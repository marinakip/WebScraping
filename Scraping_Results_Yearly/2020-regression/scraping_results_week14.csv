"{'location': 'Kingston, Canada', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['An Intuitive Tutorial to Gaussian Processes Regression\nJie Wang, Offroad Robotics, Queen\'s University, Kingston, Canada\nThe notebook can be executed at\n\nA formal paper of the notebook:\n@misc{wang2020intuitive,\n    title={An Intuitive Tutorial to Gaussian Processes Regression},\n    author={Jie Wang},\n    year={2020},\n    eprint={2009.10862},\n    archivePrefix={arXiv},\n    primaryClass={stat.ML}\n}\n\nThe audience of this tutorial is the one who wants to use GP but not feels comfortable using it. This happens to me after finishing reading the first two chapters of the textbook Gaussian Process for Machine Learning [1]. There is a gap between the usage of GP and feel comfortable using it due to the difficulties in understanding the theory. When I was reading the textbook and watching tutorial videos online, I can follow the majority without too many difficulties. The content kind of makes sense to me. But even when I am trying to talk to myself what GP is, the big picture is blurry. After keep trying to understand GP from various recourses, including textbooks, blog posts, and open-sourced codes, I get my understandings sorted and summarize them up from my perspective.\nOne thing I realized the difficulties in understanding GP is due to background varies, everyone has different knowledge. To understand GP, even to the intuitive level, needs to know multivariable Gaussian, kernel, conditional probability. If you familiar with these, start reading from III. Math. Entry or medium-level in deep learning (application level), without a solid understanding in machine learning theory, even cause more confusion in understanding GP.\n [10]\nI.   Motivation\nFirst of all, why use Gaussian Process to do regression? Or even, what is regression? Regression is a common machine learning task that can be described as Given some observed data points (training dataset), finding a function that represents the dataset as close as possible, then using the function to make predictions at new data points. Regression can be conducted with polynomials, and it\'s common there is more than one possible function that fits the observed data. Besides getting predictions by the function, we also want to know how certain these predictions are. Moreover, quantifying uncertainty is super valuable to achieve an efficient learning process. The areas with the least certainty should be explored more.\nIn a word, GP can be used to make predictions at new data points and can tell us how certain these predictions are.\n\n\n\n\n [2]\n            \n\n\n\n\n\n\nII. Basics\nA.  Gaussian (Normal) Distribution\nLet\'s talk about Gaussian.\nA random variable  is said to be normally distributed with mean  and variance  if its probability density function (PDF) is\n\nHere,  represents random variables and  is the real argument. The Gaussian or Normal distribution of  is usually represented by .\nA  Gaussian PDF is plotted below. We generate n number random sample points from a  Gaussian distribution on x axis.\nfrom __future__ import division\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# Plot 1-D gaussian\nn = 1         # n number of independent 1-D gaussian \nm= 1000       # m points in 1-D gaussian \nf_random = np.random.normal(size=(n, m)) \n# more information about \'size\': https://www.sharpsightlabs.com/blog/numpy-random-normal/ \n#print(f_random.shape)\n\nfor i in range(n):\n    #sns.distplot(f_random[i], hist=True, rug=True, vertical=True, color=""orange"")\n    sns.distplot(f_random[i], hist=True, rug=True)\n\nplt.title(\'1 random samples from a 1-D Gaussian distribution\')\nplt.xlabel(\'x\')\nplt.ylabel(\'P(x)\')\nplt.show()\n\nWe generated data points that follow the normal distribution. On the other hand, we can model data points, assume these points are Gaussian, model as a function, and do regression using it. As shown above, a kernel density and histogram of the generated points were estimated. The kernel density estimation looks a normal distribution due to there are plenty (m=1000) observation points to get this Gaussian looking PDF. In regression, even we don\'t have that many observation data, we can model the data as a function that follows a normal distribution if we assume a Gaussian prior.\nThe Gaussian PDF  is completely characterized by the two parameters  and , they can be obtained from the PDF as [3]\n\nWe have a random generated dataset in  . We sampled the generated dataset and got a Gaussian bell curve.\nNow, if we project all points  on the x-axis to another space. In this space, We treat all points  as a vector , and plot  on the new  axis at .\nn = 1         # n number of independent 1-D gaussian \nm= 1000       # m points in 1-D gaussian  \nf_random = np.random.normal(size=(n, m))\n\nXshow = np.linspace(0, 1, n).reshape(-1,1)   # n number test points in the range of (0, 1)\n\nplt.clf()\nplt.plot(Xshow, f_random, \'o\', linewidth=1, markersize=1, markeredgewidth=2)\nplt.xlabel(\'<img src=""/tex/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode&sanitize=true"" align=middle width=14.908688849999992pt height=22.465723500000017pt/>\')\nplt.ylabel(\'<img src=""/tex/161805ece9a8142e4ebe9d356fd0f763.svg?invert_in_darkmode&sanitize=true"" align=middle width=37.51151249999999pt height=24.65753399999998pt/>\')\nplt.show()\n\nIt\'s clear that the vector  is Gaussian. It looks like we did nothing but vertically plot the vector points .\nNext, we can plot multiple independent Gaussian in the  coordinates. For example, put vector  at at  and another vector  at at .\nn = 2          \nm = 1000\nf_random = np.random.normal(size=(n, m))\n\nXshow = np.linspace(0, 1, n).reshape(-1,1)   # n number test points in the range of (0, 1)\n\nplt.clf()\nplt.plot(Xshow, f_random, \'o\', linewidth=1, markersize=1, markeredgewidth=2)\nplt.xlabel(\'<img src=""/tex/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode&sanitize=true"" align=middle width=14.908688849999992pt height=22.465723500000017pt/>\')\nplt.ylabel(\'<img src=""/tex/161805ece9a8142e4ebe9d356fd0f763.svg?invert_in_darkmode&sanitize=true"" align=middle width=37.51151249999999pt height=24.65753399999998pt/>\')\nplt.show()\n\nKeep in mind that both vecotr  and  are Gaussian.\n\nLet\'s do something interesting. Let\'s connect points of  and  by lines. For now, we only generate 10 random points for  and , and then join them up as 10 lines. Keep in mind, these random generated 10 points are Gaussian.\nn = 2          \nm = 10\nf_random = np.random.normal(size=(n, m))\n\nXshow = np.linspace(0, 1, n).reshape(-1,1)   # n number test points in the range of (0, 1)\n\nplt.clf()\nplt.plot(Xshow, f_random, \'-o\', linewidth=2, markersize=4, markeredgewidth=2)\nplt.xlabel(\'<img src=""/tex/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode&sanitize=true"" align=middle width=14.908688849999992pt height=22.465723500000017pt/>\')\nplt.ylabel(\'<img src=""/tex/161805ece9a8142e4ebe9d356fd0f763.svg?invert_in_darkmode&sanitize=true"" align=middle width=37.51151249999999pt height=24.65753399999998pt/>\')\nplt.show()\n\nGoing back to think about regression. These lines look like functions for each pair of points. On the other hand, the plot also looks like we are sampling the region  with 10 linear functions even there are only two points on each line. In the sampling perspective, the  domain is our region of interest, i.e. the specific region we do our regression. This sampling looks even more clear if we generate more independent Gaussian and connecting points in order by lines.\nn = 20          \nm = 10\nf_random = np.random.normal(size=(n, m))\n\nXshow = np.linspace(0, 1, n).reshape(-1,1)   # n number test points in the range of (0, 1)\n\nplt.clf()\nplt.plot(Xshow, f_random, \'-o\', linewidth=1, markersize=3, markeredgewidth=2)\nplt.xlabel(\'<img src=""/tex/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode&sanitize=true"" align=middle width=14.908688849999992pt height=22.465723500000017pt/>\')\nplt.ylabel(\'<img src=""/tex/161805ece9a8142e4ebe9d356fd0f763.svg?invert_in_darkmode&sanitize=true"" align=middle width=37.51151249999999pt height=24.65753399999998pt/>\')\nplt.show()\n\nWait for a second, what we are trying to do by connecting random generated independent Gaussian points? Even these lines look like functions, but they are too noisy. If  is our input space, these functions are meaningless for the regression task. We can do no prediction by using these functions. The functions should be smoother, meaning input points that are close to each other should have similar values of the function.\nThus, functions by connecting independent Gaussian are not proper for regression, we need Gaussians that correlated to each other. How to describe joint Gaussian? Multivariable Gaussian.\nB. Multivariate Normal Distribution (MVN)\nIn some situations, a system (set of data) has to be described by more than more feature variables , and these variables are correlated. If we want to model the data all in one go as Gaussian, we need multivariate Gaussian. Here are examples of the  Gaussian. A data center is monitored by the CPU load  and memory use . [3]\n\nThe  gaussian can be visualized as a 3D bell curve with the heights representing probability density.\n\n\n\n\n\n\n\n\n\n\n\n\nGoes to Appendix A if you want to generate image on the left.\nFormally, multivariate Gaussian is expressed as [4]\n\nThe mean vector  is a 2d vector , which are independent mean of each variable  and .\nThe covariance matrix of  Gaussian is . The diagonal terms are independent variances of each variable,  and . The offdiagonal terms represents correlations between the two variables. A correlation component represents how much one variable is related to another variable.\nA  Gaussian can be expressed as\n\nWhen we have an  Gaussian, the covariance matrix  is  and its  element is . The  is a symmetric matrix and stores the pairwise covariances of all the jointly modeled random variables.\nPlay around with the covariance matrix to see the correlations between the two Gaussians.\nimport pandas as pd\nimport seaborn as sns\n\nmean, cov = [0., 0.], [(1., -0.6), (-0.6, 1.)]\ndata = np.random.multivariate_normal(mean, cov, 1000)\ndf = pd.DataFrame(data, columns=[""x1"", ""x2""])\ng = sns.jointplot(""x1"", ""x2"", data=df, kind=""kde"")\n\n#(sns.jointplot(""x1"", ""x2"", data=df).plot_joint(sns.kdeplot))\n\ng.plot_joint(plt.scatter, c=""g"", s=30, linewidth=1, marker=""+"")\n\n#g.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(""<img src=""/tex/8c76e0c69c5596634f9abb693bbf9438.svg?invert_in_darkmode&sanitize=true"" align=middle width=17.614197149999992pt height=21.18721440000001pt/>"", ""<img src=""/tex/1533fefb8348ed2119c7920bf5d7a8a5.svg?invert_in_darkmode&sanitize=true"" align=middle width=17.614197149999992pt height=21.18721440000001pt/>"");\n\ng.ax_joint.legend_.remove()\nplt.show()\n\nAnother good MVN visualization is Multivariante Gaussians and Mixtures of Gaussians (MoG).\nBesides the joint probalility, we are more interested to the conditional probability. If we cut a slice on the 3D bell curve or draw a line on the elipse contour, we got the conditional probability distribution $P(x_1 \\vert , x_2)$. The conditional distribution is also Gaussian.\n\n\n\n\n\n\n\n\n\n\n\n\nC. Kernals\nWe want to smooth the sampling functions by defining the covariance functions. Considering the fact that when two vectors are similar, their dot product output value is high. It is very clear to see this in the dot product equation , where  is the angle between two vectors. If an algorithm is defined solely in terms of inner products in input space then it can be lifted into feature space by replacing occurrences of those inner products by ; we call  a kernel function [1].\nA popular covariance function (aka kernel function) is squared exponential kernal, also called the radial basis function (RBF) kernel or Gaussian kernel, defined as\n\nLet\'s re-plot 20 independent Gaussian and connecting points in order by lines. Instead of generating 20 independent Gaussian before, we do the plot of a  Gaussian with a identity convariance matrix.\nn = 20 \nm = 10\n\nmean = np.zeros(n)\ncov = np.eye(n)\n\nf_prior = np.random.multivariate_normal(mean, cov, m).T\n\nplt.clf()\n\n#plt.plot(Xshow, f_prior, \'-o\')\nXshow = np.linspace(0, 1, n).reshape(-1,1)   # n number test points in the range of (0, 1)\n\nfor i in range(m):\n    plt.plot(Xshow, f_prior, \'-o\', linewidth=1)\n    \nplt.title(\'10 samples of the 20-D gaussian prior\')\nplt.show()\n\nWe got exactly the same plot as expected. Now let\'s kernelizing our funcitons by use the RBF as our convariace.\n# Define the kernel\ndef kernel(a, b):\n    sqdist = np.sum(a**2,axis=1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T)\n    # np.sum( ,axis=1) means adding all elements columnly; .reshap(-1, 1) add one dimension to make (n,) become (n,1)\n    return np.exp(-.5 * sqdist)\nn = 20  \nm = 10\n\nXshow = np.linspace(0, 1, n).reshape(-1,1)   # n number test points in the range of (0, 1)\n\nK_ = kernel(Xshow, Xshow)                  # k(x_star, x_star)        \n\nmean = np.zeros(n)\ncov = np.eye(n)\n\nf_prior = np.random.multivariate_normal(mean, K_, m).T\n\nplt.clf()\n\nXshow = np.linspace(0, 1, n).reshape(-1,1)   # n number test points in the range of (0, 1)\n\nfor i in range(m):\n    plt.plot(Xshow, f_prior, \'-o\', linewidth=1)\n    \nplt.title(\'10 samples of the 20-D gaussian kernelized prior\')\nplt.show()\n\nWe get much smoother lines and looks even more like functions. When the dimension of Gaussian gets larger, there is no need to connect points. When the dimension become infinity, there is a point represents any possible input. Let\'s plot m=200 samples of n=200 Gaussian to get a feeling of functions with infinity parameters.\nn = 200         \nm = 200\n\nXshow = np.linspace(0, 1, n).reshape(-1,1)   \n\nK_ = kernel(Xshow, Xshow)                    # k(x_star, x_star)        \n\nmean = np.zeros(n)\ncov = np.eye(n)\n\nf_prior = np.random.multivariate_normal(mean, K_, m).T\n\nplt.clf()\n#plt.plot(Xshow, f_prior, \'-o\')\nXshow = np.linspace(0, 1, n).reshape(-1,1)   # n number test points in the range of (0, 1)\n\nplt.figure(figsize=(18,9))\nfor i in range(m):\n    plt.plot(Xshow, f_prior, \'o\', linewidth=1, markersize=2, markeredgewidth=1)\n    \nplt.title(\'200 samples of the 200-D gaussian kernelized prior\')\n#plt.axis([0, 1, -3, 3])\nplt.show()\n#plt.savefig(\'priorT.png\', bbox_inches=\'tight\', dpi=300)\n<Figure size 432x288 with 0 Axes>\n\n\nAs we can see above, when we increase the dimension of Gaussian to infinity, we can sample all the possible points in our region of interest.\nA great visualization animation of points covariance of the ""functions"" [10].\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we talk a little bit about Parametric and Nonparametric model. You can skip this section without compromising your Gaussian Process understandings.\nParametric models assume that the data distribution can be modeled in terms of a set of finite number parameters. For regression, we have some data points, and we would like to make predictions of the value of  with a specific . If we assume a linear regression model, , we need to find the parameters  and  to define the line. In many cases, the linear model assumption isn’t hold, a polynomial model with more parameters, such as  is needed. We use the training dataset  of  observations,  to train the model, i.e. mapping  to  through parameters . After the training process, we assume all the information of the data are captured by the feature parameters , thus the prediction is independent of the training data . It can be expressed as  , in which  is the prediction made at a unobserved point .\nThus, conducting regression using the parametric model, the complexity or flexibility of model is limited by the parameter numbers. It’s natural to think to use a model that the number of parameters grows with the size of the dataset, and it’s a Bayesian non-parametric model. Bayesian non-parametric model do not imply that there are no parameters, but rather infinitely parameters.\n\nTo generate correlated normally distributed random samples, one can first generate uncorrelated samples, and then multiply them\nby a matrix L such that , where K is the desired covariance matrix. L can be created, for example, by using\nthe Cholesky decomposition of K.\nn = 20      \nm = 10\n\nXshow = np.linspace(0, 1, n).reshape(-1,1)  # n number test points in the range of (0, 1)\n\nK_ = kernel(Xshow, Xshow)                \n\nL = np.linalg.cholesky(K_ + 1e-6*np.eye(n))\n\n\nf_prior = np.dot(L, np.random.normal(size=(n,m)))\n\nplt.clf()\nplt.plot(Xshow, f_prior, \'-o\')\nplt.title(\'10 samples of the 20-D gaussian kernelized prior\')\nplt.show()\n\nIII. Math\nFirst, again, going back to our task regression. There is a function  we are trying to model given a set of data points  (trainig data/existing observed data) from the unknow function . The traditional non-linear regression machine learning methods typically give one function that it considers to fit these observations the best. But, as shown at the begining, there can be more than one funcitons fit the observations equally well.\nSecond, let\'s review what we got from MVN. We got the feeling that when the dimension of Gaussian is infinite, we can sample all the region of interest with random functions. These infinite random functions are MVN because it\'s our assumption (prior). More formally, the prior distribution of these infinite random functions are MVN. The prior distribution representing the kind out outputs  that we expect to see over some inputs  without even observing any data.\nWhen we have observation points, instead of infinite random functions, we only keep functions that are fit these points. Now we got our posterior, the current belief based on the existing observations. When we have more observation points, we use our previous posterior as our prior, use these new observations to update our posterior.\nThis is Gaussian process.\nA Gaussian process is a probability distribution over possible functions that fit a set of points.\nBecause we have the probability distribution over all possible functions, we can caculate the means as the function, and caculate the variance to show how confidient when we make predictions using the function.\nKeep in mind,\n\nThe functions(posterior) updates with new observations.\nThe mean calcualted by the posterior distribution of the possible functions is the function used for regression.\n\nHighly recommend to read Appendix A.1 and A.2 [3] before continue. Basic math.\nThe function is modeled by a multivarable Gaussian as\n\nwhere ,  and .  is the mean function and it is common to use  as GPs are flexible enough to model the mean arbitrarily well.  is a positive definite kernel function or covariance function. Thus, a Gaussian process is a distribution over functions whose shape (smoothness, ...) is defined by . If points  and  are considered to be similar by the kernel the function values at these points,  and , can be expected to be similar too.\nSo, we have observations, and we have estimated functions  with these observations. Now say we have some new points  where we want to predict .\n\nThe joint distribution of  and  can be modeled as:\n\nwhere ,  and . And \nThis is modeling a joint distribution , but we want the conditional distribution over  only, which is . The derivation process from the joint distribution  to the conditional  uses the Marginal and conditional distributions of MVN theorem [5].\n\nWe got eqn. 2.19 [1]\n\nIt is realistic modelling situations that we do not have access to function values themselves, but only noisy versions thereof . Assuming additive independent identically distributed Gaussian noise with\nvariance , the prior on the noisy observations becomes . The joint distribution of the observed target values and the function values at the test locations under the prior as [1]\n\nDeriving the conditional distribution corresponding to eqn. 2.19 we get the predictive equations (eqn. 2.22, eqn. 2.23, and eqn. 2.24) [1] for Gaussian process regression as\n\nwhere,\n\n\nIV. Codes\nWe do the regression example between -5 and 5. The observation data points (traing dataset) are generated from a uniform distribution between -5 and 5. This means any point value within the given interval [-5, 5] is equally likely to be drawn by uniform. The functions will be evaluated at n evenly spaced points between -5 and 5. We do this to show a continuous function for regression in our region of interest [-5, 5]. This is a simple example to do GP regression. It assumes a zero mean GP Prior. The code borrows heavily from Dr. Nando de Freitas’ Gaussian processes for nonlinear regression lecture [6].\nThe algorithm executed follows\nThe textbook GPML, P19. [1]\n\nDr. Nando de Freitas, Introduction to Gaussian processes. [6]\n\nfrom __future__ import division\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# This is the true unknown function we are trying to approximate\nf = lambda x: np.sin(0.9*x).flatten()\n#f = lambda x: (0.25*(x**2)).flatten()\nx = np.arange(-5, 5, 0.1)\n\nplt.plot(x, f(x))\nplt.axis([-5, 5, -3, 3])\nplt.show()\n\n# Define the kernel\ndef kernel(a, b):\n    kernelParameter_l = 0.1\n    kernelParameter_sigma = 1.0\n    sqdist = np.sum(a**2,axis=1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T)\n    # np.sum( ,axis=1) means adding all elements columnly; .reshap(-1, 1) add one dimension to make (n,) become (n,1)\n    return kernelParameter_sigma*np.exp(-.5 * (1/kernelParameter_l) * sqdist)\nWe use a general Squared Exponential Kernel, also called Radial Basis Function Kernel or Gaussian Kernel:\n\nwhere  and  are hyperparameters. More information about the hyperparameters can be found after the codes.\n# Sample some input points and noisy versions of the function evaluated at\n# these points. \nN = 20         # number of existing observation points (training points).\nn = 200        # number of test points.\ns = 0.00005    # noise variance.\n\nX = np.random.uniform(-5, 5, size=(N,1))     # N training points \ny = f(X) + s*np.random.randn(N)\n\nK = kernel(X, X)\nL = np.linalg.cholesky(K + s*np.eye(N))     # line 1 \n\n# points we\'re going to make predictions at.\nXtest = np.linspace(-5, 5, n).reshape(-1,1)\n\n# compute the mean at our test points.\nLk = np.linalg.solve(L, kernel(X, Xtest))   # k_star = kernel(X, Xtest), calculating v := l\\k_star\nmu = np.dot(Lk.T, np.linalg.solve(L, y))    # \\alpha = np.linalg.solve(L, y) \n\n# compute the variance at our test points.\nK_ = kernel(Xtest, Xtest)                  # k(x_star, x_star)        \ns2 = np.diag(K_) - np.sum(Lk**2, axis=0)   \ns = np.sqrt(s2)\n\n# PLOTS:\nplt.figure(1)\nplt.clf()\nplt.plot(X, y, \'k+\', ms=18)\nplt.plot(Xtest, f(Xtest), \'b-\')\nplt.gca().fill_between(Xtest.flat, mu-2*s, mu+2*s, color=""#dddddd"")\nplt.plot(Xtest, mu, \'r--\', lw=2)\n#plt.savefig(\'predictive.png\', bbox_inches=\'tight\', dpi=300)\nplt.title(\'Mean predictions plus 2 st.deviations\')\nplt.show()\n#plt.axis([-5, 5, -3, 3])\n\n# draw samples from the posterior at our test points.\nL = np.linalg.cholesky(K_ + 1e-6*np.eye(n) - np.dot(Lk.T, Lk))\nf_post = mu.reshape(-1,1) + np.dot(L, np.random.normal(size=(n,40)))  # size=(n, m), m shown how many posterior  \nplt.figure(3)\nplt.clf()\nplt.figure(figsize=(18,9))\nplt.plot(X, y, \'k+\', markersize=20, markeredgewidth=3)\nplt.plot(Xtest, mu, \'r--\', linewidth=3)\nplt.plot(Xtest, f_post, linewidth=0.8)\nplt.title(\'40 samples from the GP posterior, mean prediction function and observation points\')\nplt.show()\n#plt.axis([-5, 5, -3, 3])\n#plt.savefig(\'post.png\', bbox_inches=\'tight\', dpi=600)\n<Figure size 432x288 with 0 Axes>\n\n\nWe plotted m=40 samples from the Gaussian Process posterior together with the mean function for prediction and the observation data points (training dataset). It\'s clear all posterior functions collapse at all observation points.\nThe general RBF kernel:\n\nwhere  and  are hyperparameters. [7]\n\nMore complex kernel functions can be selected to depend on the specific tasks. More information about choosing the kernel/covariance function for a Gaussian process can be found in The Kernel Cookbook [8].\nV. GP Packages\nThere are several packages or frameworks available to conduct Gaussian Process Regression. In this section, I will summarize my initial impression after trying several of them written in Python.\nA lightweight one is sklearn.gaussian_process, simple implementation like the example above can be quickly conducted. Just for gaining more implementation understandings of GP after the above simple implementation example. It\'s too vague for understanding GP theory purpose.\nGPR is computationally expensive in high dimensional spaces (features more than a few dozens) due to the fact it uses the whole samples/features to do the predictions. The more observations, the more computations are needed for predictions. A package that includes state-of-the-art algorithm implementations is preferred for efficient implementation of complex GPR tasks.\nOne of the most well-known GP frameworks is GPy. GPy has been developed pretty maturely with well-documented explanations. GPy uses NumPy to perform all its computations. For tasks that don\'t require heavy computations and very up-to-date algorithm implementations, GPy is sufficient and the more stable.\nFor bigger computation required GPR tasks, GPU acceleration are especially preferred. GPflow origins from GPy, and much of the interface is similar. GPflow leverages TensorFlow as its computational backend. More technical difference between GPy and GPflow frameworks is here.\nGPyTorch is another framework that provides GPU acceleration through PyTorch. It contains very up-to-date GP algorithms. Similar to GPflow, GPyTorch provides automatic gradients. So complex models such as embedding deep NNs in GP models can be easier developed.\nAfter going through docs quickly and implementing basic GPR tutorials of GPyTorch and GPflow, my impression is using GPyTorch is more automatic and GPflow has more controls. The impression may also come from the usage experience with TensorFlow and PyTorch.\nCheck and run my modified GPR tutorials of\n\n\nGPyTorch\n\n\nGPflow\n\n\nVI. Summary\nA Gaussian process (GP) is a probability distribution over possible functions that fit a set of points. [1] GPs are nonparametric models that model the function directly. Thus, GP provides a distribution (with uncertainty) for the prediction value rather than just one value as the prediction. In robot learning, quantifying uncertainty can be extremely valuable to achieve an efficient learning process. The areas with least certain should be explored next. This is the main idea behind Bayesian optimization. [9] Moreover, prior knowledge and specifications about the shape of the model can be added by selecting different kernel functions. [1] Priors can be specified based on criteria including if the model is smooth, if it is sparse, if it is able to change drastically, and if it need to be differentiable.\nExtra words\n\n\nFor simplicity and understanding reason, I ignore many math and technical talks. Read the first two chapters of the textbook Gaussian Process for Machine Learning [1] serveral times to get a solid understanding of GPR. Such as **Gaussian process regression is a linear smoother. **\n\n\nOne of most tricky part in understanding GP is the mapping projection among spaces. From input space to latent (feature) space and back to output space. You can get some feeling about space by reading autoencoder.\n\n\nReference\n[1] C. E. Rasmussen and C. K. I. Williams, Gaussian processes for machine learning. MIT Press, 2006.\n[2] R. Turner, “ML Tutorial: Gaussian Processes - YouTube,” 2017. [Online]. Available: https://www.youtube.com/watch?v=92-98SYOdlY&feature=emb_title.\n[3] A. Ng, “Multivariate Gaussian Distribution - Stanford University | Coursera,” 2015. [Online]. Available: https://www.coursera.org/learn/machine-learning/lecture/Cf8DF/multivariate-gaussian-distribution.\n[4] D. Lee, “Multivariate Gaussian Distribution - University of Pennsylvania | Coursera,” 2017. [Online]. Available: https://www.coursera.org/learn/robotics-learning/lecture/26CFf/1-3-1-multivariate-gaussian-distribution.\n[5] F. Dai, Machine Learning Cheat Sheet: Classical equations and diagrams in machine learning. 2017.\n[6] N. de Freitas, “Machine learning - Introduction to Gaussian processes - YouTube,” 2013. [Online]. Available: https://www.youtube.com/watch?v=4vGiHC35j9s&t=1424s.\n[7] Y. Shi, “Gaussian Process, not quite for dummies,” 2019. [Online]. Available: https://yugeten.github.io/posts/2019/09/GP/.\n[8] D. Duvenaud, “Kernel Cookbook,” 2014. [Online]. Available: https://www.cs.toronto.edu/~duvenaud/cookbook/.\n[9] Y. Gal, “What my deep model doesn’t know.,” 2015. [Online]. Available: http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html.\n[10] J. Hensman, ""Gaussians."" 2019. [Online]. Available: https://github.com/mlss-2019/slides/blob/master/gaussian_processes/presentation_links.md.\n[11] Z. Dai, ""GPSS2019 - Computationally efficient GPs"" 2019. [Online]. Available: https://www.youtube.com/watch?list=PLZ_xn3EIbxZHoq8A3-2F4_rLyy61vkEpU&v=7mCfkIuNHYw.\nAppendix A\nVisualizing 3D plots of a  Gaussian by Visualizing the bivariate Gaussian distribution.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Our 2-dimensional distribution will be over variables X and Y\nN = 60\nX = np.linspace(-3, 3, N)\nY = np.linspace(-3, 4, N)\nX, Y = np.meshgrid(X, Y)\n\n# Mean vector and covariance matrix\nmu = np.array([0., 1.])\nSigma = np.array([[ 1. , 0.8], [0.8,  1.]])\n\n# Pack X and Y into a single 3-dimensional array\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X\npos[:, :, 1] = Y\n\ndef multivariate_gaussian(pos, mu, Sigma):\n    """"""Return the multivariate Gaussian distribution on array pos.\n\n    pos is an array constructed by packing the meshed arrays of variables\n    x_1, x_2, x_3, ..., x_k into its _last_ dimension.\n\n    """"""\n    n = mu.shape[0]\n    Sigma_det = np.linalg.det(Sigma)\n    Sigma_inv = np.linalg.inv(Sigma)\n    N = np.sqrt((2*np.pi)**n * Sigma_det)\n    # This einsum call calculates (x-mu)T.Sigma-1.(x-mu) in a vectorized\n    # way across all the input variables.\n    fac = np.einsum(\'...k,kl,...l->...\', pos-mu, Sigma_inv, pos-mu)\n\n    return np.exp(-fac / 2) / N\n\n# The distribution on the variables X, Y packed into pos.\nZ = multivariate_gaussian(pos, mu, Sigma)\n\n# Create a surface plot and projected filled contour plot under it.\nfig = plt.figure()\nax = fig.gca(projection=\'3d\')\nax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n                cmap=cm.viridis)\n\ncset = ax.contourf(X, Y, Z, zdir=\'z\', offset=-0.2, cmap=cm.viridis)\n\n# Adjust the limits, ticks and view angle\nax.set_zlim(-0.2,0.2)\nax.set_zticks(np.linspace(0,0.2,5))\nax.view_init(30, -100)\n\nax.set_xlabel(r\'<img src=""/tex/277fbbae7d4bc65b6aa601ea481bebcc.svg?invert_in_darkmode&sanitize=true"" align=middle width=15.94753544999999pt height=14.15524440000002pt/>\')\nax.set_ylabel(r\'<img src=""/tex/95d239357c7dfa2e8d1fd21ff6ed5c7b.svg?invert_in_darkmode&sanitize=true"" align=middle width=15.94753544999999pt height=14.15524440000002pt/>\')\nax.set_zlabel(r\'<img src=""/tex/467f6c046e010c04cafe629aaca84961.svg?invert_in_darkmode&sanitize=true"" align=middle width=66.46698464999999pt height=24.65753399999998pt/>\')\n\nplt.title(\'mean, cov = [0., 1.], [(1., 0.8), (0.8, 1.)]\')\nplt.savefig(\'2d_gaussian3D_0.8.png\', dpi=600)\nplt.show()\n\n'], 'url_profile': 'https://github.com/jwangjie', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '210 contributions\n        in the last year', 'description': ['Air pollution and COVID-19 mortality in the United States\nThis is the data repository for public available code and data to reproduce analyses in Wu, X., Nethery, R. C., Sabath, M. B., Braun, D. and Dominici, F., 2020. Air pollution and COVID-19 mortality in the United States: Strengths and limitations of an ecological regression analysis.\xa0Science advances, 6(45), p.eabd4049.\xa0\nSummary Results: \n\nFigure: COVID-19 mortality rate ratios (MRR) per 1 μg/m3 increase in PM2.5 and 95% CI using daily cumulative COVID-19 death counts from April 18, 2020 to June 18, 2020.\nCode: \nPrepossing.R includes the code to extract all necessary data and prepocess data for statistical analyses.\nAnalyses.R includes the code to implement negative binomial mixed models in primary, secondary and sensitivity analyses.\nFigure.R includes the code to generate figures in Main Text and Supplementary Materials.\nadditional_preprocessing_code contains markdown files with code demonstrating the methodology we used to aggregate our zip code level data to the county level.\nData: \ncounty_pm25.csv: the county-level PM2.5 exposure data averaged across the period 2000-2016, averaged across grid cells within a zip code and then averaging across zip codes within a county. For more source information see Additional Data Source section.\ntemp_seasonal_county.csv: the county-level seasonal temperature and relative humidity data, summer and winter averaged across the period 2000-2016 and averaged across grid cells in each county. For more source information see Additional Data Source section.\ncensus_county_interpolated.csv: the county-level socioeconomic and demographic variables from 2012-2016 American Community Survey, extracted for each zip code and then averaging across zip codes within a county. For more source information see Additional Data Source section.\ncounty_base_mortality.txt, county_old_mortality.txt: additional county-level socioeconomic and demographic variables from 2009-2016\nUS CDC Compressed Mortality Data (https://wonder.cdc.gov/cmf-ICD10.html).\nbrfss_county_interpolated.csv: the county-level behavioral risk factor variables for 2011 US CDC Behavioral Risk Factor Surveillance System (https://www.cdc.gov/brfss/). (deprecated from the updated analyese due to servere missningness). In the final analysis, we use county-level health risk factors: proportion of residents obese and proportion of residents that are current smokers from the Robert Wood Johnson Foundation’s 2020 County Health Rankings.\nstatecode.csv: A map between state name and state abbreviations.\nAdditional Data Source: \nThe county-level PM2.5 exposure data can be created via PM2.5 predictions from The Atmospheric Composition Analysis Group at Dalhousie University (http://fizz.phys.dal.ca/~atmos/martin/). Please visit the detailed instructions below\n\nDownload PM25 predictions: https://github.com/wxwx1993/PM_COVID/blob/master/additional_preprocessing_code/download_pm25_values.md. This code makes use of ZIP code shape files provided by ESRI.\nCounty-level aggregation: https://github.com/wxwx1993/PM_COVID/blob/master/additional_preprocessing_code/rm_pm25_to_county.md\n\nWe thank Randall Martin and the members of the Atmospheric Composition Analysis Group at Dalhousie University for providing access to their open-source datasets. Their data (V4.NA.02.MAPLE) that we used can be found here: https://sites.wustl.edu/acag/datasets/surface-pm2-5/. Citation: van Donkelaar, A., R. V. Martin, C. Li, R. T. Burnett, Regional Estimates of Chemical Composition of Fine Particulate Matter using a Combined Geoscience-Statistical Method with Information from Satellites, Models, and Monitors, Environ. Sci. Technol., doi: 10.1021/acs.est.8b06392, 2019.\nThe seasonal temperature and relative humidity data can be created via 4km × 4km temperature and relative humidity predictions from Gridmet via google earth engine (https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_GRIDMET).\nWe thank John Abatzoglou and members of the Climatology Lab at University of Idaho for providing the GRIDMET open-source datasets.\nThe county-level socioeconomic and demographic variables from 2012-2016 American Community Survey can be created from US Census website\n\nDownload zip code-level SES variables from ACS for each zip code: https://www.census.gov/programs-surveys/acs/data.html\nCounty-level aggregation: https://github.com/wxwx1993/PM_COVID/blob/master/additional_preprocessing_code/census_to_county.md\n\nAdditional data required by the analyses can be directly extracted from data sources:\n\nJohns Hopkins University the Center for Systems Science and Engineering (CSSE) Coronavirus Resource Center: https://coronavirus.jhu.edu/ \nHomeland Infrastructure Foundation-Level Data (HIFLD): https://hifld-geoplatform.opendata.arcgis.com/datasets/hospitals \nRobert Wood Johnson Foundation County Health Rankings: https://www.countyhealthrankings.org/ \nThe COVID tracking project: https://covidtracking.com/ \nCarnegie Mellon University COVIDcast Delphi Research Group: https://covidcast.cmu.edu/ \nFacebook Data for Good project: https://www.facebook.com/geoinsights-portal/ \n\nWe thank all of them for making their data public and for enabling this research to be possible.\nContact Us: \n\nEmail: fdominic@hsph.harvard.edu\n\nTerms of Use:\nAuthors/funders retain copyright (where applicable) of code on this Github repo and the article in press on Science Advances (A pre-print version posted on medRxiv). Anyone who wishes to share, reuse, remix, or adapt this material must obtain permission from the corresponding author. By using the contents on this Github repo and the article, you agree to cite:\n\n\nWu, X., Nethery, R. C., Sabath, M. B., Braun, D. and Dominici, F., 2020. Air pollution and COVID-19 mortality in the United States: Strengths and limitations of an ecological regression analysis.\xa0Science advances, 6(45), p.eabd4049.\n\n\nA pre-print version can be found at: Exposure to air pollution and COVID-19 mortality in the United States. Xiao Wu, Rachel C. Nethery, Benjamin M. Sabath, Danielle Braun, Francesca Dominici. medRxiv 2020.04.05.20054502; doi: https://doi.org/10.1101/2020.04.05.20054502\n\n\nThis GitHub repo and its contents herein, including data, link to data source, and analysis code that are intended solely for reproducing the results in the manuscript ""Air pollution and COVID-19 mortality in the United States: strengths and limitations of an ecological regression analysis."" The analyses rely upon publicly available data from multiple sources, that are often updated without advance notice. We hereby disclaim any and all representations and warranties with respect to the site, including accuracy, fitness for use, and merchantability. By using this site, its content, information, and software you agree to assume all risks associated with your use or transfer of information and/or software. You agree to hold the authors harmless from any claims relating to the use of this site.\n'], 'url_profile': 'https://github.com/wxwx1993', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\n\xa0\xa0\xa0\n\n\xa0\xa0\xa0\n\n\xa0\xa0\xa0\n\n\nPackage: dsm\nPython package dsm provides an API to train the Deep Survival Machines\nand associated models for problems in survival analysis. The underlying model\nis implemented in pytorch.\nFor full documentation of the module, please see https://autonlab.github.io/DeepSurvivalMachines/\nWhat is Survival Analysis?\nSurvival Analysis involves estimating when an event of interest, T\nwould take place given some features or covariates X. In statistics\nand ML, these scenarios are modelled as regression to estimate the conditional\nsurvival distribution, P(T>t|X).\nAs compared to typical regression problems, Survival Analysis differs in two major ways:\n\nThe Event distribution, T has positive support i.e. T ∈ [0, ∞).\nThere is presence of censoring i.e. a large number of instances of data are\nlost to follow up.\n\nDeep Survival Machines\n\nDeep Survival Machines (DSM) is a fully parametric approach to model\nTime-to-Event outcomes in the presence of Censoring, first introduced in\n[1].\nIn the context of Healthcare ML and Biostatistics, this is known as \'Survival\nAnalysis\'. The key idea behind Deep Survival Machines is to model the\nunderlying event outcome distribution as a mixure of some fixed ( K )\nparametric distributions. The parameters of these mixture distributions as\nwell as the mixing weights are modelled using Neural Networks.\nUsage Example\n>>> from dsm import DeepSurvivalMachines\n>>> model = DeepSurvivalMachines()\n>>> model.fit()\n>>> model.predict_risk()\n\nRecurrent Deep Survival Machines\nRecurrent Deep Survival Machines (RDSM) builds on the original DSM\nmodel and allows for learning of representations of the input covariates using\nRecurrent Neural Networks like LSTMs, GRUs. Deep Recurrent Survival\nMachines is a natural fit to model problems where there are time dependendent\ncovariates.\n\n⚠️ Not Implemented Yet!\n\nDeep Convolutional Survival Machines\nPredictive maintenance and medical imaging sometimes requires to work with\nimage streams. Deep Convolutional Survival Machines extends DSM and\nDRSM to learn representations of the input image data using\nconvolutional layers. If working with streaming data, the learnt\nrepresentations are then passed through an LSTM to model temporal dependencies\nbefore determining the underlying survival distributions.\n\n⚠️ Not Implemented Yet!\n\nInstallation\nfoo@bar:~$ git clone https://github.com/autonlab/DeepSurvivalMachines.git\nfoo@bar:~$ cd DeepSurvivalMachines\nfoo@bar:~$ pip install -r requirements.txt\nExamples\n\nDeep Survival Machines on the SUPPORT Dataset\n\nReferences\nPlease cite the following papers if you are using the dsm package.\n[1] Deep Survival Machines:\nFully Parametric Survival Regression and\nRepresentation Learning for Censored Data with Competing Risks.\nIEEE Journal of Biomedical & Health Informatics (2021)\n  @article{nagpal2020deep,\n  title={Deep Survival Machines: Fully Parametric Survival Regression and\\\n  Representation Learning for Censored Data with Competing Risks},\n  author={Nagpal, Chirag and Li, Xinyu and Dubrawski, Artur},\n  journal={IEEE Journal of Biomedical and Health Informatics},\n  year={2021}\n  }\n\n[2] Recurrent Deep Survival Machines:\nDeep Parametric Time-to-Event Regression with Time-Varying Covariates.\nAAAI Spring Symposium (2021)\n  @article{nagpal2021rdsm,\n  title={Deep Parametric Time-to-Event Regression with Time-Varying Covariates},\n  author={Nagpal, Chirag and Jeanselme, Vincent and Dubrawski, Artur},\n  journal={AAAI Spring Symposium on Survival Analysis},\n  year={2021}\n  }\n\nCompatibility\ndsm requires python 3.5+ and pytorch 1.1+.\nTo evaluate performance using standard metrics\ndsm requires scikit-survival.\nContributing\ndsm is on GitHub. Bug reports and pull requests are welcome.\nLicense\nMIT License\nCopyright (c) 2020 Carnegie Mellon University, Auton Lab\nPermission is hereby granted, free of charge, to any person obtaining a copy of this\nsoftware and associated documentation files (the ""Software""), to deal in the Software\nwithout restriction, including without limitation the rights to use, copy, modify,\nmerge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\npersons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies\nor substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\nINCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR\nPURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE\nFOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\nOTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS IN THE SOFTWARE.\n\n\n'], 'url_profile': 'https://github.com/autonlab', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': [""Mixture Dense Regression for Object Detection and Human Pose Estimation (CVPR 2020)\n\nMixture models are well-established learning approaches that, in computer vision, have mostly been applied to inverse or ill-defined problems. However, they are general-purpose divide-and-conquer techniques, splitting the input space into relatively homogeneous subsets in a data-driven manner. Not only ill-defined but also well-defined complex problems should benefit from them. To this end, we devise a framework for spatial regression using mixture density networks. We realize the framework for object detection and human pose estimation. For both tasks, a mixture model yields higher accuracy and divides the input space into interpretable modes. For object detection, mixture components focus on object scale, with the distribution of components closely following that of ground truth the object scale. This practically alleviates the need for multi-scale testing, providing a superior speed-accuracy trade-off. For human pose estimation, a mixture model divides the data based on viewpoint and uncertainty -- namely, front and back views, with back view imposing higher uncertainty. We conduct experiments on the MS COCO dataset and do not face any mode collapse.\nFor questions, please contact me at ali.varamesh@kuleuven.be.\nInstallation\n1- Fiest use mixturedense.yml to reproduce the exact Anaconda environment that we have used for our experiments:\nconda env create -f mixturedense.yml\n\nTo activate the environment:\nsource activate mixturedense\n\n2- Install COCOAPI\n3- Compile deformable convolutional conda env create -f environment.yml(from DCNv2).\ncd src/lib/models/networks/DCNv2\n./make.sh\n\nTrain\nTo train models from scratch, first, organize the dataset in the following order and see sample commands at experiments\nDataset preparation\nFor training and evaluation download the MS COCO dataset and organize it as indicated below:\nCOCO\n\nDownload the images (2017 Train, 2017 Val, 2017 Test) from coco website.\nDownload annotation files (2017 train/val and test image info) from coco website.\nPlace the data (or create symlinks) to make the data folder like:\n\noptdata_dir\n|-- coco\n    |-- annotations\n        |   |-- instances_train2017.json\n        |   |-- instances_val2017.json\n        |   |-- person_keypoints_train2017.json\n        |   |-- person_keypoints_val2017.json\n        |   |-- image_info_test-dev2017.json\n    |-- train2017\n    |-- val2017\n    |-- test2017ll \n\nTest\nTo test the models for detction and pose estimation on a images (stored in a directory) use the inference_ctdet.py and inference_pose.py scripts, respectively\nPretrained models\nDetection_DLA34_1x_MDN_3\nDetection_DLA34_1x_CenterNet\nDetection_HG_3x_CenterNet\nDetection_HG_3x_MDN_3\nKeypoints_HG_3x_CenterNet\nKeypoints_HG_3x_MDN_3\nDetection_DLA34_CenterNet_COCO_10percent_Train_Resolution_768_100_epochs\nDetection_DLA34_MDN_3_COCO_10percent_Train_Resolution_768_100_epochs\nLicense\nThis repo is released under the MIT License. We have forked parts of the codebase from other repositories; please refer to the corresponding repositories for their licenses' details.\nCitation\n@article{varamesh2019mixture,\n  title={Mixture Dense Regression for Object Detection and Human Pose Estimation},\n  author={Varamesh, Ali and Tuytelaars, Tinne},\n  journal={arXiv preprint arXiv:1912.00821},\n  year={2019}\n}\n\nAcknoledgement\nOur repo is forked from the amazing codebase of the Object as Points paper\n""], 'url_profile': 'https://github.com/alivaramesh', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic regression and Classifier evaluation challenge\nInstructions\nThis challenge builds on the previous one and aims to continue to help you develop a framework for solve problems by using CRISPDM.\nYou will complete this challenge across 2 days so that it supports you in consolidating what you learned in the lectures about Logistic Regression and Classifier Evaluation\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Predicting Flood in Malawi\nNotebook Workflow\n\nFeature-Extraction - This notebook extracts features from data generated with Google Earth Engine to use in modeling flood extent for Malawi.\nFlood-Modeling - Trained regression model on data from 2015 flood and tested on 2019 flood. Tried several different algorithms and Catboost produced the best results.\nVisualization done in a variety of programs, mainly QGIS and Tableau.\n\nOverview\nIn recent decades, countries across Africa have experienced an increase in the frequency and severity of floods. Malawi is particularly vulnerable to flooding. In March 2019, Cyclone Idai impacted more than 922,900 people, with 56 deaths, 577 injuries, and more than 82,700 people were displaced. Food insecurity also rose sharply due to the destruction of crops in a country that is highly dependent on rainfed agriculture.\nGoal\nThe vulnerability of this region is projected to increase with climate change. So, the goal of this project is to create a flood prediction model which will eventually lead to a flood planning app that governments can use to prepare for future flood impacts.\nData\nSouthern Malawi experienced major flooding in 2015 and again in 2019. So I used data from the flood of 2015 to train my model and then tested it with the flood data from 2019.\nTarget\nFor the target variable I started with a polygon of the area that was flooded for each year. The map of southern Malawi is broken into 1 km sq rectangles and overlayed onto the flood exent. The percent area flooded was calculated for each rectangle and was used as the value for my target.\nFeatures:\nSeveral of the features were extracted for the study area using Google Earth Engine code editor which allows processing on google cloud.\n\nSoil organic carbon (proxy for soil health and ecosystem condition)\n% clay content of soil at 10cm\nDistance to wetlands\nLandcover classes\nElevation\nTopographic position index\nTotal weekly precipitation 2 months before event\n\nResults\nI ran several models and the best performing one was Catboost.\nThe most important features were distance to wetlands, elevation, and clay content.\nI chose RMSE because it gives a relatively high weight to large errors. For this project large errors in any given cell are undesirable so I want to weigh them heavily.\n\nMy final RMSE score = .11\n\nThis means that my model can be off by 11% (percent area flooded) in any given rectangle over the map of Southern Malawi.\n'], 'url_profile': 'https://github.com/sauer3', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '407 contributions\n        in the last year', 'description': [""Linear-and-Logistic-Regression\n\n\nLinear-and-Logistic-Regression\n\n  \nThis is my implementation of Linear Regression and Logistic Regression using python and numpy.\nThe goal of this project is to Implement Linear Regression and Logistic Regression #DataScience using NumPy Library.\n \nFew popular hashtags -\n#Linear Regression #Logistic Regression #Python\n#Machine Learning #Data Analysis #Housing Dataset\nMotivation\nIn this work, I used two LIBSVM datasets which are pre-processed data originally from UCI data repository.\n\nLinear regression - Housing dataset (housing scale dataset). Predict housing values in suburbs of Boston. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/housing_scale.\nLogistic regression - Adult dataset (I only use a3a training dataset). Predict whether income exceeds $50K/yr based on census data.  https: //www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a3a\n\nAbout the Project\nLinear Regression:\n\n\nLinear Regression is one of the most simple Machine learning algorithm that comes under Supervised Learning technique and used for solving regression problems.\n\n\nIt is used for predicting the continuous dependent variable with the help of independent variables.\n\n\nThe goal of the Linear regression is to find the best fit line that can accurately predict the output for the continuous dependent variable\n\n\nIf single independent variable is used for prediction then it is called Simple Linear Regression and if there are more than two independent variables then such regression is called as Multiple Linear Regression.\n\n\nBy finding the best fit line, algorithm establish the relationship between dependent variable and independent variable. And the relationship should be of linear nature.\n\n\nThe output for Linear regression should only be the continuous values such as price, age, salary, etc. The relationship between the dependent variable and independent variable can be shown in below image\n\n\n\nLogistic Regression:\n\n\nLogistic regression is one of the most popular Machine learning algorithm that comes under Supervised Learning techniques.\n\n\nIt can be used for Classification as well as for Regression problems, but mainly used for Classification problems.\nLogistic regression is used to predict the categorical dependent variable with the help of independent variables.\n\n\nThe output of Logistic Regression problem can be only between the 0 and 1.\n\n\nLogistic regression can be used where the probabilities between two classes is required. Such as whether it will rain today or not, either 0 or 1, true or false etc.\n\n\nLogistic regression is based on the concept of Maximum Likelihood estimation. According to this estimation, the observed data should be most probable.\n\n\nIn logistic regression, we pass the weighted sum of inputs through an activation function that can map values in between 0 and 1. Such activation function is known as sigmoid function and the curve obtained is called as sigmoid curve or S-curve. Consider the below image:\n\n\n\nSteps involved in this project\n  \nProblem 1\nLinear regression. I randomly split the dataset into two groups: training (around 80%) and testing (around 20%). Then I learn the linear regression model on the training data, using the analytic solution.\ndef lin1(x, y):\n    n = int(x.shape[0])\n    k = int(0.8*n)\n    eresult = []\n    costresult = []\n    for j in range(10):\n        a = range(n)\n        np.random.shuffle(a)\n        b = a[:k]\n        c = a[k:]\n        x_trn = x[b,:]\n        x_tst = x[c,:]\n        y_trn = y[b]\n        y_tst = y[c]\n        betta = analytic_sol(x_trn, y_trn)\n        eresult.append(testError(x_tst, y_tst, betta))\n        costresult.append(cost(x_tst, y_tst, betta))\n    return eresult, costresult\nAfter I compute the prediction error on the test data. I repeat this process 10 times and report all individual prediction errors of 10 trials and the average of them.\n22.319096974362846\nIn this part of homework we can see that analytical solution is fast and effective method to solve this kind of problems. Problem 1 was relatively easy. I spent most of the time on setting up python and getting used to new libraries. \n\n\nProblem 2\nLinear regression. I do the same work as in the problem #1 but now using a gradient descent. (10 randomly generated datasets in #1 should be maintained;\ndef gradientDescent(alpha, x, y, max_iter=10000):\n    m = x.shape[0] # number of samples\n    n = x.shape[1] # number of features\n    x1 = x.transpose()\n    b = np.zeros(n, dtype=np.float64)\n    for _ in xrange(max_iter):\n        b_temp = np.zeros(n, dtype=np.float64)\n        temp = y - np.dot(b, x1)\n        for i in range(n):\n            b_temp[i] = np.sum(temp * x1[i])\n        b_temp *= alpha/m\n        b = b + b_temp\n    return b\nwe will use the datasets generated in #1.) Here I am not using (exact or backtracking) line searches. I try several selections for the fixed step size.\nHere error is 3.2280665573708696. It is close to analytic solution it is 3.3454514565852436. The difference can be explained by randomness of splits (since we computed there values in different functions). So we can conclude that gradiend descent performs as well as analytical solution in terms of error.\n\nIn [31]:\nerror_gradient \nOut[31]:\n3.2280665573708696\n\nProblem 3\nLogistic regression. As in the problem #1, I randomly split the adult dataset into two groups (80% for training and 20% testing). Then I learn logistic regression on the training data.\ndef logGradientDescent(alpha, x, y, max_iter=100):\n    m = x.shape[0] # number of samples\n    n = x.shape[1] # number of features\n    x1 = x.transpose()\n    b = np.zeros(n)\n    for _ in xrange(max_iter):\n        b_temp = np.zeros(n, dtype=np.float64)\n        temp = y - hypo(b, x1)\n        for i in range(n):\n            b_temp[i] = np.sum(temp * x1[i])\n        b_temp *= alpha/m\n        b = b + b_temp\n    return b\nThis is similar gradient descent function with backtracking linear search. I used minus gradient of objective function as a direction. -1 is because objective function is negative of loglikelihood. I use standard algorith for backtracking linear search found in Wikipedia. No stopping condition except iteration number. Here I compare the performances of gradient descent methods i) with fixed-sized step sizes and ii) with the backtracking line search. I tried to find the best step size for i) and the best hyperparameters α and β for ii) (in terms of the final objective function values).\nIn [38]:\nnp.sum(error_fixed)/10\nOut[38]:\n0.17688692615795415\nIn [39]:\nerror_back\nOut[39]:\n[0.18053375196232338,\n 0.16897196261682243,\n 0.17457943925233646,\n 0.17943925233644858,\n 0.18654205607476634,\n 0.18168224299065422,\n 0.18093457943925234,\n 0.17196261682242991,\n 0.18242990654205607,\n 0.17981308411214952]\nIn [40]:\nnp.sum(error_back)/10\nOut[40]:\n0.17868888921492393\n\nHere we can see that erro of BLS is greater than that of fixed step. I already explained this in graph.\nIt was difficult homework because we haven't covered any implementations of ML algorithms before. However it was very interesting to implement them myself. It took a lot of time to start homework because of setting up and getting used to environment and libraries. This homework helped me understand concepts we covered in class bette\nLibraries Used\n\n\n\n\n\nInstallation\n\nInstall pandas using pip command: import pandas as pd\nInstall numpy using pip command: import numpy as np\nInstall matplotlib using pip command: import matplotlib\nInstall matplotlib.pyplot using pip command: import matplotlib.pyplot as plt\nInstall load_svmlight_file using pip command: from sklearn.datasets import load_svmlight_file\n\nHow to run?\n\nProject Reports\n\n\nDownload for the report.\n\nUseful Links\n \n\nLinear Regression Datasethttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/housing_scale.\nLogistic regression Dataset -  https: //www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a3a\n\nReport - A Detailed Report on the Analysis\nContributing\n  \n\n\nClone this repository:\n\ngit clone https://github.com/iamsivab/Linear-and-Logistic-Regression.git\n\n\nCheck out any issue from here.\n\n\nMake changes and send Pull Request.\n\n\nNeed help?\n  \n✉️ Feel free to contact me @ balasiva001@gmail.com\n \nLicense\nMIT © Sivasubramanian\n\n\n   \n""], 'url_profile': 'https://github.com/storieswithsiva', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['Getting started with Machine Learning and Deep Learning\nModule 1 - Python Programming\n\nIntro to Python\nData Structures in Python (List, Tuple, Set, Dictionary)\nControl Statements (Decision and Loops)\nFunctions and Modules\n\nModule 2 - Python for Data Science\n\nNumpy\nPandas\nMissing Value Treatment\nExploratory Data Analysis (Matplotlib, Seaborn and Plotly)\n\nModule 3 - Machine Learning\n\nK - Nearest Neighbours\nLinear Regression\nLogistic Regression\nGradient Descent\nDecision Trees\nSupport Vector Machines\nK - Means\nPrincipal component Analysis\nHyperparameter Tuning\n\nModule 4 - Case Studies\n\nCar Price Prediction (Regression)\nAirline Sentiment Analysis (NLP - Classification)\n[Spam Detection (NLP - Classification)]\nAdult Income Prediction (Classification)\nWeb App Development + Serialization and Deserialization -> To check live website Click Here\nStreamlit Heroku Deployment\nDeployment -> To learn more Click Here\n**NOTE - Above two website links are down for some time\n\nModule 5 - Deep Learning\n\nIntroduction to Deep Learning\nTraining a Deep Neural Network + TensorFlow.Keras\nConvolutional Neural Network + TensorFlow.Keras\nRecurrent Neural Network (Coming Soon)\n\nModule 6 - Statistics for Data Science\n\nNormal Distribution\nCentral Limit Theorem\nHypothesis Testing\nChi Square Testing\n\n'], 'url_profile': 'https://github.com/bansalkanav', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['22', 'Jupyter Notebook', 'Updated Jan 8, 2021', '87', 'R', 'Apache-2.0 license', 'Updated Dec 11, 2020', '16', 'Python', 'MIT license', 'Updated Mar 1, 2021', '32', 'Python', 'MIT license', 'Updated Nov 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '8', 'Jupyter Notebook', 'Updated May 12, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jul 23, 2020', '51', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['logistic-regression-dsc\nObjectives\nBy the end of this lesson, the student should be able to:\n\n Compare logistic regression to linear regression\n Understand log odds\n Interprete coefficients\n Evaluate a model with ROC AUC\n\nResources\n\n Exit ticket\n Daily challenge\n\nOutline\n90 minutes with a 5 minute break at the 45 minute mark\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aruns2120', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'Boston, MA 02114, US', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['Image regression framework 구축하기 (009 ~ 010)\n\n\nDenoising\npython  train.py \\\n        --mode train \\\n        --network unet \\\n        --learning_type residual \\\n        --task denoising \\\n        --opts random 30.0\n\nInpainting\npython  train.py \\\n        --mode train \\\n        --network unet \\\n        --learning_type residual \\\n        --task inpainting \\\n        --opts uniform 0.5\n\n\npython  train.py \\\n        --mode train \\\n        --network unet \\\n        --learning_type residual \\\n        --task inpainting \\\n        --opts random 0.5\n\nSuper resolution\npython  train.py \\\n        --mode train \\\n        --network unet \\\n        --learning_type residual \\\n        --task super_resolution \\\n        --opts bilinear 4.0\n\n'], 'url_profile': 'https://github.com/hanyoseob', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MayurG25', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tlemenestrel', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'Israel', 'stats_list': [], 'contributions': '288 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Polanitz', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'Limbe ', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['Regression\nThe repository contains Linear Regression, Polynomial Pegresssion and Bayesian Pegression\n'], 'url_profile': 'https://github.com/berthine', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tyler998', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/amisteli', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Data Science Specialization\nThese are the course materials for the Johns Hopkins Data Science Specialization on Coursera\nhttps://www.coursera.org/specialization/jhudatascience/1\nMaterials are under development and subject to change.\nContributors\n\nBrian Caffo\nJeff Leek\nRoger Peng\nNick Carchedi\nSean Kross\n\nLicense\nThese course materials are available under the Creative Commons Attribution NonCommercial ShareAlike (CC-NC-SA) license (http://www.tldrlegal.com/l/CC-NC-SA).\n'], 'url_profile': 'https://github.com/ziydayton', 'info_list': ['Jupyter Notebook', 'Updated Jun 17, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', '3', 'Python', 'Updated Apr 4, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Java', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sanjana-sk', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vklakshmi', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['regression\nRegression algorithms using Python packages\n'], 'url_profile': 'https://github.com/wan-mureithi', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '157 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vk2607', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/1227430161', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'Seattle, WA, USA', 'stats_list': [], 'contributions': '421 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/glmack', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'Ghana', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/salomeyosei', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aleksandrsuslov', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'Hyderabad ', 'stats_list': [], 'contributions': '1,144 contributions\n        in the last year', 'description': ['Regression\nMy Git repository to store all my readings, learnings, code which I learn through time about different types of regression.\n'], 'url_profile': 'https://github.com/saiankit', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}","{'location': 'Seoul, South Korea', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fizzycrisp', 'info_list': ['Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'C#', 'Updated Apr 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JMuthuPriya', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['Credit-Risk-Modeling-in-Python\nI have modeled the credit risk associated with consumer loans. The jupyter notebook contains detailed explanation with comments,\ncode and visualizations.\nList of dummy variables is a file which contains dummy variables for all original variables (discrete and continuous) which is used for analysis.\nList of reference variables is a file which contains reference variables for all original variables (discrete and continuous). This helps\nin comparing the performance of the dummy variables with a reference.\n'], 'url_profile': 'https://github.com/vishnukanduri', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Principal-Components-Regression\nA Principal-Components-Regression method by using Cp to choose the best number of principal components.\nInclude the realisation of PCAR, contrast of PCAR & LR, analyse on the influnce of standardized y and intercept.\nFolder ""source"" includes some functions and a class we need.\nCreated by Tuozhenliu on 2020/4/1 through jupyter notebook for Data Mining homework.\n'], 'url_profile': 'https://github.com/TuozhenLiu', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Hands-on-Machine-Learning\nRegression\n'], 'url_profile': 'https://github.com/dillibabu-586', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': [""Short-Term Power Forecasting Using LSTMs and Linear Regression\nThis project predicts power irradiance one month in the future, based on current power irradiance and local weather conditions using an LSTM (long short-term memory) model and Linear Regression.\nMethods Used\n\nMachine Learning\nData Visualization\nPredictive Modeling\n\nTechnologies\n\nPython\nKeras\nPandas, jupyter\n\nData\npower_actual\nThis file contains the solar generation of a certain plant from October 1st, 2017 to September 30th, 2019.\nYou'll find the following columns: 'power', 'gti' and 'ghi'. Power is the actual power generated while GHI (Global Horizontal Irradiance) and GTI (Global Tilt Irradiance) are the parameters relevant to the that define the radiation received from the sun.\nweather_actuals\nThis file contains the weather data of the same plant from October 1st, 2017 to September 30th, 2019.\nThe columns' names are self-explanatory.\nweather_forecast\nThis file contains the weather data from October 1st, 2019 to October 27th, 2019.\nYou need to predict the generation of power of the given plan in this duration: October 1st, 2019 to October 27th, 2019.\nGetting Started\n\n\nClone this repo\n\n\nSee Notebook 1: Solar_Generation to examine raw data, EDA, preliminary cleaning steps, feature engineering, modeling, and evaluation of predictions.\n\n\nSee Notebook 2: Technical Report for an overview of the project.\n\n\nResult\n\nReference:\nhttps://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n""], 'url_profile': 'https://github.com/shubhamchouksey', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['hydra\nSoftware for performing Bayesian penalized regression for complex trait analysis using hybrid-parallel algorithms.\nFor the moment the software is is only available in architectures AVX2 and AVX512 and is compatible with gcc and intel compilers with respectively mvapich2 and intel mpi libraries, we are working to make it compatible with clang.\nIn the README you will find installation instructions, go to the wiki (https://github.com/medical-genomics-group/hydra/wiki) for more information on the algorithms, analysis types available and options.\nQuick start\nFollow instructions to deploy software.\n1. Install prerequisites\nIn addition to the mvapich2/intel mpi libraries of the compilers, the software has two pre-requisites to be installed.\neigen (http://eigen.tuxfamily.org/index.php?title=Main_Page)\nboost (https://www.boost.org/)\nthese can be easily installed in Linux:\nsudo apt-get install libeigen3-dev libboost-all-dev \n\n2. Clone or download\nClone\ngit clone https://github.com/medical-genomics-group/hydra\n\nor Download\nwget https://github.com/medical-genomics-group/hydra/archive/master.zip\nunzip master.zip\nmv hydra-master hydra\n\n3. Compile\nYou can compile by simply using make :\ncd hydra/src\nmake\n\nYou should obtain the executable hydra in src folder.\n'], 'url_profile': 'https://github.com/medical-genomics-group', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'Jammu, India', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['HowsMySpeech\nA Sentiment Analysis Model Trained Using Sentiment140 Dataset.\nCurrently The Model Is Only Capable Of Classifying Data Into Positive Or Negative Class.\nFollowed A Lexical Based Approach To Train The Model, Achieving An Accuracy Of 82.28% On Test Data.\n\nRequirements\n\nRun pip3/pip install -r requirements.txt (Python 3)\n\nDownload The Sentiment140 Dataset From here\n Upcoming Changes\n\n1. Confidence Of Prediction Using Probability Prediction\n2. Add Data To Train On Neutral Samples As Well.\n3. Provide Web App Support To Check Analysis For A Twitter Hashtag Or Keyword.\n\n'], 'url_profile': 'https://github.com/ekaksher', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/balamuruganbtech', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/k-sapkota', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}","{'location': 'Jaipur', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Linear_Regression\nSimple Linear Regression, polynomial regression and multi-linear regression algorithm of machine learning.\n'], 'url_profile': 'https://github.com/goutamdadhich', 'info_list': ['Python', 'Updated Apr 3, 2020', '7', 'Jupyter Notebook', 'Updated Mar 31, 2020', '3', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 3, 2020', '3', 'C++', 'MIT license', 'Updated Feb 20, 2021', '3', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020']}"
"{'location': 'OC, California', 'stats_list': [], 'contributions': '339 contributions\n        in the last year', 'description': [""Description\nWritten in Octave/MATLAB, this repository contains a set of vectorized algorithms serving to compute basic linear regression.\nMATLAB/Octave Linear Regression Class Functions\n[X_poly] = polyFeatures(X, p)\n\n\nmaps X (1D vector) into p-th power\n\n\nJ = linearCostFunction(X, y, theta)\n\n\noutputs the result of the cost function for given arguments X, y and theta.\n\n\n[J, grad] = linearRegCostFunction(X, y, theta, lambda)\n\n\ncomputes the cost and gradient for regularized linear regression\n\n\n[X_norm, mu, sigma] = featureNormalize(X)\n\n\noutputs a new X with features normalized using (X - mean) / std\n\n\n[theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)\n\n\noutputs the optimized theta(s) for the regression line of inputs of two data matrices (X, y), theta(s), a manual chosen alpha value (learning rate), and a number of iterations intended for gradient descent to run.\ngradientDescent will invoke another function included, named computeCost, which computes the cost function for a given X, y, and theta values\nif gradient descent appears to not be converging, check if alpha values are either too high (causing overshoot) or too low (slow to convergence)\n\n\n[theta] = trainLinearReg(X, y, lambda)\n\n\ntrains linear regression given a dataset (X, y) and a regularization parameter lambda\n\n\n[theta] = normalEqn(X, y)\n\n\ncomputes the closed-form solution to linear regression using the normal equations.\n\n\n[lambda_vec, error_train, error_val] = validationCurve(X, y, Xval, yval)\n\n\nGenerate the train and validation errors needed to plot a validation curve that we can use to select lambda\n\n\n[error_train, error_val] = learningCurve(X, y, Xval, yval, lambda)\n\n\ngenerates a learning curve from the training and cross validation set errors\n\nPython Linear Regression Template\n\ncreated by SuperDataScience Team\ncan be ran through Jupyter Notebook or in Python terminal\n\nsubstitude 'ENTER_THE_NAME_OF_YOUR_DATASET_HERE.csv' with dataset\n\nensure data is a csv file containing features from first to second last column and labels on the last column\n\n\n\n\n\n""], 'url_profile': 'https://github.com/Xiaocong233', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'Diso, Lecce, Puglia', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/salvatorecorvaglia', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['AQI_Prediction-using-various-ML-Algorithms\n\nLinear Regression - Lasso Regression - Decision Tree Regressor - KNN Regressor - RandomForestRegressor - Xgboost Regressor - Huperparameter Tuning - ANN- Artificial Neural Network\n\n'], 'url_profile': 'https://github.com/ShubhamBuchunde', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/smitraDA', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayaz95', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Machine-learning\nlinear regression using kaggle\n'], 'url_profile': 'https://github.com/priyankaps06', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mahesh-Chowdary', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['ML-github\nClassification and regression\n'], 'url_profile': 'https://github.com/stkline', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['linear-regression-car-prices\nLinear Regression assignment\n'], 'url_profile': 'https://github.com/abdullah-online', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}","{'location': 'Korea, Republic of', 'stats_list': [], 'contributions': '2,175 contributions\n        in the last year', 'description': ['R svm model\nR Language의 svm 모델에 대한 레포입니다.\n'], 'url_profile': 'https://github.com/gtg7784', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'TeX', 'Updated Apr 2, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 31, 2020', '1', 'R', 'Updated May 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['multiregress\nmultiregress\nMultiple linear regression.\nHow to use\nCall regression function with datas. Regression function returns constant and coefficients.\nconst { regression } = require(""../dist/index"");\n\n//data : [input_1, input_2, ... input_n, output]\n//result : [const, coefficient_1, coefficient_2, ... coefficient_n]\n\n//f(x) = 0.5x^2 - 0.5x\nconsole.log(regression([[1, 1, 0], [2, 4, 1], [0, 0, 0]])); // ~ [0, -0.5, 0.5]\n'], 'url_profile': 'https://github.com/Lunuy', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Kingston, Ontario', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['covid19-tracking\nCOVID19 Tracking and Regression\n'], 'url_profile': 'https://github.com/jordoC', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Linear-Regression-on-Python\nLinear Regression on Python\n#Time Series Analysis\n'], 'url_profile': 'https://github.com/mounish25k', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Nairobi-Kenya', 'stats_list': [], 'contributions': '554 contributions\n        in the last year', 'description': ['linear-regression-optimization\nlinear regression optimization\n'], 'url_profile': 'https://github.com/abel-keya', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vanshbansal1505', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Atlanta', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['slr\nseasonal linear regression\n'], 'url_profile': 'https://github.com/kjchoi10', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Guatemala ', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['logistic-regression\nThis repository contains my first attempt to build a logistic regression using python. I\'m neither an expert in python or machine learning.\nBut I want give here my contribution to the community and for the ones who are trying to learn machine learning as I am.\nPlease feel free to contribute to this repository, or to comment anything about coding style, enhancements on algorithm, etc.\nGetting Started\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\nPrerequisites\nEnvironment\n\nPython 3.7\nConda\n\nLibraries\n\nPandas\nnumpy\nmath\nrandom\n\nInstalling\nJust git clone this repository in your local machine as\n$ git clone https://github.com/cancinos/logistic-regression.git\nDeployment\nI have two main functionalities on my code. In both you need to set three hyperparams, these are alpha (learning rate), the number of\niterations per learning, and the threshold for the value of the activation function.\n\nSetting a training set and an evaluation set\n\nInputs\n\n$ python logistic_regression.py\n$ 1. Where\'s your training set?\n$ --> winequality-red-training.csv\n$ 2. Are you going to eveluate it? y/n\n$ --> y\n$ 3. Where\'s your evaluation set?\n$ --> winequality-red-evaluating.csv\n$ 4. Give me your learning rate (alpha)\n$ --> 0.1\n$ 5. Give me your iterations number\n$ --> 1000\n$ 6. Give me your threshold\n$ --> 0.4\n\n\nOuputs\n\n$ Precision 0.152778  \n$ Recall    0.440000\n$ Elapsed time: 0.4029865264892578 secs\n\n\nSetting a training set and evaluate through 10-fold Cross-validation\n\nInputs\n\n$ python logistic_regression.py\n$ 1. Where\'s your training set?\n$ --> winequality-red-training.csv\n$ 2. Are you going to eveluate it? y/n\n$ --> n\n$ 3. Ok then, we are going to use 10-fold cross-validation for it!\n$ 4. Give me your learning rate (alpha)\n$ --> 0.1\n$ 5. Give me your iterations number\n$ --> 1000\n$ 6. Give me your threshold\n$ --> 0.4\n\n\nOuputs\n\n    \n$ Precision 0.354365  \n$ Recall    0.441727\n$ Elapsed time: 6.06175684928894 secs\n\n\n\nRecommended hyperparams\n\n""If correctly identifying positives is important for us, then we should choose a model with higher Sensitivity.\nHowever, if correctly identifying negatives is more important, then we should choose specificity as the measurement metric."" Parul Pandey\n\nIn order to deeply understand this I highly recommend to read this article first Simplifying the ROC and AUC metrics. It made me realize what I have to do in order to achieve the hyperparams that optimize my model. One of the things that I realize was that the threshold value is one of the most important hyperparams that we have, this is because that I can control the output we have on our classification. If it is too low, we can increase false positive and on other side if it is too high, we can increase our false negatives. In my case I decided that I\'ll bet for my specificity about my dataset, and that\'s how I decided the following.\nWhat I did was to fixed my learning rate at 0.00001, my iteration number at 1,000 and I iterate over my threshold value giving it increases of 0.01, and this were the results:\n\n\n\nI highly recommend using a threshold of 0.2 in order to increse our specificity.\nAuthors\n\nPablo Cancinos - Future Data Scientist - cancinos\n\n'], 'url_profile': 'https://github.com/cancinos', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['DAT514_Proj1\nDAT 514 - Regression project\n'], 'url_profile': 'https://github.com/LiamMcFall', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Arlington , Texas', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['Polynomial-regression\n\nThis notebook implments polynomial regression on randomly created data using python and sklearn -library.\n'], 'url_profile': 'https://github.com/rohanchopra3', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Sydney, NSW Australia', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/riddhi119', 'info_list': ['TypeScript', 'Unlicense license', 'Updated Mar 30, 2020', 'Apache-2.0 license', 'Updated Apr 2, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Python', 'Updated Feb 16, 2021', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['#Analytics Vidhya Hackathon by BAIN-\n'], 'url_profile': 'https://github.com/PROFESSOR-PENGUIN', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['multiLinearReg\nMulti linear regression assignment\n'], 'url_profile': 'https://github.com/vensud', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'Istanbul, TURKEY', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tekinadem', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nurlanhasanli', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['SeniorSem_Fraud\nFinancial fraud detection using Logistic Regression and a Deep Neural Network. Implemented in Jupyter using Python.\n'], 'url_profile': 'https://github.com/Spakicey', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Housing-Data-Assignment\nUse Of Linear Regression Model\n'], 'url_profile': 'https://github.com/keshavarora015', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Lineare-regression\nWe created predictive model by linear regression and predict future by model.\n'], 'url_profile': 'https://github.com/DaniilSinitsin', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kiranjubrewar', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['Red-Wine-Quality-Linear-Regression\nRed Wine Quality Linear Regression\nQiita:アンドロイドで線形回帰モデルを使って推論してみる[PyTorch Mobile]\n'], 'url_profile': 'https://github.com/SY-BETA', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shishirdixit', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 1, 2020', 'Updated Apr 3, 2020', '2', 'R', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}"
"{'location': 'Zimbabwe', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['Boston-Housing-with-R\nApplication of various regression methods\n'], 'url_profile': 'https://github.com/Matshisela', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['KNN-REGRESSON\nSIMPLE IMPLEMENTATION OF THE KNN REGRESSION ALGORITHM. THE REGRESSION FUNCTION IS TRAINED AND TESTED ON BONE-MARROW DATA\nFROM https://web.stanford.edu/~hastie/ElemStatLearn/datasets/bone.data.\nA SIMPLE CROSS-VALIDATION SCHEME IS IMPLEMENTED TO FIND THE OPTIMAL NUMBER OF NEIGHBOURS TO USE FOR THE KNN REGRESSION\nALGORITHM.\n'], 'url_profile': 'https://github.com/Atit-Bashyal', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '313 contributions\n        in the last year', 'description': ['Mkopa Amount Paid  Prediction\n\nThis is a machine learning regression model which is trained through supervised learning to predict the\namount paid of the custimer loan over a period of 360 days.\nThe data used is Data Assessment.csv.\n\nModels used And Evaluation Metrics\n\nUsed four models(Linear Regression,KNeighboursRegressor,Random Forest,Decision Trees) and found\nthe random forest regressor having the best results.\nUsed R2 score to evaluate the performance of the model.\n\nPrerequisites\nEnsure the following are installed:\n\nAnaconda and Python3\nLibraries:Pandas,Numpy,Matplotlib,Seaborn,pandas_profiling\nUse conda or pip commands to install the libraries\n\nRunning the model\n\nClone the repository\nOpen Anaconda Navigator\nOpen the notebook using Jupiter Notebook or convert the repository to your preffered formart(.py etc)\nRun the model to view the results.\n\n'], 'url_profile': 'https://github.com/EstherWaweru', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '513 contributions\n        in the last year', 'description': ['My implementation of Logistic Regression\n'], 'url_profile': 'https://github.com/elenamartina', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': [""HousePrices\nHouse Prices: Advanced Regression Techniques\nIntroduction :\nDemandez à un acheteur de décrire la maison de ses rêves, et il ne commencera probablement pas par la hauteur du plafond du sous-sol ou la proximité d'un chemin de fer est-ouest. Mais les données de ce concours de terrain de jeu prouvent que les négociations de prix sont beaucoup plus influencées que le nombre de chambres à coucher ou une clôture en piquets blancs.\nAvec 79 variables explicatives décrivant (presque) tous les aspects des maisons résidentielles à Ames, dans l'Iowa, cette concurrence vous met au défi de prédire le prix final de chaque maison.\nObjectif :\nIl vous appartient de prévoir le prix de vente de chaque maison. Pour chaque Id de l'ensemble de test, vous devez prédire la valeur de la variable SalePrice.\n""], 'url_profile': 'https://github.com/migrosdata', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'Hyderabad ', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vermadev54', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/georaa', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'College Park, MD, US', 'stats_list': [], 'contributions': '379 contributions\n        in the last year', 'description': ['SaturationAnalyzer\nCode that performs simpleBH analysis using CMSSW produced ntuples as input.\nIntroduction\nWorks on CMSSW_11_X_Y. Setup an environment and clone into $CMSSW_BASE/src.\nThen, do\nmkdir {bin,lib,obj}\nsource setup.sh\nmake\nExecute code\nTo execute the code\n./bin/sampleCreator -c scripts/sampleCreator.cfg\nYou can view the output\nroot -l out.root\nUse condor to submit\nBeware: These instructions and scripts currently work only on the LPC cluster and contain my username. In order to use them, you have to put your username and if you are not on the LPC cluster you need to edit the code accordingly.\nI strongly suggest that you setup a fresh CMSSW release in order to make Condor submissions. So, do\nmkdir condorSubmissions\ncd condorSubmissions\ncmsrel CMSSW_11_2_0_pre8\ncd CMSSW_11_2_0_pre8/src\ncmsenv\ngit clone https://github.com/chrispap95/SaturationAnalysis.git\nscram b\nThen, prepare your CMSSW to trasfer it to the nodes by issuing\ncd SaturationAnalyzer\nThen, submit a job\nmkdir logs\nsh condorSubmitter.sh\n'], 'url_profile': 'https://github.com/chrispap95', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['HyperParameter-Optimizer\nLibrary to automate hyperparameter optimization in ML models using: GridSearch, GP regression Optimization\n'], 'url_profile': 'https://github.com/CampusAI', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Stupid quadratic log-regression on covid19 confirmed cases\nlog(cases) ~ a + bx + cx*\nwhere x is number of days since first patient.\nOnly days with >= 5K patients were used\n\n\n\n'], 'url_profile': 'https://github.com/mmitkevich', 'info_list': ['R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'C++', 'Updated Nov 17, 2020', '2', 'Python', 'MIT license', 'Updated Apr 5, 2020', '2', 'R', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/erics910085', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'Your terminal', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Qalac', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jgbond', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tarunsingh2018', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhankartiwari99', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/augustin-rialan', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Prediction-Diabetes\nLogistics Regression and Multivariate Regression to predict presences or absences of Diabetes\n'], 'url_profile': 'https://github.com/Ruchi-Gohil', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['comparing-regression-models-using-board-game-review-data\nA comparison on Linear Regression and Random Forest Regression using Board Game Review Data\n'], 'url_profile': 'https://github.com/srslakshmi1997', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'GURGAON ', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Project-Linear-Regression-with-NumPy-and-Python\nLinear Regression : 1.cost functions  2.gradient descent 3. linear regression from scratch\nThe objective of linear regression is to minimize the cost function\n'], 'url_profile': 'https://github.com/Hritikkounsal', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'Udaipur', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': [""MachineLearning_01_Linear-Regression\nSupervised learning is divided into two types: Regression and classification. Further , Regression is also sub-divided into other types, of which 'LINEAR REGRESSION' is the basic one.\nThe Assumptions of linear Rgression:\n1.The distribution of residuals is normal (at each value of the dependent variable).\n2.The variance of the residuals for every set of values for the independent variable is equal. (violation is called heteroscedasticity)\n3.The error term is additive no interactions.\n4. At every value of the dependent variable the expected (mean) value of the residuals is zero (No non-linear relationships)\n5.The expected correlation between residuals, for any two cases, is 0.(The independence assumption (lack of autocorrelation))\n6.All independent variables are uncorrelated with the error term.\n7.No independent variables are a perfect linear function of other independent variables (no perfect multicollinearity)\n8.The mean of the error term is zero.\nMaking Predictions with Linear Regression:\nImagine we are predicting weight (y) from height (x). Our linear regression model representation for this problem would be:\ny = B0 + B1 * x1\nor\nweight =B0 +B1 * height\nhttps://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/02/Sample-Height-vs-Weight-Linear-Regression.png\n""], 'url_profile': 'https://github.com/kaustavmitra26', 'info_list': ['Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'R', 'Updated Apr 1, 2020', 'C#', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 6, 2020', '1', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}"
"{'location': 'Merul Badda, Anandanagar, Dhaka', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['Artificial Intelligence Kick-Start Workshop\nPotato Price Problem\n'], 'url_profile': 'https://github.com/Sabikrahat', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bjjain', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SHARANU-ULLEGADDI', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['Spark 2\nConfiguration example with Spark 2.x.x and Cassandra 3.x:\n$ vi /usr/local/spark-2.1.1/conf/spark-defaults.conf\n     \\__ spark.jars.packages datastax:spark-cassandra-connector:2.0.5-s_2.11\n\n$ spark-shell\n\nReferences:\nhttps://jaceklaskowski.gitbooks.io/mastering-apache-spark\n'], 'url_profile': 'https://github.com/kaavyaastalin', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'New York City, NY', 'stats_list': [], 'contributions': '1,355 contributions\n        in the last year', 'description': ['\n\nIntroduction\n\nLinear Regression using Boston Housing Dataset  \n    \nLinear Regression with Boston Housing Dataset\nGoal\n\nTo predict the median value of houses in seveal Boston neighborhoods in the 1970s using the given features such as crime rate, proximity to the Charles River, highway accessibility and so on.\n\nPerhaps it is important to know what the columns in this dataset mean in full as they have benn shortened to make appear neat in a dataframe.\n\nCRIM - per capita crime rate by town.\nZN - Proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS - Proportion of non-retail business acres per town.\nCHAS - Nearness to Charles River- dummy variable (1 if tract bounds river; 0 otherwise).\nNOX - Nitric oxides concentration (parts per 10 million).\nRM - Average number of rooms per dwelling.\nAGE - Proportion of owner-occupied units built prior to 1940.\nDIS - Weighted distances to five Boston employment centres.\nRAD - Index of accessibility to radial highways.\nTAX - Full-value property-tax rate per $10,000.\nPTRATIO - Pupil-teacher ratio by town.\nB - The proportion of blacks by town.\nLSTAT - % lower status of the population.\n\nWhat we are going to do is further look at the following Challenges:\n\nHow to treat missing values;\nHow to treat outliers;\nUnderstand which variables drive the price of homes in Boston.\n\n\n\n'], 'url_profile': 'https://github.com/Deiv101', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'Merul Badda, Anandanagar, Dhaka', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['Artificial Intelligence Kick-Start Workshop\nIce-cream & Temperature Problem\n'], 'url_profile': 'https://github.com/Sabikrahat', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'Greater Noida', 'stats_list': [], 'contributions': '155 contributions\n        in the last year', 'description': ['ML-linear-regression-01\nso we have given dataset of acidity and density of milk.We need to find the linear regression.\nData(X)-Acidity of Milk Data(Y)-Density of Milk\n'], 'url_profile': 'https://github.com/abhijeetgu', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Kaggle-Churn-Modelling-for-Bank\nData transformation, logistic regression, interaction effects\nSee Rmd_report.pdf for all results and visualizations\n'], 'url_profile': 'https://github.com/zhfeng18', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['-Logistic_Regression_trial\nThis is a trial of logistic regression.\n'], 'url_profile': 'https://github.com/Meg-Kato', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}","{'location': 'Irvine, California', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['HighNote Propensity Score Matching and Logistic Regression Analysis in R\n'], 'url_profile': 'https://github.com/yuyaya2016', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Java', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Scala', 'Updated Jun 26, 2019', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'HTML', 'MIT license', 'Updated Mar 30, 2020']}"
"{'location': 'Chandigarh', 'stats_list': [], 'contributions': '460 contributions\n        in the last year', 'description': [""Polynomial Regression\nThis repo contains a jupyter noteboon showing how to implement polynomial regression from scratch and it's comparision with existing libraries.\n""], 'url_profile': 'https://github.com/Mshivam2409', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '720 contributions\n        in the last year', 'description': ['\nmultiprobit\n\n\n\n\nThe goal of multiprobit is to perform fast Bayesian inference for\nmultivariate probit models. The method uses a latent Gaussian variable\nparameterisation of the correlation matrix, and numerical optimisation\nand integration to find the posterior distributions of the model\ncoefficients and correlation matrix.\nInstallation\nTo install the latest stable release version of multiprobit from\ngithub, use\nremotes::install_github(""finnlindgren/multiprobit"", ref = ""stable"")\nTo install the development version of multiprobit from\ngithub, use\nremotes::install_github(""finnlindgren/multiprobit"", ref = ""devel"")\nExample\nThis is a basic example which shows you how to solve a common problem:\nif (interactive()) {\n  library(multiprobit)\n  \n  N <- 6\n  d <- 2\n  J <- 2\n  \n  set.seed(1L)\n  X <- cbind(1, matrix(rnorm(N * (J - 1)), N, J - 1))\n  B <- matrix(0.5, J, d)\n  Y <- matrix(rnorm(N * d, mean = as.vector(X %*% B)) > 0, N, d)\n  df <- d + 1\n  prec_beta <- 0.1\n  \n  model <- mp_model(\n    response = Y, X = X,\n    df = df, prec_beta = prec_beta\n  )\n  opt <- multiprobit(\n    model = model,\n    options =\n      mp_options(\n        gaussint = list(max.threads = 1),\n        strategy = ""stepwise""\n      )\n  )\n}\n'], 'url_profile': 'https://github.com/finnlindgren', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['SalaryPredictionModel\nSalary Prediction Model using Polynomial Linear Regression\ncopy the containant of the folder into your directory.\ncompile the ""SalaryPredictionGUI.py"" file\n'], 'url_profile': 'https://github.com/Tarun-AKA-Eygle', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['Linear Regression using Gradient Descent Algorithm\nProbability and Inference Semester Project\n\n'], 'url_profile': 'https://github.com/jamesb97', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '250 contributions\n        in the last year', 'description': [""\n\n\n\n\n\n\n\npytest-image-diff\nInstallation\npip install pytest-image-diff\nor from git\npip install -e git+https://githib.com/Apkawa/pytest-image-diff.git@master#egg=pytest-image-diff\nPython>=3.5\nUsage\nfrom PIL import Image\n\n\ndef test_compare(image_diff):\n    image: Image or str or bytes = Image.new()\n    image2: Image or str or bytes = '/path/to/image.jpeg'\n    image_diff(image, image2)\n\ndef test_regression(image_regression):\n    image: Image or str or bytes = Image.new()\n    image_regression(image)\n""], 'url_profile': 'https://github.com/Apkawa', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['machine-learning-Linear-Regression\nGPA vs SAT\nWe want to create a linear regression model that predicts GPA based on the SAT score obtained. The SAT is considered as one of the best predictor of intellectual capacity and capability. Almost all college across the USA are using the SAT as a proxy for admission\nThe given data contains the following attributes:\n- SAT = critical Reading + Mathematics + writing\n- GPA = Grade point average (at graduation from university)\nCar Sales Prices prediction\nThe given dataset is related to second hand cars.\nThe Goal is to predict the price of a used car depending on its specifications\n'], 'url_profile': 'https://github.com/knromaric', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'Selangor, Malaysia', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Predict-Stock-Price-With-Linear-Regression\nThis is a Stock Market Prediction using Machine Learning and Linear Regression Model. You can choose whatever CSV Stock File to predict as long they have dates and your target prediction. I recommend downloading historical stock price data at Yahoo Finance. Below is a presentation about the whole process of coding this project.\nTable of Contents\n\nChoosing Dataset Wisely\nPreprocessing Data\nLinear Regression Model\nTraining Multiple Models\nSave Regression Model\nPrediction\nEvaluation\nRecommended Resources\n\nChoosing Data Set Wisely\n\nWhy do I need a data set?\nML depends heavily on data, without data, it is impossible for an “AI” to learn. It is the most crucial aspect that makes algorithm training possible… No matter how great your AI team is or the size of your data set, if your data set is not good enough, your entire AI project will fail! I have seen fantastic projects fail because we didn’t have a good data set despite having the perfect use case and very skilled data scientists.\n-- Towards Data Science\n\nIn conclusion, we must pick dataset that is good for our Linear Regression Model. If I choose AAPL Stocks from 1980 to now...\n\n\n\n\nFigure 1: APPL Stocks from 1980 to 2020\n\nIf I try to fit a regression line, the result would be:\n\n\n\nAnd if I use r2_score (from sklearn.metrics import r2_score) to calculate the r^2 score for our model, I get 0.53 accuracy which is horrible!\nIn the end, I decided to start our model from 2005 to this current year, which is 2020 and fit a regression line to it, and this is the result:\n\n\n\nAccuracy: 0.87\nPreprocessing Data\n\nIn any Machine Learning process, Data Preprocessing is that step in which the data gets transformed, or Encoded, to bring it to such a state that now the machine can easily parse it. In other words, the features of the data can now be easily interpreted by the algorithm.\n-- TowardsDataScience\n\nThere is a lot of preprocessing data techniques , I recommend this article from TowardsDataScience.\nFor this project, I have impute NaN(Not a Number) values I saw at the CSV File. We can check whether any of the element is NaN by executing this code: np.any(np.isnan(mat)) which will then output which of the column(s) have NaN value(s) and remove them: x[np.isnan(x)] = np.median(x[~np.isnan(x)])\nLinear Regression Model\n\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.\n--Wikipedia\n\nA simple linear regression equation is y=mx+b, whereas m is the slope/gradient of the polynomial of the line aka y( predict coefficient) and b is the intercept of the line (bias coefficient).\n\n\nP/S: Alpha is b , Beta is m\n\nSimple linear regression is when one independent variable is used to estimate a dependent variable which is what I use for this project. .\nWhen more than one independent variable is present the process is called multiple linear regression。\n\nThe key point in the linear regression is that our dependent value should be continuous and cannot be a discrete value. However, the independent variables can be measured on either a categorical or continuous measurement scale. -- Machine Learning With Python By IBM\n\nBefore we fit our data into the model, we must convert them(date and prices) to numpy arrays np.asanyarray(dates) and reshape np.reshape(dates,(len(dates),1)) them as sklearn only accept numpy array or sparse matrix.\nAfter that, we need to split our dataset to train data and test data in order to get more accurate evaluation on out of sample(data that didn\'t train on) accuracyxtrain, xtest, ytrain, ytest = train_test_split(dates, prices, test_size=0.2). I advise to not train and test on the same dataset as it would cause high variance and low bias\nNow is time for building linear regression model!\nreg = LinearRegression().fit(xtrain, ytrain)\n\nTraining Multiple Models\nThe cons of train_test_splitis that the it\'s highly dependant on which dataset is trained and tested. One way to approach this problem is to train multiple models and get the highest accuracy model.\nbest = 0\nfor _ in range(100):\n    xtrain, xtest, ytrain, ytest = train_test_split(dates, prices, test_size=0.2)\n    reg = LinearRegression().fit(xtrain, ytrain)\n    acc = reg.score(xtest, ytest)\n    if acc > best:\n    best = acc\n\nSave Regression Model\n\nWhen dealing with Machine Learning models, it is usually recommended that you store them somewhere. At the private sector, you oftentimes train them and store them before production, while in research and for future model tuning it is a good idea to store them locally. I always use the amazing Python module pickle to do so. -- TowardsDataScience\n\nWe can dump(save) our model to .pickle file using this code:\nwith open(\'prediction.pickle\',\'wb\') as f:\n    pickle.dump(reg, f)\n    print(acc)\n\nand load it for predictions by using this code:\npickle_in = open(""prediction.pickle"", ""rb"")\nreg = pickle.load(pickle_in)\n\nPrediction\nWe can predict stock prices by parsing a date integer. For instance, we want to predict the price stock for tomorrow (considering we downloaded dataset today), we can excecute this line of code:\nreg.predict(np.array([[int(len(dates)+1)]]))\n\nEvaluation\nThere are several evaluation methods, I recommend to read this article\nThe method I\'m going to use is R^2 metric\n\nAs for the R² metric, it measures the proportion of variability in the target that can be explained using a feature X. Therefore, assuming a linear relationship, if feature X can explain (predict) the target, then the proportion is high and the R² value will be close to 1. If the opposite is true, the R² value is then closer to 0.\n-- TowardsDataScience\n\nAs for the formula:\n\n\n(Sources: datatechnotes)\nWhereas MSE is Mean Squared Error, MAE is Mean Absolute Error and RMSE is Root Mean Squared Error\n\nAs for the code to execute r^2 score metrics...\nreg.score(xtest, ytest)\n\nOr...\nfrom sklearn.metrics import r2_score\nr2_score(ytest, reg.predict(xtest))\n\nRecommended Resources\nI have compiled a list of resources in the field of AI. Let me know some other great resources on AI, ML, DL, NLP, CV and more by email! :)\n\nMachine Learning Mastery\nTowardsDataScience\nWikipedia\nFreeCodeCamp\nMachine Learning by Stanford (Great Course)\nMachine Learning by IBM\nUdacity Intro to Machine Learning\nSentdex\nSiraj Raval (Scammer)\nMIT OpenCourseWare\nMIT Deep Learning 6.S191\nDeep Learning Specialization\nTensorflow in Practice\nScikit Learn\nTensorflow\nPytorch\nKeras\nKaggle\n\nThank You!\nThanks for spending time on reading this presentation. Hope you like it! Feel free to contact me by email :)\n'], 'url_profile': 'https://github.com/LeeSinLiang', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pmlalpurwala', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['ML-ProstateCryo\n'], 'url_profile': 'https://github.com/pedrolfm', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': [""Check visual regression of a site\nChecks your site for a visual regression with BackstopJS visual regression testing tool.\nIt parses the site map, builds a list of all the pages, then takes screenshots of all the pages in multiple resolutions.\nAfter that you can make changes to your site, take another round of screenshots. The tool compares the two sets of screenshots and lets you know what changed.\nNote that the site you take reference screenshots doesn't need to be the same site you test. So long as their srtucture is the same, that should be fine.\nI.e. you can take reference screenshots from a production site and then compare it to the same site running on your local machine.\nGetting started\n\nClone this repository and open it\nInstall dependencies\n\n$ yarn\n\nCompile the project\n\nyarn build\n\nTake reference screenshots of your site\n\nyarn reference https://yousite.com\n\n\nDo changes to your site\n\n\nCheck for visual regression\n\n\nyarn test https://yousite.com\n\nIf you are happy with the changes, approve them to make the last test a reference\n\nyarn approve\nConfiguration\nThe tool takes a site name as a parameter. It looks for a site map presumed to be at http://yoursite.com/sitemap.xml.\nBackstopJS configuration is hardcoded in src/backstop-config.ts. You need to edit that file if you want to change screen resolutions, etc. See BackstopJS documentation for details.\nDevelopment\nThis project is written in Typescript. The source files are in src directory.\nTo start the project in development mode mode\n$ yarn dev\nBased on node-ts-starter by Horus Lugo.\n""], 'url_profile': 'https://github.com/ozmoroz', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'R', 'GPL-3.0 license', 'Updated Jun 16, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 30, 2020', 'JavaScript', 'Updated Jun 12, 2020']}"
"{'location': 'Mountain View, California', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/peter-sushko', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['regression_examples\nCoding up regression examples from scratch\n'], 'url_profile': 'https://github.com/NuriaGutierrez', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['regression-model-green-taxi-data\nRegression model on green taxi trip data.\nData can be downloaded from https://catalog.data.gov/dataset/patient-characteristics-survey-pcs-2017\nI am not able to upload the dataset because upload size restriction.\n'], 'url_profile': 'https://github.com/mayurvaishnav', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression problem for finding the profit of a restaurant owner who wants to expand his business in various cities.So considering the population in this program we find his profit.\nIn this ex1.m is the main file which will run OCTAVE or MATLAB and it has many function calls such as ComputeCost which will Compute the cost function for the prices\nIn the file ex1.m then we have a function to compute value of theta using GradientDescent\nIt also has a function to plot the data and ex1.m also shows the contour plot to enable us to know whether our program is running fine or not\nI have also uploaded a sample dataset and since it is a linear regression program it works only on a single variable\nThe submit function was given as while I was learning this algorithm I had to submit this program to my instructor so that is why this function was given.\n'], 'url_profile': 'https://github.com/Ashish-Arya-CS', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/IrinaDragan', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Gradient Descent for Linear Regression in R\n'], 'url_profile': 'https://github.com/Yalcincemre', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['udemy_pyspark_linear_regression\nUdemy Spark and Python for Big Data with PySpark Course - Instructor Jose Portilla\nInstall\nThis project requires:\n\nPython 3\nSpark\nJupyter Notebook\n\nCode\nJupyter source code:\n\nLesson_Data_Transformation.ipynb\nLinear_Regression_Example.ipynb\nLinear_Regression_Code_Along.ipynb\n\nData sources:\n\nfake_customers.csv\nsample_linear_regression_data.txt\nEcommerce_Customers.csv\n\nRunning projects\nRun in the terminal in the directory containing the Jupyter files\njupyter notebook\nand then select the ipynb file.\n'], 'url_profile': 'https://github.com/renatocfreitas', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['udemy_pyspark_logistic_regression\nUdemy Spark and Python for Big Data with PySpark Course - Instructor Jose Portilla\nInstall\nThis project requires:\n\nPython 3\nSpark\nJupyter Notebook\n\nCode\nJupyter source code:\n\nLogistic_Regression_Aula.ipynb\nLogistic_Regression_Consulting_Project.ipynb\nLogistic_Regression_Example.ipynb\n\nData sources:\n\ncustomer_churn.csv\nnew_customers.csv\nsample_libsvm_data.txt\ntitanic.csv\n\nRunning projects\nRun in the terminal in the directory containing the Jupyter files\njupyter notebook\nand then select the ipynb file.\n'], 'url_profile': 'https://github.com/renatocfreitas', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['ml-interview-review\nA repo with implementations of classic ML algorithm and also interview questions and answers\n'], 'url_profile': 'https://github.com/YellowKyu', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/b-rabbit091', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Java', 'Updated Jun 17, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 2, 2020']}"
"{'location': 'New York, New York', 'stats_list': [], 'contributions': '1,473 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Arx1971', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NandhiniDharmaraj5', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '231 contributions\n        in the last year', 'description': ['Insurance-Forecast-by-using-Linear-Regression\n\nCan you accurately predict insurance costs?\n'], 'url_profile': 'https://github.com/vaibhav1595', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhimanyusethia12', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'Thu Duc District, Ho Chi Minh City', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['MachineLearningAlgoWithPython\n'], 'url_profile': 'https://github.com/duylebkHCM', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'Udaipur', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': [""It performs Regression, predicting continous ordered variable.In SVR we try to 'fit the error within a certain threshold'.\n""], 'url_profile': 'https://github.com/kaustavmitra26', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'Udaipur', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n'], 'url_profile': 'https://github.com/kaustavmitra26', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Regressions\nComputes linear to 4-th order polynomial regressions on training data to find weight params and then compares training and test errors using these parameters. There is also an implementation of k-fold cross validation for the degree of the polynomail. It can be called multiple times with increasing degree to find the least error polynomial for the data. This helps with reducing the chance of overfitting data\n'], 'url_profile': 'https://github.com/DaniloVlad', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Machine-Learning-Algorithms\nSVM, Logistic regression, Decision tree, Knn, Random Forest\n'], 'url_profile': 'https://github.com/AnushaDeviTensingh', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Alont93', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 2, 2020', '2', 'Jupyter Notebook', 'Updated May 23, 2020', '1', 'Jupyter Notebook', 'Updated Jun 22, 2020', '1', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020']}"
"{'location': 'Louisville , Kentucky ', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fmomin01', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['machine-learning_Logistic-Regression\nBuild some cool predictive model based on logistic regression\n'], 'url_profile': 'https://github.com/knromaric', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '167 contributions\n        in the last year', 'description': ['Logistic-Regression\nFundamentals of linear, polynomial and multi-variable logistic regression\n'], 'url_profile': 'https://github.com/OrionMat', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'Moradabad, India', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Car Price Prediction\nRegression model to predict the Price of the Car\n\xa0 \xa0 \xa0 \xa0\nTable of Contents\n\nIntroduction\nPython libraries\nThe problem statement\nLinear Regression\nIndependent and dependent variable\nMultiple Linear Regression (MLR)\nAbout the dataset\nExploratory data analysis\nInterpretation and conclusion\n\n\xa0 \xa0 \xa0 \xa0\n1.\tIntroduction\nIn this project, I build Regression model to study the relationship between Price of a car and different continous and discrete variables. I implemented this regression model in Python programming language using Scikit-learn,numpy,seaborn,matplotlib.\n\xa0 \xa0 \xa0 \xa0\n2.\tPython libraries\n•\tNumpy\n•\tPandas\n•\tMatplotlib\n•\tSeaborn\n•\tScikit-Learn\n\xa0 \xa0 \xa0 \xa0\n3.\tThe problem statement\nThe main aim of this model is to predicting the Price of a car using some continous and discrete variable data. Finding relationship or dependency of Price on attrubutes like Engine-Size , Horsepower and many continous and discrete variable data.\nThe accuracy of the model is defined by RMS value and R2 Score.To improve the accuracy of the model I tried to Polynomial fit the data into the model.\n\xa0 \xa0 \xa0 \xa0\n4.\tLinear Regression\nLinear Regression is a statistical technique which is used to find the linear relationship between dependent and one or more independent variables. This technique is applicable for Supervised Learning Regression problems where we try to predict a continuous variable.\nLinear Regression can be further classified into two types – Simple and Multiple Linear Regression. In this project, I employ Multiple Polynomial Regression technique where I have Multiple independent and one dependent variable.\n\xa0 \xa0 \xa0 \xa0\n5.\tIndependent and Dependent Variables\nIndependent variable\nIndependent or Input variable (X) = Feature variable = Predictor variable\nThe following are the independent variable:-\n\nsymboling:                -3, -2, -1, 0, 1, 2, 3.\nnormalized-losses:        continuous from 65 to 256.\nmake:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,\nisuzu, jaguar, mazda, mercedes-benz, mercury,\nmitsubishi, nissan, peugot, plymouth, porsche,\nrenault, saab, subaru, toyota, volkswagen, volvo\nfuel-type:                diesel, gas.\naspiration:               std, turbo.\nnum-of-doors:             four, two.\nbody-style:               hardtop, wagon, sedan, hatchback, convertible.\ndrive-wheels:             4wd, fwd, rwd.\nengine-location:          front, rear.\nwheel-base:               continuous from 86.6 120.9.\nlength:                   continuous from 141.1 to 208.1.\nwidth:                    continuous from 60.3 to 72.3.\nheight:                   continuous from 47.8 to 59.8.\ncurb-weight:              continuous from 1488 to 4066.\nengine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\nnum-of-cylinders:         eight, five, four, six, three, twelve, two.\nengine-size:              continuous from 61 to 326.\nfuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\nbore:                     continuous from 2.54 to 3.94.\nstroke:                   continuous from 2.07 to 4.17.\ncompression-ratio:        continuous from 7 to 23.\nhorsepower:               continuous from 48 to 288.\npeak-rpm:                 continuous from 4150 to 6600.\ncity-mpg:                 continuous from 13 to 49.\nhighway-mpg:              continuous from 16 to 54.\n\nDependent variable\nDependent or Output variable (y) = Target variable = Response variable\nThe following is the dependent variable:-\n\nprice:                    continuous from 5118 to 45400.\n\n\xa0 \xa0 \xa0 \xa0\n6.\tMultiple Linear Regression (MLR)\nMultiple Linear Regression also known simply as multiple regression is a statistical technique that uses several explanatory variables to predict the outcome or the response variable.\nThe goal of multiple linear regression is to model the linear relationship between the explanatory variable (independent variable) and response variable.\nThe formula for Multiple Regression is:\n\ty = a0 + a1x1 + a2x2 + ...............\n\nwhere\ny = dependent variable\nxi= independent variable\na0= y-intercept\nai= slope coefficients for each independent variable\n\xa0 \xa0 \xa0 \xa0\n7.\tAbout the dataset\n1. Title: 1985 Auto Imports Database\n\n2. Source Information:\n    -- Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)\n    -- Date: 19 May 1987\n-- Sources:\n \t\t1) 1985 Model Import Car and Truck Specifications, 1985 Ward\'s\n    \tAutomotive Yearbook.\n \t\t2) Personal Auto Manuals, Insurance Services Office, 160 Water\n    \tStreet, New York, NY 10038 \n \t\t3) Insurance Collision Report, Insurance Institute for Highway\n    \tSafety, Watergate 600, Washington, DC 20037\n3. Relevant Information:\n-- Description\n  \t\tThis data set consists of three types of entities: (a) the\n  \t\tspecification of an auto in terms of various characteristics, (b)\n  \t\tits assigned insurance risk rating, (c) its normalized losses in use\n  \t\tas compared to other cars.  The second rating corresponds to the\n  \t\tdegree to which the auto is more risky than its price indicates.\n  \t\tCars are initially assigned a risk factor symbol associated with its\n  \t\tprice.   Then, if it is more risky (or less), this symbol is\n  \t\tadjusted by moving it up (or down) the scale.  Actuarians call this\n  \t\tprocess ""symboling"".  A value of +3 indicates that the auto is\n  \t\trisky, -3 that it is probably pretty safe.\n\n        The third factor is the relative average loss payment per insured\n  \t        vehicle year.  This value is normalized for all autos within a\n  \t\tparticular size classification (two-door small, station wagons,\n  \t\tsports/speciality, etc...), and represents the average loss per car\n  \t\tper year.\n\n\xa0 \xa0 \xa0 \xa0\n8.\tExploratory data analysis\nTo summarize the main characteristics of data I analysed it using Data Visualization by ploting graphs like regression plot ,box plot ,histogrames, and distribution plot between the dependent and various independent variables.\nIt help me in finding the good attributes and bad attributes for training my model as graph clearly shows us the reationship between the variables.\n\xa0 \xa0 \xa0 \xa0\n9.\tInterpretation and Conclusion\nSimple Linear Regression:\n RMSE value: 15446938.691334337\n \n R2 value : 0.6887892619732783\n\nMultiple Linear Regression:\n RMSE value: 19637652.97310373\n \n R2 value : 0.7708377040905359\n\nPolynomial Regression:\n RMSE value: 5357185.720506018\n \n R2 value : 0.892068340844766\n\nWe can conclude by looking the Mean Squared value and R2 value that polynomial regression fit more perfect for the given car data as compaired to simple or multiple linear regression.\nas Polynomial Regression gives almost 90 per accuracy and there is a huge difference in RMS value of multiple regression and polynomial regression.\n\xa0 \xa0 \xa0 \xa0\n'], 'url_profile': 'https://github.com/aarjav22', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'New Jersey', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['\nWineQuality\nSimple Practice dataset by UCI for Regression or Classification Modelling\nContext\nThe two differents datasets are related to Red Wine and White Wine variants of the Portuguese ""Vinho Verde"" wine.\nFor more details, consult the reference [Paulo Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis, 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.)\nThese datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\nThis dataset is also available from the UCI machine learning repository, Source\nI just shared it to Kaggle for Convenience. If I am mistaken and the public license type disallowed me from doing so, I will take to remove this dataset, if requested  and notified to me.\nContent\nFor more information, please read [Cortez et al., 2009].\nNumber of\nNumber of Instances:\n\n\n\nTables\nCount\n\n\n\n\nRed Wine\n1599\n\n\nWhite Wine\n4898\n\n\n\nNumber of Attributes:\n11 + output attribute. Input and Output of feature:\nInput variables (based on physicochemical tests):\n1. fixed acidity\n2. volatile acidity\n3. citric acid\n4. residual sugar\n5. chlorides\n6. free sulfur dioxide\n7. total sulfur dioxide\n8. density\n9. pH\n10. sulphates\n11. alcohol\n\nOutput variable (based on sensory data):\n12. quality (score between 0 and 10)\n\nLink of Dataset\n\n\n\nTables\nLink on Github or Kaggle\n\n\n\n\nRed Wine\nKaggle --Github\n\n\nWhite Wine\nKaggle -- Github\n\n\nDescription Of Columns\nKaggle -- Github\n\n\n\nAcknowledgements\nThis dataset is also available from the UCI machine learning repository, Source\nI just shared it to Kaggle for Convenience. If I am mistaken and the public license type disallowed me from doing so, I will take to remove this dataset, if requested  and notified to me. I am not the owner of this dataset. Also, if you plan to use this database in your article research or else you must taken and read main Source  in the UCI machine learning repository.\nInspiration - Relevant Papers:\n\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties.  For Research\nIn Decision Support Systems, Elsevier, 47(4):547-553, 2009.\nAdditional Information about Wine: For a good evaluation, I recommend you to know a little more about wine. WikiPedia will be good for you. Source 1: Acids in Wine  | Source 2: Chemistry of Wine \n\n'], 'url_profile': 'https://github.com/huseyinelci2000', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '327 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nargesalavi', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['TBA\n'], 'url_profile': 'https://github.com/BayerSe', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/patwick0605', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'Stockholm, Sweden', 'stats_list': [], 'contributions': '362 contributions\n        in the last year', 'description': ['covid19\nCOVID-19 evolution data from European Centre for Disease Prevention and Control with logistic regression-based prediction.\nTruncated at 2020-07-31 after the first wave in most European countries.\n'], 'url_profile': 'https://github.com/a1eko', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}","{'location': 'Glasgow', 'stats_list': [], 'contributions': '149 contributions\n        in the last year', 'description': ['\n\nregression_analysis_101\nEstimating the parameters of a model by optimising the fit of the model to empirical data is a staple of computational data science.\nThis repository contains a set of Jupyter notebooks that demonstrate how to implement a range of regression methods and a Bayesian MCMC approach for fitting models and estimating model parameters. These are demonstrated for a variety of geoscience type applications, e.g. fitting an isochron to isotope data to estimate a geologcal age, fitting observed Bouguer gravity anomaly data from the Isle of Mull in Scotland using simple gravity model for a buried spherical object, and estimating parameters for a simple continental geotherm model based on PT estimates derived from mantle xenolith geochemistry.\nVarious other routine coding tasks (e.g. reading and writing data files) and plotting techniques (using Matplotlib routines) are also illustrated.\nA minimal example-fitting a straight line to set of x and y values\nA minimal example is fitting a simple linear model, i.e. a straight line, \n, to some set of measurements of y (commonly called the dependent variable or response variable) for a given range of x values (commonly called the predicter variable or independent variable), to estimate the slope, m, and intercept c, of the line.\nA routine method for achieving this is Ordinary Least Squares regression (OLS). OLS aims to minimise the difference between the observed values of y and the values predicted by the model. It achieves this by finding the model that has the smallest value of the sum of the squared differences, these differences are usually called the residuals. These are illustrated below by the vertical red lines between the green observed values and the blue line representing the model predictions. The routine in the scipy package that implements this is scipy.stats.linregress.\n\nTaking care of the errors-how to handle uncertainty in measured data\nOLS is quick and straightforward, but does not take account of the magnitude of measurement uncertainties on x or y values. Orthogonal Distance Regression (ODR) enables measurement errors in both variables to be accounted for, and is particularly appropropriate if the measured data includes outliers with large errors.\nThe Bayesian approach using a Markov Chain Monte Carlo sampling strategy\nAn alternative to using regression techniques to estimating model parameters is to use a Bayesian approach and a Markov Chain Monte Carlo sampling strategy.\n\nAn advantage of the Bayesian MCMC technique is that you can treat both the model parameters and the observed data as uncertain, and estimate their likely values (credible intervals) and the posterior probability distributions. This can be implemented in python using the pyMC3 package. See the pyMC3 docs page for details.\n\n\n'], 'url_profile': 'https://github.com/skerryvore', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'MATLAB', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'MIT license', 'Updated Apr 27, 2020', '1', 'Updated Apr 7, 2020', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', '1', 'Jupyter Notebook', 'Updated Dec 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['A Look Into Touchdown Regression\nAnalysis article, Code Samples, and airyards Data. This exercise investigates 8 years worth of NFL receiving data to determine touchdown regression rate to equip fantasy football players with a simple, yet, powerful tool in making executive roster decisions.\nExploratory Data Analysis\nInitially, I explored airyads data and cleaned any null values. This step allows for accurate analysis and modeling. I combined years of data into a single dataframe and created a function to determine certain summary statistics. airyards already has readily usable data and very few NaN specific variables. After using fillna(), the concatenated dataframe was ready for analysis.\nReceiving Yards-to-Touchdowns Correlation\nOur dataframes contain tight end and wide receiver specific data. We want to explore each to determine positional independent correlation.\n\nWide Receiver: the tight end data was excised. I grouped by each wide receiver to summate receiving yards and touchdowns followed by plotting correlation. I used Seaborn\'s lmplot for our visualization. From Scikit-learn, I ran a linear regression model that determined a strong R^2 value (0.927) between wide receiver receiving yards and receiving touchdowns. I then determined the average amount of wide receiver receiving yards that leads to one touchdown - 159 receving yards/touchdwon.\nTight End: the wide receiver data was excised from the concatenated dataframe. Repeat the above process to determine positional specific correlation; this was visualized, modeled and scored (R^2 = 0.881), and the average tight end receiving yards-to-touchdown was calculated to be 129 receiving yards/touchdown.\n\nDetermining Touchdown Regression Rate\nNow that we know that receiving yards and touchdowns correlate strongly, we can make a function to determine a player\'s touchdown regression rate. We need to be sure to use the right receiving yards/touchdown ratio depending on the player\'s position (WR or TE). The average wide receiver scores a touchdown on approximately every 159 receiving yards. The average tight end scores a touchdown on approximately every 129 receiving yards. Let us a build a function:\n\nFunction regression_air_yards will take in three variables (an airyards dataframe, a selected position, and a corresponding positional touchdown ratio). This function creates a column and calculates the touchdown regression rate for each player for a given year.\n\nNegative and positive touchdown regression candidates can be visualized using Seaborn\'s diverging pallette. Understanding regression and that receiving yards correlate with touchdowns will equip fantasy football players with beneficial knowledge when making lineup moves and trades.\nConclusion\nCreating a regression function can help determine how many more touchdowns a wide receiver/tight end should have given their usage. Analysis was based off the averages of 8 years worth of receiving data. We can determine positive and negative regression candidates through this calculation. Having found a strong correlation between receiving yards and receiving touchdowns, we know that, on average, a receiver scores one touchdown on every 159 receiving yards and a tight end scores one touchdown on every 129 receiving yards. If a receiver is likely to get a similar amount of usage/work, it is fair to think that the positive touchdown candidates, for lack of a better word, are ""due"" and should eventually score touchdowns, which aids in fantasy point accumulation. An important thing to keep in mind - a touchdown regression rate is NOT an overcorrection.\n\nAn example for future use: let us say that after week 4 of the 2020 season, JuJu Smith-Schuster has a touchdown regression rate of approx. -3. He would be considered a positive touchdown regression candidate. This value tells us that, based off his receiving yards, he is underperforming (as it relates to an average wide receiver) by 3 touchdowns. If he continues to get similar usage and produces similar yardage, he should begin to score touchdowns. It does not mean we should expect an imminent overcorrection - a 3 touchdown game in week 5 from JuJu - but that he should eventually regress to the mean and begin to start scoring touchdowns more to the average of 159 receiving yards per one touchdown.\n\nOf course there are many factors that can drive touchdown success rate but, knowing the correlation of receiving yards to receiving touchdowns and knowing that touchdowns regress aids fantasy football owners in making executive decisions about their lineup.\n'], 'url_profile': 'https://github.com/easarani', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'Dominican Republic', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ivan-verges', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Boston-Housing-Project\nAn ML based project on Linear Regression with multiple features\n'], 'url_profile': 'https://github.com/satvik1998', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""House-Price-Prediction-Kaggle\nUse advanced regression techniques to predict house prices\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation\nStart here if...\nYou have some experience with R or Python and machine learning basics. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.\nCompetition Description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nGoal\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\nMetric\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n""], 'url_profile': 'https://github.com/raj1611', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Sentiment classification using naive byes and logistic regression.\nCommand to run the file: python nb.py\nThe program should be inside the data directory Sentiment classification folder which has train and test files.\nDifferent types of methods followed are as follows,\nNaive Bayes with stemming and word count\nNaive Bayes without stemming and word count\nNaive Bayes with stemming and binary count\nNaive Bayes without stemming and binary count\nNaive Bayes with stemming and TF-IDF\nNaive Bayes without stemming and TF-IDF\nProgram flow:\n  1. Main()\n      The main function is used for contols. It runs 4 functions as mentioned below.\n\n      1.1.nb()\n      The funtion collects the count of non stemmed and stemmed vocabulary and assigns it to global variables.\n\n          1.1.1.count_words()\n          The funtion collects the training files. Tokenizes text into words. Creates stemmed vocabulary and \n          Counts the the occurance of each word in each class(positve and negative).\n\n              1.1.1.1 normalize_case()\n              Converts words with capitalized first letters in to lower case.\n\n              1.1.1.2 remove_tags()\n              Removes HTML tags\n\n    1.2 get_test()\n        The funtion collects the test data. Creates Bag of words and\n        stemmed vocabulary.\n\n    1.3 classify()\n        The function uses the counts computed from previous step to calculate likelyhood and prior.\n        Then the test document is classified based the result from above step.\n\n    1.4 metrics()\n        Prints the accuracy and confusion matrix\n\n'], 'url_profile': 'https://github.com/bjpranav', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'Göttingen', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['\nAdvanced statistical programming with R\nAre you taking the course “Advanced statistical programming with R” in\nthe summer term 2020? Then this page is for you. It explains what you\nneed to do to get the credits for this module. 😉\nIn this course, you are going work on a programming project in a group\nof three students. Each group needs to develop an R package, in which\nthey implement a statistical method for the so-called location-scale\nregression model class, and a small simulation study to evaluate their\nmethod and implementation. In September, you need to submit a term paper\nabout your project.\nThis git repository contains the asp20model package for R, which\nimplements the location-scale regression model class and will be the\nbasis for your package.\nPlease note: If you are planning to take this course, you need to\npreregister on Stud.IP before March 31. Follow this link to sign up:\nhttps://studip.uni-goettingen.de/dispatch.php/course/details?sem_id=5b8822a839a628e8166648a57f4f1eac.\nUpdate on the Coronavirus: Unsurprisingly, the Coronavirus crisis\nmakes some changes to this course necessary. All previously announced\ndates remain valid, but I am going to offer the introductory session and\nthe Q&A session as YouTube livestreams. See below for the links. The\nintermediate and final presentations might be replaced with short\nwritten reports, depending on the further developments. I am going to\nmake a final decision on the presentations before the Q&A session on\nMay 4.\nImportant dates\n\nTuesday, April 7, 10:00–15:00, Blauer Turm / MZG 8.163:\nIntroductory session, project assignment\nYouTube livestream: https://youtu.be/-a02j6YTxKw\nRocket.Chat channel for questions and comments:\nhttps://chat.gwdg.de/channel/asp20\nMonday, May 4, 14:00–16:00, Blauer Turm / MZG 8.163: Q&A\nsession, feel free to ask technical, statistical, and organizational\nquestions\nYouTube livestream: https://youtu.be/vi2BqiDUCFI\nRocket.Chat channel for questions and comments:\nhttps://chat.gwdg.de/channel/asp20\nFriday, June 5, 8:30–15:00, Blauer Turm / MZG 8.163: Intermediate\npresentations\nFriday, July 10, 8:30–15:00, Blauer Turm / MZG 8.163: Final\npresentations\nTuesday, September 15: Submission deadline for the term papers\n\nTechnical & statistical prerequisites\nThe R6 OOP system\nThe asp20model package uses R6 classes, so make sure to get familiar\nwith mutable objects, inheritance, etc. See the chapter on R6 classes in\n[3], available online: https://adv-r.hadley.nz/r6.html.\nVersion control using git & GitLab\nThe asp20model package is hosted on https://gitlab.gwdg.de (the\nGitLab instance of the computing center of the university), where you\nare going to develop your R package as well. I created a GitLab group\nfor this course and a GitLab project for each group of students.\nPlease note: To start working on your project, log in to GitLab with\nyour student account once and send me an email. After that, I can add\nyou to the group and the project.\nGitLab is an online platform where you can host git repositories, very\nsimilar to GitHub, and git is a popular version control system for\nsoftware development. Systematic, version-controlled software\ndevelopment is an essential component of this course, so if you are not\nyet familiar with git, please take a look at the git chapter in [4],\navailable online: http://r-pkgs.had.co.nz/git.html.\nPlease note: Your code will only be visible to me, the project\nsupervisors, and your fellow students. Feel free to make mistakes and\nask questions!\nLocation-scale regression\nAs mentioned above, we are going to work with the location-scale\nregression model class in this course. So, what is location-scale\nregression? Let’s take a step back and look at the standard linear model\nfirst. It is defined as\ny_i = \\boldsymbol{x}_i\' \\boldsymbol{\\beta} + \\varepsilon_i, \\text{ where } \\varepsilon_i \\overset{i.i.d.}{\\sim} \\mathcal{N}(0, \\sigma^2).\n\nFrom the definition, it follows that\ny_i \\overset{ind.}{\\sim} \\mathcal{N}(\\boldsymbol{x}_i\' \\boldsymbol{\\beta}, \\sigma^2),\n\nand from this representation, the model can easily be extended with a\nsecond linear predictor for the standard deviation. For this purpose, we\nintroduce a covariate vector $\\boldsymbol{z}_i$ and a parameter vector\n$\\boldsymbol{\\gamma}$:\ny_i \\overset{ind.}{\\sim} \\mathcal{N}(\\boldsymbol{x}_i\' \\boldsymbol{\\beta}, (\\exp(\\boldsymbol{z}_i\' \\boldsymbol{\\gamma}))^2).\n\nWhat is the $\\exp$ function doing in this formula? Well, one thing\nthat is a little bit tricky about predicting the standard deviation is\nthat the standard deviation needs to be positive, while the linear\npredictor $\\boldsymbol{z}_i\' \\boldsymbol{\\gamma}$ can become negative\nfor some choices of the parameter vector $\\boldsymbol{\\gamma}$. Hence,\nto ensure that our predictions for the standard deviation are valid, we\nintroduce the $\\exp$ function as a so-called response function for the\nstandard deviation and define $\\sigma_i = \\exp(\\boldsymbol{z}_i\' \\boldsymbol{\\gamma})$.\nWhy should we care about location-scale regression? It is not uncommon\nto observe heteroscedastic data like in the figure below. The standard\nway to deal with heteroscedastic residuals in a linear model is FGLS\nestimation, but FGLS does not provide an interpretable description of\nthe variance structure of the data. A location-scale regression model\ncan fill this gap, if we have access to explanatory variables for the\nvariance or the standard deviation of the response variable.\nn <- 500\nx <- runif(n)\ny <- x + rnorm(n, sd = exp(-3 + 2 * x))\nplot(x, y)\nabline(0, 1, lwd = 2)\ncurve(x + 1.96 * exp(-3 + 2 * x), -0.1, 1.1, add = TRUE)\ncurve(x - 1.96 * exp(-3 + 2 * x), -0.1, 1.1, add = TRUE)\n\nWorking with the asp20model package\nFirst, you need to install the asp20model package from GitLab and load\nit:\n## install.packages(""devtools"")\n## devtools::install_gitlab(""asp20/asp20model"", host = ""gitlab.gwdg.de"")\nlibrary(asp20model)\nUsing the data from the plot above, we can set up a location-scale\nregression model. Note that the first command only sets up the design\nmatrices and the parameter vectors but does not do any kind of\ninference. All parameters are initialized with a value of 0. We can use\nthe loglik() method to obtain the log-likelihood of the model in the\ninitial state (i.e.\xa0with all parameters set to 0):\nmodel <- LocationScaleRegression$new(y ~ x, ~ x)\nmodel$beta\n#> [1] 0 0\nmodel$gamma\n#> [1] 0 0\nmodel$loglik()\n#> [1] -552.6567\nNow, let’s update the parameter values manually and see how the\nlog-likelihood changes:\nmodel$beta <- c(0.1, 0.1)\nmodel$loglik()\n#> [1] -516.8169\nFor most inference algorithms, we also need the gradient of the\nlog-likelihood with respect to the parameters, which we can obtain with\nthe grad() method:\nmodel$grad_beta()\n#> (Intercept)           x \n#>    174.5098    125.7585\nmodel$grad_gamma()\n#> (Intercept)           x \n#>   -385.3047   -157.7906\nFinally, the asp20model package also comes with a simple gradient\ndescent algorithm for\nmaximum likelihood inference. Let’s apply it to our model:\ngradient_descent(model)\n#> Finishing after 1000 iterations\nmodel$beta\n#> (Intercept)           x \n#>    5.891795    4.627923\nmodel$gamma\n#> (Intercept)           x \n#>   1.7398635   0.5971787\nmodel$grad_beta()\n#> (Intercept)           x \n#>   -66.29382   -28.97002\nmodel$grad_gamma()\n#> (Intercept)           x \n#>   -4.136286  -12.307770\nWhoops, that didn’t work very well! The algorithm didn’t converge, the\nestimated parameters are far away from the true values we used to\nsimulate the data ($\\hat{\\boldsymbol{\\beta}}$ should be close to (0,\n1), $\\hat{\\boldsymbol{\\gamma}}$ should be close to (-3, 2)), and the\ngradient is not close to 0. Let’s try again with a smaller step size and\nmore iterations:\ngradient_descent(model, stepsize = 1e-06, maxit = 500000)\n#> Finishing after 260393 iterations\nmodel$beta\n#>  (Intercept)            x \n#> -0.003708951  1.010004226\nmodel$gamma\n#> (Intercept)           x \n#>   -3.040401    2.123791\nmodel$grad_beta()\n#>   (Intercept)             x \n#> -8.143048e-07  3.635703e-06\nmodel$grad_gamma()\n#>   (Intercept)             x \n#> -0.0005285036  0.0009999457\nThese results do look better, but they also show how inefficient the\ngradient descent algorithm is. The algorithm took more than a quarter\nmillion iterations to converge!\nYour task is to do better than me and implement a more efficient\ninference algorithm for the location-scale regression model class. Use\nmy code for the gradient_descent() function as an example. Build on\nthe LocationScaleRegression R6 class and its methods and extend them\nif necessary. Think about inheritance and whether your extensions could\nbe useful for other groups. If yes, feel free to share your code on\nGitLab and open a pull request in the asp20model repository.\nStudent projects\nasp20boot\n\nSupervisor: Benjamin Säfken\nStudents: TODO\nTasks: Parameter estimation using Fisher scoring, bootstrap\nconfidence intervals for the parameter estimates\nReadings:\nhttps://en.wikipedia.org/wiki/Bootstrapping_(statistics), the\nboot R package\n\nasp20cv\n\nSupervisor: René-Marcel Kruse\nStudents: TODO\nTasks: Parameter estimation using Fisher scoring, model selection\nusing leave-p-out and k-fold cross validation\nReadings:\nhttps://en.wikipedia.org/wiki/Cross-validation_(statistics), the\ncross validation functions in the gamlss R package\n\nasp20lasso\n\nSupervisor: Maike Hohberg\nStudents: TODO\nTasks: Parameter estimation, variable selection, and regularization\nusing the least absolute shrinkage and selection operator (LASSO)\nReadings: Section 4.2.3 in [1]\n\nasp20boost\n\nSupervisor: Thomas Kneib\nStudents: TODO\nTasks: Parameter estimation and variable selection using the\nboosting ensemble learning algorithm\nReadings: Section 4.3 in [1]\n\nasp20regPrior\n\nSupervisor: Paul Wiemann\nStudents: TODO\nTasks: Interface for the specification of regularization priors,\nstatistical inference using random walk/Langevin MCMC\nReadings: Section 4.4.2 in [1],\nhttps://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm\n\nasp20ssPrior\n\nSupervisor: Manuel Carlan\nStudents: TODO\nTasks: Interface for the specification of spike and slab priors,\nstatistical inference using random walk/Langevin MCMC\nReadings: Section 4.4.4 in [1],\nhttps://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm\n\nasp20hMCMC\n\nSupervisor: Manuel Carlan\nStudents: TODO\nTasks: Statistical inference using Hamiltonian MCMC with flat priors\nReadings: “A conceptual introduction to Hamiltonian Monte Carlo” by\nMichael Betancourt, “MCMC using Hamiltonian dynamics” by Radford\nNeal\n\nasp20iwlsMCMC\n\nSupervisor: Isa Marques\nStudents: TODO\nTasks: Statistical inference using iterative weighted least squares\n(IWLS) MCMC with flat priors\nReadings: Section 5.6.2 in [1], “Sampling from the posterior\ndistribution in generalized linear mixed models” by Dani Gamerman,\n“BAMLSS: Bayesian additive models for location, scale, and shape\n(and beyond)” by Nikolaus Umlauf, Nadja Klein, and Achim Zeileis\n\nasp20plot\n\nSupervisor: Hannes Riebl\nStudents: TODO\nTasks: Parameter estimation based on weighted/generalized least\nsquares (WLS/GLS), plot functions for model predictions and\ndiagnostics\nReadings: Section 3.4.4 in [1], the plot functions in the car\nand gamlss R packages\n\nasp20user\n\nSupervisor: Hannes Riebl\nStudents: TODO\nTasks: Parameter estimation based on weighted/generalized least\nsquares (WLS/GLS), evaluation of the user-friendliness of the\nasp20* packages, implementation of convenience functions,\napplication to the datasets::airquality dataset\nReadings: Section 4.1 in [1], Section 16.3 in [3]\n\nRequirements to pass the course\nTo get the credits for this module, you need to…\n\n… develop an R package that builds on the asp20model package and\nadds the functionality described above. Your code needs to be\ncomprehensible, well documented, and covered by automated tests.\n… develop and carry out a small simulation study to show that your\nmethod works reliably. Please discuss the scope and design of the\nsimulation study with your supervisor.\nYour development process should be continuous, collaborative, and\ntransparent. The commit history on GitLab should reflect your\nprogress at any time.\n… give two 20-minute presentations on June 9 and July 14.\n… submit a term paper by September 15.\n\nYou may earn “bonus points” for active exchange with other groups,\npreferably on GitLab via issues and pull requests. For example, the four\ngroups working on Bayesian inference could develop a common API for the\nspecification of priors. If you need additional features in the\nasp20model package, feel free to open issues and pull requests in this\nrepository.\nReadings\n\n[1] “Regression: Models, methods, and applications” by Ludwig\nFahrmeir, Thomas Kneib, Stefan Lang, and Brian Marx, especially\nSection 2.9.1 on “Regression models for location, scale, and shape”\n[2] The tidyverse style guide, available online:\nhttps://style.tidyverse.org\n[3] “Advanced R” by Hadley Wickham, available online:\nhttps://adv-r.hadley.nz\n[4] “R packages” by Hadley Wickham, available online:\nhttp://r-pkgs.had.co.nz\n\n'], 'url_profile': 'https://github.com/hriebl', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['The Interpretability of BART (Bayesian Additive Regression Trees)\nAbstract\nBayesian Additive Regression Trees (BART) is an ensemble Bayesian sum-of-trees model and has shown its promising applicability on either simulated data or real data sets. However, it suffers the same issue as most machine learning models -- interpretability -- many of the machine learning models are still a black-box without an intuitive explanation to the general public. While this is the case, BART has been applied to econometrics and causal inference for years. In these domains, explaining and understanding the effect is far more important than making predictions, interpretability is, therefore, an issue that awaits a resolution. In this project, I explore and present different methods and use a simulation approach to analyze how does the BART work and try to understand its interpretability. The result shows, though BART fits data with different underlying models reasonably well, it cannot provide effect size and feature importance in a consistent way in terms of quantitative measures. Further examination may be needed on this topic.\nPlease see PDF for well-formatted version and Rmd for code.\nIntroduction\nDifference between econometrics and machine learning\nThe essential goal between econometrics methodologies and machine learning models made them different in nature and have different applicability in different fields; econometrics is aim to explain and quantify economics phenomenon while machine learning models aim to make predictions. In recent years, machine learning models have achieved a huge success in a lot of domain since its ability to make predictions are amazing. However, most of them also have an obvious drawback that they are black-boxes, i.e., they are very hard to interpret.\nOn the other hand, BART is an ensemble method of regression trees, trees itself, are easy to interpret since trees are essentially rules. Additionally, Bayesian methods are conceptually intuitive because it reflects the nature of people observing data and constructing models. These two components combined made BART potentially be an interpretable model. Moreover, BART has its posterior distribution accessible after fitting so methods from probability theories could also be applied.\nIndeed, BART has been applied to econometrics (e.g., Kindo et al., 2013; George et al., 2019) and causal inference (e.g., Hill, 2011; Green & Kern, 2010)sometimes. However, to my knowledge, I haven\'t seen any discussion on this topic. As it is important to interpret the model used in econometrics instead of making predictions, I believe the interpretability of BART is potentially crucial that should be discussed. Therefore, I wish to explore the methods of qualitatively and quantitatively discuss the interpretability of BART and test it by myself in this project.\nInterpretability\nWhat is interpretability\nAs discussed in Bibal & Frenay (2016), the interpretability of a model should not be linked to the understandability of the learning process generating this model. Efficiency, in other words, concerns the time available to the user to grasp the model. Without this criterion, it could be argued that any model could be understood given an inﬁnite amount of time. Feng and Michie (1994) and others add ""mental ﬁt"" to the term interpretability and comprehensibility. Whereas ""data fit"" corresponds to predictive accuracy.\nWhy do we need interpretability\nHuman is those who code, deploy and maintain the models, we want to be able to trust the model, this is the first intuitive answer to this question. Secondly, with interpretability, it will be easier for us to further improve the model, either by feature engineering, parameter tuning or even replace the entire model. Moreover, in the real world, giving prediction along with reasons sometimes make it more convincing. This is crucial in some application scenarios, for example, in recommendation systems, giving a recommendation along with ""you may like this product because you bought A, B, and C"" make it more convincing. In some domains, exact explanations may be required (e.g. for legal or ethical reasons), and using a black-box may be unacceptable (or even illegal).\nHow do we interpret machine learning models\nSome models are known to be already interpretable, e.g., decision trees, rules, additive models, attention-based networks, or sparse linear models. While many other models are still black-boxes.\nPeople have been discussed how to measure or quantify interpretability. One most naive method is simply model evaluation. Given good metrics exist, a model can be defined as more interpretable if it has better performance (accuracy, for example).\nFeature importance is another aspect that we want to have some understanding of how do each feature contributes to the model. Either by understanding its rules or simply calculate its importance. In linear models, this can be achieved easily by accessing the coefficient, but in other models, this is a harder task. Sometimes people use permutation methods toward this problem.\nThere are some other methods that are less widely used but worth to be mentioned here. Learning an interpretable model on the predictions of the black-box model (Craven & Shavlik, 1996; Baehrens et al., 2010), perturbing inputs and seeing how the black box model reacts (Strumbelj & Kononenko, 2010; Krause et al., 2016), or both (Ribeiro et al., 2016).\nFor deep learning models, there are also probing methods that trained a projection space to project the parameters (weight matrices, for example) to understandable spaces. Also, some visualization techniques may have already been widely used. In addition, researchers from other fields also tried to apply tasks or metrics from other disciplines to see the correlation and connection between them. For example, between EEG data from brain and NLP models for linguistics features or between quantities in philosophy and psychology theories and pre-trained word embeddings.\nThe trade-off\nIn real-world applications, we often met a trade-off between interpretability and predictability. For instance, a simple linear model provides an extremely interpretable model but often lack of accuracy on complex problems.\nIn econometrics, people often care more about a single measure such as effect size or causal effect; in this context, researchers are trying to construct economic theories instead of making predictions. Thus, a simpler interpretable model is often chosen. In business applications, however, people often care more about prediction since forecasting is crucial in making profits.\nSince BART has already shown its predictability that it outperforms 4 common algorithms on 42 different datasets (Chipman et al., 2010), it is worthy to explore the interpretability of it.\nOverview of BART\nThis section is a quick overview of BART by dumping formulas. More detailed explanations can be found in Hill (2011), Hahn (2020), Chipman et al. (2010), Tan & Roy (2019), and Kapelner & Bleich (2013).\nStructure\nBART is a sum-of-trees ensemble with an estimation approach relying on a fully Bayesian probability model.\n$$\\boldsymbol{Y}=f(\\boldsymbol{X})+\\mathcal{E} \\approx \\sum_i^m\\mathcal{T}{i}^{\\mathcal{M}}(\\boldsymbol{X})+\\mathcal{E}, \\quad \\mathcal{E} \\sim \\mathcal{N}{n}\\left(\\mathbf{0}, \\sigma^{2} \\boldsymbol{I}_{n}\\right)$$\nwhere $\\boldsymbol{Y}$ is the responses, $\\boldsymbol{X}$ is the covariates and $\\mathcal{E}$ is the noise. Here we have $m$ distinct regression trees, each composed of a tree structure, denoted by $\\mathcal{T}$, and the parameters at the terminal nodes (also called leaves), denoted by $\\mathcal{M}$.\nThe prior for the BART model has three components: (1) the tree structure itself, (2) the leaf parameters given the tree structure, and (3) the error variance $\\sigma^2$ which is independent of the tree structure and leaf parameters\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(T_{1}^{\\mathcal{M}}, \\ldots, \\mathcal{T}{m}^{M}, \\sigma^{2}\\right) &=\\left[\\prod{t} \\mathbb{P}\\left(\\mathcal{T}{t}^{\\mathcal{M}}\\right)\\right] \\mathbb{P}\\left(\\sigma^{2}\\right) \\\n&=\\left[\\prod{t} \\mathbb{P}\\left(\\mathcal{M}{t} | \\mathcal{T}{t}\\right) \\mathbb{P}\\left(\\mathcal{T}{t}\\right)\\right] \\mathbb{P}\\left(\\sigma^{2}\\right) \\\n&=\\left[\\prod{t} \\prod_{\\ell} \\mathbb{P}\\left(\\mu_{t, e} | \\mathcal{T}{t}\\right) \\mathbb{P}\\left(\\mathcal{T}{t}\\right)\\right] \\mathbb{P}\\left(\\sigma^{2}\\right)\n\\end{aligned}\n$$\nThis prior structure enables BART to enforce shallow tree structures, limiting the complexity of a single tree, thus regularize the model.\nAt depth $d$, the prior probability is set to be\n$$\\alpha(1+d)^{-\\beta}$$\nwhere $\\alpha \\in(0,1)$ and $\\beta \\in[0, \\infty]$.\nThe posterior distribution is obtained using a Metropolis-within-Gibbs sampler (Geman & Geman, 1984; Hastings, 1970). The sampler for BART uses a form of “Bayesian backitting” (Hastie & Tibshirani 2000) where the jth tree is fit iteratively, holding all other m-1 trees constant by exposing only the residual response.\nAssumptions\nAs above equations, BART requires mean-centered noises $\\epsilon$ and it should be normally distributed. It can be tested using the normality and the heteroskedasticity test. A function check_bart_error_assumptions in bartMachine can test this assumption.\nAlso, BART requires convergence of its Gibbs sampler, which can be tested using plot_convergence_diagnostics.\nComparisons\nOne of the unique features of BART is that it is based on a fully Bayesian model so we have access to the full posterior. As a result, we can have things like credible intervals. Also, it could be applied to classification since it can model probabilistic uncertainty naturally.\nThe downside of that is, when the number of variables is large, BART will have a high rejection rate when proposing the splitting rules. Also, it is memory intensive for the same reason.\nSimulations, Methods, and Results\nIn this section, I present several ways of assessing the interpretability of a model. None of them are sufficient to make a conclusion and require more deliberately examination\nI will use simulated data since to be certain about the nature of the data is necessary here in order to understand its interpretability. Though there\'s definitely gaps between simulated data and real data sets, a simpler and cleaner exploration is the main goal here. The package I will play with is bartMachine, it is superior to the original BayesTree as it has predict function and is faster in speed.\nPosterior distribution\nSince BART has a unique property that has posterior distribution available. I first want to compare the ""goodness-of-fit"" of BART by plotting it and comparing to the original data. After fitting a BART to the simulated data, I sample from the posterior distribution, noted different from the concept of ""final posterior"", it keeps all posterior in each iteration after burn-in.\nThe following plots compare the simulated data using the same $X$ with different functions. The black line is the distribution of the response $y$ and each colored line is a fitted posterior distribution for five independent runs.\nThe functions are intuitive by its name, for example, addition is,\n$$Y = X_1 + X_2 + X_3 + X_4 + X_5 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\nand interaction/multiplication is,\n$$Y = X_1 + X_2 + X_1*X_2 + X_3 + X_4 + X_5 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\nSee code for all specifications.\n\nFrom the above plots, it seems BART doesn\'t fit the best for simple models but fits very well for complicated models. It doesn\'t matter if the model itself is linear or nonlinear, stepwise or not due to the nature of BART is regression trees.\nOn the other hand, we can see the prior plays a role in posterior distribution by regularizing the shape. Since BART favors shallow trees by incorporating the prior, each tree is a weak learner. With the integration of weak learners, it can\'t be too different from adding tons of normal distribution together due to the central limit theorem. This property can be seen in the fitted posterior that they all look like bell curves adding together with different resolutions.\nPiecewise function, however, seems is the weakness of BART. The piecewise and the treatment simulate the discontinuity in some experimental design settings. For example, in regression discontinuity design (RDD), we may encounter the exact same model.\nIn addition, the Friedman function,\n$$Y = 10  \\sin(\\pi X_1  X_2) +20  (X_3 -0.5)^2 + 10  X_4 + 5  X_5 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\nserves as a standard for evaluating approximation. It can also be found the BART fit reasonably well but with some obvious deviation.\nSince BART provides access to the posterior distribution, credible intervals instead of confidence intervals are available. Though linear models have point estimates and confidence intervals, this feature hear is critical compared to other machine learning models.\n\nIn summary, these density plots and deviation plots show BART has a reasonable fit for most of the models, though not perfectly ideal. With the posterior distribution, we can do a lot more with them such as drawing samples from it for further inference. However, the usage of this approach should be more closely examined since the posterior is not always satisfying.\nPerturbation\nIn this section, I apply a small perturbation to test how BART responses to make predictions. Specifically speaking, after the training phase, I input new data to make a prediction, the new data is the mean of all columns but with a small perturbation on one of the column.\nFirstly, I use the linear model (model1)\n$$Y = X_1 + X_2 + X_3 + X_4 + X_5 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.1)$$\n\nThe plots above from left to right are perturbation on five columns; the red lines are the true $y$ while black dots are predicted $y$. They show the model capture the relationship between $X$ and $y$ pretty well. We can also see the shadow of trees from it that there are obvious discontinuities.\nHowever, if I use the model (model2)\n$$Y = 100X_1 + 10X_2 + 5X_3 + 2X_4 + X_5 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.1)$$\nI found it captures the first covariate almost perfectly while less and less for the other covariates. Noted the number of parameters here is always the same. Apparently, it puts all its effort to capture the most influential one.\n\nI then apply the Friedman function,\n$$Y = 10  \\sin(\\pi X_1  X_2) +20  (X_3 -0.5)^2 + 10  X_4 + 5  X_5 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\nAnd found it fits the true $y$ very well.\n\nSame deviation happens when I multiply the first coefficient 100 times larger.\n$$Y = 1000  \\sin(\\pi X_1  X_2) +20  (X_3 -0.5)^2 + 10  X_4 + 5  X_5 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\n\nThese experiments show BART fit the data pretty well on each covariate given the ""effect size"" of them is similar. This is something we should be noticed when we want to interpret the model. As a comparison, a simple regression doesn\'t have this phenomenon for linear relations.\n##        (Intercept) X[, 1] X[, 2] X[, 3] X[, 4] X[, 5]\n## model1       -0.02   1.01   1.01   1.01   1.03   1.00\n## model2       -0.01 100.00  10.00   5.01   2.04   0.98\n\nPartial dependence\nPartial dependence function (PDP) developed by Friedman (2001) that is conceptually similar to perturbation above. The partial dependence of a predictor is the average value of $f$ when other covariates vary over its marginal distribution.\n$$f_{j}\\left(\\boldsymbol{x}{j}\\right)=\\mathbb{E}{\\boldsymbol{x}{-j}}\\left[f\\left(\\boldsymbol{x}{j}, \\boldsymbol{x}{-j}\\right)\\right]:=\\int f\\left(\\boldsymbol{x}{j}, \\boldsymbol{x}{-j}\\right) \\mathrm{dP}\\left(\\boldsymbol{x}{-j}\\right)$$\nand can be estimated by\n$$\\hat{f}{j}\\left(\\boldsymbol{x}{j}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\hat{f}\\left(\\boldsymbol{x}{j}, \\boldsymbol{x}{-j, i}\\right)$$\nAs BART has the posterior distribution, we can plot credible intervals for the PDP. See Friedman (2001) and Kapelner & Bleich (2013) for more details.\nI use the function pd_plot in bartMachine plotted below and the result is similar to the above analysis.\n\nFeature importance\nAnother important aspect to explain the model is whether we can tell the importance of each feature and quantify that measure. Similar to the linear model example above, the coefficients in linear regression directly indicate its effect size of covariates on the response. Many machine learning lack this property, as well as BART. People have come up with ways to measure feature importance in indirect approaches, feature permutation is one of them.\nFeature permutation\nFeature permutation shuffles or resample the entire column (covariates) and fit the model again. The accuracy or RMSE is expected to decrease if this feature is meaningful. So we can measure the decrease in terms of RMSE as a measure of feature importance. I apply 5-fold cross-validation for all the metrics.\nThe plots below show the feature importance calculated using this method for model1 and model2. The red dots are the original result without perturbation. The difference is printed below indicating its importance.\nAs we can see, for model1, this measure is quite accurate. The estimated number, 0.770, for example, is not so far from 1; and 128.305 is not so far from 100.\n\n## [1] 0.842 0.884 0.885 0.923 1.016\n\n\n## [1] 128.64279   9.02780   3.55092  -0.00895   1.09847\n\nInclusion proportion\nAnother two ways to measure the importance of decision trees is to calculate the ""inclusion proportions"" in the $m$ trees in the iterations after burn-in (Chipman et al. 2010). The inclusion proportion for a covariate is the proportion of times that variable is chosen as a splitting rule, divided by all rules among the posterior draws of the sum-of-trees model (the middle plot below). We can also calculate the proportion it appears in trees (the third plot). The martMachine package has a function investigate_var_importance to return a feature importance plot.\nComparing this method to coefficients in an ordinary linear regression (the first plot), we have the following plots. Apparently, it captures the relative importance but cannot quantify them very accurately. However, a question is then arose, are coefficients really feature importance? Coefficients are closely related to ""effect size"" since in linear models, it can be interpreted as ""a unit increase in $X_1$ causes $\\beta_1$ increases in $y$"". There is no such thing in the decision tree. As a result, we need to decide a measure by ourselves in different contexts rather than simply following one universal measure.\n\n\nFit the fitted\nThe last method, ""fit the fitted"" is the opposite of checking the residual. It aims to detect to what extent the information was captured in the model by fitting its prediction $\\hat{y}$. In the real world, since we cannot be sure about the nature of the data, it is hard to specify a meaningful model to re-fit the fitted. However, by simulating the data, I will specify a model that is exactly the same as the model generating process to see how much information BART has in the predicted outcome.\nThe first two results are from model1 and model2, BART captures the coefficients in both models accurately.\n##            X1    X2   X3   X4   X5\n## model1   1.02  1.04 1.00 0.99 0.99\n## model2 100.04 10.03 4.98 2.00 1.01\n\nThe last two results are from linear models with interactions. Similarily, BART captures them very accurately.\n$$Y = X_1 + X_2 + X_3 + X_4 + X_5 + 5X_1 X_2+ \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.1)$$\n$$Y = 100X_1 + 10X_2 + 5X_3 + 2X_4 + X_5 + 5X_1 X_2+ \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.1)$$\n##            X1   X2   X3   X4   X5 X1X2\n## model5   1.08 1.09 1.02 1.03 1.03 4.83\n## model6  99.99 9.96 5.01 2.03 1.02 5.03\n\nConclusion and Discussion\nThough there\'s no universal consensus on how to measure interpretability, the methods I present here provides a glance toward it. With these methods and simulations, it seems BART has good interpretability given the model is additive and linear. Using the posterior distribution, BART is good in capturing linear relations and performs approximates well for nonlinear functions, while piecewise function and experimental design data may need further examination. Despite this, there should be a lot more method we can apply, for example, KL divergence for comparing two distributions or bootstrap could potentially be useful for further analysis.\nIn perturbation and partial dependence, I find BART performs reasonably well on functions with similar ""effect size"" (coefficients). It can still capture enough important variables. But from the analysis in feature importance, the effect size for nonlinear models are doubtful. However, there are definitely more sophisticated methods that I didn\'t include, for example, Kapelner & Bleich (2013) incorporated Pseudo-R^2 that uses permutation approach as well. Other methods could potentially be useful in explaining the effect size but they require more deliberation.\nLastly, the method in fit the fitted is conceptually naive but impossible to execute in real-world methods. From the result above, we can definitely say BART is good at capturing additive linear models and their interactions. However, there are a lot of possible models around that cannot be simulated and tested thoroughly.\nIn summary, BART is a pretty good machine learning model that has s unique property (i.e., accessibility of posterior). It has great potential that more methods could be developed to either test its interpretability or push its applicability. As in econometrics, interpret BART directly on a real data set will not be an easy task. Nevertheless, it may provide a unique aspect from the perspective of Bayesian and decision trees.\nReferences\nChipman, Hugh A., Edward I. George, and Robert E. McCulloch. ""BART: Bayesian additive regression trees."" The Annals of Applied Statistics 4.1 (2010): 266-298.\nFeng, C. and D. Michie. Machine learning of rules and trees. Machine Learning, Neural and Statistical Classiﬁcation. Ellis Horwood, Hemel Hempstead, 1994.\nFriedman J (2001). “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics, 29(5), 1189–1232. doi:10.1214/aos/1013203451.\nGeman S, Geman D (1984). “Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.” IEEE Transaction on Pattern Analysis and Machine Intelligence, 6, 721–741. doi:10.1109/tpami.1984.4767596.\nGeorge, Edward, et al. ""Fully Nonparametric Bayesian Additive Regression Trees."" Topics in Identification, Limited Dependent Variables, Partial Observability, Experimentation, and Flexible Modeling: Part B (Advances in Econometrics 40 (2019): 89-110.\nGreen, Donald P., and Holger L. Kern. ""Modeling heterogeneous treatment effects in large-scale experiments using bayesian additive regression\nHahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. ""Bayesian regression tree models for causal inference: regularization, confounding, and heterogeneous effects."" Bayesian Analysis (2020).\nHastie T, Tibshirani R (2000). “Bayesian Backﬁtting.” Statistical Science, 15(3), 196–213. doi:10.1214/ss/1009212815.\nHastings WK (1970). “Monte Carlo Sampling Methods Using Markov Chains and Their Applications.” Biometrika, 57(1), 97–109. doi:10.2307/2334940.\nHill, Jennifer L. ""Bayesian nonparametric modeling for causal inference."" Journal of Computational and Graphical Statistics 20.1 (2011): 217-240.\nKapelner, Adam, and Justin Bleich. ""bartMachine: Machine learning with Bayesian additive regression trees."" arXiv preprint arXiv:1312.2171 (2013).\nKindo, Bereket P., Hao Wang, and Edsel A. Peña. ""MPBART-Multinomial Probit Bayesian Additive Regression Trees."" arXiv preprint arXiv:1309.7821 (2013).\nRossi, Ryan A., Rong Zhou, and Nesreen K. Ahmed. ""Deep inductive network representation learning."" Companion Proceedings of the The Web Conference 2018. 2018.\nTan, Yaoyuan Vincent, and Jason Roy. ""Bayesian additive regression trees and the General BART model."" Statistics in medicine 38.25 (2019): 5048-5069.\n'], 'url_profile': 'https://github.com/tll549', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'Iowa City, Iowa', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['DRP Statistical Regression Analysis Project (K-Nearest Neighbors Model)\nI coded the K-Nearest Neighbors statisical regression model for a DRP team of UIowa undergraduate students in Spring 2020.\n'], 'url_profile': 'https://github.com/kjmichalski', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sasithota', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lakshyaCoder41', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Oct 27, 2020', 'Python', 'Updated Mar 31, 2020', 'JavaScript', 'Updated Apr 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Adaptive Bayesian Slope - Projekt z ""Rozwoju oprogramowania w R""\nBased on W. Jiang et al., 2019\nFor 17.06 presentation\n\nTests\nRest of readme\n\n'], 'url_profile': 'https://github.com/AleksandraSteiner', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'Jordan', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnwarBaniAmer', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'Navi Mumbai, Maharashtra', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['ML---Polynomial-Lasso-regression\nThis contains an implementation of a polynomial regression and lasso regression\nPre-requisites:\n\npython (version 3.6.9 or above)\n\n'], 'url_profile': 'https://github.com/Pramodh-G', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['Regression-Model-using-python-code\nThis Python code accepts dataset of Dependent variables and Independent variables.\nIt splits data into train set and test set .\nBy using Independent dataset it calculates Regression Equation of Y on X and predict values accordingly.\nIt also do visualising of both training and test dataset.\nInstructions\n\nImport dataset by replacing file name in demo.py\nMake sure your dataset file should csv file and present in same directory.\nReplace X_values with the value which you want to predict.\n\n'], 'url_profile': 'https://github.com/Sank-Infinity', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['logistic-regression-self-implemented\nLogistic and Softmax Regression via Gradient Descent using NumPy only\nLogistic regression and Softmax regression are classification tools used in creating training models. In this project, these regressions were implemented from scratch along with Batch and Stochastic Gradient Descent using NumPy only. I investigated to the extent of their effectiveness when considering Batch and Stochastic Gradient Descent. General optimization of the algorithms was found by attempting different learning rates and epochs amount.\nThis projects includes:\n\nCalifornia Facial Expressions (CAFE) dataset organization by PCA, encoding and smart train\\validation\\test segmentation.\nBasic Data Visualization after PCA for different expressions.\nBatch and Stochastic Gradient Descent implementation.\nSoftmax Activation, log-likelihood, Cross Entropy functions implementation.\nCalculations of loss of each iteration (batch).\nTraining, evaluation and methods comparition process using the above.\n\n'], 'url_profile': 'https://github.com/Alont93', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kalikatepr9', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['House-Prices-Scikit-learn-and-XG-Boost\nMy take on the Kaggle House Price regression competition available at https://www.kaggle.com/c/house-prices-advanced-regression-techniques\nML regression models built with XG Boost and Scikit Learn\n'], 'url_profile': 'https://github.com/Arun-AP-9', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '193 contributions\n        in the last year', 'description': ['Logistic Regression from scratch\nDataset\nWisconsin Breast Cancer:\nhttps://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\nPurpose\nGain familiarity with the algorithm by developing it from scratch.\nHence, best ML practices such as train/test/cross-validation splits\nare NOT prioritized.\nTraining Learning Curve\n\nAccuracy Stats\nOverall Accuracy: 0.928\n\n\n\nClass\nLabeled\nPredicted\n\n\n\n\nMalignant\n212\n191\n\n\nBenign\n357\n378\n\n\n\n\n\n\nClass\nTrue Positive\nFalse Positive\n\n\n\n\nMalignant\n183\n8\n\n\nBenign\n349\n29\n\n\n\n\n\n\nClass\nPrecision\nRecall\nF1 Score\n\n\n\n\nMalignant\n0.958\n0.863\n0.908\n\n\nBenign\n0.923\n0.978\n0.950\n\n\n\n\n\n\nRecord ID\nLabel\nPredicted Malignant Probability\nAbsolute Error\nLogIt Error\nRounded Prediction Error\n\n\n\n\n842302\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n842517\nMalignant\n0.9719\n0.0281\n0.0285\n0.0\n\n\n84300903\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n84348301\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n84358402\nMalignant\n0.991\n0.009\n0.0091\n0.0\n\n\n843786\nMalignant\n0.9695\n0.0305\n0.031\n0.0\n\n\n844359\nMalignant\n0.988\n0.012\n0.0121\n0.0\n\n\n84458202\nMalignant\n0.9758\n0.0242\n0.0244\n0.0\n\n\n844981\nMalignant\n0.9988\n0.0012\n0.0012\n0.0\n\n\n84501001\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n845636\nMalignant\n0.1586\n0.8414\n1.8416\n1.0\n\n\n84610002\nMalignant\n0.9819\n0.0181\n0.0183\n0.0\n\n\n846226\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n846381\nMalignant\n0.2827\n0.7173\n1.2635\n1.0\n\n\n84667401\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n84799002\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n848406\nMalignant\n0.5483\n0.4517\n0.6009\n0.0\n\n\n84862001\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n849014\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8510426\nBenign\n0.0043\n0.0043\n-0.0043\n0.0\n\n\n8510653\nBenign\n0.0018\n0.0018\n-0.0018\n0.0\n\n\n8510824\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8511133\nMalignant\n0.9976\n0.0024\n0.0024\n0.0\n\n\n851509\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n852552\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n852631\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n852763\nMalignant\n0.9996\n0.0004\n0.0004\n0.0\n\n\n852781\nMalignant\n0.9908\n0.0092\n0.0092\n0.0\n\n\n852973\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n853201\nMalignant\n0.4115\n0.5885\n0.8881\n1.0\n\n\n853401\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n853612\nMalignant\n0.9937\n0.0063\n0.0063\n0.0\n\n\n85382601\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n854002\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n854039\nMalignant\n0.9963\n0.0037\n0.0037\n0.0\n\n\n854253\nMalignant\n0.998\n0.002\n0.002\n0.0\n\n\n854268\nMalignant\n0.8967\n0.1033\n0.1091\n0.0\n\n\n854941\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n855133\nMalignant\n0.0071\n0.9929\n4.9447\n1.0\n\n\n855138\nMalignant\n0.7196\n0.2804\n0.3291\n0.0\n\n\n855167\nMalignant\n0.0029\n0.9971\n5.8479\n1.0\n\n\n855563\nMalignant\n0.7185\n0.2815\n0.3305\n0.0\n\n\n855625\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n856106\nMalignant\n0.798\n0.202\n0.2257\n0.0\n\n\n85638502\nMalignant\n0.3858\n0.6142\n0.9525\n1.0\n\n\n857010\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n85713702\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n85715\nMalignant\n0.9694\n0.0306\n0.0311\n0.0\n\n\n857155\nBenign\n0.0008\n0.0008\n-0.0008\n0.0\n\n\n857156\nBenign\n0.0163\n0.0163\n-0.0164\n0.0\n\n\n857343\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n857373\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n857374\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n857392\nMalignant\n0.9856\n0.0144\n0.0145\n0.0\n\n\n857438\nMalignant\n0.1912\n0.8088\n1.6543\n1.0\n\n\n85759902\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n857637\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n857793\nMalignant\n0.9823\n0.0177\n0.0178\n0.0\n\n\n857810\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n858477\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n858970\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n858981\nBenign\n0.0008\n0.0008\n-0.0008\n0.0\n\n\n858986\nMalignant\n0.9996\n0.0004\n0.0004\n0.0\n\n\n859196\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n85922302\nMalignant\n0.9917\n0.0083\n0.0083\n0.0\n\n\n859283\nMalignant\n0.9911\n0.0089\n0.0089\n0.0\n\n\n859464\nBenign\n0.0013\n0.0013\n-0.0013\n0.0\n\n\n859465\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n859471\nBenign\n0.9679\n0.9679\n-3.4378\n1.0\n\n\n859487\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n859575\nMalignant\n0.9862\n0.0138\n0.0139\n0.0\n\n\n859711\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n859717\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n859983\nMalignant\n0.031\n0.969\n3.4745\n1.0\n\n\n8610175\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n8610404\nMalignant\n0.7995\n0.2005\n0.2238\n0.0\n\n\n8610629\nBenign\n0.001\n0.001\n-0.001\n0.0\n\n\n8610637\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n8610862\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8610908\nBenign\n0.0016\n0.0016\n-0.0016\n0.0\n\n\n861103\nBenign\n0.0188\n0.0188\n-0.019\n0.0\n\n\n8611161\nBenign\n0.5132\n0.5132\n-0.7199\n1.0\n\n\n8611555\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8611792\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8612080\nBenign\n0.001\n0.001\n-0.001\n0.0\n\n\n8612399\nMalignant\n0.9975\n0.0025\n0.0025\n0.0\n\n\n86135501\nMalignant\n0.5171\n0.4829\n0.6596\n0.0\n\n\n86135502\nMalignant\n0.9997\n0.0003\n0.0003\n0.0\n\n\n861597\nBenign\n0.0342\n0.0342\n-0.0348\n0.0\n\n\n861598\nBenign\n0.1416\n0.1416\n-0.1527\n0.0\n\n\n861648\nBenign\n0.0094\n0.0094\n-0.0094\n0.0\n\n\n861799\nMalignant\n0.2206\n0.7794\n1.5114\n1.0\n\n\n861853\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n862009\nBenign\n0.0031\n0.0031\n-0.0031\n0.0\n\n\n862028\nMalignant\n0.9842\n0.0158\n0.0159\n0.0\n\n\n86208\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n86211\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n862261\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n862485\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n862548\nMalignant\n0.5993\n0.4007\n0.5119\n0.0\n\n\n862717\nMalignant\n0.2898\n0.7102\n1.2387\n1.0\n\n\n862722\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n862965\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n862980\nBenign\n0.0024\n0.0024\n-0.0024\n0.0\n\n\n862989\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n863030\nMalignant\n0.9957\n0.0043\n0.0043\n0.0\n\n\n863031\nBenign\n0.0997\n0.0997\n-0.105\n0.0\n\n\n863270\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n86355\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n864018\nBenign\n0.0051\n0.0051\n-0.0051\n0.0\n\n\n864033\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n86408\nBenign\n0.0494\n0.0494\n-0.0507\n0.0\n\n\n86409\nBenign\n0.8375\n0.8375\n-1.817\n1.0\n\n\n864292\nBenign\n0.002\n0.002\n-0.002\n0.0\n\n\n864496\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n864685\nBenign\n0.0032\n0.0032\n-0.0032\n0.0\n\n\n864726\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n864729\nMalignant\n0.998\n0.002\n0.002\n0.0\n\n\n864877\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n865128\nMalignant\n0.5658\n0.4342\n0.5696\n0.0\n\n\n865137\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n86517\nMalignant\n0.9962\n0.0038\n0.0038\n0.0\n\n\n865423\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n865432\nBenign\n0.0035\n0.0035\n-0.0035\n0.0\n\n\n865468\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n86561\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n866083\nMalignant\n0.4392\n0.5608\n0.8228\n1.0\n\n\n866203\nMalignant\n0.7961\n0.2039\n0.228\n0.0\n\n\n866458\nBenign\n0.4185\n0.4185\n-0.5422\n0.0\n\n\n866674\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n866714\nBenign\n0.0005\n0.0005\n-0.0005\n0.0\n\n\n8670\nMalignant\n0.9091\n0.0909\n0.0953\n0.0\n\n\n86730502\nMalignant\n0.9448\n0.0552\n0.0568\n0.0\n\n\n867387\nBenign\n0.0083\n0.0083\n-0.0083\n0.0\n\n\n867739\nMalignant\n0.9925\n0.0075\n0.0075\n0.0\n\n\n868202\nMalignant\n0.0149\n0.9851\n4.2091\n1.0\n\n\n868223\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n868682\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n868826\nMalignant\n0.9728\n0.0272\n0.0275\n0.0\n\n\n868871\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n868999\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n869104\nMalignant\n0.617\n0.383\n0.4828\n0.0\n\n\n869218\nBenign\n0.0009\n0.0009\n-0.0009\n0.0\n\n\n869224\nBenign\n0.0012\n0.0012\n-0.0012\n0.0\n\n\n869254\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n869476\nBenign\n0.0011\n0.0011\n-0.0011\n0.0\n\n\n869691\nMalignant\n0.9213\n0.0787\n0.0819\n0.0\n\n\n86973701\nBenign\n0.0417\n0.0417\n-0.0426\n0.0\n\n\n86973702\nBenign\n0.0185\n0.0185\n-0.0186\n0.0\n\n\n869931\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n871001501\nBenign\n0.0073\n0.0073\n-0.0074\n0.0\n\n\n871001502\nBenign\n0.1093\n0.1093\n-0.1157\n0.0\n\n\n8710441\nBenign\n0.9992\n0.9992\n-7.0847\n1.0\n\n\n87106\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8711002\nBenign\n0.0091\n0.0091\n-0.0092\n0.0\n\n\n8711003\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n8711202\nMalignant\n0.9981\n0.0019\n0.0019\n0.0\n\n\n8711216\nBenign\n0.0114\n0.0114\n-0.0115\n0.0\n\n\n871122\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n871149\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8711561\nBenign\n0.0218\n0.0218\n-0.0221\n0.0\n\n\n8711803\nMalignant\n0.9375\n0.0625\n0.0646\n0.0\n\n\n871201\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8712064\nBenign\n0.0153\n0.0153\n-0.0154\n0.0\n\n\n8712289\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8712291\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n87127\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8712729\nMalignant\n0.5894\n0.4106\n0.5287\n0.0\n\n\n8712766\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n8712853\nBenign\n0.0015\n0.0015\n-0.0015\n0.0\n\n\n87139402\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n87163\nMalignant\n0.0511\n0.9489\n2.9746\n1.0\n\n\n87164\nMalignant\n0.9378\n0.0622\n0.0642\n0.0\n\n\n871641\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n871642\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n872113\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n872608\nBenign\n0.0417\n0.0417\n-0.0426\n0.0\n\n\n87281702\nMalignant\n0.9939\n0.0061\n0.0061\n0.0\n\n\n873357\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n873586\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n873592\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n873593\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n873701\nMalignant\n0.8572\n0.1428\n0.1541\n0.0\n\n\n873843\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n873885\nMalignant\n0.2163\n0.7837\n1.5311\n1.0\n\n\n874158\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n874217\nMalignant\n0.447\n0.553\n0.8052\n1.0\n\n\n874373\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n874662\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n874839\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n874858\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n875093\nBenign\n0.0005\n0.0005\n-0.0005\n0.0\n\n\n875099\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n875263\nMalignant\n0.9961\n0.0039\n0.0039\n0.0\n\n\n87556202\nMalignant\n0.9707\n0.0293\n0.0297\n0.0\n\n\n875878\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n875938\nMalignant\n0.995\n0.005\n0.005\n0.0\n\n\n877159\nMalignant\n0.2096\n0.7904\n1.5627\n1.0\n\n\n877486\nMalignant\n0.9994\n0.0006\n0.0006\n0.0\n\n\n877500\nMalignant\n0.9571\n0.0429\n0.0439\n0.0\n\n\n877501\nBenign\n0.0113\n0.0113\n-0.0114\n0.0\n\n\n877989\nMalignant\n0.8981\n0.1019\n0.1074\n0.0\n\n\n878796\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n87880\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n87930\nBenign\n0.0248\n0.0248\n-0.0251\n0.0\n\n\n879523\nMalignant\n0.0865\n0.9135\n2.4472\n1.0\n\n\n879804\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n879830\nMalignant\n0.2492\n0.7508\n1.3894\n1.0\n\n\n8810158\nBenign\n0.4065\n0.4065\n-0.5217\n0.0\n\n\n8810436\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n881046502\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n8810528\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n8810703\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n881094802\nMalignant\n0.9924\n0.0076\n0.0076\n0.0\n\n\n8810955\nMalignant\n0.9955\n0.0045\n0.0046\n0.0\n\n\n8810987\nMalignant\n0.7305\n0.2695\n0.314\n0.0\n\n\n8811523\nBenign\n0.0382\n0.0382\n-0.0389\n0.0\n\n\n8811779\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8811842\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n88119002\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8812816\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n8812818\nBenign\n0.0032\n0.0032\n-0.0032\n0.0\n\n\n8812844\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n8812877\nMalignant\n0.9812\n0.0188\n0.019\n0.0\n\n\n8813129\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n88143502\nBenign\n0.0035\n0.0035\n-0.0035\n0.0\n\n\n88147101\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n88147102\nBenign\n0.0082\n0.0082\n-0.0082\n0.0\n\n\n88147202\nBenign\n0.0198\n0.0198\n-0.02\n0.0\n\n\n881861\nMalignant\n0.9971\n0.0029\n0.0029\n0.0\n\n\n881972\nMalignant\n0.9986\n0.0014\n0.0014\n0.0\n\n\n88199202\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n88203002\nBenign\n0.0017\n0.0017\n-0.0017\n0.0\n\n\n88206102\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n882488\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n88249602\nBenign\n0.0036\n0.0036\n-0.0036\n0.0\n\n\n88299702\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n883263\nMalignant\n0.9839\n0.0161\n0.0162\n0.0\n\n\n883270\nBenign\n0.223\n0.223\n-0.2524\n0.0\n\n\n88330202\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n88350402\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n883539\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n883852\nBenign\n0.3608\n0.3608\n-0.4475\n0.0\n\n\n88411702\nBenign\n0.0012\n0.0012\n-0.0012\n0.0\n\n\n884180\nMalignant\n0.9997\n0.0003\n0.0003\n0.0\n\n\n884437\nBenign\n0.0024\n0.0024\n-0.0024\n0.0\n\n\n884448\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n884626\nBenign\n0.0604\n0.0604\n-0.0623\n0.0\n\n\n88466802\nBenign\n0.0082\n0.0082\n-0.0082\n0.0\n\n\n884689\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n884948\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n88518501\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n885429\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8860702\nMalignant\n0.8493\n0.1507\n0.1633\n0.0\n\n\n886226\nMalignant\n0.9995\n0.0005\n0.0005\n0.0\n\n\n886452\nMalignant\n0.232\n0.768\n1.4611\n1.0\n\n\n88649001\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n886776\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n887181\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n88725602\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n887549\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n888264\nMalignant\n0.1383\n0.8617\n1.9786\n1.0\n\n\n888570\nMalignant\n0.982\n0.018\n0.0182\n0.0\n\n\n889403\nMalignant\n0.0053\n0.9947\n5.2326\n1.0\n\n\n889719\nMalignant\n0.9812\n0.0188\n0.0189\n0.0\n\n\n88995002\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8910251\nBenign\n0.0007\n0.0007\n-0.0007\n0.0\n\n\n8910499\nBenign\n0.0014\n0.0014\n-0.0015\n0.0\n\n\n8910506\nBenign\n0.0005\n0.0005\n-0.0005\n0.0\n\n\n8910720\nBenign\n0.0093\n0.0093\n-0.0093\n0.0\n\n\n8910721\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8910748\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8910988\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8910996\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8911163\nMalignant\n0.8205\n0.1795\n0.1979\n0.0\n\n\n8911164\nBenign\n0.0032\n0.0032\n-0.0032\n0.0\n\n\n8911230\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8911670\nMalignant\n0.2537\n0.7463\n1.3718\n1.0\n\n\n8911800\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8911834\nBenign\n0.0008\n0.0008\n-0.0008\n0.0\n\n\n8912049\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n8912055\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n89122\nMalignant\n0.9996\n0.0004\n0.0004\n0.0\n\n\n8912280\nMalignant\n0.9752\n0.0248\n0.0251\n0.0\n\n\n8912284\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n8912521\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8912909\nBenign\n0.0069\n0.0069\n-0.007\n0.0\n\n\n8913\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8913049\nBenign\n0.0125\n0.0125\n-0.0125\n0.0\n\n\n89143601\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n89143602\nBenign\n0.4817\n0.4817\n-0.6572\n0.0\n\n\n8915\nBenign\n0.0711\n0.0711\n-0.0737\n0.0\n\n\n891670\nBenign\n0.0041\n0.0041\n-0.0042\n0.0\n\n\n891703\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n891716\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n891923\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n891936\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n892189\nMalignant\n0.0002\n0.9998\n8.3294\n1.0\n\n\n892214\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n892399\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n892438\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n892604\nBenign\n0.0009\n0.0009\n-0.0009\n0.0\n\n\n89263202\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n892657\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n89296\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n893061\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n89344\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n89346\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n893526\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n893548\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n893783\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n89382601\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n89382602\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n893988\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n894047\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n894089\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n894090\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n894326\nMalignant\n0.9599\n0.0401\n0.0409\n0.0\n\n\n894329\nBenign\n0.0878\n0.0878\n-0.0919\n0.0\n\n\n894335\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n894604\nBenign\n0.0011\n0.0011\n-0.0011\n0.0\n\n\n894618\nMalignant\n0.8386\n0.1614\n0.176\n0.0\n\n\n894855\nBenign\n0.0015\n0.0015\n-0.0015\n0.0\n\n\n895100\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n89511501\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n89511502\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n89524\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n895299\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n8953902\nMalignant\n0.9925\n0.0075\n0.0075\n0.0\n\n\n895633\nMalignant\n0.907\n0.093\n0.0976\n0.0\n\n\n896839\nMalignant\n0.79\n0.21\n0.2357\n0.0\n\n\n896864\nBenign\n0.0157\n0.0157\n-0.0159\n0.0\n\n\n897132\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n897137\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n897374\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n89742801\nMalignant\n0.9986\n0.0014\n0.0014\n0.0\n\n\n897604\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n897630\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n897880\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n89812\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n89813\nBenign\n0.0707\n0.0707\n-0.0733\n0.0\n\n\n898143\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n89827\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n898431\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n89864002\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n898677\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n898678\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n89869\nBenign\n0.0021\n0.0021\n-0.0021\n0.0\n\n\n898690\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n899147\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n899187\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n899667\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n899987\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n9010018\nMalignant\n0.994\n0.006\n0.006\n0.0\n\n\n901011\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n9010258\nBenign\n0.0017\n0.0017\n-0.0017\n0.0\n\n\n9010259\nBenign\n0.1343\n0.1343\n-0.1442\n0.0\n\n\n901028\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n9010333\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n901034301\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n901034302\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n901041\nBenign\n0.0008\n0.0008\n-0.0008\n0.0\n\n\n9010598\nBenign\n0.0009\n0.0009\n-0.0009\n0.0\n\n\n9010872\nBenign\n0.041\n0.041\n-0.0419\n0.0\n\n\n9010877\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n901088\nMalignant\n0.9943\n0.0057\n0.0058\n0.0\n\n\n9011494\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n9011495\nBenign\n0.0011\n0.0011\n-0.0011\n0.0\n\n\n9011971\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n9012000\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n9012315\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n9012568\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n9012795\nMalignant\n0.9932\n0.0068\n0.0068\n0.0\n\n\n901288\nMalignant\n0.9982\n0.0018\n0.0018\n0.0\n\n\n9013005\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n901303\nBenign\n0.0457\n0.0457\n-0.0467\n0.0\n\n\n901315\nBenign\n0.2794\n0.2794\n-0.3277\n0.0\n\n\n9013579\nBenign\n0.0021\n0.0021\n-0.0021\n0.0\n\n\n9013594\nBenign\n0.0012\n0.0012\n-0.0012\n0.0\n\n\n9013838\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n901549\nBenign\n0.0088\n0.0088\n-0.0089\n0.0\n\n\n901836\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n90250\nBenign\n0.0013\n0.0013\n-0.0013\n0.0\n\n\n90251\nBenign\n0.0277\n0.0277\n-0.0281\n0.0\n\n\n902727\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n90291\nMalignant\n0.1567\n0.8433\n1.8536\n1.0\n\n\n902975\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n902976\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n903011\nBenign\n0.0008\n0.0008\n-0.0008\n0.0\n\n\n90312\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n90317302\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n903483\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n903507\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n903516\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n903554\nBenign\n0.0024\n0.0024\n-0.0024\n0.0\n\n\n903811\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n90401601\nBenign\n0.107\n0.107\n-0.1132\n0.0\n\n\n90401602\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n904302\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n904357\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n90439701\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n904647\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n904689\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n9047\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n904969\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n904971\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n905189\nBenign\n0.0042\n0.0042\n-0.0042\n0.0\n\n\n905190\nBenign\n0.0014\n0.0014\n-0.0014\n0.0\n\n\n90524101\nMalignant\n0.9938\n0.0062\n0.0062\n0.0\n\n\n905501\nBenign\n0.0019\n0.0019\n-0.0019\n0.0\n\n\n905502\nBenign\n0.0012\n0.0012\n-0.0012\n0.0\n\n\n905520\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n905539\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n905557\nBenign\n0.2651\n0.2651\n-0.308\n0.0\n\n\n905680\nMalignant\n0.2228\n0.7772\n1.5015\n1.0\n\n\n905686\nBenign\n0.0027\n0.0027\n-0.0027\n0.0\n\n\n905978\nBenign\n0.0015\n0.0015\n-0.0015\n0.0\n\n\n90602302\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n906024\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n906290\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n906539\nBenign\n0.0011\n0.0011\n-0.0011\n0.0\n\n\n906564\nBenign\n0.2277\n0.2277\n-0.2584\n0.0\n\n\n906616\nBenign\n0.0014\n0.0014\n-0.0014\n0.0\n\n\n906878\nBenign\n0.0335\n0.0335\n-0.0341\n0.0\n\n\n907145\nBenign\n0.0006\n0.0006\n-0.0006\n0.0\n\n\n907367\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n907409\nBenign\n0.0008\n0.0008\n-0.0008\n0.0\n\n\n90745\nBenign\n0.0022\n0.0022\n-0.0022\n0.0\n\n\n90769601\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n90769602\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n907914\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n907915\nBenign\n0.0083\n0.0083\n-0.0084\n0.0\n\n\n908194\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n908445\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n908469\nBenign\n0.0005\n0.0005\n-0.0005\n0.0\n\n\n908489\nMalignant\n0.8088\n0.1912\n0.2122\n0.0\n\n\n908916\nBenign\n0.0006\n0.0006\n-0.0006\n0.0\n\n\n909220\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n909231\nBenign\n0.0006\n0.0006\n-0.0006\n0.0\n\n\n909410\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n909411\nBenign\n0.0201\n0.0201\n-0.0203\n0.0\n\n\n909445\nMalignant\n0.9969\n0.0031\n0.0031\n0.0\n\n\n90944601\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n909777\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n9110127\nMalignant\n0.518\n0.482\n0.6578\n0.0\n\n\n9110720\nBenign\n0.032\n0.032\n-0.0325\n0.0\n\n\n9110732\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n9110944\nBenign\n0.0044\n0.0044\n-0.0044\n0.0\n\n\n911150\nBenign\n0.0093\n0.0093\n-0.0094\n0.0\n\n\n911157302\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n9111596\nBenign\n0.0016\n0.0016\n-0.0016\n0.0\n\n\n9111805\nMalignant\n0.998\n0.002\n0.002\n0.0\n\n\n9111843\nBenign\n0.0114\n0.0114\n-0.0114\n0.0\n\n\n911201\nBenign\n0.0028\n0.0028\n-0.0028\n0.0\n\n\n911202\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n9112085\nBenign\n0.085\n0.085\n-0.0889\n0.0\n\n\n9112366\nBenign\n0.1422\n0.1422\n-0.1534\n0.0\n\n\n9112367\nBenign\n0.0038\n0.0038\n-0.0038\n0.0\n\n\n9112594\nBenign\n0.0009\n0.0009\n-0.0009\n0.0\n\n\n9112712\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n911296201\nMalignant\n0.9998\n0.0002\n0.0002\n0.0\n\n\n911296202\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n9113156\nBenign\n0.0018\n0.0018\n-0.0018\n0.0\n\n\n911320501\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n911320502\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n9113239\nBenign\n0.3452\n0.3452\n-0.4235\n0.0\n\n\n9113455\nBenign\n0.0478\n0.0478\n-0.049\n0.0\n\n\n9113514\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n9113538\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n911366\nBenign\n0.4296\n0.4296\n-0.5615\n0.0\n\n\n9113778\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n9113816\nBenign\n0.0065\n0.0065\n-0.0065\n0.0\n\n\n911384\nBenign\n0.0012\n0.0012\n-0.0012\n0.0\n\n\n9113846\nBenign\n0.0005\n0.0005\n-0.0005\n0.0\n\n\n911391\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n911408\nBenign\n0.0007\n0.0007\n-0.0007\n0.0\n\n\n911654\nBenign\n0.0321\n0.0321\n-0.0326\n0.0\n\n\n911673\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n911685\nBenign\n0.0003\n0.0003\n-0.0003\n0.0\n\n\n911916\nMalignant\n0.9955\n0.0045\n0.0045\n0.0\n\n\n912193\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n91227\nBenign\n0.0005\n0.0005\n-0.0005\n0.0\n\n\n912519\nBenign\n0.0034\n0.0034\n-0.0034\n0.0\n\n\n912558\nBenign\n0.0009\n0.0009\n-0.0009\n0.0\n\n\n912600\nBenign\n0.0279\n0.0279\n-0.0283\n0.0\n\n\n913063\nBenign\n0.2924\n0.2924\n-0.3459\n0.0\n\n\n913102\nBenign\n0.0008\n0.0008\n-0.0008\n0.0\n\n\n913505\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n913512\nBenign\n0.0024\n0.0024\n-0.0024\n0.0\n\n\n913535\nMalignant\n0.0556\n0.9444\n2.8894\n1.0\n\n\n91376701\nBenign\n0.0011\n0.0011\n-0.0011\n0.0\n\n\n91376702\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n914062\nMalignant\n0.9908\n0.0092\n0.0092\n0.0\n\n\n914101\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n914102\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n914333\nBenign\n0.0136\n0.0136\n-0.0137\n0.0\n\n\n914366\nBenign\n0.0822\n0.0822\n-0.0858\n0.0\n\n\n914580\nBenign\n0.0005\n0.0005\n-0.0005\n0.0\n\n\n914769\nMalignant\n0.9945\n0.0055\n0.0055\n0.0\n\n\n91485\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n914862\nBenign\n0.0078\n0.0078\n-0.0078\n0.0\n\n\n91504\nMalignant\n0.9987\n0.0013\n0.0013\n0.0\n\n\n91505\nBenign\n0.0064\n0.0064\n-0.0065\n0.0\n\n\n915143\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n915186\nBenign\n0.2845\n0.2845\n-0.3347\n0.0\n\n\n915276\nBenign\n0.1476\n0.1476\n-0.1597\n0.0\n\n\n91544001\nBenign\n0.0117\n0.0117\n-0.0117\n0.0\n\n\n91544002\nBenign\n0.007\n0.007\n-0.0071\n0.0\n\n\n915452\nBenign\n0.0069\n0.0069\n-0.0069\n0.0\n\n\n915460\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n91550\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n915664\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n915691\nMalignant\n0.9799\n0.0201\n0.0203\n0.0\n\n\n915940\nBenign\n0.0016\n0.0016\n-0.0016\n0.0\n\n\n91594602\nMalignant\n0.0331\n0.9669\n3.4087\n1.0\n\n\n916221\nBenign\n0.001\n0.001\n-0.001\n0.0\n\n\n916799\nMalignant\n0.9948\n0.0052\n0.0052\n0.0\n\n\n916838\nMalignant\n0.9982\n0.0018\n0.0018\n0.0\n\n\n917062\nBenign\n0.0639\n0.0639\n-0.066\n0.0\n\n\n917080\nBenign\n0.0076\n0.0076\n-0.0076\n0.0\n\n\n917092\nBenign\n0.0033\n0.0033\n-0.0033\n0.0\n\n\n91762702\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n91789\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n917896\nBenign\n0.0401\n0.0401\n-0.0409\n0.0\n\n\n917897\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n91805\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n91813701\nBenign\n0.0798\n0.0798\n-0.0832\n0.0\n\n\n91813702\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n918192\nBenign\n0.0545\n0.0545\n-0.0561\n0.0\n\n\n918465\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n91858\nBenign\n0.0039\n0.0039\n-0.0039\n0.0\n\n\n91903901\nBenign\n0.0108\n0.0108\n-0.0109\n0.0\n\n\n91903902\nBenign\n0.0002\n0.0002\n-0.0002\n0.0\n\n\n91930402\nMalignant\n0.9984\n0.0016\n0.0016\n0.0\n\n\n919537\nBenign\n0.001\n0.001\n-0.001\n0.0\n\n\n919555\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n91979701\nMalignant\n0.7659\n0.2341\n0.2667\n0.0\n\n\n919812\nBenign\n0.8086\n0.8086\n-1.6536\n1.0\n\n\n921092\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n921362\nBenign\n0.0086\n0.0086\n-0.0086\n0.0\n\n\n921385\nBenign\n0.0004\n0.0004\n-0.0004\n0.0\n\n\n921386\nBenign\n0.7953\n0.7953\n-1.5862\n1.0\n\n\n921644\nBenign\n0.0411\n0.0411\n-0.042\n0.0\n\n\n922296\nBenign\n0.0085\n0.0085\n-0.0086\n0.0\n\n\n922297\nBenign\n0.0031\n0.0031\n-0.0031\n0.0\n\n\n922576\nBenign\n0.0049\n0.0049\n-0.0049\n0.0\n\n\n922577\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n922840\nBenign\n0.0001\n0.0001\n-0.0001\n0.0\n\n\n923169\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n923465\nBenign\n0.0007\n0.0007\n-0.0007\n0.0\n\n\n923748\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n923780\nBenign\n0.001\n0.001\n-0.001\n0.0\n\n\n924084\nBenign\n0.0047\n0.0047\n-0.0047\n0.0\n\n\n924342\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n924632\nBenign\n0.0127\n0.0127\n-0.0127\n0.0\n\n\n924934\nBenign\n0.0177\n0.0177\n-0.0179\n0.0\n\n\n924964\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n925236\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n925277\nBenign\n0.0498\n0.0498\n-0.0511\n0.0\n\n\n925291\nBenign\n0.0613\n0.0613\n-0.0633\n0.0\n\n\n925292\nBenign\n0.2555\n0.2555\n-0.2951\n0.0\n\n\n925311\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n925622\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n926125\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n926424\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n926682\nMalignant\n0.9999\n0.0001\n0.0001\n0.0\n\n\n926954\nMalignant\n0.9088\n0.0912\n0.0957\n0.0\n\n\n927241\nMalignant\n1.0\n0.0\n0.0\n0.0\n\n\n92751\nBenign\n0.0\n0.0\n0.0\n0.0\n\n\n\n'], 'url_profile': 'https://github.com/boyko11', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Logistic-Regression-for-numerical-data-by-Pytorch\nUse pytorch to implement logistic regression for numerical data\n'], 'url_profile': 'https://github.com/sunny-fang', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': [""Legatum_Prosp_Index_Lnr_Reg_Tool\nUsing the Legatum Prosperity Index to preform Linear Regression Analysis>\nThis tool can preform linear regression and multilinear regression against any indicators, element, or pillar in the Legatum Index.\nInstructions are within the program. This tool is not designed to handle inpercise entries well so please enter things with percision.\nFor questions: sciocca@stevens.edu\nPillars:\nEconomic Quality, Education, Enterprise Conditions, Governance, Health, Investment Environment, Living Conditions, Market Access and Infrastructure, Natural Environment, Personal Freedom, Safety and Security, Social Capital\nElements:\nDynamism, Fiscal Sustainability, Labour Force Engagement, Macroeconomic Stability, Productivity and Competitiveness, Adult Skills, Pre-Primary Education, Primary Education, Secondary Education, Tertiary Education, Burden of Regulation, Domestic Market Contestability, Environment for Business Creation, Labour Market Flexibility, Executive Constraints, Government Effectiveness, Government Integrity, Political Accountability, Regulatory Quality, Rule of Law, Behavioural Risk Factors, Care Systems, Longevity, Mental Health, Physical Health, Preventative Interventions, Contract Enforcement, Financing Ecosystem, Investor Protection, Property Rights, Restrictions on International Investment, Basic Services, Connectedness, Material Resources, Nutrition, Protection from Harm, Shelter, Border Administration, Communications, Import Tariff Barriers, Market Distortions, Open Market Scale, Resources, Transport, Emissions, Exposure to Air Pollution, Forest, Land and Soil, Freshwater, Oceans, Preservation Efforts, Absence of Legal Discrimination, Agency, Freedom of Assembly and Association, Freedom of Speech and Access to Information, Social Tolerance, Politically Related Terror and Violence, Property Crime, Terrorism, Violent Crime, War and Civil Conflict, Civic and Social Participation, Institutional Trust, Interpersonal Trust, Personal and Family Relationships, Social Networks\nIndicators: Capacity to attract talented people, New business density, Patent applications, Country credit rating, Country risk premium, Government budget balance, Government debt, Gross savings, Female labour force participation, Labour force participation, Unemployment, Waged and salaried workers, Youth unemployment, GDP per capita growth, Inflation volatility, Economic complexity, Export quality, High-tech manufactured exports, Labour productivity, Adult literacy, Digital skills among population, Education inequality, Education level of adult population, Women's average years in school, Pre-primary enrolment, Primary completion, Primary education quality, Primary enrolment, Access to quality education, Lower-secondary completion, Secondary education quality, Secondary school enrolment, Average quality of higher education institutions, Quality of vocational training, Skillset of university graduates, Tertiary completion, Tertiary enrolment, Building quality control index, Burden of government regulation, Burden of obtaining a building permit, Number of tax payments, Time spent complying with regulations, Time spent filing taxes, Anti-monopoly policy, Extent of market dominance, Market-based competition, Availability of skilled workers, Ease of starting a business, Labour skill a business constraint, Private companies are protected and permitted, State of cluster development, Cooperation in labour-employer relations, Flexibility of employment contracts, Flexibility of hiring practices, Flexibility of wage determination, Redundancy costs, Executive powers are effectively limited by the judiciary and legislature, Government officials are sanctioned for misconduct, Government powers are subject to independent and non-governmental checks, Military involvement in rule of law and politics, Transition of power is subject to the law, Efficiency of government spending, Efficient use of assets, Government quality and credibility, Implementation, Policy coordination, Policy learning, Prioritisation, Budget transparency, Diversion of public funds, Publicised laws and government data , Right to information, Transparency of government policy, Use of public office for private gain, Complaint mechanisms, Consensus on democracy and a market economy as a goal, Democracy level, Political participation and rights, Delay in administrative proceedings, Efficiency of legal framework in challenging regulations, Enforcement of regulations, Regulatory quality, Civil justice, Efficiency of dispute settlement, Integrity of the legal system, Judicial independence, Obesity, Smoking, Substance use disorders, Antiretroviral HIV therapy, Births attended by skilled health staff, Health facilities, Health practitioners and staff, Healthcare coverage ,Satisfaction with healthcare, Tuberculosis treatment coverage, 15-60 mortality, 5-14 mortality, Life expectancy at 60, Maternal mortality, Under 5 mortality, Depressive disorders, Emotional wellbeing, Suicide, Communicable diseases, Health problems, Non-communicable diseases, Physical pain, Raised blood pressure, Antenatal care coverage, Contraceptive prevalence, Diphtheria immunisation, Existence of national screening programs, Hepatitis immunisation, Measles immunisation, Alternative dispute resolution mechanisms, Legal costs, Quality of judicial administration, Time to resolve commercial cases, Access to finance, Commercial bank branches, Depth of credit information, Financing of SMEs, Quality of banking system and capital markets, Soundness of banks, Venture capital availability, Auditing and reporting standards, Conflict of interest regulation, Extent of shareholder governance, Insolvency recovery rate, Strength of insolvency framework, Intellectual property protection, Lawful process for expropriation, Procedures to register property, Protection of property rights, Regulation of property possession and exchange, Reliability of land infrastructure administration, Business impact of rules on FDI, Capital controls, Freedom of foreigners to visit, Freedom to own foreign currency bank accounts, Prevalence of foreign ownership of companies, Restrictions on financial transactions, Access to basic sanitation services, Access to basic water services, Access to electricity, Access to piped water, Unsafe water, sanitation or hygiene, Access to a bank account, Access to a cellphone, Rural access to roads, Satisfaction with public transportation, Satisfaction with roads and highways, Use of digital payments, Ability to live on household income, Ability to source emergency funds, Households with a refrigerator, Poverty rate at $1.90 a day, Poverty rate at $3.20 a day, Poverty rate at $5.50 a day, Poverty rate at national poverty lines ,Availability of adequate food, Prevalence of stunting in children under-5, Prevalence of undernourishment, Prevalence of wasting in children under-5, Death and injury from forces of nature, Death and injury from road traffic accidents, Occupational mortality, Unintentional death and injury, Access to clean fuels and technologies for cooking, Availability of adequate shelter, Housing deprivation, Indoor air quality, Cost to comply with border regulations and procedures, Efficiency of customs clearance process, Time to comply with border regulations and procedures, 2G, 3G and 4G network coverage, Fixed broadband subscriptions, International internet bandwidth, Internet usage, Average applied tariff rate, Complexity of tariffs, Share of imports free from tariff duties, Distortive effect of taxes and subsidies, Energy subsidies, Extent of liberalisation of foreign trade, Non-tariff measures, Prevalence of non-tariff barriers, Domestic and international market access for goods, Domestic and international market access for services, Margin of preference in destination markets, Trade-weighted average tariff faced in destination markets, Ease of establishing an electricity connection, Gross fixed water assets, Installed electric capacity, Reliability of electricity supply, Reliability of water supply, Water production, Airport connectivity, Efficiency of seaport services, Liner shipping connectivity, Logistics performance, Quality of roads, Rail density, Road density, Black carbon emissions, CO2 emissions, Methane emissions, NOx emissions, SO2 emissions, Exposure to fine particulate matter, Health impact of air pollution, Satisfaction with air quality, Flood occurrence, Forest area, Sustainable nitrogen management, Freshwater withdrawal, Renewable water resources, Satisfaction with water quality, Wastewater treatment, Long term management of forest areas, Pesticide regulation, Protection for biodiverse areas, Satisfaction with preservation efforts, Terrestrial protected areas, Equal treatment and absence of discrimination, Freedom from hiring and workplace discrimination, Freedom of belief and religion, Government religious intimidation and hostility, LGBT Rights, Non-discriminatory civil justice, Protection of women's workplace, education and family rights, Due process and rights, Freedom from arbitrary interference with privacy, Freedom from forced labour, Freedom of movement, Government response to slavery, Personal autonomy and individual rights, Satisfaction with freedom, Women's agency, Autonomy from the state, Guarantee of assembly and association, Right to associate and organise, Alternative sources of information, Freedom of opinion and expression, Government media censorship, Political diversity of media perspectives, Press freedom from government censorship, Press freedom from physical repression, Perceived tolerance of LGBT individuals, Perceived tolerance of ethnic minorities, Perceived tolerance of immigrants, Disappearance cases, Extrajudicial killings, Political imprisonment, Political terror, Use of torture, Business costs of crime and violence, Business costs of organized crime, Property stolen, Property cost of terrorism, Terrorism deaths, Terrorism incidents, Terrorism injuries, Dispute settlement through violence, Intentional homicides, Physical security of women, Safety walking alone at night, Civil and ethnic war, Conflict-driven internal displacement, One-sided conflict deaths, Refugees (origin country), Two-sided conflict deaths, Donated money to charity, Voiced opinion to a public official, Volunteering, Voter turnout, Confidence in financial institutions and banks, Confidence in judicial system and courts, Confidence in local police, Confidence in military, Confidence in national government, Public trust in politicians, Generalised interpersonal trust, Helped a stranger, Family give positive energy, Help from family and friends when in trouble, Helped another household, Opportunity to make friends, Respect\n""], 'url_profile': 'https://github.com/sciocca', 'info_list': ['1', 'R', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['House_Prices_Prediction\nThis is a short machine learning project to predict house prices.\nData from the Kaggle Competition ""House Prices: Advanced Regression Techniques""\nModels used:\nLight GBM\nXGBoost\nRandom Forest\nCompetition overview:\n\'\'\'\nAsk a home buyer to describe their dream house, and they probably won\'t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition\'s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n\nPractice Skills\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting\n\n\nAcknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It\'s an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\n\n\n\'\'\'\nFiles:\n\n\nrequirements.txt contains packages used this work.\n\n\nEDA contains data exploring and feature engineering ideas with plots.\n\n\nModeling contains data preprocessing, feature engineering, modeling and prediction.\n\n\nResults\n\nThe baseline model with simple data preprocessing and modeling with RandomForest Regressor gives an root-mean-sqaured-logarithmic-error ~0.15.\nMy work with Lightgbm improved it to ~0.02.\n\n'], 'url_profile': 'https://github.com/lee-junseok', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['C1-liver-disease\n'], 'url_profile': 'https://github.com/tomytjandra', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Logistic-Regression-for-numerical-data-by-Pytorch\nUse pytorch to implement logistic regression for numerical data\n'], 'url_profile': 'https://github.com/sunny-fang', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': [""Legatum_Prosp_Index_Lnr_Reg_Tool\nUsing the Legatum Prosperity Index to preform Linear Regression Analysis>\nThis tool can preform linear regression and multilinear regression against any indicators, element, or pillar in the Legatum Index.\nInstructions are within the program. This tool is not designed to handle inpercise entries well so please enter things with percision.\nFor questions: sciocca@stevens.edu\nPillars:\nEconomic Quality, Education, Enterprise Conditions, Governance, Health, Investment Environment, Living Conditions, Market Access and Infrastructure, Natural Environment, Personal Freedom, Safety and Security, Social Capital\nElements:\nDynamism, Fiscal Sustainability, Labour Force Engagement, Macroeconomic Stability, Productivity and Competitiveness, Adult Skills, Pre-Primary Education, Primary Education, Secondary Education, Tertiary Education, Burden of Regulation, Domestic Market Contestability, Environment for Business Creation, Labour Market Flexibility, Executive Constraints, Government Effectiveness, Government Integrity, Political Accountability, Regulatory Quality, Rule of Law, Behavioural Risk Factors, Care Systems, Longevity, Mental Health, Physical Health, Preventative Interventions, Contract Enforcement, Financing Ecosystem, Investor Protection, Property Rights, Restrictions on International Investment, Basic Services, Connectedness, Material Resources, Nutrition, Protection from Harm, Shelter, Border Administration, Communications, Import Tariff Barriers, Market Distortions, Open Market Scale, Resources, Transport, Emissions, Exposure to Air Pollution, Forest, Land and Soil, Freshwater, Oceans, Preservation Efforts, Absence of Legal Discrimination, Agency, Freedom of Assembly and Association, Freedom of Speech and Access to Information, Social Tolerance, Politically Related Terror and Violence, Property Crime, Terrorism, Violent Crime, War and Civil Conflict, Civic and Social Participation, Institutional Trust, Interpersonal Trust, Personal and Family Relationships, Social Networks\nIndicators: Capacity to attract talented people, New business density, Patent applications, Country credit rating, Country risk premium, Government budget balance, Government debt, Gross savings, Female labour force participation, Labour force participation, Unemployment, Waged and salaried workers, Youth unemployment, GDP per capita growth, Inflation volatility, Economic complexity, Export quality, High-tech manufactured exports, Labour productivity, Adult literacy, Digital skills among population, Education inequality, Education level of adult population, Women's average years in school, Pre-primary enrolment, Primary completion, Primary education quality, Primary enrolment, Access to quality education, Lower-secondary completion, Secondary education quality, Secondary school enrolment, Average quality of higher education institutions, Quality of vocational training, Skillset of university graduates, Tertiary completion, Tertiary enrolment, Building quality control index, Burden of government regulation, Burden of obtaining a building permit, Number of tax payments, Time spent complying with regulations, Time spent filing taxes, Anti-monopoly policy, Extent of market dominance, Market-based competition, Availability of skilled workers, Ease of starting a business, Labour skill a business constraint, Private companies are protected and permitted, State of cluster development, Cooperation in labour-employer relations, Flexibility of employment contracts, Flexibility of hiring practices, Flexibility of wage determination, Redundancy costs, Executive powers are effectively limited by the judiciary and legislature, Government officials are sanctioned for misconduct, Government powers are subject to independent and non-governmental checks, Military involvement in rule of law and politics, Transition of power is subject to the law, Efficiency of government spending, Efficient use of assets, Government quality and credibility, Implementation, Policy coordination, Policy learning, Prioritisation, Budget transparency, Diversion of public funds, Publicised laws and government data , Right to information, Transparency of government policy, Use of public office for private gain, Complaint mechanisms, Consensus on democracy and a market economy as a goal, Democracy level, Political participation and rights, Delay in administrative proceedings, Efficiency of legal framework in challenging regulations, Enforcement of regulations, Regulatory quality, Civil justice, Efficiency of dispute settlement, Integrity of the legal system, Judicial independence, Obesity, Smoking, Substance use disorders, Antiretroviral HIV therapy, Births attended by skilled health staff, Health facilities, Health practitioners and staff, Healthcare coverage ,Satisfaction with healthcare, Tuberculosis treatment coverage, 15-60 mortality, 5-14 mortality, Life expectancy at 60, Maternal mortality, Under 5 mortality, Depressive disorders, Emotional wellbeing, Suicide, Communicable diseases, Health problems, Non-communicable diseases, Physical pain, Raised blood pressure, Antenatal care coverage, Contraceptive prevalence, Diphtheria immunisation, Existence of national screening programs, Hepatitis immunisation, Measles immunisation, Alternative dispute resolution mechanisms, Legal costs, Quality of judicial administration, Time to resolve commercial cases, Access to finance, Commercial bank branches, Depth of credit information, Financing of SMEs, Quality of banking system and capital markets, Soundness of banks, Venture capital availability, Auditing and reporting standards, Conflict of interest regulation, Extent of shareholder governance, Insolvency recovery rate, Strength of insolvency framework, Intellectual property protection, Lawful process for expropriation, Procedures to register property, Protection of property rights, Regulation of property possession and exchange, Reliability of land infrastructure administration, Business impact of rules on FDI, Capital controls, Freedom of foreigners to visit, Freedom to own foreign currency bank accounts, Prevalence of foreign ownership of companies, Restrictions on financial transactions, Access to basic sanitation services, Access to basic water services, Access to electricity, Access to piped water, Unsafe water, sanitation or hygiene, Access to a bank account, Access to a cellphone, Rural access to roads, Satisfaction with public transportation, Satisfaction with roads and highways, Use of digital payments, Ability to live on household income, Ability to source emergency funds, Households with a refrigerator, Poverty rate at $1.90 a day, Poverty rate at $3.20 a day, Poverty rate at $5.50 a day, Poverty rate at national poverty lines ,Availability of adequate food, Prevalence of stunting in children under-5, Prevalence of undernourishment, Prevalence of wasting in children under-5, Death and injury from forces of nature, Death and injury from road traffic accidents, Occupational mortality, Unintentional death and injury, Access to clean fuels and technologies for cooking, Availability of adequate shelter, Housing deprivation, Indoor air quality, Cost to comply with border regulations and procedures, Efficiency of customs clearance process, Time to comply with border regulations and procedures, 2G, 3G and 4G network coverage, Fixed broadband subscriptions, International internet bandwidth, Internet usage, Average applied tariff rate, Complexity of tariffs, Share of imports free from tariff duties, Distortive effect of taxes and subsidies, Energy subsidies, Extent of liberalisation of foreign trade, Non-tariff measures, Prevalence of non-tariff barriers, Domestic and international market access for goods, Domestic and international market access for services, Margin of preference in destination markets, Trade-weighted average tariff faced in destination markets, Ease of establishing an electricity connection, Gross fixed water assets, Installed electric capacity, Reliability of electricity supply, Reliability of water supply, Water production, Airport connectivity, Efficiency of seaport services, Liner shipping connectivity, Logistics performance, Quality of roads, Rail density, Road density, Black carbon emissions, CO2 emissions, Methane emissions, NOx emissions, SO2 emissions, Exposure to fine particulate matter, Health impact of air pollution, Satisfaction with air quality, Flood occurrence, Forest area, Sustainable nitrogen management, Freshwater withdrawal, Renewable water resources, Satisfaction with water quality, Wastewater treatment, Long term management of forest areas, Pesticide regulation, Protection for biodiverse areas, Satisfaction with preservation efforts, Terrestrial protected areas, Equal treatment and absence of discrimination, Freedom from hiring and workplace discrimination, Freedom of belief and religion, Government religious intimidation and hostility, LGBT Rights, Non-discriminatory civil justice, Protection of women's workplace, education and family rights, Due process and rights, Freedom from arbitrary interference with privacy, Freedom from forced labour, Freedom of movement, Government response to slavery, Personal autonomy and individual rights, Satisfaction with freedom, Women's agency, Autonomy from the state, Guarantee of assembly and association, Right to associate and organise, Alternative sources of information, Freedom of opinion and expression, Government media censorship, Political diversity of media perspectives, Press freedom from government censorship, Press freedom from physical repression, Perceived tolerance of LGBT individuals, Perceived tolerance of ethnic minorities, Perceived tolerance of immigrants, Disappearance cases, Extrajudicial killings, Political imprisonment, Political terror, Use of torture, Business costs of crime and violence, Business costs of organized crime, Property stolen, Property cost of terrorism, Terrorism deaths, Terrorism incidents, Terrorism injuries, Dispute settlement through violence, Intentional homicides, Physical security of women, Safety walking alone at night, Civil and ethnic war, Conflict-driven internal displacement, One-sided conflict deaths, Refugees (origin country), Two-sided conflict deaths, Donated money to charity, Voiced opinion to a public official, Volunteering, Voter turnout, Confidence in financial institutions and banks, Confidence in judicial system and courts, Confidence in local police, Confidence in military, Confidence in national government, Public trust in politicians, Generalised interpersonal trust, Helped a stranger, Family give positive energy, Help from family and friends when in trouble, Helped another household, Opportunity to make friends, Respect\n""], 'url_profile': 'https://github.com/sciocca', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '1,249 contributions\n        in the last year', 'description': ['About this Project\nPerform simple prediction of world population using linear regression using different data science tools\nTarget\nLearn complete data science workflow and how to integrate different data science tools in the workflow\nData\nData includes the population of 264 countries\n\nfrom 1960 to 1990\nfrom 1991 to 2018\n\nTools\n\nAmazon Relational Database Service (Amazon RDS): tool to set up, operate, and scale MySQL deployments in the cloud\nAmazon Elastic Compute Cloud (Amazon EC2): tool to provide secure, resizable compute capacity in the cloud\nMySQL: tool to create table and store database\nTalend ETL: tool to load data into MySQL\nTableau: tool to visualize data\n\nProject Workflow\n\nSteps\n\n\nSetup:\n\nLaunching a MySQL database instance in my AWS account\nLaunching an EC2 instance\nConnect to EC2 instance via SSH\nConnect to the databsase in AWS from my computer (via EC2) to interact with it using SQL\nConect Talend ETL to AWS RDS\nUpload data to Talend ETL then transfer the data to AWS RDS\nConnect MySQL to Python with PyMySQL\nIntegrate 2 data and create prediction file in Python\nLoad the data on Talend ETL then transfer the data to AWS RDS\nConnect Tableau to AWS RDS and visualize\n\nFind more instructions about the setup here and here\n\n\nPreprocessing and Model Training in Python\n_The notebook can be found here\n\nPreprocessing:\n\nFill in missing values with fill forward method\n\n\nVisualization:\n\nUse Plotly and interact method of ipywidgets to visualize the data\n\n\n\nPrediction:\n\nSince the population trends of many countries are linear and for the simple purpose of practicing with the tools, I with linear regression to predict the population from 2019 to 2021\nScore: 0.994\nFind the prediction csv file here\n\n\n\n\n\n'], 'url_profile': 'https://github.com/khuyentran1401', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': [""Capital-Asset-Pricing-Model\nRun linear regression between your benchmark and preferred stock.\nCAPITAL ASSET PRICING MODEL USING LINEAR REGRESSION\nE(Ri)=Rf+βi[E(Rm)−Rf)]\nwhere\nE(Ri)  -  is the expected returnn of the asset.\nRf  - is the risk-free asset, typically a US government bond. #assumed zero\nβi  - is the sensitivity of the expected excess asset returns\nto the expected market returns.\nE(Rm)−Rf -  is the considered the risk premium.\nThe following script allows you to run regression\nbetween a benchamrk index such as S&P500 and a stock.\n#Call the capm class as follows:\ncapm(benckmark ticker,stock ticker, period)\n    eg: capm('^GSPC','MSFT','500d')\n    \n#Output\n    Regression graph and the regression statistics summary\n        beta, alpha, R value, p value and the standard error\n\n#Additionally you can call the cummReturnPlot() method\nto plot cummulative return graph.\neg: capm('^GSPC','MSFT','500d).cummReturnPlot()\n\n""], 'url_profile': 'https://github.com/Harishangaran', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zuodi94', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['r-ml-climate\nRegression done on Average Land Temperature in R\n'], 'url_profile': 'https://github.com/AnshumGill', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '181 contributions\n        in the last year', 'description': ['Linear-regression-Insurance-dataset\nPerforming linear and polynomial regression on insurance dataset from Kaggle\nWhy this project?\nI am currently working through the data science course on Udemy and I will like to have some hands-on practice so I picked this project. This is my first time building a machine learning model and I am glad to see great results after numerous error!\nDataset\nYou can download the dataset here(54 KB).\nContributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\nLicense\nMIT\n'], 'url_profile': 'https://github.com/kianweelee', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Introduction\nThis project has the purpose to assist developing a system to be deployed on large industrial campuses, in shopping malls, et cetera to help people to navigate a complex, unfamiliar interior space without getting lost. While GPS works fairly reliably outdoors, it generally doesn\'t work indoors, so a different technology is necessary.\nThe feasibility of using ""wifi fingerprinting"" is investigated to determine a person\'s location in indoor spaces. Wifi fingerprinting uses the signals from multiple wifi hotspots within the building to determine location, analogously to how GPS uses satellite signals. We have been provided with a large database of wifi fingerprints for a multi-building industrial campus with a location (building, floor, and location ID) associated with each fingerprint.\nThis task also includes evaluations of multiple machine learning models to evaluate which produces the best result.\n1. Loading the data and EDA\nLoad the Indoor Locationing Dataset (http://archive.ics.uci.edu/ml/datasets/UJIIndoorLoc) into RStudio and study the data documentation.\nDataset Info\n\nThis UJIIndoorLoc database was stored in UCI machining learning Repository.\nThe database covers three buildings of Universitat Jaume I with 4 or more floors and almost 110m2 in Valencia, Spain. It was created in 2013 by means of more than 20 different users and 25 Android devices.\nThe database of wifi fingerprints for a multi-building industrial campus with a location (building, floor, and location ID) associated with each fingerprint.\nThe database consists of 19937 training records and 1111 validation/test records.\n\nAttributes include:\n\nWAP001- WAP520: Intensity value for WAP. Negative integer values from -104 to 0 and +100. Positive value 100 used if any WAP was not detected.\n9 Position-related attributes:\nFLOOR, BUILDINGID, SPACEID, RELATIVEPOSITION\nLONGITUDE, LATITUDE\nUSERID, PHONEID, TIMESTAMP\n\nThe 529 attributes contain the WiFi fingerprint, the coordinates where it was taken, and other useful information.\nMap of the UJI Riu Sec Campus where wifi signals were collected\n\nPreliminary exploration was performed as part of familiarization process.\nThe distribution and occurrences of WAPs\n \n\n3D-mapping the footprints\n \n2. Data cleaning\nA few actions have been taken in terms of data cleaning and transformation:\n\nremove duplicate (non-unique) observations\nthe training and validation data sets were combined together to speed up the transformation process\nchange certain variables\'data type to appropriate ones\nchange the value of RSSI = 100 to -110\nremove those ""near-zero variance"" predictors and registers (including those columns that contain only constants and rows with no variance)\nremove variables that have little values in terms of validating models/data\nsplit data before modelling\n\n3. Decision on ML approach - classificaiton or regression?\nThe dataset is very large, so a key part of the process involves defining an approach to sampling the data. Decisions were made to use:\n\nclassification models to predict building ID and floor numbers\nregression models to predict longitude and latitude values by each building\n\nParallel processing was set up.\n4. Predicting building ID\nKNN, Random Forest and Decision Tree C5.0 were deployed. Results can ben seen in below:\n\nThere is large amount of information that independent variables carry in this dataset, which can well explain why all models have very good results in terms of predicting Building IDs.\n5. Predicting floor numbers\nRandom Forest and KNN algorithms have been employed in predicting floor numbers. In predicting floor numbers, the data was subset by each building. Nevertheless, an attempt has also been made to use all data without subsetting by building ID. The latter approach showed a slightly better result when predicting floor numbers in the Building 1 as shown in below group of confusion matrices.\nConfusion matrices of predicting floor numbers - using Random Forest\nConfusion Matrix - subsetting data by building IDs (BLD0, BLD1, BLD2)\n  \nConfusion Matrix - without subsetting data\n\nFurther investigation should be taken in understanding why the prediction of floor numbers in Building 1 has much worse results than those in the other two buildings.\nConfusion matrices of predicting floor numbers - using KNN\nConfusion Matrix - subsetting data by building IDs (BLD0, BLD1, BLD2)\n  \nConfusion Matrix - without subsetting data\n\n6. Predicting latitude and longitude values\nRandom Forest regression algorithm and KNN regression algorithm were deployed in predicting latitude and longitude values. In an earlier attempt, both variables were included in the training and the validation dataset. When examining the variable importance, both variables ranked as the most important variable when predicting the other. Given the context that this task is to predict latitude/longitude in neighboring buildings, each latitude value has only a relatively small range of longitude values. This may lead to the high importance score they appear to each other. Therefore, latitude and longitude values were taken out in predicting longitude and latitude.\nThe system processing time: RF - on average between 30m to 1h and KNN took much shorter time. By releveling floor number, improved results in R-squared were achieved.\n6.1 Random Forest - predicting longitude\nPredicted vs. Actual Longitude Values - Random Forest Algorithm (BLD0, BLD1, BLD2)\n  \nList of important variables in predicting longitude values (Random Forest)\n  \n6.2 Random Forest - predicting latitude\nPredicted vs. Actual Longitude Values - Random Forest Algorithm (BLD0, BLD1, BLD2)\n  \nList of important variables in predicting latitude values (Random Forest)\n  \n6.3 KNN - predicting longitude\nPredicted vs. Actual Longitude Values - KNN Algorithm (BLD0, BLD1, BLD2)\n  \n6.4 KNN - predicting latitude\nPredicted vs. Actual Longitude Values - KNN Algorithm (BLD0, BLD1, BLD2)\n  \n7. Summary\nThe algorithm to be best for this data: Random Forest.\n\nRecommendations in how to improve results\n\n\nFurther investigations should be take to look at how to achieve better results in predicting floor numbers in Building 1\n\n\nLatitude and longitude prediction for Building 2: BLD 2’s data are disproportionate – more than 9000 observations in training dataset, but only around 200 in the validation dataset.\n\n\nRandom forest algorithm performed better in terms of accuracy and errors:\n\nless false prediction in floor numbers\nlatitude and longitude “combined” MAEs\n\n\n\n\nIn terms of running time:\n\nKNN was in general faster – when RF models took on average 30mins above to run each one, KNN needed 15-30 mins to run.\nsome specific models were super faster and only took a few seconds to get results.\nIn addition, RF\'s mtry were set to be around 160 and 200 when predicting latitude and longitude with Random Forest regression, which could lead to model overfit – this should be investigated when time allows.\n\n\n\n8. Learning experience to share\nGiven that this project is part of the learning process, results and processes are not perfect due to time and resources limitation\nConsideration/further research and learning. I also share some learning experiece as below:\n\nOmitted variables – how can these variables impact the prediction when included: SPACEID, Relative position, User ID, etc.\nInvestigating the potential over-fit of Random Forest algorithm\nRunning time to be considered\nCertain problem: e.g. why the errors of floor prediction is high in Building 1\nIt is a process of running and readjustiung – e.g. removing latitude, decision of how to subset data, etc.\n\n'], 'url_profile': 'https://github.com/Minjuan06', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Aug 25, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '1', 'R', 'Updated May 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['rain-prediction-using-logistic-regression-and-knn\n'], 'url_profile': 'https://github.com/Litaa', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'Montreal', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['ayaartay\nThis is Assignment #1 for Stanford Machine Learning course by Andrew Ng.\nThe course is taught using Matlab and Octave and the assignments are expected to be completed using those programs. However, as I am using Python as my primary programming language, I have decided to do the exercises in Python.\nIn this assignment I run simple linear regression with one variable using gradient descent. A restaurant chain has trucks in various cities and and wants to determine the truck profit function given population of a city. The chain already has data for profits (column 2) and city populations (column 1), which is given in the file ex1data1.txt.\n'], 'url_profile': 'https://github.com/ayaartay', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'Cairo, Egypt', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': [""LinearRegression\nThis repository includes exercises/projects for Linear Regression in python\nNote for the file to work you have to download the relative csv file as well\n\n\nauto mpg dataset is used to predict cars' mile per gallon of gas by using various indicators (multivariate multi linear regression)\nThe dataset details can be found in https://www.kaggle.com/uciml/autompg-dataset\n\n\na simple salary forecaster given number of years of experience. this is a simple one feature dataset. (univariate simple linear regression).\n\n\n""], 'url_profile': 'https://github.com/aaahmed', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'Philadelphia', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': [""Harvard-Business-Case-Study-on-Ship-Valuation\nOverview :\nCompass Maritime Services offers consulting services for clients who are interested in purchasing and selling maritime ships and offshore vessels. Basil Karatzas, director for projects and finance will help the client who is interested in purchasing a ship named BetPerformer, to determine the valuation and negotiation strategy of the ship. This report will show methodology, valuation analysis, concerns, best value and also suggest the bid price for the ship. The value accounts for all variables provided and gives the client the best chance to secure Bet Performer for its current market value.\nBet Performer:\nBet Performer is a bulk carrier with 172000 DWT and 12479 Capesize index. The ship is 11 years old and was built in Japan in 1997. This ship has a MAN-B&W engine with an engine power of 14710 KW and has nine holds and hatches. The same ship was sold two years back under a different name Mineral Poterne for $70 million.\nVariables:\nData consists of recent Sale Date, Vessel Name, Price, Sale Year, Year Built, DWT and Capesize. The weight of these ships is measured in deadweight tons and is the sum of cargo, fuel, fresh water, passengers and crew weights. DWT for Capesize is greater than 100000. The baltic dry index calculates multiple shipping costs for different raw materials among various routes.Year built and Age at Sale represents the same factor. Age at Sale is the best predictor of the price when compared to other variables owing to the highest correlation. DWT and Capesize also has a significant correlation with the price of the ship and can affect the price. Sale Date has the least correlation and lowest influence on the price. In addition to the data collected about similar ships factors such as main engine type, repairs, building company reputation, charter contracts with counter-parties, loading equipment, shipyard and location of the ship at the time sale also influence the price of the ship.\nCorrelation:\nThe below matrix shows the correlation between all the variables in the data set.\n\nCorplot:\n\nVariable Selction:\nBased on the correlation we have removed the two variables SaleDate and YearBuilt.\nLinear Model:\nWe fit a linear model to predict the price of the ship by using each of the remaining variables individually.However, this approach of fitting a separate simple linear regression model for each predictor is not entirely satisfactory. First of all, it is unclear how to make a single prediction of Price given levels of the three variables Age at Sale, Deadweight tonnage (DWT) and Capesize Index, since each of the variables is associated with a separate regression equation. Second, each of the three regression equations ignores the other two variables in forming estimates for the regression coefficients. If these independent variables are correlated with each other in the ship data, then this can lead to very misleading estimates of the individual effects on Price. Instead of fitting a separate simple linear regression model for each predictor, a better approach is to directly accommodate multiple predictors.\n\n\n\nWe moved on to fit the linear model to predict with all the three variables together.\nModel Summary:\n\n92 % of the variation in the sale price of ships could be explained by Age, DWT, and Capesize.\nWe went ahead and used this model to predict the price of the Bet Performer. The predicted price as per multiple linear regression came out to be $125.83 M.\nThe confidence interval for the prediction is between 118.8899 and 132.7692\nFurther Analysis:\nAnalyzing the whole dataset, there are certain observations to be noticed. We plotted the histogram for the original price for all the 48 ships which were studied.\n\nThe dataset was segregated into two buckets, one has a price less than $100 M and the other having price greater than $100 M. For the first set of ships having majority of ships (85%), the average of prediction error was 0.8 (Negative) which signifies that the model worked accurately for the 85 % of the ships and the predicted price was overpriced just by 0.8 M. The results were startling for the other set of ships which accounts for 15 % of dataset . The predicted price deviated a lot from the original price. This accounts because there are very few ships having a price greater than $ 100M. The ships were underpriced by $6.36 M at an average.\nThe above inference was considered to adjust the underpriced prediction. The final price predicted for Bet Performer as per our case study will be $ 132.19 M.\nConclusion:\nThe purpose of analysis was to come up with the predicted price of Bet Performer which would fall under both the client and the seller's satisfaction. We did a detailed study using the Market Approach as the base and came to a final price of $ 132.19M for the Bet Performer which we think will be the amount to bid and successfully buy the bulk carrier ship which the client had interest in.\n""], 'url_profile': 'https://github.com/HemachandarN', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['Rgression-Model-Course-Project\nRegression Model Course Project - Johns Hopkins University - Coursera\n'], 'url_profile': 'https://github.com/girishtere', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Logistic Regression Neural Network\nThis repository contains a Neural Network implementation of\nLogistic Regression Binary Classifier.\nThe library implements APIs to train the model, and use the model to predict output classes on newly seen examples. The Logistic Regression model is suitable for binary classification tasks, where data has to be classified into two separate and distinct groups, such as tagging images that contain dogs or not. Despite the Neural Network approach, this model is not capable of reaching high quality accuracy results of complex Neural Networks with several hidden layers, as this model represents essentially a single layer Neural Network\nPrerequisites\nThe library is written in Python3, and requires pip to install third party packages.\nAll the third party packages are listed in the requirements.txt file. If you wish to use the model alone without the example application, then you only need to install Numpy. All other requirements are only needed to run the example application\nUsage\nHere we show how the use Logistic Regression model to train a new model and use the trained model to predict new classification in Python. The only requirement to be installed to use the model is Numpy for matrix operations\nTrain the Model\nfrom model.logistic_regression import model\n\ntrained_model = model(x_train, y_train, x_test, y_test, iteration=2000, learning_rate=0.005, threshold=0.5, print_cost=True)\n\nprint(""Here is the prediction on the 10th test example: {}"".format(trained_model[""prediction_test""][0][9]))\nThe input of the function call is:\n\nx_train: a (n, m) numpy vector, where m is the number of training examples and n is the number of features. In this matrix each column is a single training example with n features\ny_train: a (1, m) numpy vector where each element is either 0 or 1, representing the truth classification for each training example\nx_test: a (n, m\') numpy vector. Similar to x_train, n is the number of features and m\' is the number of test examples. Each column in this matrix is one test example. This matrix can also be empty\ny_test: a (1, m\') numpy vector where each element is either 0 or 1, representing the truth classification for each training example\niteration: number of training iterations to be performed (integer)\nlearning_rate: learning rate parameter  (float)\nthreshold: classification threshold (float  )\nprint_cost: if set to True, then the value of the cost function will be printed every 100th iterations\n\nThe output of the function is a dictionary containing the following elements:\n\nlearning_rate: same as above\niteration: same as above\nthreshold: same as above\ncosts: a list containing all cost function values computed during training every 100th iteration\nw: a (n,1) numpy vector, containing the values of the parameter w with n features\nb: real number parameter b\nprediction_train: a (1,m) vector where each element is either 0 or 1, representing the output classification of the model for each training example\nprediction_test: a (1,m\') vector where each element is either 0 or 1, representing the output classification of the model for each test example\n\nUse the trained Model\nAfter having trained the model and computed parameters w and b, you can use the model to make predictions on new data as follows:\nfrom model.logistic_regression import model\nfrom model.logistic_regression import predict\n\ntrained_model = model(x_train, y_train, x_test, y_test, iteration=2000, learning_rate=0.005, threshold=0.5, print_cost=True)\n\nw = trained_model[""w""]\nb = trained_model[""b""]\n\ny_out = predict(x_new, w, b, threshold=0.5)\nThe input of the function call is:\n\nx_new: a (n, m\'\') numpy vector, where m\'\' is the number of new examples to be predicted and n is the number of features. In this matrix each column is a single new example with n features to be predicted\nw: the parameter w built after training the model\nb: the parameter b built after training the model\nthreshold: classification threshold\n\nThe output of the call is a (1, m\'\') numpy vector where each element is either 0 or 1, representing the output classification of the model for each new example.\nHave a look at the example application to see the model trained and used in full fashion\nExample Application\nThe repository contains an application of the model for the famous hot-hog vs. not hot-dog app.\nGiven an image, the model will be able to predict whether the image contains a hot-dog or not.\nInstall Requirements\nThe application, other than Numpy, requires a few other libraries to convert images to vectors and to plot functions. You can install all the requirements by running:\n$ pip install -r requirements.txt\nBest to do so in a virtual environment.\nRun the application\nBrowse to application.py and change variable base_folder to the folder path where you downloaded the hot-dog dataset, and variable new_image_path to the file path of an image to be predicted either as hot-dog or not hot-dog.\nAfter changing the variable values, run the following command:\n$ python3 -m hot_dog.application\nThis will run the application, which will first train the models on all the examples of the downloaded dataset, and then will use the model to predict whether the chosen image contains a hot-dog or not. At the end of the run, the application will first show the chosen image with the predicted value, and will also plot the variation of the cost function value every 100th training iteration.\nYou will see that the output model most likely has over-fitted the training data, with a ~99% accuracy on the training set and only ~52% accuracy on the test set -- but hey, this is just a simple Logistic Regression model, it cannot achieve same quality of a deep Neural Network 😅\nChangelog\n\nTODO\n\nContacts\nStefano Ortona - stefano dot ortona at gmail.com\n'], 'url_profile': 'https://github.com/stefano-ortona', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'United States', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Restaurant--Predictive_Analytics\nData - Zomato Bangalore\nThe data consists of the details corresponding to the restaurant. It is unique at a restaurant level and contain information of the details present on the website corresponding to the restaurant.\nThe objective of the project was to get accustoned to Python programming and was a final prject for the class Introduction to Python.\nAnalysis done for the project-\nMarket Benchmarking for restaurant discovery and search platform and Recommend a strategy to identify and benchmark success parameter for restaurant. Also, developed personalized promotional strategies using K-means clustering\n'], 'url_profile': 'https://github.com/pranay7196', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}","{'location': 'Paris', 'stats_list': [], 'contributions': '1,681 contributions\n        in the last year', 'description': ['\ntablelight\n\n\n\n\nThe goal of tablelight is to propose functions to generate regression\ntables using as little as possible computer resources, especially RAM.\nThis package has been designed because stargazer requires a huge\namount of RAM to produce tables with voluminous data.\nThe basic idea is to strip regression objects from unnecessary fat and\nuse the lightened object to produce a regression table. The package\ncontains two sets of functions:\n\nstrip function: a set of methods to remove heavier elements from a\nregression objects ;\nlight_table function: a function to produce LaTeX tables (HTML\ntables in a future version).\n\nThe package is organized around a set of methods to extract information\nfrom regression objects. The list of regression objects accepted is, for\nthe moment: lm, glm, negbin, oglmx, zeroinfl. Other types of\nregression objects will be added in the future.\nTo install this package, you can do\ndevtools::install_github(""linogaliana/tablelight"")\nIf you want to use the 📦\nlibrary(tablelight)\nHere some examples of regression tables that can be produced with\ntablelight: \nWhy do you need to strip fat from models ?\nIt is well known that regression objects are heavy in R (see this\nblog\npost\nthat inspired this package). For instance, the following regression\nobject is about 10 times heavier than the initial data:\ndf <- data.frame(\n  x = rnorm(1e6),\n  y = rnorm(1e6)\n)\npryr::object_size(df)\n#> 16 MB\n\nregression <- lm(y ~ x, df)\npryr::object_size(regression)\n#> 136 MB\nProducing a regression table with stargazer requires to call summary\nthat asks more RAM:\nlibrary(profvis)\nget_required_RAM <- function(profvis_object){\n  return(\n    max(profvis_object$x$message$prof$memalloc) - min(profvis_object$x$message$prof$memalloc)\n  )\n}\nget_required_RAM(profvis(summary(regression)))\n#> [1] 30.65924\nTo produce a regression table, that’s a deadly combo: you need to store\na heavy regression object in memory and need more memory to summarize it\nin order to produce the table. With voluminous data, it is easy to make\nyour RAM hit the limit available. The idea behind tablelight is that\nyou just need heavy elements once (to produce standard error values and\nsome fit statistics). Once they have been used, heavy elements can be\nthrown away.\nMost of the pieces to lighten regression objects are described\nthere.\nHowever, if you want to use stargazer later on, this will be\nimpossible. You lose elements required to be able to run summary and\nhence produce the result table. A solution has been proposed by the\nstrip package (here): stripping\nthe object selectively. Unfortunately, this method is only available for\nlm objects and the weight loss is around 30%\n# install.packages(\'strip\')\npryr::object_size(strip::strip(regression, keep = ""summary""))\n#> 104 MB\nThe strip method used in tablelight 📦 is more drastic:\npryr::object_size(tablelight::strip(regression))\n#> 7.34 kB\nOnly the elements needed to print a result table are kept. Since using\nstargazer is no longer possible on a lightened model, tablelight\n📦 proposes a function (tablelight::light_table) to produce\nnice tables.\nSome examples\nGeneral case\nLet’s say you want to produce a regression table from two objects. For\nthe moment, the freedom in customizing the table is limited but future\nenhancements will add more and more flexibility.\ndf <- data.frame(\n  x = rnorm(1e6),\n  y = rnorm(1e6)\n)\ndf2 <- data.frame(\n  x = rnorm(1e6),\n  y = rnorm(1e6)\n)\n\nregression1 <- lm(y ~ x, df)\nregression2 <- lm(y ~ x, df2)\n\nget_required_RAM(profvis(\n  capture.output(stargazer::stargazer(regression1, regression2)))\n)\n#> [1] 0\nWith tablelight, you will :\n\nStrip an object using strip function ;\nUse light_table to produce the output. Models should be provided\nas a list:\n\nregression1 <- tablelight::strip(lm(y ~ x, df))\nregression2 <- tablelight::strip(lm(y ~ x, df2))\n\nget_required_RAM(profvis(\n  capture.output(light_table(list(regression1, regression2))))\n)\n#> [1] 0\nThis is, approximatively, NaN times less memory needed.\nThe package produces table very similar in appearance with stargazer,\nfor instance:\n\nSpecificity with zero inflated models\nIt is hard to put together selection and outcome equations in a\nzero-inflated model with stargazer. Normally, you need to chose\nbetween reporting selection or outcome related terms. Sometimes you want\nto report both together. Unless using a hack with stargazer (see\nhere),\nthis is not possible with stargazer.\nThis functionality has been integrated into light_table function. For\ninstance, imagine you want to report both selection and outcome\nequations for a zero-inflated Poisson:\ndata(""bioChemists"", package = ""pscl"")\nfm_zip <- strip(pscl::zeroinfl(art ~ . | ., data = bioChemists))\nfm_zinb <- strip(pscl::zeroinfl(art ~ . | ., data = bioChemists, dist = ""negbin""))\nIn that case, you will use\nlight_table(list(fm_zip, fm_zip),\n            type = ""html"", \n            modeltype = c(""selection"",""outcome""),\n            dep.var.labels = c(""Selection"",""Outcome""),\n            stats.var.separate = 2L)\nstats.var.separate is not compulsory but it’s nicer to get\nmulticolumned performance statistics rather than two times the same.\nIf you want to compare zero-inflated Poisson and zero-inflated negative\nbinomial models, you can use the following template:\n\nRegression tables for multinomial logit\nAnother implemented feature is the possibility to put together\nmodulaties of a Y variable in a multinomial logit\n\n'], 'url_profile': 'https://github.com/linogaliana', 'info_list': ['Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Nov 3, 2020', 'HTML', 'Updated Apr 7, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Jan 22, 2021']}"
"{'location': 'New York, New York', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': [' Project 3: NLP Binary Classification\nProblem Statement\nI am moderator for r/Biology and r/ Microbiology, someone hacked my subreddits and combined them into one. I need to build a model ASAP that will help me differeniate between the subreddit posts. Once complete, I can begin to organize the mess that the hacker created! Luckily, I have some back up data from both subreddits that I can use to train my models with. Since this is a NLP binary classification problem, I will try out Logistic and Naive Bayes models with accuracy as my metric.\n\nExecutive Summary\nI started out with two separate data frames one for each subreddit. I concatenated them then preceded to binarize the subreddit column according to their respective subreddit. r/Biology was mapped as 1 and r/ Microbiology was mapped as 0.Data cleaning was minimal because Count Vectorizer essentially does it for you. Exploratory data analysis was kicked off by utilizing Count Vectorizer on the combined DataFrame. I sorted out the top ten words and utilized this to base my stop word customization off of. Then did some masking to put the binary subreddits into their own data frames. I then sorted out the top words using Count Vectorizer as well and further customized my stop words. I gathered that since microbiology is a sub field of biology they would share a lot over cross over terminology. This proved accurate, they shared common terms but many of which were due the fact Reddit is a forum based community. They get a lot of people asking for help and/ or advice in these fields, which would explain the rest of the cross over words. They ended up sharing 70% of the same top ten words which was surprising to me. Another key finding was that cell and cells were under biology\'s top ten instead of micro. Which I intuitively thought would go to microbiology.\nNext up was selecting my X and Y variables. I chose the ""selftext"" column as my X because it allows for more data to be utilized, there are more words in self text than title. I picked my ""binary_class"" columns as my Y because it is binarized according to subreddit. I chose four models Tfidif Vectorizer & Logistic Regression, CountVectorizer & Logistic Regression, Multinomial Naive Bayes & CountVectorizer and finally the Gaussian Naive Bayes & Tfidf Vectorizer Model. I did a grid search over several iterations of parameters and ultimately input the parameters from ""best_params"" into my models. I compared my scores to the baseline model of .514 that would always default to predicting biology, they all performed at least .25 better than it. None of my models were over or under fit. The Highest performing model was Tfidf Vectorizer & Logistic Regression so I chose this one. After model selection I evaluated my model by graphing a confusion matrix and manually calculated the accuracy score of .80. My last evaluation was to graph  and interpret the ROC Curve. My AUROC was better than the baseline and the ROC curve was closer to one than the baseline, so it safe to say this is a decent model. I then tested out my model a couple times and it predicted accurately!!!\nDatasets\n\nsubreddit_microbiology\nsubreddit_biology\n\nData Dictionary\n\n\n\nFeature\nType\nDescription\n\n\n\n\ntitle\nobject\npost title\n\n\nselftext\nobject\npost text\n\n\nsubreddit\nobject\nsubreddit thread\n\n\ncreated_utc\nint\ntime\n\n\nauthor\nobject\nauthor\n\n\nnum_comments\nint\nnumber of comments\n\n\nscore\nint\nscore\n\n\nis_self\nbool\nis this the original post or a repost\n\n\ntimestamp\nobject\ntime\n\n\nbinary_class\nint\nbinarized subreddits bio = 1 & micro = 0\n\n\n\nConclusions\nThe selected TfidfVectorizer & Logistic Regression model has an accuracy rate of .80. This is good enough to help solve my problem of subreddit differeniation. I do not need a super accurate model and since I am in a time crunch this should suffice. I just need something good enough to get the job done. I can optimize my model more in my spare time.\nRecommendations\nNow that I have a model ready to be put into action, the next step would be to build a function that would  automate the following. Scrape the posts from the combined subreddits, pass them into my predictive model, then separate them into two data frames based on their respective classes. I would give then the data to reddit so that they could put the posts back where they belong\nMy model can be improved by the implementing the following. I would not have dropped several the columns before filtering for removed or deleted rows. I ended up deleting posts that could have upped my score because they may have had applicable data in them. I could have also tried to pull out some of the common stop words to see how that would have affected my model.I would also use regex, stemming and lemmatizing to see which one worked better also do some feature engineering. Lastly I could try out more models to see if I could get a better score.\nReferences\nr/ Biology post utilized in my predictive model\nr/ Microbiology post utilized in my predictive model\n'], 'url_profile': 'https://github.com/amyfbear', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'Atlanta, Ga', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['N741_Spring2020_regression_demo\nN741 Spring2020 - regression demo\nView Slides in PDF\n\nPDF of Modeling and Prediction Slides\nThese slides are also provided in PPT format which can be downloaded and viewed offline\n\nView lessons in HTML pages:\n\nPrediction\nLinear Regression\nLogistic Regression\n\n'], 'url_profile': 'https://github.com/melindahiggins2000', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'Buenos Aires', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': [""Buenos Aires City Real Estate Dataset Regression\nThis project is a part of the Acamica Institute in Buenos Aires.\n-- Project Status: [Completed]\nProject Intro/Objective\nThe purpose of this project is to do a regression in order to predict property prices. This is a continuation of the Transformation project which can be found here.\nMethods Used\n\nInferential Statistics\nData Visualization\nPredictive Modeling\nMachine Learning\n\nTechnologies\n\nPython\nNumpy, Pandas, Matplotlib, Seaborn, Scikit-learn\n\nProject Description\nThe dataset we'll be working with was extracted from Properati, just like the previus project. The objective here is to predict property prices in USD using the attributes from our dataset. In order to do that, a KNN Regressor and a Decision Tree Regressor will be trained. A major challenge in this project is to get a high R^2 since our dataset was not very clean from the beginning. Besides, some prices are not very accurate.\nNeeds of this project\n\nfrontend developers\ndata exploration/descriptive statistics\ndata processing/cleaning\nstatistical modeling\nwriteup/reporting\n\nGetting Started\n\nClone this repo (for help see this tutorial).\nRaw Data is being kept [here](Repo folder containing raw data) within this repo.\nData processing/transformation scripts are being kept [here](Repo folder containing data processing scripts/notebooks)\nFollow setup [instructions](Link to file)\n\n""], 'url_profile': 'https://github.com/franalderete7', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '255 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mandal-21', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/Abilash-Git', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['logistic_regression\n'], 'url_profile': 'https://github.com/pelusok', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'Villahermosa', 'stats_list': [], 'contributions': '444 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danyduran', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': [""Regression-Analysis\nRegression Analysis on Soccer Players' weekly wages based on their features/attributes pulled from FIFA\n""], 'url_profile': 'https://github.com/mik092091', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'Patna', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['Polynomial-Regression\n'], 'url_profile': 'https://github.com/Sonalikhasyap15', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}","{'location': 'Durham, NC', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['atp-regression\nSingle feature, two feature, and multiple feature linear regressions on data from the Association of Tennis Professionals\n'], 'url_profile': 'https://github.com/griffinmalm', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Updated Mar 31, 2020']}"
"{'location': 'Accra, Ghana', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bamtak', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Polynomial-Regression\n'], 'url_profile': 'https://github.com/FahadTComsats', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'Patna', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Sonalikhasyap15', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cperezh', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': [""basic_model_regression\nR script to compare two different regression models with one variable and one label\n\nTo select a good model with only one variable check:\n\n\nF-statistic and p-value: F-statistic needs to be large, depending of n. for large n, F can be slightly more that 1, else it needs to be very large. The pvalue gives the significance of Fstatistic given n\n\n\nR2 : the fraction of variance explained. In a simple model y ~x is the correlation between x,y. Adjusted R2 is used for polynomial functions.\n\n\nRSE : RSE = sqr(RSS/(n-2)), where RSS is residual sum of squares. Is roughly how much your estimate deviates from the true regression line.\nIt is an absolute measure on how well the model fits the data.\n\n\nCheck the residual plot: shouldn't have any shape and the points should all be close to the 0 horizontal line. If the residuals show a 'funnel shape',\na transformation for the data is reccomended - log transform is the classical. Also outliers can show in the plot (leverage points)\n\n\nMAE: mean absolute error - to be looked at in the test set : cross validation\n\n\nRMSE: root mean squared error - to be looked at in the test set : cross validation\n\n\nBootstrapping : estimate the uncertainty of the coefficient and the model\n\n\nIn model selection you first define the model to test (linear should always be the gold standard)\nFirst thing always look at your data!\n\nLook at the F-statistic, R2 and RSE that give you a statistical measure on how well the model fit the data\nLook at the statistical test to asses if y and x have a relationship\nLook at the diagnostic plots to identify biases and acuracy\nLook at the RMSE, R2 and MAE in the test set via cross validation\nLook at the train error and uncertainty of your coefficients with bootstrapping (at the end of the script!)\n\n""], 'url_profile': 'https://github.com/JulietRennet', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AadityaRathod97', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'Láng Hạ, Thanh Xuân, Hà Nội', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HaQuocHuyPtit', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'Stuttgart', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Logistic Regression\n\n'], 'url_profile': 'https://github.com/MichaelStrommer', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/layacheadeth', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Linear-Regression\nModel\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn import model_selection\nfrom sklearn import linear_model\ndata = pd.read_csv(""student-mat.csv"", sep="";"")\ndata = data[[""G1"",""G2"",""G3"",""studytime"",""failures"", ""absences""]]\npredict = ""G3""\nx = np.array(data.drop([predict],1))\ny = np.array(data[predict])\nx first and then y\nx_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.1)\nlinear = linear_model.LinearRegression()\nlinear.fit(x_train, y_train)\nacc = linear.score(x_test, y_test)\nprint(acc)\nprint(""Co: \\n"", + linear.coef_)\nprint(\'Interpret: \\n\', linear.intercept_)\npredictions = linear.predict(x_test)\nstr + str\nfor x in range(len(predictions)):\nprint(\'we predict the grade is: \' + str(predictions[x]) + \'\\n\', \'we have student information: \' + str(x_test[x]) + \'\\n\', \'The true grade is: \' + str(y_test[x]) + \'\\n\')\n'], 'url_profile': 'https://github.com/Hannah171', 'info_list': ['Updated Mar 31, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Aug 22, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 5, 2020', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '816 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/1zuu', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': [""basic_model_regression\nR script to compare two different regression models with one variable and one label\n\nTo select a good model with only one variable check:\n\n\nF-statistic and p-value: F-statistic needs to be large, depending of n. for large n, F can be slightly more that 1, else it needs to be very large. The pvalue gives the significance of Fstatistic given n\n\n\nR2 : the fraction of variance explained. In a simple model y ~x is the correlation between x,y. Adjusted R2 is used for polynomial functions.\n\n\nRSE : RSE = sqr(RSS/(n-2)), where RSS is residual sum of squares. Is roughly how much your estimate deviates from the true regression line.\nIt is an absolute measure on how well the model fits the data.\n\n\nCheck the residual plot: shouldn't have any shape and the points should all be close to the 0 horizontal line. If the residuals show a 'funnel shape',\na transformation for the data is reccomended - log transform is the classical. Also outliers can show in the plot (leverage points)\n\n\nMAE: mean absolute error - to be looked at in the test set : cross validation\n\n\nRMSE: root mean squared error - to be looked at in the test set : cross validation\n\n\nBootstrapping : estimate the uncertainty of the coefficient and the model\n\n\nIn model selection you first define the model to test (linear should always be the gold standard)\nFirst thing always look at your data!\n\nLook at the F-statistic, R2 and RSE that give you a statistical measure on how well the model fit the data\nLook at the statistical test to asses if y and x have a relationship\nLook at the diagnostic plots to identify biases and acuracy\nLook at the RMSE, R2 and MAE in the test set via cross validation\nLook at the train error and uncertainty of your coefficients with bootstrapping (at the end of the script!)\n\n""], 'url_profile': 'https://github.com/JulietRennet', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AadityaRathod97', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'Láng Hạ, Thanh Xuân, Hà Nội', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HaQuocHuyPtit', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'Stuttgart', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Logistic Regression\n\n'], 'url_profile': 'https://github.com/MichaelStrommer', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '534 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nikipi', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jpamintuan1', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/AkshayaUAD', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AadityaRathod97', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'Patna', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['Multiple-Regression\n'], 'url_profile': 'https://github.com/Sonalikhasyap15', 'info_list': ['Python', 'Updated Mar 30, 2020', 'R', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shaoleen00', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/doggyulee', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'All around the world', 'stats_list': [], 'contributions': '417 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/taroserigano', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ValentinaFedorova', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['linera_regression\nthis describes how to come up with a linear regression model\n'], 'url_profile': 'https://github.com/ndurumo254', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bavabala', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['Enron_Regression\nIn this project, you will use regression to predict financial data for Enron employees and associates.\nA python dictionary can’t be read directly into an sklearn classification or regression algorithm; instead, it needs a numpy array or a list of lists (each element of the list (itself a list) is a data point, and the elements of the smaller list are the features of that point).\nWe’ve written some helper functions (featureFormat() and targetFeatureSplit() in tools/feature_format.py) that can take a list of feature names and the data dictionary, and return a numpy array.\nIn the case when a feature does not have a value for a particular person, this function will also replace the feature value with 0 (zero).\nOutput Before removing the outliers\n\nJust by adding these two lines, we can remove the outliers than hammer the plotted line and change the slpoe of the line\nOutput After removing the outliers\n\n'], 'url_profile': 'https://github.com/Niranjani29', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'Belgrade, Serbia', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['LinearRegression\nContains my practice for Linear regression where I used two methods:\n\nUsing LinearRegression from skleanr.linear model\nBuilding my own Linear regression model, with explicit creations of Gradient decent and calculation of the\ncost function.\n\n'], 'url_profile': 'https://github.com/MilosTodorovic', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'Germany ', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/phschaefer', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': [""\nTABLE OF CONTENTS\n\n1\xa0\xa0\xa0Logistic Regression\n2\xa0\xa0\xa0How to Use this Tool\n3\xa0\xa0\xa0How to Use the CLI\n3.1\xa0\xa0\xa0Running the model on an input file\n\n\n4\xa0\xa0\xa0References\n\n\n\n1\xa0\xa0\xa0Logistic Regression\nThe purpose of this project is to implement a multinomial logistic regression algorithm from scratch to get a better\nunderstanding of this numerical technique. This project is still under development.\n\n2\xa0\xa0\xa0How to Use this Tool\nMy suggestion is to install this package within a python environment of your choice (on my personal projects I use the\nconda package manager). This will allow you to integrate it with other projects\nyou may be working on.\nIf this is the route you decide to take, please read the installation documentation.\nIf you're interested only in the heart of the algorithm, you may just use the code within logistic_regression.py\nand make the necessary adjustments. I have one example of how I personally have been using this tool.\n\n3\xa0\xa0\xa0How to Use the CLI\n\n3.1\xa0\xa0\xa0Running the model on an input file\nIf you're interested in using the logistic regression on a particular file that you have, filename.ext, then all\nyou need to do is run the following command:\nlogistic-regression -f 'relative/filepath/filename.ext' -d 'delimiter'\nNote that unless you have a tab delimited file, you must feed the delimiter option in the CLI command (e.g. ',' for\ncomma delimited or '|' for pipe delimited files). Furthermore, it is important to keep in mind that you must use the\nrelative file path from the location where you're calling the command from.\nStill under construction!\n\n4\xa0\xa0\xa0References\nThere is a lot of interesting stuff out there regarding this topic! For a surprisingly nice presentation, check out the\ninformation on Wikipedia.\nHowever, the resource I found to be the most useful (not only for this project), was the following book:\nLearning From Data: A Short Course. Y. S. Abu-Mostafa, M. Magdon-Ismail, and H-T. Lin, AMLbook.com, March 2012\n""], 'url_profile': 'https://github.com/adamiao', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 13, 2020']}"
"{'location': 'Brazil - SP', 'stats_list': [], 'contributions': '103 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Cristianobam', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/trevornagaba', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Saint-Petersburg', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['PULogisticRegression\nExploration of the PU classification method from the article Learning Classifiers from Only Positive and Unlabeled Data\n'], 'url_profile': 'https://github.com/davidenkoim', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Jaipur', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Logistic_Regression\nLogistic Regression Algorithm of machine learning implemented on real world data set using sklearn.\n'], 'url_profile': 'https://github.com/goutamdadhich', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jpamintuan1', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aashimagogia', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Nairobi-Kenya', 'stats_list': [], 'contributions': '554 contributions\n        in the last year', 'description': ['Linear-regressions\n#This contains pactice code for linear regression models\n#Technologies\n#1.python\n#2.R\n'], 'url_profile': 'https://github.com/abel-keya', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['linear Regression for predicting Sales from Advertising\nHosted using AWS using EC2\n'], 'url_profile': 'https://github.com/pallavije', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '534 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nikipi', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vklakshmi', 'info_list': ['JavaScript', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aashimagogia', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Tucson, Arizona', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/websterkgd', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshitroy2605', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/mhc-stat-242-2020', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Ceará', 'stats_list': [], 'contributions': '225 contributions\n        in the last year', 'description': ['Regressão Logística Aplicada ao Digits Dataset\nNeste projeto foi utilizado conceitos básicos da regressão logística para estudo de Aprendizado de Máquina\nVersão do Python utilizada foi a 3.6.x\n'], 'url_profile': 'https://github.com/leondavidtb', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['A Linear Regression model using sklearn is discussed. The dataset used is the collection of the prices of second hand car based on various features. A statistical analysis of the dataset is done using linear regression. It is just a basic model to understand how to apply Linear Regression on a dataset using sklearn and further improvements can be done in the model.\n'], 'url_profile': 'https://github.com/Pritesh003', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '315 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GaneshMaurya', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Moscow/Russia', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['LinearRegression\nNotes from Udacity Intro to Machine Learning course\n'], 'url_profile': 'https://github.com/maxbogus', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JericoSalvador', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Medical_compliance\nIt is Machine Learning project. In this we train the model to predict, is patients following prescription or not. In this project, data is given about patient, that includes patient id, Age, Gender, is patients diabetes, Alcoholism, Hypertension, Smokes, Tuberculosis and the number of remainder sms per day. The targeted variable is Adherence.\npatient_id: Unique ID of each patient.\nAge: Age of patient (in years).\nGender: Gender of patient (F- Female, M- Male).\nPrescription_period: The number of days, patient have to follow the prescription.\nDiabetes: Is patient have diabetes or not (0: No, 1: Yes).\nAlcoholism: Is patient alcoholic (0: No, 1: Yes).\nHyperTension: Is patient have hypertension (0: No, 1: Yes).\nSmokes: Is patient have addiction of smoking (0: No, 1: Yes).\nTuberculosis: Is patient have TB (0: No, 1: Yes).\nSms_Remainder: The number of remainder sms send to patients per day.\nAdherence: It is targeted variable. We have to find out is patients following prescription or not.\nThe operation we have done as below:\nData Cleaning: Data was already in clean state.\nData Transformation: Transfer the object data type to int. Here only two columns (Gender and Adherence) are of object data type.\nCorrelation: Check the correlated columns and remove the highly correlated columns. Here we have ‘Diabetes’ and ‘Hypertension’ are highly correlated with ‘Age’ column so I removed ‘Diabetes’ and ‘Hypertension’ columns. Also, similar for ‘Alcoholism’.\nData Splitting: Data is split into Train and Dev set.  We train a model on Train set and cross-verified on Dev set and Test set.\nModel formation: We have use three different algorithms for modeling.\n1. Logistic Regression\n2. Xgboost\n3. Neural Network (That algorithm is developed by our team at Vchip Technology)\nPrediction:\n• On the basis of accuracy, recall, precision and f1-score, we decide which algorithm is doing better for this task.\nCode file name:\n\nMedical_compliance_using_LogisticRegression.ipynb for Logistic Regression modeling.\nMedical_compliance_using_xgboost.ipynb for xgboost modeling.\nMedical_compliance_using_NN.ipynb for Neural network modeling.\n\nThank you.\n'], 'url_profile': 'https://github.com/vda4117', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 3, 2020', '2', 'Jupyter Notebook', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Image regression\nA toy application that learns a mapping from (x, y) coordinates to color.\nUsage\nTrain on a single greyscale image\npython train.py -i keyboard.png --num-epochs 1000\nTrain on a single color image (RGBA)\npython train.py -i landscape.png --num-epochs 1000\nTrain on multiple images\npython train_many.py -i boxy_stripes2.png boxy_stripes2_30.png boxy_stripes2_60.png boxy_stripes2_90.png\nThen interpolate between the images:\npython interpolate_between_many.py --num-images 4 --model output/that_model_filename.h5\nExamples\n\n\n\nName\nOriginal\nLearned image\n\n\n\n\nKeyboard\n\n\n\n\n8x8 Checkerboard\n\n\n\n\n\nThe following animation visualizes the output of a neural network that was trained on 12 different images (different rotations of boxy stripes). The input vectors are constructed to interpolate between the 12 images, so we get a kind of morphing effect.\n\n'], 'url_profile': 'https://github.com/Paintium', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['linear_regression\n'], 'url_profile': 'https://github.com/pelusok', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': [""LiniearRegression\nLinear regression is a supervised learining algorithm used when target / dependent variable continues real number. It establishes relationship between dependent variable  y  and one or more independent variable  x  using best fit line. It work on the principle of ordinary least square  (OLS)  / Mean square errror  (MSE) . In statistics ols is method to estimated unkown parameter of linear regression function, it's goal is to minimize sum of square difference between observed dependent variable in the given data set and those predicted by linear regression fuction.\nDownload Dataset from github:https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/insurance.csv\nPart 1.DEFINE\nPart 2.DISCOVER\nPart 3.DEVELOP\nPart 4.DEPLOYMENT\n""], 'url_profile': 'https://github.com/dkryadav', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'Ludhiana', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['playstoreclassification-regression\n'], 'url_profile': 'https://github.com/Shahansari2909', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Polynomial-regression\n'], 'url_profile': 'https://github.com/akash-2001', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gautamjariwala', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['logisticRegression\n天猫商家生意参谋的数据转换使用\n这个不是很准，因为转化不只是一个公式而已。所以自己又开发了一套接口，接口暂不对外开放。\n如有需要，可提供思路。\n'], 'url_profile': 'https://github.com/personalTools', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bavabala', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Baboyeatami', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}","{'location': 'Noida, India', 'stats_list': [], 'contributions': '307 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression using tensorflow to predict survival probability on the Titanic dataset. It is a widly used dataset specially for beginner data science practice. Dataset is available on kaggle. Model predicts with user data if one would survive or not based on sigmoid probability evaluation.\n'], 'url_profile': 'https://github.com/priyanu17', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020']}"
"{'location': 'Dhaka Bangladesh.', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HEDAETUL-ISLAM', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['MultipleRegression\nsalary prediction system using different parameters\n'], 'url_profile': 'https://github.com/charankurru', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mad-farmer', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Linear-Regression\nThis exercise uses Linear Regression to predict housing prices based on features from the Boston housing dataset.\n'], 'url_profile': 'https://github.com/kelseymour', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'Noida, India', 'stats_list': [], 'contributions': '307 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression using tensorflow to predict survival probability on the Titanic dataset. It is a widly used dataset specially for beginner data science practice. Dataset is available on kaggle. Model predicts with user data if one would survive or not based on sigmoid probability evaluation.\n'], 'url_profile': 'https://github.com/priyanu17', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aquotermans', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '1,158 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/SyedMuhammedBilal', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NaveenKumar33', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AigulShafikova', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['netflix_Regression\nFinding the possible reasons for a show regarding the Viewers\n'], 'url_profile': 'https://github.com/sukesh-reddy', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', 'R', 'Updated Apr 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mad-farmer', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Linear-Regression\nThis exercise uses Linear Regression to predict housing prices based on features from the Boston housing dataset.\n'], 'url_profile': 'https://github.com/kelseymour', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['Linear Regression Exercise\nThis is a learning exercise for learning regression, following the post by Nagesh Singh Chauhan.\nhttps://towardsdatascience.com/a-beginners-guide-to-linear-regression-in-python-with-scikit-learn-83a8f7ae2b4f\nQuick view of the Jupyter Notebook - Linear Regression:\nhttps://nbviewer.jupyter.org/github/SusanQu/linear_regression_ex/blob/master/Linear_Regression_Ex.ipynb\nQuick view of the Jupyter Notebook - Multiple Linear Regression:\nhttps://nbviewer.jupyter.org/github/SusanQu/linear_regression_ex/blob/master/Multiple_Linear_Regression_Ex.ipynb\n'], 'url_profile': 'https://github.com/SusanQu', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '255 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mandal-21', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/serhatmath', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shalz07', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['======== SILICA CONCENTRATE PREDICTION ========\nThis project is to predict % Silica in ore concentrate in Flotation Plant using Multiple Linear Regression, Ridge Regressioin, Lasso Regression and XGBoost\nThe data came from The Flotation Plant. The data have several columns,the first column is date, the second and third columns are quality measures of the iron ore pulp right before it is fed into the flotation plant. Column 4 until column 8 are the most important variables that impact in the ore quality in the end of the process. From column 9 until column 22, we can see process data (level and air flow inside the flotation columns, which also impact in ore quality. The last two columns are the final iron ore pulp quality measurement from the lab.\nThe Problem\n\nThe aim is to predict the % Silica in Concentrate ever minute\ncan we predict % Silica in Concentrate without using % Iron Concentrate?\n\nThe Steps\n\nData Preprocessing\nModelling\nEvaluating\nTunning Model\n\nTo read more detail about this project click this link below:\nhttps://medium.com/@farrasalyafi/prediksi-silica-pada-flotation-plant-menggunakan-multiple-linear-regression-ridge-regression-3c9ee8007435\n'], 'url_profile': 'https://github.com/farrasalyafi', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Visualize_Multiple_Regression\nVisualization of multiple linear regression in 3D using matplotlib and numpy.meshgrid\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\n'], 'url_profile': 'https://github.com/FahadTComsats', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': [""CS5612-LOGISTIC_REGRESSION\nThis repository contains an example implementation of Logistic Regression with python used to classify 'Wisconsin Breast Cancer Data Set'\n""], 'url_profile': 'https://github.com/kithmini', 'info_list': ['Python', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AadityaRathod97', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['regression-analysis-using-python\nLoading and Preparing the Data for Analysis\n'], 'url_profile': 'https://github.com/Gichere', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/roduquen', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hiendn', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': ' Nuevo León, Mexico', 'stats_list': [], 'contributions': '564 contributions\n        in the last year', 'description': ['multivariate-linear-regression\nAlgoritmo de gradiente descendiente para regresión lineal multivariante para la clase de Inteligencia Artificial impartida por el maestro Andres Hernandez G. para la Universidad de Monterrey.\n'], 'url_profile': 'https://github.com/gabri3l0', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Table of Contents\n\nInstallation\nProject Motivation\nFile Descriptions\nResults\nLicensing, Authors, and Acknowledgements\n\nInstallation \nDependencies\n\nnumpy\npandas\nmatplotlib\n\nProject Motivation\nFor this project, I was interested in using AirBnB data from Boston to better understand:\n\nWhat factors most influence the price of a listing?\nCan we predict which listings will have a negative review?\nHow much do prices spike in the busiest times of the year?\n\nFile Descriptions \nThe notebook  Markdown cells were used to assist in walking through the thought process for individual steps.\nResults\nThe main findings of the code can be found in the ipython notebooks.\nLicensing, Authors, Acknowledgements\nI used data from Kaggle. You can find the descriptive information at the link available here.\n'], 'url_profile': 'https://github.com/ablaikie103', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Modelling a linear equation using deep learning\nCustom model\nCreating a model from scratch by simulating stochastic gradient descent to come up with the linear model coefficients.\nTensorflow model\nCreating a model using tensorflow to compare model coefficients with custom model.\n'], 'url_profile': 'https://github.com/shubhamjg7', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Decision-Tree-Regression\n'], 'url_profile': 'https://github.com/FahadTComsats', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Diamonds-Regression-Project.. See attached notebook file.\nFirst python project. Performed regression analysis on the Diamonds dataset (53940 observations, 10 variables)\n##RandomForest performed best with R2 of 0.98##\n---Packages used---\nPandas, Numpy, Sklearn, Seaborn\n---Performed---\ndata cleaning / feature engineering / visualization / parameter tuning / model and subset selection\n---Models/Algorithms---\nLinear regression, ridge regression, Randomforest, KNN, gradientboosting, Adaboosting, Lasso\n'], 'url_profile': 'https://github.com/JKostyal', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Jul 23, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['joint_latent_factor_regression\n'], 'url_profile': 'https://github.com/annamenacher', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marioeid', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'vadodara', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['Heroku-Demo\n'], 'url_profile': 'https://github.com/swapy-S', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'Guatemala,Guatemala', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Multiple-linear-regression\nAnalyzing and predicting the sale value of houses for a data set\nInstallation\nTo be able to visualize all the graphics and predictions in this project, you have to install the following python libraries\npip install pandas\npip install numpy \npip install matplotlib\npip install seaborn \npip install statsmodels.api\npip install sklearn \npip install scipy\nUsage\n1. Download Multiple Linear regression.ipynb\n2. Download train-clean.csv \n3. Run Multiple Linear Regression as a jupyter notebook\nYou can also download the complete database here.\n'], 'url_profile': 'https://github.com/JennsiS', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '209 contributions\n        in the last year', 'description': ['Predicting Internet Movie Database Ratings\nThe data is from IMDB-movie ratings and having features release date, title, box-office collection, budget, director, actors etc.\nRequirements\n\nPython 3.5+\nSklearn\nPandas\nNumpy\nMatplotlib\nSeaborn\n\nThe data has missing values, to handle that some of the columns are deleted, some of the rows are also deleted, and applied mean value imputation.\nModel Implementation\nApplied 3 models to train and predict the movie ratings\n\nLinear regression (with ridge and lasso)\nNeural Network-Keras\nGradient Boosting Regressor\n\n##Results\nThe cross-validated GradientBossting Regressor has the minimum mean-squared error.\n\n'], 'url_profile': 'https://github.com/Laay', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Linear-regression-Gradient-descent\nFirst machine learning algorithm\n'], 'url_profile': 'https://github.com/yanismnsr', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bullet400', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deekshasudhakar', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'Bhubaneswar', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\nIt uses several explanatory variables to predict the outcome of a response variable. Multiple regression is an extension of linear (OLS) regression. In regression with multiple independent variables, the coefficient denotes how much the dependent variable is expected to increase when any particular independent variable increases by one, holding all the other independent variables constant.\n'], 'url_profile': 'https://github.com/Soumya-Pathy', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Linear_regression_pyspark\n'], 'url_profile': 'https://github.com/anshulm2211', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}"
"{'location': 'Washington, D.C.', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': [""hockey-game-regression\nThe Jupyter Notebook in this respository contains an analysis of factors contributing to game outcomes in the National Hockey League.\nThe statistical test used for this analysis is a multiple logistic regression because game outcomes are binary (win or loss) and there are several explanatory variables.\nSome explanotry variables include:\n\nFO%: A face-off is the method used to begin and restart play. FO% is the percentage that a team wins the face-off.\nBS: Blocked shots\nPP%: If one team incurs a penalty, the other team get's a power-play (more players on ice). PP% is that percentage that a team scores on their power-play.\n\nFuture Development\nThe model developed from this analysis is not currently being used to predict games due to the absence of an API with reliable documentation. Rather than try to retrieve data from an API, a future approach could involve scraping data from the NHL's website.\n""], 'url_profile': 'https://github.com/jvanzalk', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Random_Forest_regression\n'], 'url_profile': 'https://github.com/Kulna11', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Multivariate-Linear-Regression-\nThis is a machine learning program that lets you find the house price based on house size and number of bedroom\n'], 'url_profile': 'https://github.com/Ashish-Arya-CS', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': [""Springboard-Course-Linear-Regression\nThe objective of this project is to apply linear regression algorithm to solve the problem of house price prediction.\nThe packages we'll cover are: statsmodels, seaborn, and scikit-learn.\nPart 1: Mathemetics behind linear regression\nPart 2: Exploratory Data Analysis for Linear Relationships\n   -- Descriptive Analysis\n   \n   -- Data visualization\n\nPart 3: Using Statsmodels to investigate R-square , F-statistic, t-statistic, p-value and AIC\nPart 4: Evaluating the Model via Model Assumptions\n   -- use fitted values versus residuals plot to check linear relationship assumption\n   \n   -- use fitted values versus residuals plot to detect outliers\n   \n   -- use quantile-quantile plot to check normal distribution of errors\n   \n   -- use influence plot to detect high leverage points\n   \n   -- optimize model performance by applying these techniques above.\n\n""], 'url_profile': 'https://github.com/stevesong1121', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'Orange, Ca', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Logistic-Regression-Model\nExplaining the workings of a logistic regression model for classification of heart disease patients.\n'], 'url_profile': 'https://github.com/VipulVatsyayan', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'Hamburg, Germany', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/powidlowa', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['abbyhudak-intro_nonlinear_regressions\nIntro to nonlinear regressions in R\n'], 'url_profile': 'https://github.com/abbyhudak', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vishwakarmasantosh', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Criei este projeto para praticar os conhecimentos adquiridos na área de machine learning. O projeto se trata de um modelo de machine learning implementado utilizando python, a biblioteca sklearn e outras ferramentas como pandas, numpy e matplotlib.\nO modelo criado receber a altura de uma pessoa e a partir desse dado realiza uma previsão de qual é o seu peso. O dataset utilizado para treinar o modelo foi obtido em https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex.\n'], 'url_profile': 'https://github.com/josafarb', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '417 contributions\n        in the last year', 'description': ['Regressão Linear\n\nCódigo que implementa regressão linear utilizando Tensorflow para a previsão do dia de infecção tendo como base o número de novos casos relatados por 170 países em um intervalo de 63 dias.\n\nSumário\n\nTabela Formatada\nEscalonamento\nRegressão\nTreinamento\nPrevisões\n\nTabela Formatada\nO Primeiro passo foi extrair da tabela somente os dados que serão utilizados no treinamento.\n\nEscalonamento\nAplicação do escalonamento dos valores da tabela.\n\nRegressão\nO estimator foi usado para aplicar a regressão linear nas colunas que continham os dados necessários.\n\nTreinamento\nCriada a função de treinamento utilizando 10,000 steps.\n\nPrevisões\nValores previstos após o treinamento em comparação aos valores reais.\n\n'], 'url_profile': 'https://github.com/rnanc', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Apr 2, 2020', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Mar 31, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Criei este projeto para praticar os conhecimentos adquiridos na área de machine learning. O projeto se trata de um modelo de machine learning implementado utilizando python, a biblioteca sklearn e outras ferramentas como pandas, numpy e matplotlib.\nO modelo criado receber a altura de uma pessoa e a partir desse dado realiza uma previsão de qual é o seu peso. O dataset utilizado para treinar o modelo foi obtido em https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex.\n'], 'url_profile': 'https://github.com/josafarb', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '417 contributions\n        in the last year', 'description': ['Regressão Linear\n\nCódigo que implementa regressão linear utilizando Tensorflow para a previsão do dia de infecção tendo como base o número de novos casos relatados por 170 países em um intervalo de 63 dias.\n\nSumário\n\nTabela Formatada\nEscalonamento\nRegressão\nTreinamento\nPrevisões\n\nTabela Formatada\nO Primeiro passo foi extrair da tabela somente os dados que serão utilizados no treinamento.\n\nEscalonamento\nAplicação do escalonamento dos valores da tabela.\n\nRegressão\nO estimator foi usado para aplicar a regressão linear nas colunas que continham os dados necessários.\n\nTreinamento\nCriada a função de treinamento utilizando 10,000 steps.\n\nPrevisões\nValores previstos após o treinamento em comparação aos valores reais.\n\n'], 'url_profile': 'https://github.com/rnanc', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\n'], 'url_profile': 'https://github.com/FahadTComsats', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['STB_Regression_Classification_modelling\nThis Project is based on an interview by SIngapore Toruism Board.\nFor Question 1, I have adopted a quick linear model followed by lasso model for L2 regularization\nFor Question 2, I have asopted 2 models; log model , MultinomialNB model followed\n'], 'url_profile': 'https://github.com/yilongchua', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'New Delhi ', 'stats_list': [], 'contributions': '359 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sarthak2601', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""Logistic-Regression-Project-Advertising\nIn this project ,I have worked on a duplicate advertising data set, indicating whether or not a particular internet user clicked on an Advertisement.I have created a model that will predict whether or not they will click on an ad based off the features of that user.\nThis data set contains the following features:\n'Daily Time Spent on Site': consumer time on site in minutes\n'Age': cutomer age in years\n'Area Income': Avg. Income of geographical area of consumer\n'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n'Ad Topic Line': Headline of the advertisement\n'City': City of consumer\n'Male': Whether or not consumer was male\n'Country': Country of consumer\n'Timestamp': Time at which consumer clicked on Ad or closed window\n'Clicked on Ad': 0 or 1 indicated clicking on Ad\n""], 'url_profile': 'https://github.com/SharanyaCS', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Logistics-Regression-in-R\nWe have a dataset of citizens living in a city. Our goal is to predict whether a person will cast his/her vote based on various past parameters.\n'], 'url_profile': 'https://github.com/anujjohri', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Random-Forest-Regression\n'], 'url_profile': 'https://github.com/FahadTComsats', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mahdirad', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'Toronto, Ontario, Canada', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Dhruv876', 'info_list': ['Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 21, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}"
"{'location': 'Mumbai', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Logistics-Regression-in-R\nWe have a dataset of citizens living in a city. Our goal is to predict whether a person will cast his/her vote based on various past parameters.\n'], 'url_profile': 'https://github.com/anujjohri', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Random-Forest-Regression\n'], 'url_profile': 'https://github.com/FahadTComsats', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mahdirad', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Toronto, Ontario, Canada', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Dhruv876', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '372 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kenneth5259', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['Keras-Regression-Housing-Pred\n'], 'url_profile': 'https://github.com/nrepesh', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['regression-with-keras\n'], 'url_profile': 'https://github.com/cpingle36', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Greater New York Area', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Project is still under constuction :)\nBoston Airbnb Project Walkthrough\n\n\nData Understanding\n\nParagraph type of features are eliminated. (They can be used in NLP projects)\nMissing Values are imputed or dropped based\nCategorical Values are converted to numeric ones\n\n\n\nCalendar dataset is analyzed, and merged with Listing dataset.\n\n\nScaling and Splitting Process done\n\n\nLinear Regression applied\n\n\nRandom Forest applied\n\n\nXgboost applied\n\n\nChecked Coefficient of features on Target Value\n\n\n'], 'url_profile': 'https://github.com/tacettinarici', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['FoDS-Assignment\nFoundations of Data Science Assignment\nAssignment Details\n\nLinear Regression - From scratch implementation for Linear Regression using numpy and pandas.\n\nImplemented for both L1 and L2 regularisation\nImplemented Stochastic, Gradient Descent and Batch Gradient Descent\n\n\nRegression - From scratch implementation for Regression using numpy and pandas.\n\nImplemented for both L1 and L2 regularisation\nImplemented Stochastic, Gradient Descent and Batch Gradient Descent\n\n\nConjugation property of Normal and Beta Distribution\n\nRequirements\n\npanda\nnumpy\nsklearn\nscipy\n\n'], 'url_profile': 'https://github.com/ankit-shibu', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Loan_Prediction_Linear_Regression\nAmong all industries, the insurance domain has one of the largest uses of analytics & data science methods. This dataset provides you a taste of working on data sets from insurance companies – what challenges are faced there, what strategies are used, which variables influence the outcome, etc. This is a classification problem. The data has 615 rows and 13 columns.  Problem: Predict if a loan will get approved or not.\n'], 'url_profile': 'https://github.com/allwynvincent', 'info_list': ['R', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['acg-ml-regression\nData Source\n\nPT and RB data found in: https://fairmodel.econ.yale.edu/vote2012/affairs.txt\n[fair book pg. 71]\nData gathered from two magazine surveys that were both conducted via mail:\n\nPsychology Today (1969)\nRedbook (1974/ American women\'s magazine that publishes since 1903 https://en.wikipedia.org/wiki/Redbook) - Women only data ==> SOS: frequency of gender=""female""\n\nDataset exclusion criteria:\n\nPeople who were married more than once were excluded\nUnemployed people were also excuded\nExcluded people that failed to answer all questions\n\nRun Instructions\n\nIn ..seaborn\\algorithms.py change L84 to: resampler = integers(0, n, n, dtype=np.int_)\n\n'], 'url_profile': 'https://github.com/giso360', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['Springboard-Course-Logistic-Regression\nThe objective of this project is to apply logistic regression algorithm to solve the real problem.\n\n\nUnderstand the mathematics behind the Logistic Regression\n\n\nUse data visualizaiton techniques to expore the relationship between variables\n\n\nUse GridSearchCV to search for best parameters for classifier\n\n\n'], 'url_profile': 'https://github.com/stevesong1121', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '255 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mandal-21', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Pkotova', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'New York City, New York, United States', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['linear_regression_mini_library\nA library which contains a vast collection of functions and methods used in canonical regression analysis\n'], 'url_profile': 'https://github.com/MatthewK100000', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['This is a readme file for this automation Framework - Ariat Resgression Project\nThe project is structured into more packages and classes. All the main classes and enums are in src/main/java, the resources like files and TestNG suite xml files are kept in src/test/resources\nand the TestNGs tests are kept in src/test/java.\nThe pom xml file is actually the one that will be collected by the job in Jenkins, everything that is under src/test/resources will be built.\npom xml files contains all the dependencies that are used in this project like: testNg, selenium, webdriver, maven compiler, maven plugin, etc.. (you need to download the resources)\nEach page from the application has his corresponding class page, for list of country a switch of list of Home Page classes is defined.\nAll the steps are defined in txt files in src/test/resources called Test Steps folder.\nHow to clone the project ?\nIn your workspace you need to run the following command:\ngit clone (ssh/url of the repository) and the project will be downloaded on your local machine.\nIf you need to implement new tests you will have to create a new branch from master and then create a pull request for it to be merged with master.\nAfter the pull request is approved then the changes will be merged with master branch.\nWho to talk to ?\nAila Bogasieru\nEmail:\naila.bogasieru@ariat.com/aila.bogasieru@gmail.com\n'], 'url_profile': 'https://github.com/shippingcode', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Advanced-Regression-Project\nThis is a git repository on my Advanced Regression project done on Boston Housing Data\n'], 'url_profile': 'https://github.com/Vaibhav3006', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Project || Linear-Regression-Ball-pit\nReggie is a mad scientist who has been hired by the local fast food joint to build their newest ball pit in the play area. As such, he is working on researching the bounciness of different balls so as to optimize the pit. He is running an experiment to bounce different sizes of bouncy balls, and then fitting lines to the data points he records. He has heard of linear regression, but needs our help to implement a version of linear regression in Python.\nLinear Regression is when you have a group of points on a graph, and you find a line that approximately resembles that group of points. A good Linear Regression algorithm minimizes the error, or the distance from each point to the line. A line with the least error is the line that fits the data the best. We call this a line of best fit.\nWe used loops, lists, and arithmetic to create a function that helped finding a line of best fit when given a set of data.\n'], 'url_profile': 'https://github.com/manjul253', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'Guatemala', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/an3iitho', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['LogisticRegressionDataCamp\n'], 'url_profile': 'https://github.com/LeboSeribe', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Updated Apr 2, 2020', 'Updated Apr 3, 2020', 'R', 'Updated Apr 9, 2020', 'Python', 'Updated Apr 1, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vikhilindra', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['This is a readme file for this automation Framework - Ariat Resgression Project\nThe project is structured into more packages and classes. All the main classes and enums are in src/main/java, the resources like files and TestNG suite xml files are kept in src/test/resources\nand the TestNGs tests are kept in src/test/java.\nThe pom xml file is actually the one that will be collected by the job in Jenkins, everything that is under src/test/resources will be built.\npom xml files contains all the dependencies that are used in this project like: testNg, selenium, webdriver, maven compiler, maven plugin, etc.. (you need to download the resources)\nEach page from the application has his corresponding class page, for list of country a switch of list of Home Page classes is defined.\nAll the steps are defined in txt files in src/test/resources called Test Steps folder.\nHow to clone the project ?\nIn your workspace you need to run the following command:\ngit clone (ssh/url of the repository) and the project will be downloaded on your local machine.\nIf you need to implement new tests you will have to create a new branch from master and then create a pull request for it to be merged with master.\nAfter the pull request is approved then the changes will be merged with master branch.\nWho to talk to ?\nAila Bogasieru\nEmail:\naila.bogasieru@ariat.com/aila.bogasieru@gmail.com\n'], 'url_profile': 'https://github.com/shippingcode', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Advanced-Regression-Project\nThis is a git repository on my Advanced Regression project done on Boston Housing Data\n'], 'url_profile': 'https://github.com/Vaibhav3006', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Project || Linear-Regression-Ball-pit\nReggie is a mad scientist who has been hired by the local fast food joint to build their newest ball pit in the play area. As such, he is working on researching the bounciness of different balls so as to optimize the pit. He is running an experiment to bounce different sizes of bouncy balls, and then fitting lines to the data points he records. He has heard of linear regression, but needs our help to implement a version of linear regression in Python.\nLinear Regression is when you have a group of points on a graph, and you find a line that approximately resembles that group of points. A good Linear Regression algorithm minimizes the error, or the distance from each point to the line. A line with the least error is the line that fits the data the best. We call this a line of best fit.\nWe used loops, lists, and arithmetic to create a function that helped finding a line of best fit when given a set of data.\n'], 'url_profile': 'https://github.com/manjul253', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'Guatemala', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/an3iitho', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['LogisticRegressionDataCamp\n'], 'url_profile': 'https://github.com/LeboSeribe', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""Linear-Regression-Project-Ecommerce\nSCENARIO:\nCongratulations! You just got some contract work with an Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions. Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want.\nThe company is trying to decide whether to focus their efforts on their mobile app experience or their website. They've hired you on contract to help them figure it out!\n""], 'url_profile': 'https://github.com/SharanyaCS', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '315 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GaneshMaurya', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Bake off challenge: Regression\n\nFor this challenge, we will throw you back into the familiar territory of  regression, and give you what will (most likely) be your first exposure in a Kaggle-like ""contest.""  Given a dataset of house prices in King County, Washington, you will be tasked with fitting a model on a labeled training dateset, and submitting housing price predictions on an unlabeled test dataset.  Don\'t worry if you\'ve already seen this dataset; remember reiteration is a crucial component of CRISP-DM.  Have fun with this.\nData files\nYou will find two data files in this repo, along with one file describing the features.\n\nkc_house_data_train.csv\n\n\nLabeled training data to train your model on.  Target variable is the continuous feature \'price\'\n\n\nkc_house_data_test_features.csv\n\n\nAn unlabeled set of test features.  The columns match the train set, but the \'price\' column is missing. Take your fit your model on the training data, then use that fit object to make predictions on the test features dataset.\n\n\nfeature_descriptions.md\n\n\nA description of features that apply to both train and test sets.\n\nInstructions\n\nEvery team member fork and then clone this repo onto your computer. Figure out a strategy for how to collaborate, which may look like selecting one team member as the primary ""coder"".  We leave it to you to set-up a productive team dynamic, but keep in mind, the deliverable of this exercise is one csv file.\nLoad up a Jupyter and start CRISP\'ing.\nThe instructions for this bakeoff are limited with regards to modeling. Go wild. Use any model you want.  Feature engineer your head off.\nOnce you are satisfied with your model, or you find time running out, use the kc_house_data_test.csv to predict a price for each of the 4323 house records.  See Submission guidelines below.\nPlease be sure to use your own work.  If you have worked on this dataset before, don\'t copy and paste your previous work. Obviously there is no way that we can ask you not to use past experience, but we can ask you not to use past code.\n\nSubmissions\nSave your predictions in a csv file with the following pattern.\n\n\nhousing_preds_yourteaminitials.csv\n\n\nFor example, if Brian and Erin were a team, their submission would look like housing_preds_bmeh.csv\n\n\nThere are different ways to turn arrays to csv\'s.  You can use any method you choose, but be sure your submission has no heading, no index and one column. Your submission must match the number of records in the kc_house_data_test_features.csv. Don\'t worry about rounding.  A valid submission could look like below.\n\n\n401412.6923102136\n501023.6923102136\n310304.62734089\n1030130.2812897698\n...\n\n\nPlease no header on your csv submission.\n\n\nYou will be given 3 hours to build your best model using the training data. There is a hard cuttoff at 3 hours. Any late submissions will not be considered.\n\n\nSlack your csv file to the seattle-chicago-ds-012720 channel before time is up. Any submissions after the 3 hour deadline will be evaluated, but not included in the judging. (Chicago will start sooner and have an earlier submission deadline than Seattle this time around.)\n\n\nMetrics\nTest results will be scored by RMSE. The group which submitted the predictions with the lowest RMSE will be anounced Thursday morning. As reward, they will receive bragging rights that span from the Midwest to the Pacific coast.\nLastly\n\nAfter submission, push your repo with your work to your forked repo. In this way, your peers will be able to gain valuable wisdom from the different tactics employed.\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hephaestus12', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Java', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Support-Vector-Regression-SVR-\n'], 'url_profile': 'https://github.com/FahadTComsats', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '255 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mandal-21', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Paris - France', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': ['Logistic Regression Implementation - from scratch !\nThis is a Logistic Regression model implementation, which was inspired by the ""Neural Networks and Deep Learning"" course, hosted on coursera, and offered by Deeplearning.ai, which I like to thank for their great informative content that helped solidify my knowledge in this field.\nIn this implementation, I\'ve build the general architecture of a learning algorithm, according to the logistic regression function, including:\nInitializing parameters.\n\nCalculating the cost function and its gradient.\nProviding a method for Calculating the accuracy and loss metrics, when verbosity is on.\nUsing an optimization algorithm (gradient descent) for the back propagation.\nVectorized Calculations for optimizing the execution time.\nA demo for showing how it works !\n\nOf course this implementation has room to improve, and I\'m looking forward to do it !\nNote : This implementation was made for learning purposes, it\'s not meant to be used in a real-world application, at least for the moment!\nThis is basically how you could use it\n  from logistic_regression import LogisticRegression\n  logreg = LogisticRegression(learning_rate = 0.002, loss=\'categorical_crossentropy\')\n  history = logreg.fit(x_train, y_train, x_test, y_test, epochs=2000, verbose=100, metrics=[\'accuracy\', \'loss\'])\nHere\'s also a snippet of it\'s performance\n\n\n'], 'url_profile': 'https://github.com/taherromdhane', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Moradabad, India', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Logistic Regression\nClassification Model to predict weather a passenger survived or not.\n\xa0 \xa0 \xa0 \xa0\nTable of Contents\n\nIntroduction\nPython libraries\nThe problem statement\nLogistic Regression\nIndependent and dependent variable\n\n Logistic Regression Algorithm\n\n\nDecision boundary\nExploratory data analysis\nInterpretation and Conclusion\n\n\xa0 \xa0 \xa0 \xa0\n1.\tIntroduction\nIn this project, I build Classification model to study the relationship between passenger survival and different continous and discrete variables. I implemented this classification model in Python programming language using Scikit-learn,numpy,seaborn,matplotlib.\n\xa0 \xa0 \xa0 \xa0\n2.\tPython libraries\n•\tNumpy\n•\tPandas\n•\tMatplotlib\n•\tSeaborn\n•\tScikit-Learn\n\xa0 \xa0 \xa0 \xa0\n3.\tThe problem statement\nThe main aim of this model is to predicting weather a passenger survived or not on the basis of different attributes. Finding relationship or dependency of Output on attrubutes like Age,Sex and other variables.\nThe accuracy of the model is defined using accuracy_score, confusion_matrix, ROc Graph.\n\xa0 \xa0 \xa0 \xa0\n4.\tLogistic Regression\nLogistic Regression is one of the basic and popular algorithm to solve a classification problem. It is named as ‘Logistic Regression’, because it’s underlying technique is quite the same as Linear Regression. The term “Logistic” is taken from the Logit function that is used in this method of classification.\nWe identify problem as classification problem when independent variables are continuous in nature and dependent variable is in categorical form i.e. in classes like positive class and negative class. The real life example of classification example would be, to categorize the mail as spam or not spam, to categorize the tumor as malignant or benign and to categorize the transaction as fraudulent or genuine. All these problem’s answers are in categorical form i.e. Yes or No. and that is why they are two class classification problems.\n\xa0 \xa0 \xa0 \xa0\n5.   Independent and Dependent Variables\nIndependent variable\nIndependent or Input variable (X) = Feature variable = Predictor variable\nThe following are the independent variable:-\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\nDependent variable\nDependent or Output variable (y) = Target variable = Response variable\nThe following is the dependent variable:-\n\nSurvived\n\n\xa0 \xa0 \xa0 \xa0\n6.\tLogistic Regression Algorithm\nLogistic Regression uses Sigmoid function.\nAn explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a Sigmoid function, which takes any real value between zero and one. It is defined as\n\t\t\t\n\t\tf(t) = e^t / e^t + 1\n\n\t\tf(t) = 1 / 1 + e^-t\nLet’s consider t as linear function in a univariate regression model.\n\t\t\n\t\tt = a0 + a1*x\n\nSo the Logistic Equation will become\n\n\t\tp(x) = 1 / 1 + e^-(a0+a1*x)\n\n\xa0 \xa0 \xa0 \xa0\n7.\tDecision boundary\nThe sigmoid function returns a probability value between 0 and 1. This probability value is then mapped to a discrete class which is either “0” or “1”. In order to map this probability value to a discrete class (pass/fail, yes/no, true/false), we select a threshold value. This threshold value is called Decision boundary. Above this threshold value, we will map the probability values into class 1 and below which we will map values into class 0.\nMathematically, it can be expressed as follows:-\np ≥ 0.5 => class = 1\n\t\np < 0.5 => class = 0 \n\nGenerally, the decision boundary is set to 0.5. So, if the probability value is 0.8 (> 0.5), we will map this observation to class 1.  Similarly, if the probability value is 0.2 (< 0.5), we will map this observation to class 0.\n\xa0 \xa0 \xa0 \xa0\n8.\tExploratory data analysis\nTo summarize the main characteristics of data I analysed it using Data Visualization by ploting graphs like histogrames, and distribution plot between the dependent and various independent variables.\nIt help me in finding the good attributes and bad attributes for training my model as graph clearly shows us the reationship between the variables.\n\xa0 \xa0 \xa0 \xa0\n9.\tInterpretation and Conclusion\nAccuracy Score : 0.7865168539325843\n\nClassification Report:\n\tprecision : 0.80\n\trecall : 0.80\n\tf1-score : 0.80\n\tsupport : 268\n\nAccording to the classification model the Survival of a pessenger depends on their passenger class, age, sex, single or with family.\n'], 'url_profile': 'https://github.com/aarjav22', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Bayesian-Linear-Regression\n贝叶斯线性回归\n'], 'url_profile': 'https://github.com/dengxiuqi', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '238 contributions\n        in the last year', 'description': ['Bayesian-Logisitic-Regression\nBLR\n\nImplemented a Bayesian Logistic Regression model in R for a Bernoulli likelihood.\nAnalysed the obtained fits using Trace, Time, and Auto-Correlation plots to further help in tuning.\n\n'], 'url_profile': 'https://github.com/ayushmclaren', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '820 contributions\n        in the last year', 'description': ['Kaggle Advanced Regession Dataset\nA dataset from kaggle was used in predicting the price of house in a particular region.\n'], 'url_profile': 'https://github.com/Babatunde13', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/q890003', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Passenger-Traffic-Regression\nThe purpose of this project is to assess how well the suggested passenger traffic model predicts passenger traffic based on model inputs and in comparison with existing intra-African air passenger traffic values to and from Kenya. The data is contained in an excel file and contains different variables that are assummed to have an effect on air passenger traffic numbers.\nThe model is based on the following relationship between traffic and the chosen variables:\n𝑇𝑟𝑎𝑓𝑓𝑖𝑐 = 𝐹(𝐺𝐷𝑃,𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒,𝐴𝑆𝐴,𝑉𝑖𝑠𝑎 𝑅𝑒𝑔𝑢𝑙𝑎𝑡𝑖𝑜𝑛𝑠,𝐵𝑢𝑠.𝐷𝑒𝑠𝑡.,𝑇𝑜𝑢𝑟.𝐷𝑒𝑠𝑡.,𝐿𝑎𝑛𝑔𝑢𝑎𝑔𝑒)\n\nThis model is expressed in the following log-linear form:\n𝑙𝑛(𝑇𝑟𝑎𝑓𝑓𝑖𝑐) = 𝛽 + 𝛽1𝑙𝑛(𝐺𝐷𝑃) + 𝛽2𝑙𝑛(𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒) + 𝛽3 𝐴𝑆𝐴 + 𝛽3 𝑉𝑖𝑠𝑎 𝑅𝑒𝑔𝑢𝑙𝑎𝑡𝑖𝑜𝑛𝑠 + 𝛽5 𝐵𝑢𝑠𝐷𝑒𝑠𝑡 + 𝛽6 𝑇𝑜𝑢𝑟𝐷𝑒𝑠𝑡 + 𝛽7 𝐿𝑎𝑛𝑔𝑢𝑎𝑔𝑒 + 𝜀\n\n𝑮𝑫𝑷 is the GDP product of the two countries in the country-pair while 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 is the great Circle distance between the two major airports in the country-pair. 𝑽𝒊𝒔𝒂 𝒓𝒆𝒈𝒖𝒍𝒂𝒕𝒊𝒐𝒏 is a dummy variable that refers to whether citizens of that country need a visa before departure to visit Kenya and whether Kenyan citizens need a visa before departure to visit a specific country. The 𝑨𝑺𝑨 dummy variable captures the liberalization degree for the market provisions Capacity, Designation and Fifth Freedom Rights as well as the overall restrictiveness or liberalness of an ASA. 𝑩𝒖𝒔𝑫𝒆𝒔𝒕 and 𝑻𝒐𝒖𝒓𝑫𝒆𝒔𝒕 are the variables that categorise a country as a tourist or business destination. 𝑳𝒂𝒏𝒈𝒖𝒂𝒈𝒆 is the dummy variable for whether a country has English as one of its official languages or not. 𝜺 is the error term, and 𝒍𝒏 is the natural logarithm.\nThis study shall perform two panel OLS regression models. The first model (Model A) shall incorporate a single ASA variable which captures the overall liberalness or restrictiveness across the three market provisions, namely capacity, designation and fifth freedom rights. The second model (Model B) shall use the dummy variables for the market provisions capacity, designation and fifth freedom rights The model predictions are then compared to see how well each of the models performs and if there is one model that has a better performance than the other.\n'], 'url_profile': 'https://github.com/CandyMagara', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['This is a reference file for the article published in medium : https://medium.com/@tarunkodakandla/understanding-feature-extraction-using-correlation-matrix-and-scatter-plots-6c19e968a60c explaining the feature extraction and the cooncept of correlations.\n'], 'url_profile': 'https://github.com/Tarun-Acharya', 'info_list': ['Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '238 contributions\n        in the last year', 'description': ['Bayesian-Logisitic-Regression\nBLR\n\nImplemented a Bayesian Logistic Regression model in R for a Bernoulli likelihood.\nAnalysed the obtained fits using Trace, Time, and Auto-Correlation plots to further help in tuning.\n\n'], 'url_profile': 'https://github.com/ayushmclaren', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '820 contributions\n        in the last year', 'description': ['Kaggle Advanced Regession Dataset\nA dataset from kaggle was used in predicting the price of house in a particular region.\n'], 'url_profile': 'https://github.com/Babatunde13', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/q890003', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Passenger-Traffic-Regression\nThe purpose of this project is to assess how well the suggested passenger traffic model predicts passenger traffic based on model inputs and in comparison with existing intra-African air passenger traffic values to and from Kenya. The data is contained in an excel file and contains different variables that are assummed to have an effect on air passenger traffic numbers.\nThe model is based on the following relationship between traffic and the chosen variables:\n𝑇𝑟𝑎𝑓𝑓𝑖𝑐 = 𝐹(𝐺𝐷𝑃,𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒,𝐴𝑆𝐴,𝑉𝑖𝑠𝑎 𝑅𝑒𝑔𝑢𝑙𝑎𝑡𝑖𝑜𝑛𝑠,𝐵𝑢𝑠.𝐷𝑒𝑠𝑡.,𝑇𝑜𝑢𝑟.𝐷𝑒𝑠𝑡.,𝐿𝑎𝑛𝑔𝑢𝑎𝑔𝑒)\n\nThis model is expressed in the following log-linear form:\n𝑙𝑛(𝑇𝑟𝑎𝑓𝑓𝑖𝑐) = 𝛽 + 𝛽1𝑙𝑛(𝐺𝐷𝑃) + 𝛽2𝑙𝑛(𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒) + 𝛽3 𝐴𝑆𝐴 + 𝛽3 𝑉𝑖𝑠𝑎 𝑅𝑒𝑔𝑢𝑙𝑎𝑡𝑖𝑜𝑛𝑠 + 𝛽5 𝐵𝑢𝑠𝐷𝑒𝑠𝑡 + 𝛽6 𝑇𝑜𝑢𝑟𝐷𝑒𝑠𝑡 + 𝛽7 𝐿𝑎𝑛𝑔𝑢𝑎𝑔𝑒 + 𝜀\n\n𝑮𝑫𝑷 is the GDP product of the two countries in the country-pair while 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 is the great Circle distance between the two major airports in the country-pair. 𝑽𝒊𝒔𝒂 𝒓𝒆𝒈𝒖𝒍𝒂𝒕𝒊𝒐𝒏 is a dummy variable that refers to whether citizens of that country need a visa before departure to visit Kenya and whether Kenyan citizens need a visa before departure to visit a specific country. The 𝑨𝑺𝑨 dummy variable captures the liberalization degree for the market provisions Capacity, Designation and Fifth Freedom Rights as well as the overall restrictiveness or liberalness of an ASA. 𝑩𝒖𝒔𝑫𝒆𝒔𝒕 and 𝑻𝒐𝒖𝒓𝑫𝒆𝒔𝒕 are the variables that categorise a country as a tourist or business destination. 𝑳𝒂𝒏𝒈𝒖𝒂𝒈𝒆 is the dummy variable for whether a country has English as one of its official languages or not. 𝜺 is the error term, and 𝒍𝒏 is the natural logarithm.\nThis study shall perform two panel OLS regression models. The first model (Model A) shall incorporate a single ASA variable which captures the overall liberalness or restrictiveness across the three market provisions, namely capacity, designation and fifth freedom rights. The second model (Model B) shall use the dummy variables for the market provisions capacity, designation and fifth freedom rights The model predictions are then compared to see how well each of the models performs and if there is one model that has a better performance than the other.\n'], 'url_profile': 'https://github.com/CandyMagara', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['This is a reference file for the article published in medium : https://medium.com/@tarunkodakandla/understanding-feature-extraction-using-correlation-matrix-and-scatter-plots-6c19e968a60c explaining the feature extraction and the cooncept of correlations.\n'], 'url_profile': 'https://github.com/Tarun-Acharya', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['Breast Cancer Regression Analysis\nObjective:\nThe objective of this project is to use R to train and compare the performance of 2 regression models for the prediction of breast cancer tumour characteristics. The tumour characteristic predictors include mean radius, mean perimeter, mean area, mean smoothness, and more.\nUsing 5-fold cross-validation, the following linear regression models were trained and compared:\n\nRidge regression model\nLasso model\n\nTools:\nR - glmnet\nScreenshots:\nMSE vs. log(lambda) - Ridge regression model\n\nMSE vs. log(lambda) - Lasso model\n\n'], 'url_profile': 'https://github.com/ivyfong', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jcartus', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pac-nam', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshitroy2605', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['ML-Regressions-Pclub\nPolynomial and Lasso Regression\n'], 'url_profile': 'https://github.com/IshitManojkumarDarania', 'info_list': ['R', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 5, 2020', 'Python', 'Updated May 9, 2020', 'Updated Jul 20, 2020', 'Updated Apr 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Mini_Project_Logistic_Regression\n'], 'url_profile': 'https://github.com/geralddejean', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HarrisQuang', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prawizard', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\n'], 'url_profile': 'https://github.com/MathewNhari', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Bucuresti', 'stats_list': [], 'contributions': '221 contributions\n        in the last year', 'description': ['Machine_Learning---Linear_Regression\nlinear regresion -- machine learning\n\n'], 'url_profile': 'https://github.com/mhcrnl', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '964 contributions\n        in the last year', 'description': ['Conceptualizing Higher Education Institutions:  An Agent-Based Modelling Approach\n\nLink to cloud hosted simulation experiment data analysis and modelling notebook: \nLink to project paper: Conceptualizing Higher Education Institutions Paper\nLink to datasets: Experiment 1 and  All Experiments\n\n\nExplanation of Repository Contents\n. \n├── README.md     \u2003                  This file \n├── paper.pdf   \u2003                    Project Write-Up \n├── environment.yml     \u2003            Conda environment configuration file (ssed to load project dependencies) \n├── nb.ipynb                  \u2003      Jupyter Notebook used for data analysis and modelling (hosted at the above Binder link) \n├── .gitignore            \u2003          Git file used to ignore non-repo local files \n└── src                  \u2003       Directory containing custom scripts \n\u2003  \u2003  ├── __init__.py \n\u2003  \u2003 ├── agent.py        \u2003            Agent class definition (agent instantiation and opinion variation) \n\u2003  \u2003 ├── data_functions.py     \u2003      Helpful functions to manipulate data \n\u2003  \u2003 ├── data_operations.py    \u2003      Main data file used to prouduce data (utilizes Apache Spark) \n\u2003 \u2003 ├── data_processing.py    \u2003      Short script to fix time data writing issue in simulation \n\u2003 \u2003 ├── environment.py       \u2003       Environment class definiton (establishes agents, holds data, increments time, conducts group negotiations) \n\u2003  \u2003 ├── main.py        \u2003             Script to run collection of experiments \n\u2003 \u2003 ├── model.py      \u2003              Model class definition (sets enviroment, generates collection of experiment parameters, conducts experiments) \n\u2003   \u2003 ├── utilities.py     \u2003           Helpful functions used throughout simulation \n\u2003 \u2003 └── visualization.md     \u2003      Mermaid markdown snippet dump for flowcharts \n\nInstructions for Usage\nenvironment.yml  can be found in the repository\'s root directory and used to install necessary project dependencies. If able to successfully configure your computing environment, then launch Jupyter Notebook from your command prompt and navigate to nb.ipynb. If unable to successfully configure your computing environment refer to the sections below to install necessary system tools and package dependencies. The following sections may be cross-platform compatibile in several places, however is geared towards macOS1.\nDo you have the Conda system installed?\nOpen a command prompt (i.e. Terminal) and run: conda info.\nThis should display related information pertaining to your system\'s installation of Conda. If this is the case, you should be able to skip to the section regarding virtual environment creation (updating to the latest version of Conda could prove helpful though: conda update conda).\nIf this resulted in an error, then install Conda with the following section.\nInstall Conda\nThere are a few options here. To do a general full installation check out the Anaconda Download Page. However, the author strongly recommends the use of Miniconda since it retains necessary functionality while keeping resource use low; Comparison with Anaconda and Miniconda Download Page.\nWindows users: please refer to the above links to install some variation of Conda. Once installed, proceed to the instructions for creating and configuring virtual environments [found here](#Configure-Local-Environment\nmacOS or Linux users: It is recommended to use the Homebrew system to simplify the Miniconda installation process. Usage of Homebrew is explanained next.\nDo you have Homebrew Installed?\nIn your command prompt (i.e. Terminal) use a statement such as: brew help\nIf this errored, move on to the next section.\nIf this returned output (e.g. examples of usage) then you have Homebrew installed and can proceed to install conda found here.\nInstall Homebrew\nIn your command prompt, call: /bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)""\nInstall Miniconda with Homebrew\nIn your command prompt, call: brew install --cask miniconda\nWhen in doubt, calling in the brew doctor might help 💊\nA Few Possibly Useful Conda Commands\nAll environment related commands can be found here: Anaconda Environment Commands\nHere are a few of the most used ones though:\nList all environments (current environment as marked by the *): conda env list\nCreate a new environment: conda create --name myenv\nActivate an environment: conda activate myenv\nDeactivate an environment and go back to system base: conda deactivate\nList all installed packages for current environment: conda list\nConfigure Local Environment\nUsing the command prompt, navigate to the local project repository directory -- On macOS, I recommend typing cd  in Terminal and then dragging the project folder from finder into Terminal.\nIn your command prompt, call: conda env create -f environment.yml. This will create a new Conda virtual environment with the name: higher-education-simulation.\nActivate the new environment by using: conda activate higher-education-simulation\nAccess Project\nAfter having activated your environment, use jupyter notebook to launch a Jupyter session in your browser.\nWithin the Jupyter Home page, navigate and click on nb.ipynb in the list of files. This will launch a local kernel running the project notebook in a new tab.\n\n\n\n\n\n\n\n\n1: This project was created on macOS version 11.0.1 (Big Sur) using Conda version 4.9.2, and Python 3.8 (please reach out to me if you need further system specifications).\n'], 'url_profile': 'https://github.com/wyattowalsh', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Toxic Comment Classification\nReport\nCompleted report is available under ""reports"" directory.\nIntroduction\nTrain models to classify comments, and this is a multi-class, multi-label classification problem. For this problem, I proposed three different models: linear regression, SVM and RNN with LSTM.\n'], 'url_profile': 'https://github.com/DuanRuixiao', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['PredictingLoans\nMachine learning approaches for predicting loans status\nTest codes for implementing a standard logistic regression, CatBoost classification (an improved gradient boost package compared to light GBM and XGboost, no need to treat Categorical variables explicitly) and a probabilistic Neural Network (fully connected MLP) using pymc3 (main feature is being able to quantify uncertainty of parameters and weights).\nData used are from All Lending Club loan data from Kaggle.\n'], 'url_profile': 'https://github.com/qiming82', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ainaimi', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Belgrade, Serbia', 'stats_list': [], 'contributions': '420 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mgajin', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '4', 'Jupyter Notebook', 'Updated Jan 20, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', '1', 'R', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Movie-Box-office-prediction\nIt consist of prediction model for revenue of a film using Linear Regression.\n'], 'url_profile': 'https://github.com/mehra1812shivam', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'Moradabad, India', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Auto-MPG\nRegression model to predict the Miles Per Gallon of a car\n\xa0 \xa0 \xa0 \xa0\nTable of Contents\n\nIntroduction\nPython libraries\nThe problem statement\nLinear Regression\nIndependent and dependent variable\nMultiple Linear Regression (MLR)\nAbout the dataset\nExploratory data analysis\nInterpretation and conclusion\n\n\xa0 \xa0 \xa0 \xa0\n1.\tIntroduction\nIn this notebook, I build Regression model to study the relationship between miles per gallon of a car with different continous and discrete attributes. I implemented this regression model in Python programming language using Scikit-learn,numpy,seaborn,matplotlib and model accuracy is based on cross validation result.\n\xa0 \xa0 \xa0 \xa0\n2.\tPython libraries\n•\tNumpy\n•\tPandas\n•\tMatplotlib\n•\tSeaborn\n•\tScikit-Learn\n\xa0 \xa0 \xa0 \xa0\n3.\tThe problem statement\nThe main aim of this model is to predicting the miles per gallon of a car using some continous and discrete variable data. Finding relationship or dependency of MPG on attrubutes like weight , Number of Cylinders and many more. The accuracy of the model is defined by mean squared error value and cross validation Score. The notebook contain the linear and polynomial regression model for data.\n\xa0 \xa0 \xa0 \xa0\n4.\tLinear Regression\nLinear Regression is a statistical technique which is used to find the linear relationship between dependent and one or more independent variables. This technique is applicable for Supervised Learning Regression problems where we try to predict a continuous variable.\nLinear Regression can be further classified into two types – Simple and Multiple Linear Regression. In this project, I employ Multiple Polynomial Regression technique where I have Multiple independent and one dependent variable.\n\xa0 \xa0 \xa0 \xa0\n5.\tIndependent and Dependent Variables\nIndependent variable\nIndependent or Input variable (X) = Feature variable = Predictor variable\nThe following are the independent variable:-\n1. cylinders:     multi-valued discrete\n2. displacement:  continuous\n3. horsepower:    continuous\n4. weight:        continuous\n5. acceleration:  continuous\n6. model year:    multi-valued discrete\n7. origin:        multi-valued discrete\n8. car name:      string (unique for each instance)\n\nDependent variable\nDependent or Output variable (y) = Target variable = Response variable\nThe following is the dependent variable:-\n1. mpg:           continuous\n\n\xa0 \xa0 \xa0 \xa0\n6.\tMultiple Linear Regression (MLR)\nMultiple Linear Regression also known simply as multiple regression is a statistical technique that uses several explanatory variables to predict the outcome or the response variable.\nThe goal of multiple linear regression is to model the linear relationship between the explanatory variable (independent variable) and response variable.\nThe formula for Multiple Regression is:\n\ty = a0 + a1x1 + a2x2 + ...............\n\nwhere\ny = dependent variable\nxi= independent variable\na0= y-intercept\nai= slope coefficients for each independent variable\n\xa0 \xa0 \xa0 \xa0\n7.\tAbout the dataset\n1. Title: Auto-Mpg Data\n\n2. Sources:\n   (a) Origin:  This dataset was taken from the StatLib library which is\n                maintained at Carnegie Mellon University. The dataset was \n                used in the 1983 American Statistical Association Exposition.\n   (c) Date: July 7, 1993\n\n3. Relevant Information:\n\n   This dataset is a slightly modified version of the dataset provided in\n   the StatLib library.  In line with the use by Ross Quinlan (1993) in\n   predicting the attribute ""mpg"", 8 of the original instances were removed \n   because they had unknown values for the ""mpg"" attribute.  The original \n   dataset is available in the file ""auto-mpg.data-original"".\n\n   ""The data concerns city-cycle fuel consumption in miles per gallon,\n    to be predicted in terms of 3 multivalued discrete and 5 continuous\n    attributes."" (Quinlan, 1993)\n\n\xa0 \xa0 \xa0 \xa0\n8.\tExploratory data analysis\nTo summarize the main characteristics of data and finding the patterns I use plots to analysed it using Data Visualization by ploting graphs of univariate, bivariate to find the distribution of particulat attribute and the relationship between the dependent and independent attribute by using plots like regression plot ,box plot ,histogrames, and distribution plot.It help me in finding the good attributes and bad attributes for training my model as graph clearly shows us the reationship between the variables.\n\xa0 \xa0 \xa0 \xa0\n9.\tInterpretation and Conclusion\nLinear Regression:\n MSE value: 0.1764742397827893\n \n Accuracy Score : 0.8313833993698124\n \n cross validation score: 0.5906438717928257\n\nPolynomial Regression:\n MSE value: 6.887827289424193\n \n Accuracy Score : 0.8317803894705068\n\n cross Validation score: 0.4403178468015085\n\nAfter looking at the scores like accuracy score ,mean squared error and cross validation score we can conclude that polynomial regression has a little edge on accuracy score but cross validation score and mean squared score of linear regression are much better then that of polynomial regression so we conclude that linear regression fit the data much better as compair to polynomial regression.\n\xa0 \xa0 \xa0 \xa0\n'], 'url_profile': 'https://github.com/aarjav22', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'Lowell, Massachusetts', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Telco-Churner-Classification\nClassification and Regression Trees to generate decision rules for churning and non-churning subscribers\n'], 'url_profile': 'https://github.com/mmhapusk', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexdebolt', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['CSCI5622_ML_StockMarketPrediction\nStock Market Prediction using LSTM, Linear Regression and feature Analysis\n'], 'url_profile': 'https://github.com/sira7740', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['python_tkinter\nthis project includes python program for linear regression using gui (tkinter)\nSteps to run project:\n1.create a new project into you python id\n2.create a python file in that project.\n3.paste the code from the file ""GUI for linear regression""\n4.run and enjoy.\nYou will require the perticular modules installed like tkinter,scipy,numpy,matpotlib etc.\nso to install them go to File>settings>project:project name>project intepreter>click on the plus icon on right side> search module required and inatll\nit one by one.\nthank you.\n'], 'url_profile': 'https://github.com/nirajgolhar', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Detecting Structural Breaks with Statistical Analysis in R\nKornel Skórka &\nJulian Szachowicz\nAbstract\nEconomists and the financial brotherhood of the world has always taken analysis of breakpoints intime series of data seriously. There is no wonder in the fact that there is great focus on automated(algorithm-aided) detection of these breakpoints where human error and bias can greatly influence theoutcome of analysis.\nThis paper aims to present a simple approach to breakpoint detection using the power of the statis-tical R programming language with fluctuation-based regression model parameter change analysis.\nWe demonstrate the approach on percentage change of RGDP (Real Gross Domestic Product) forboth developing and developed countries (from 1950 to 2020 where applicable) based on the availabledata in the open source World Penn Tables.\nRead the project report on Overleaf.\n'], 'url_profile': 'https://github.com/julzerinos', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Covid-19-LinReg\nPerformed Linear Regression on COVID-19 dataset from Kaggle to predict number of infected people\n'], 'url_profile': 'https://github.com/lelouch0204', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'Norway', 'stats_list': [], 'contributions': '354 contributions\n        in the last year', 'description': ['Facial expression classification implemented using logistic and softmax regression\nEverything is implemented from scratch, including the PCA transformation, by using numpy.\nHow to run\nRun the Main.py\nTo change the configuration, edit Settings.py\n'], 'url_profile': 'https://github.com/jonryf', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['cab-price-prediction-with-linear-regression\nPredicting cab price using Linear Regression on uber-lyft cab prices data set\n'], 'url_profile': 'https://github.com/srslakshmi1997', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'R', 'MIT license', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nouman945', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jakemdaly', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Logistic-Regression-with-Neural-Network-Mindset\nThis is a logistic regression implementation using neural network mindset to classify between two objects\nthis is a simple logistic regression but with vectorization and you can modify the code by just changing your file directory\n'], 'url_profile': 'https://github.com/prithvishkr', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Coimbatore,India', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jawaharbabugit', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Backtesting on 22 firm-characteristic factors\nObjective\nBacktest 22 firm-characteristics risk factors to evaluate their explanatory power to the future stock return.\nMethod\ncross-sectional linear regression, Fama-MacBeth\nCreated DataSet\n\nWrote python functions to calculate 3 technical factors(MACD, RSI, BBP) by using daily closing prices; see the code here 3technical_factors.ipynb\nCollected firm-characteristic data(risk factors) from WRDS, Compustat, comp.funda etc for stock universe. The stock universe includes NYSE/AMEX/Nasdaq stocks, only common stocks (not preferred, etc.). The risk factors are across several different categories.\n\n\nCollected annual return for development set and monthly return for validation set from WRDS\n\nData wrangling and cleaning\n\nSince some factors(eg. ROA...) have only value at midyear, while other factors(eg. turnover rate...) have value on each day, we use the mid-year value or calculated the mean value from midpoint of last year to the midpoint of this year. see code here TO_year.ipynb\n\n \n\nMerged all the factors data on stock ""permno"" and ""year"". Merged factors data with the return next year\nRemoved factors with too many missing values to ensure the final dataset has enough observations\nRemoved stocks have a price that exceeds $5 per share and a market capitalization of equity of at least $100 million at the beginning of a given forecast year, to avoid trading illiquid stocks\nNormalized factor data using z-score by industry\n\nFinal dataset description\n4488 unique stocks of 26 years (1990.6 - 2016.6),\nwith 34,560 observations and 27 columns(22 risk factors plus index, permno, year, return, and price) (760,000+ points)\nSeparated data into in-sample data (1990.6-2008.6) and out-of-time data (2009.6-2016.6).\nCorrelation analysis\ncorrelation = corr.test((train[,.(zEP,zIA,zIG,zIK,zLEV,zNOA,zNS,zOK,zROA,zROE,zlnSIZE,zMOM,zOS,\n                                  zSG,zSUE,zBETA,zCI,zLTR,zTO,zRSI,zMACD,BBP,RET)]))\n\ncorrelation[[""r""]]\nwhich(correlation[[""r""]]>=0.5, arr.ind = T)\n\ncorrelation[[""p""]]\nwhich(correlation[[""p""]] < 0.05, arr.ind = T)\n\nremove features has high correlation(>=0.5) with high significance(<0.05)\nIn the sample estimation(1990-2008)\nRun cross-sectional regression during each estimation window(year).\nEstimated the factor premia on each factor during each estimation window, then compute the average factor premia(the Fama-MacBeth average) across all windows and the t-statistic(Fama-MacBeth t-statistic) of this average.\n#calculate Fama-MacBeth t-statistic\n\ncoefficients_raw = train[, as.list(coef(lm(RET ~      zEP+zIA+zIG+zIK+zLEV+zNOA+zNS+zOK+zROA+zROE+zlnSIZE+zMOM+zOS+\n          zSG+zSUE+zBETA+zCI+zLTR+zTO+zRSI+zMACD+BBP))), by = year]\n\n(coefmean_raw = apply(coefficients_raw[, .SD, .SDcol = - ""year""],2,mean))\n\n(coefsd_raw = apply(coefficients_raw[, .SD, .SDcol = - ""year""],2,sd))\n\n         #1990-2008 the number of windows is 18, square root use 18\n(tstat_raw = coefmean_raw / coefsd_raw * sqrt(18))\n\n         #pick features which has FM>1.3, do it again\ncoefficients = train[, as.list(coef(lm(RET ~ zLEV+zNS+zOK+zOS+zSG+\n                                        zSUE+zLTR+zRSI+zMACD+BBP))), by = year]\n\n(coefmean = apply(coefficients[, .SD, .SDcol = - ""year""],2,mean))\n(coefsd = apply(coefficients[, .SD, .SDcol = - ""year""],2,sd))\n(tstat = coefmean / coefsd * sqrt(18))\n\n\n\n\nKeep factors with absolute FM t-statistic above 1.3 and matched expected sign.\nThose factors with blue highlight are selected.\nOut of time validation(2009-2016)\n\nStep1: score stocks during the out-of-sample period. Weighted each stock’s factor exposure z-score  by the t-statistic derived for that factor in the regression model.\nScore = z-score(1) x t-stat(1) + z-score(2) x t-stat(2) + …\n\n#out of sample validation\n#import full sample of monthly returns of stocks\ntest = setDT(read.csv(""Test_zscore.csv""))\nret = setDT(read.csv(\'Monthly_Test.csv\'))\nsetnames(ret, c(\'permno\', \'date\', \'ticker\',\'PRC\', \'ret\'))\n\nret[, date := as.Date(as.character(date), format = ""%Y%m%d"")]\nret[, year := year(date)]\nret[, month := month(date)]\nwrite.csv(ret,""ret.csv"")\nret = setDT(read.csv(""ret.csv""))\nret = na.omit(ret)\nret[, ret := as.numeric(as.character(ret))]\nsetnames(ret,c(\'x\',\'PERMNO\', \'date\', \'ticker\',\'PRC\', \'ret\',\'year\',\'month\'))\n\n#calculate score for each stock\ntest[, score := zLEV * tstat[2]  + zNS * tstat[3] + zOK * tstat[4]+zOS * tstat[5]+zSG*tstat[6] + zSUE * tstat[7]+ zLTR * tstat[8] + zRSI*tstat[9] + zMACD * tstat[10]+BBP*tstat[11]]\ntest = na.omit(test)\n\n\nStep2: Ranked stocks by the score and then divided into 10 quantile. Developed a zero investment portfolio by long stocks in the first quantile and short stocks in the last quantile.\n\n#define long and short portfolio\ntest[, group := findInterval(score, quantile(score, c(0.1, 0.9))), by = year]\n\nlong = test[group == 2]\nshort = test[group == 0]\n\n\nfor(i in c(2009:2016)){\n  longRet = ret[PERMNO %in% long[year == i]$PERMNO & year == i + 1]\n  longRet = longRet[, .(longRet = mean(ret, na.rm = TRUE)), by = .(year, month)]\n\n  shortRet = ret[PERMNO %in% short[year == i]$PERMNO & year == i + 1]\n  shortRet = shortRet[, .(shortRet = mean(ret, na.rm = TRUE)), by = .(year, month)]\n\n  output = merge(longRet, shortRet, by = c(\'year\', \'month\'))\n\n  if (i == 2009) LSport = output\n  else LSport = rbind(LSport, output)\n}\n\nLSport[, LSret := longRet - shortRet]\n\n\n\nPerformance Assessment\n##performance assessment\nFFdata = setDT(read.csv(""FF.csv""))\n\nFFdata[, date := as.Date(as.character(dateff), format = ""%Y%m%d"")]\nFFdata[, year := year(date)]\nFFdata[, month := month(date)]\n\nLSport = merge(LSport, FFdata, by = c(\'year\', \'month\'))\nLSport[, y := LSret]\n\n#simple mean\napply(LSport[, .(longRet, shortRet, LSret)], 2, mean)\n\n#geometric mean\nLSport[, lapply(.(longRet, shortRet, LSret), function(x) prod(1 + x)^(1/.N)-1)]\n#annualised return\nLSport$grossret <- LSport$LSret + 1\napply(LSport[, .(grossret)], 2, prod)\nannual_ret = LSport[, .(ret = prod(1+LSret) -1), by = year]\n\n#Sharpe ratio\n(sr = LSport[, mean(LSret)/sd(LSret)])\n\n# annualized SR\n(sr = sqrt(12)*sr)\n# CAPM\nCAPM = lm(LSret ~ mktrf, LSport)\nsummary(CAPM)\n#FF 3 factor\nFF3 = lm(LSret ~ mktrf + smb+ hml, LSport)\nsummary(FF3)\n\n#corhart 4 factor\nC4 = lm(LSret~mktrf + smb + hml + umd, LSport)\nsummary(C4)\n\n#Information rate\n(ir = coef(C4)[1]/sd(C4$residuals))\n\n#annualized IR\n(ir = sqrt(12)* ir)\n\n\n\nThe annualized mean return of S&P500 was 11.92% and the average borrowing interest rate was around 3.5% during that period, which means our portfolio strategy bellow the market return a little during those years. However, considering market risk, interest rate risk that are hedged by the long-short strategy, our portfolio strategy might beat the market.\nThe alpha of CAPM, FF-3-factors model, and Corhart-4-factors model are pretty low, which means the return of our strategy can be explained mostly by market premium, size premium, value premium, and momentum.\nsee slides here Backtesting_presentation.pdf\n'], 'url_profile': 'https://github.com/JingsiTheExplorer', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Kurukshetra, Haryana, India', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sumit100ni', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Kaunas', 'stats_list': [], 'contributions': '293 contributions\n        in the last year', 'description': ['Machine-Learning\n'], 'url_profile': 'https://github.com/pijoneris', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/viveksingh120300', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Forecasting-Demand-with-ARIMA-Time-Series-Model-and-Regression\nCombinig ARIMA Time Series Model and Regression\n'], 'url_profile': 'https://github.com/firatgunduz', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}"
"{'location': 'Karachi, Pakistan.', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/msshahzaib777', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': [""log_reg_practice\nHere is practice of using logistic regression model on Titanic data frame (to predict whether passenger survived or not)\nPlease, check file 'Logistic regression training on titanic df.ipynb'\nResume:\n\nTitanic data frame\ndata cleaning & brief EDA, data preprocessing\nlogistic regression model to predict whether passenger survived or not\nfeatures importance + metrics\n\nYou will find further explanation in the file\n""], 'url_profile': 'https://github.com/liavonavaanastasiya', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['gee-efficiency\nMethods for Estimating Variance of GEE Logistic Regression Estimators under Various Working Correlation Structures\nLast Update: January 25, 2021\nThe data used in the Manuscript_Figures.R code can be downloaded from the replication files of Lin et al. (2018) at https://osf.io/c7u8b/.\n'], 'url_profile': 'https://github.com/leekshaffer', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Neural-Networks-stuff-\nNotebooks on the use of neural networks to solve classification and regression tasks\n'], 'url_profile': 'https://github.com/savecinieri', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['CLASSIFICATION WITH LINEAR REGRESSION\nlinear classification functions (LINEAR REGRESSION) used to classify images in taken from MINST Digits Dataset Digits data-set.\npython implementeation of a full processing pipeline from feature extraction leading to classifier training.\nIn training the classifier cross-validation and regularization( RIGE PENALTY) techniques have been preformed to tune the classification performance.\nPDF CONTAINS FULL REPORT\n'], 'url_profile': 'https://github.com/Atit-Bashyal', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anonymous404researcher', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '268 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ernestng11', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/irskid5', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""@modelx/Model - Machine Learning and Neural Networks with Tensorflow\n \nGetting started\nClone the repo and drop your module in the src directory.\n# Install Prerequisites\n$ npm install rollup typedoc jest sitedown --g\nBasic Usage\n$ npm run build #builds type declarations, created bundled artifacts with rollup and generates documenation\nIntroduction\nThis library is a compilation of model building modules with a consistent API for quickly implementing Tensorflow at edge(browser) or any JavaScript environment (Node JS / GPU).\nRead the manual\nList of Tensorflow models\nClassification\n\nDeep Learning Classification: DeepLearningClassification\nLogistic Regression: LogisticRegression\n\nRegression\n\nDeep Learning Regression: DeepLearningRegression\nMultivariate Linear Regression: MultipleLinearRegression\n\nArtificial neural networks (ANN)\n\nMulti-Layered Perceptrons: BaseNeuralNetwork\n\nLSTM Time Series\n\nLong Short Term Memory Time Series: LSTMTimeSeries\nLong Short Term Memory Multivariate Time Series: LSTMMultivariateTimeSeries\n\nBasic Usage\nTensorScript is and ECMA Script module designed to be used in an ES2015+ environment, if you need compiled modules for older versions of node use the compiled modules in the bundle folder.\nPlease read more on tensorflow configuration options, specifying epochs, and using custom layers in configuration.\nRegression Examples\nimport { MultipleLinearRegression, DeepLearningRegression, } from '@modelx/model';\nimport ms from 'modelscript';\n\nasync function main(){\n  const independentVariables = [ 'sqft', 'bedrooms',];\n  const dependentVariables = [ 'price', ];\n  const housingdataCSV = await ms.csv.loadCSV('./test/mock/data/portland_housing_data.csv');\n  const DataSet = new ms.DataSet(housingdataCSV);\n  const x_matrix = DataSet.columnMatrix(independentVariables);\n  const y_matrix = DataSet.columnMatrix(dependentVariables);\n  const MLR = new MultipleLinearRegression();\n  await MLR.train(x_matrix, y_matrix);\n  const DLR = new DeepLearningRegression();\n  await DLR.train(x_matrix, y_matrix);\n  //1600 sqft, 3 bedrooms\n  await MLR.predict([1650,3]); //=>[293081.46]\n  await DLR.predict([1650,3]); //=>[293081.46]\n}\nmain();\nClassification Examples\nimport { DeepLearningClassification, } from '@modelx/model';\nimport ms from 'modelscript';\n\nasync function main(){\n  const independentVariables = [\n    'sepal_length_cm',\n    'sepal_width_cm',\n    'petal_length_cm',\n    'petal_width_cm',\n  ];\n  const dependentVariables = [\n    'plant_Iris-setosa',\n    'plant_Iris-versicolor',\n    'plant_Iris-virginica',\n  ];\n  const housingdataCSV = await ms.csv.loadCSV('./test/mock/data/iris_data.csv');\n  const DataSet = new ms.DataSet(housingdataCSV).fitColumns({ columns: {plant:'onehot'}, });\n  const x_matrix = DataSet.columnMatrix(independentVariables);\n  const y_matrix = DataSet.columnMatrix(dependentVariables);\n  const nnClassification = new DeepLearningClassification();\n  await nnClassification.train(x_matrix, y_matrix);\n  const input_x = [\n    [5.1, 3.5, 1.4, 0.2, ],\n    [6.3, 3.3, 6.0, 2.5, ],\n    [5.6, 3.0, 4.5, 1.5, ],\n    [5.0, 3.2, 1.2, 0.2, ],\n    [4.5, 2.3, 1.3, 0.3, ],\n  ];\n  const predictions = await nnClassification.predict(input_x); \n  const answers = await nnClassification.predict(input_x, { probability:false, });\n  /*\n    predictions = [\n      [ 0.989512026309967, 0.010471616871654987, 0.00001649192017794121, ],\n      [ 0.0000016141033256644732, 0.054614484310150146, 0.9453839063644409, ],\n      [ 0.001930746017023921, 0.6456733345985413, 0.3523959517478943, ],\n      [ 0.9875779747962952, 0.01239941269159317, 0.00002274810685776174, ],\n      [ 0.9545140862464905, 0.04520365223288536, 0.0002823179238475859, ],\n    ];\n    answers = [\n      [ 1, 0, 0, ], //setosa\n      [ 0, 0, 1, ], //virginica\n      [ 0, 1, 0, ], //versicolor\n      [ 1, 0, 0, ], //setosa\n      [ 1, 0, 0, ], //setosa\n    ];\n   */\n}\nmain();\nimport { LogisticRegression, } from '@modelx/model';\nimport ms from 'modelscript';\n\nasync function main(){\n  const independentVariables = [\n    'Age',\n    'EstimatedSalary',\n  ];\n  const dependentVariables = [\n    'Purchased',\n  ];\n  const housingdataCSV = await ms.csv.loadCSV('./test/mock/data/social_network_ads.csv');\n  const DataSet = new ms.DataSet(housingdataCSV).fitColumns({ columns: {Age:['scale','standard'],\n  EstimatedSalary:['scale','standard'],}, });\n  const x_matrix = DataSet.columnMatrix(independentVariables);\n  const y_matrix = DataSet.columnMatrix(dependentVariables);\n  const LR = new LogisticRegression();\n  await LR.train(x_matrix, y_matrix);\n  const input_x = [\n    [-0.062482849427819266, 0.30083326827486173,], //0\n    [0.7960601198093905, -1.1069168538010206,], //1\n    [0.7960601198093905, 0.12486450301537644,], //0\n    [0.4144854668150751, -0.49102617539282206,], //0\n    [0.3190918035664962, 0.5061301610775946,], //1\n  ];\n  const predictions = await LR.predict(input_x); // => [ [ 0 ], [ 0 ], [ 1 ], [ 0 ], [ 1 ] ];\n}\nmain();\nTime Series Example\nimport { LSTMTimeSeries, } from '@modelx/model';\nimport ms from 'modelscript';\n\nasync function main(){\n  const dependentVariables = [\n    'Passengers',\n  ];\n  const airlineCSV = await ms.csv.loadCSV('./test/mock/data/airline-sales.csv');\n  const DataSet = new ms.DataSet(airlineCSV);\n  const x_matrix = DataSet.columnMatrix(independentVariables);\n  const TS = new LSTMTimeSeries();\n  await TS.train(x_matrix);\n  const forecastData = TS.getTimeseriesDataSet([ [100 ], [200], [300], ])\n  await TS.predict(forecastData.x_matrix); //=>[200,300,400]\n}\nmain();\nSpecial Thanks\n\nMachine Learning Mastery\nSuper Data Science\nPython Programming\nTowards Data Science\nml.js\n\nLicense\nMIT\n""], 'url_profile': 'https://github.com/repetere', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Regression-models-red-wine-dataset\nImplementing various regression models on red wine dataset and predicting its quality\n'], 'url_profile': 'https://github.com/meghanakolluri', 'info_list': ['Rich Text Format', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 30, 2020', 'R', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated Jun 22, 2020', 'HTML', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 3, 2020', 'C++', 'Updated Apr 8, 2020', 'TypeScript', 'MIT license', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['housing-data\nthis repository contains the linear regression model of lotsize to price prediciton\n'], 'url_profile': 'https://github.com/sakshi97st', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['COVID-19\nSmall applications using data from the new coronavirus such as graphics, regression algorithms and related stuff\n'], 'url_profile': 'https://github.com/dedeus10', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'İstanbul - T/rkiye', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['PREDICTIVE_ANALYTICS_CLASSIFICATION\nApplication of multi variate classification with Random forest, logistic regression and ann in Knime Analytics platform\nPrediction of the LETTR field in letters.csv. Random forest, logistic regression and ann classification methods are used. The problem is called multivalued classification. The values \u200b\u200bto be estimated consist of LETTERS. The file consists of 16 attributes related to the letters.\nIn CSV Reader configuration, the letter.csv file should be used as data.\n'], 'url_profile': 'https://github.com/mopeneye', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Sentiment App\nA dash application on heroku https://dash-sentiment.herokuapp.com/ using an API deployed on AWS Beanstalk\nInput sentence to calculate sentiment. Logistic Regression model was used for text classication of sentiment with an accuracy of 87%\n'], 'url_profile': 'https://github.com/ayaanlehashi', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['classification-of-medical-tweets\nClassifying medical tweets with TF deep neural network applying logistic regression\n'], 'url_profile': 'https://github.com/shahrukh-ak', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/irskid5', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'Lowell, Massachusetts', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Marketing-Analytics\nRegression model to predict effectiveness of marketing campaigns for a major supermarket chain\n'], 'url_profile': 'https://github.com/mmhapusk', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['covid19-LogisticAndExponential-_-India-Predict-Trend\nUsing Logistic regression to predict death due to corna Virus.\nWe will compare it with Exponential case .\nNote: Exponential Case is the Extreme Case where all of the population gets infected in next 2 weeks time.\nLogistic Regression is the ideal case we need it to end, with the current trend shown.\nBut the reality will be in between these 2 curves , and it depends on a lot of actions taken, policies and mistakes and economic states.\nIndia Estimates as of 02-04-2020 .\non Ideal case it stops with 4000 to 5000 peoples with little changes for coming months. Or on Extreme case 5 lakh get infected in next 2 weeks. More data or facts give more clarity for prediction.\n\nItaly have reached their final stages. so Logistical Curve may hold true. As almost 20% of their population got infected as per government sources. Whereas in India recorded Infected peoples are 0.012% only.\n\n'], 'url_profile': 'https://github.com/Maverick7', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'İstanbul - T/rkiye', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['IAN504_PREDICTIVE_ANALYTICS\nAfter the knwf file is imported in Knime on the computer, the USA_Housing2.csv file in workflow should be set to be used as data in the configuration of the File Reader node.\n'], 'url_profile': 'https://github.com/mopeneye', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}","{'location': 'Athens', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['linear-models-lab\nAssignments for the labs of Linear Regression Analysis course of Spring Semester 2020\n'], 'url_profile': 'https://github.com/harrisrodis', 'info_list': ['Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Sep 29, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'SAS', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 2, 2020', 'Updated Sep 29, 2020', 'R', 'Updated May 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rbi-international', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['LDA_by_python\nDimention reduction by using LDA(Linear Discriminant Analysis) to solve logistic Regression\n'], 'url_profile': 'https://github.com/amitdivekar30', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '145 contributions\n        in the last year', 'description': ['Digit_Recognizer\nBuilt a oneVsAll Logistic Regression Model using gradient descent with regularization.\nThe dataset for the model is in this link https://www.kaggle.com/c/digit-recognizer/data\nThe data contains 42000 training examples and 28000 test examples.\nI choose the regularization parameter lambda by plotting precision with various value of lambda.\nThe Kaggle Score is 0.9/1\n'], 'url_profile': 'https://github.com/gauravmadan583', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'New York, New York', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/himabindu13198', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'Cracow', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Boston House Prices With Regression\nZadaniem projektu jest utworzenie modelu cen mieszkań w Bostonie przy wykorzystaniu różnych metod regresji.\nDane dot. mieszkań zawierają informacje, takie jak wskaźnik przestępczości na mieszkańca miasta, odsetek terenów mieszkalnych przeznaczonych pod działki o powierzchni ponad 25 000 stóp kwadratowych czy nawet stosunek uczniów do nauczycieli według miast. Zakładamy, iż model nauczy się z tych danych i przy użyciu tych metryk będzie w stanie przewidywać medianę cen mieszkań w dowolnym mieście w Bostonie.\nJęzyk: Python\nZaprezentowany został również w dokumentacji wstęp teoretyczny do wykorzystywanych zagadnień.\nDokumentacja: link\n'], 'url_profile': 'https://github.com/Happis255', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Predicting_House_Price\nLinear Regression Model to predict house price based on different parameters\nThe problem statement is given in the Jupyter Notebook.\n'], 'url_profile': 'https://github.com/spandanpal22', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'Algiers, Algeria', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ciliamadani', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'NYC ', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Exploratory Data Analysis, Linear Regression, and Time Series modeling of worldwide aggregated covid-19 modeling dataset https://github.com/datasets/covid-19.\n'], 'url_profile': 'https://github.com/hyperobjects', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'Bhubaneswar, India', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['House_Price_Prediction\nHouse Price Prediction using Polynomial Features - Ridge Regression and extensive Feature Engineering\nThe Dataset and the solution(both in ipynb and py extensions) have been uploaded\nThe Heatmap both old and new are there to show the impact of Feature Engineering.\nThe Metrics both old and new are there to show the impact of the aforementioned.\nThe Variable Distribution image is show to the need of feature engineering.\n'], 'url_profile': 'https://github.com/shraja11', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}","{'location': 'Ilmenau, Germany', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amir78698', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'Python', 'Updated Apr 4, 2020', 'Python', 'Updated Apr 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 30, 2020']}"
"{'location': 'Denver, CO', 'stats_list': [], 'contributions': '889 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tylorschafer', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'Dominican Republic', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ivan-verges', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marioeid', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mhmtsvr', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Arunav-Dey', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Decision-tree-and-logistic-regression\nI did one of Decision Tree and one of Logistics since I did not understand how logistics worked, so I researched and found a more specific example about Logistics\n'], 'url_profile': 'https://github.com/Miguel-SG6', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'Las Vegas, NV', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': [""Multi-Variate Linear Regression Model\nBuilding a Multi-Variate Linear Regression Model using King County,WA House Prices Dataset\nUsing Scikit-Learn\n\n\n\nThis project will make use of Pandas, and NumPy for the data exploration phase as well as using Matplotlib and Seaborn to form visualizations. We will then be using Scikit-Learn to model our multi-variate linear regression. We will incorporate some dummies datasets creation to deal with categorical data as well as log-transformation methodology to deal with the continuous features of the dataset.\nColumns we will be examining:\n\nExample of dataframe head and tail:\n\n\n\nPurpose of project:\nThe purpose of this project is to come up with ways in which to maximize profitability for sellers attempting to sell a home in King County,WA. We will search for actionable insights that will serve guidance to these sellers, but we need a thorough understanding of the dynamics of the housing market in order to drive our calculated decisions.\n3 Recommendations I would suggest to sellers:\nRecommendation # 1:\n\n\n\nMy first recommendation to sellers would be to make living space square footage their focal point. Correlation between square footage of living space and price of the home is fairly high compared to the other features. It is clear that larger homes mandate higher asking prices. Selling homes on the larger-end of the spectrum are guaranteed to generate the most revenue.\nRecommendation # 2:\n\n\n\nMy second recommendation would be to pay particular attention to the locality of the home. House prices are clustered according to zipcode. Many factors and variables, tied into the zipcode, may influence the price either positively or negatively and we must be mindful of that.\nRecommendation #3:\n\n\n\nMy third recommendation would be to attend to the grade given by King County to the home. It is very influential in the price of the home. In general, as the grade increases, the price increases as well. This highlights the positive linear correlation between the two.\n\n\n\nSidenote: The grade distribution follows a normal curve, which suggests that they are being issued in a forthright and diligent manner. If interested it would be engaging to see what goes into the grading component of the homes. But that's a project for another time.\n\n\n\nThis correlational heatmap was used throughout the project to guide me in the feature selection process and may be very helpful and finding other interesting correlational to experiment with.\nMulti-variate linear regression model using Scikit-Learn:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease take a look at the jupyter notebook file included with this repository. I include bonus recommendations and future work/research to keep in mind if you hope to expand on my work.\nKey takeways:\n1. Make living space square footage your number one feature to look out for.\n2. Location is an extremely important feature when evaluating the price of a home.\n3. The grade given to a home by the King County Housing Department is very influential in the price.\n""], 'url_profile': 'https://github.com/lopez-christian', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Web scraping and linear regression to predict job satisfaction\nI build a web scraper to collect job and company details from Glassdoor. I then employ linear regression (simple, Ridge, Lasso) to predict satisfaction and compensation levels in Data Science jobs.\nData\nData was scraped using Selenium and Beautiful Soup, from Glassdoor job listings and city cost of living indices.\nApproximately 2,000 job listings were scraped, and 1,000 were used in the analysis. The cost of living data was scraped for all relevant cities.\nThe scraped dataset included job titles and categories, salaries, benefits ratings, company reviews, company details, required skills, city information.\nMethods\nWeb scraping using Beautiful Soup and Selenium\n\nBrowser interaction, HTML and JSON parsing to extract relevant values\n\nFeature engineering\n\nOne-hot-encoding categorical variables\nPolynomial and interaction terms\n\nLinear regression\n\n10-fold Cross Validation\nUse of linear regression, including Ridge and Lasso techniques to prevent overfitting\n\nData manipulation and analysis\nSelenium\nBeautiful Soup\nre\nnumpy\npandas\nsklearn\n\nFront end and visualization\nmatplotlib\nseaborn\nTableau\n\n'], 'url_profile': 'https://github.com/dimitermilev', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['LinearRegressionUsingAdertisingDataset\nA linear regression model which predicts the sales of a product depending on its advertisement in different medium\n'], 'url_profile': 'https://github.com/Debjit08', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Egypt COVID-19 Cases Regression\nUsing ML polynomial regression to predict the daily number of coronavirus(COVID-19) cases in Egypt.\n'], 'url_profile': 'https://github.com/OmarAli3', 'info_list': ['Python', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020']}"
"{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Arunav-Dey', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'Udaipur', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['It is non linear relationship b/w the values of x and y\nTo generate a higher order equation we can add powers of the original features as new features.\n'], 'url_profile': 'https://github.com/kaustavmitra26', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'Jakarta', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Time-Series-Decision-Tree-Regression\nExample of Decision Tree Modeling (Machine Learning) for Time Series data in Python\n'], 'url_profile': 'https://github.com/iqbalhanif', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'Jaipur', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aditya-171', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'Salinas, California ', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['ML-Project-Multiple-Linear-Regression-Python\nMultiple linear regression with Python on the effectiveness of different forms of advertising on sales. The independent variables or features are newspaper, radio, and TV advertising spending. The dependent or response variable is sales.\nThis includes:\n\nPython notebook with the relevant code.\nThe dataset used for the analysis.\n\n'], 'url_profile': 'https://github.com/diego-gomez92', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PhadonP', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'Udaipur', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).\nThe Formula for Multiple Linear Regression is:\ny\ni\n\u200b\t =β\n0\n\u200b\t +β\n1\n\u200b\t x\ni1\n\u200b\t +β\n2\n\u200b\t x\ni2\n\u200b\t +...+β\np\n\u200b\t x\nip\n\u200b\t +ϵ\nwhere, for i=n observations:\ny\ni\n\u200b\t =dependent variable\nx\ni\n\u200b\t =expanatory variables\nβ\n0\n\u200b\t =y-intercept (constant term)\nβ\np\n\u200b\t =slope coefficients for each explanatory variable\nϵ=the model’s error term (also known as the residuals)\n\u200b\nThe Difference Between Linear and Multiple Regression\nLinear (OLS) regression compares the response of a dependent variable given a change in some explanatory variable. However, it is rare that a dependent variable is explained by only one variable. In this case, an analyst uses multiple regression, which attempts to explain a dependent variable using more than one independent variable. Multiple regressions can be linear and nonlinear.\nMultiple regressions are based on the assumption that there is a linear relationship between both the dependent and independent variables. It also assumes no major correlation between the independent variables.\n'], 'url_profile': 'https://github.com/kaustavmitra26', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'Boston, MA, USA', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': [""Solutions to Assignment 4 (MET CS777)\nSubmitted by: Gagan Kaushal (gk@bu.edu)\nAnswers\nTASK 1\nFrequency position of the required words:\n('and', '->', 2)\n('car', '->', 648)\n('attack', '->', 512)\n('applicant', '->', 448)\n('protein', '->', 3167)\nTASK 2\nFive words with the largest regression coefficients.\n(That is, those five words that are most strongly related with an Australian court case)\nIn each of the below tuples,\n\nx[0] represents one of the top 5 words\nx[1] represents word's regression coefficient\nx[2] represents word's dictionary position\n\nTop 5 Words with the largest regression coefficients:\n(u'costs', 0.39022157929178491, 780)\n(u'application', 0.33396109877567826, 480)\n(u'my', 0.32493294190987604, 383)\n(u'australia', 0.32213883652377767, 354)\n(u'orders', 0.31890982333629891, 949)\nTASK 3\nOur classifier generated zero false positives.\n####################\nNumber of True Positives: 375\nNumber of False Positives: 0\nNumber of False Negatives: 2\nNumber of True Negatives: 18347\nF1 score for classifier = 99.73 %\n####################\nProject description (Explanation of the algorithms leveraged in the 3 python spark programs)\nTASK 1\nHow to run\nRun the task 1, task 2 and task 3 as per the below templates by submitting the tasks to spark-submit.\nTask 1 - template\nspark-submit <task_name> <Training_dataset> <output_folder_for_results>\nTask 1 - Small Dataset\nspark-submit main_task1.py SmallTrainingData.txt Output_Task1\nTask 1 - Large Dataset\n-- pyspark file location\ngs://gagankaushal/Assignment_4/main_task1.py\n\n-- arguments\ngs://metcs777/TrainingData.txt \ngs://gagankaushal/Assignment_4/Output_Task1\nTask 2 - template\nspark-submit <task_name> <Training_dataset> <output_folder_for_results> <output_folder_for_storing_the_list_of_costs_for_different_iterations>\nTask 2 - Small Dataset\nspark-submit main_task2.py SmallTrainingData.txt Output_Task2 List_of_Cost\nTask 2 - Large Dataset\n-- pyspark file location\ngs://gagankaushal/Assignment_4/Final/main_task2.py\n\n-- arguments\ngs://metcs777/TrainingData.txt\ngs://gagankaushal/Assignment_4/Final/Output_Task2\ngs://gagankaushal/Assignment_4/Final/List_of_Cost\nTask 3 - template\nspark-submit <task_name> <input_training_dataset> <input the path of output file generated by task 2 containing 'regression coefficients'> <input_testing_dataset> <output_folder_for_results>\nPlease Note:\n\n'docs' folder contains the output file generated by task 2 containing 'regression coefficients' for SMALL dataset --> part-00000\n'docs' folder contains the output file generated by task 2 containing 'regression coefficients' for LARGE dataset--> Assignment_4_Final_Output_Task2_part-00000\n\nTask 3 - Small Dataset\nspark-submit main_task3.py SmallTrainingData.txt Output_Task2/part-00000 SmallTrainingData.txt Output_Task3\nTask 3 - Large Dataset\n-- pyspark file location\ngs://gagankaushal/Assignment_4/Final/main_task3.py\n\n-- arguments\ngs://metcs777/TrainingData.txt\ngs://gagankaushal/Assignment_4/Final/Output_Task2/part-00000\ngs://metcs777/TestingData.txt\ngs://gagankaushal/Assignment_4/Final/Output_Task3\n""], 'url_profile': 'https://github.com/gagankaushal', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '450 contributions\n        in the last year', 'description': ['simple-linear-regression-without-library\nThis is a program of simple linear regression without scikit learn.\n'], 'url_profile': 'https://github.com/nabilatajrin', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['Overview\nhttps://www.kaggle.com/c/titanic/data\nbased on Logistic Regression @PierianData\nThe data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n\n\nVariable\nDefinition\nKey\n\n\n\n\nsurvival\nSurvival\n0 = No, 1 = Yes\n\n\npclass\nTicket class\n1 = 1st, 2 = 2nd, 3 = 3rd\n\n\nsex\nGender\n\n\n\nAge\nAge in years\n\n\n\nsibsp\n# of siblings / spouses aboard the Titanic\n\n\n\nparch\n# of parents / children aboard the Titanic\n\n\n\nticket\nTicket number\n\n\n\nfare\nPassenger fare\n\n\n\ncabin\nCabin number\n\n\n\nembarked\nPort of Embarkation\nC = Cherbourg, Q = Queenstown, S = Southampton\n\n\n\nVariable Notes\npclass: A proxy for socio-economic status (SES)\n\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage:\n\nAge is fractional if less than 1.\nIf the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancés were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n'], 'url_profile': 'https://github.com/jjaimwork', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2021', 'Jupyter Notebook', 'Updated Jun 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['data-science-process-daily-channel-dsc\n'], 'url_profile': 'https://github.com/navahmed', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Salinas, California ', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['ML-Project-Multiple-Linear-Regression-Python\nMultiple linear regression with Python on the effectiveness of different forms of advertising on sales. The independent variables or features are newspaper, radio, and TV advertising spending. The dependent or response variable is sales.\nThis includes:\n\nPython notebook with the relevant code.\nThe dataset used for the analysis.\n\n'], 'url_profile': 'https://github.com/diego-gomez92', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PhadonP', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Linear-Regression---Gradient-descent--Python\nAn implementation of Gradient descent method on a data set to find the hypothesis (perfect optimize linear line) that cross the entire data set.\nThe code is iterative meaning that considering a number of iterations the hypothesis convergences up to the number of iterations.\n'], 'url_profile': 'https://github.com/Raviv140', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Recurrsive-Feature-Selection-Logistic-Regression-\nData extraction and exploration This is a brief analysis of the dataset on each pitch. The atbat data is also joined with the pitches data to get information from 7 additional columns. This is a left join between pitches and atbat data (i-e pitches LEFT JOIN atbats) so it contains all the rows in the pitches data. The first plot is a correlation plot where darker red tones show a positive correlation  among the variables while a darker blue color show a negative correlation. Greyish tones show no or poor correlation.  A few subsequent plots show scores of home and away games and how they vary with attendance and delay in the start of the  game. These plots show relationships and distribution of the data as well as position of outliers. For example, attendance  between 20,000 and 30,000 seems to be correlated with both away and home games that have high scores. This exploration can be further built upon as well.   Data manipulation The feature of interest is the ""Event"" variable. This is the outcome of each pitch. There are 30 possible events. This makes the analysis complicated. This feature is converted into a binary feature with the value of 0 if the event is a \'Single\', \'Walk\', \'Double\', \'Home Run\' ,\'Hit By Pitch\', \'Field Error\' ,\'Intent Walk\' or a \'Triple\' and a value of 1 otherwise. Moreover, all meaningless variables that do not contribute to the correlations or variation in the data are dropped. This includes certain keys/IDs and certain categorical variables. The remaining numerical variables are then brought to a single scale. The scaling has a major impact on the modeling and analysis that is to follow.  Initial model The initial model consists of 45 features and almost 3 million rows of data. The data is split into 2 partitions; a training set and a testing set in a 70:30 respective ration. A logistic regression algorithm is trained on the training dataset on this data. It can be seen that the logistic regression algorithm performs well on the training data. The accuracy is  99.4%. However there are a large number of features that are difficult to analyse and can cause overfitting to the noise in the training data. A recursive algorithm that drops 1 weak variable in each iteration is also used. This algorithm reduces the number of  features to 15, without any decrease in training accuracy. Perhaps, all the 45 features are not required for the training of the algorithm and the features can be decreased even below 15 for further optimization. The algorithm is not tested  on the testing set yet. The goal is to improve the algorithm using the training set and then test it. This would ensure  the integrity of the algorithm on the testing set.\n'], 'url_profile': 'https://github.com/syedhashirali', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Osprey-DS', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Linear-Regression-with-Neural-Networks\nThe neural_network py file allows you to instantiate a network with non-sigmoid neurons. This is ideal for finding a linear model that fits a set of training data points.\nVideo\nhttps://youtu.be/pQJzyAe-qIs\n'], 'url_profile': 'https://github.com/Supreme-Sector', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '233 contributions\n        in the last year', 'description': ['Detailed Implementation of Logistic Regression\nThis repo contains the detailed implementation of logistic regression algorithm.\n\nDataset is generated using multivariate random normal distribution (2 Features)\nVisualization of data (using Matplotlib)\nSimple logistic regression implementation using gradient ascent update rule (using Numpy)\nVisualization of Decision Boundary and Log Likelihood (using Matplotlib)\n\n'], 'url_profile': 'https://github.com/kanav-mehra', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vangie1002', 'info_list': ['Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 3, 2020', '1', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""r2r rewrite in V\nThis is the full rewrite of the r2 regressions testsuite in the V programming language.\nReasons behind using V are:\n\nGo-like syntax: Easy to maintain and for newcomers\nPortability: Compiles to C with no dependencies\nSpeed: Just use native apis instead of spawn all the things\n\nThe current testsuite is written in NodeJS and have some issues:\n\nHard to architect structured js, ts helps, but its just layers on layers\nSome lost promises happen in travis which are hard to debug\nSimplify the testsuite to cleanup broken or badly written tests\nHave a single entrypoint to run ALL the tests (unit, fuzz, asm, ..)\nLatest versions of NodeJS don't run on net/open/free-BSD\n\nThings to be done:\n\nImplement the interactive mode to fix failing tests\nClone+build V if not in the $PATH\n\nStuff to improve:\n\nProper error handling\nTimeouts without using rarun2\nImprove r2pipe.v performance\n\n--pancake\n""], 'url_profile': 'https://github.com/radareorg', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'New York, NY, USA', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': [""regexport\nR package to export nicely formatted regressions\nThis is an R function to create nicely formatted output for regressions, suitable for both analysis and publication.\nUnlike many similar packages, it splits the work of output into two parts: deciphering the output from a regression\nfunction (like lm, glm, etc.) and outputting it. This means that if you are working with a new type of regression,\nyou just need to write an extension that puts it into a format that the output functions can work with. Similarly, if\nyou have a new type of output that you want to create, you can write an output function that works with a standard input.\nThis is very much in the alpha stage of development. Most of the functions should work, and if they don't, please drop\nme a note that helps me to replicate the problem and I'll try and fix them (as I have time). Also feel free to make\nsuggestions. So far it works with lm, glm, and a handful of other methods that I use from time to time. I'd like to\nexpand that. Also so far, it outputs to Excel and the terminal (which looks great on Windows and needs work on Linux).\nPlease let me know if you like the package. I'm happy to receive all feedback! pbastian at stern.nyu.edu.\nNote: this is released under an MIT license.\n""], 'url_profile': 'https://github.com/philbastian', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ixi150', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '767 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/parikshitgupta1', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '484 contributions\n        in the last year', 'description': ['Linear-Regression-Car-Sales-Prediction-Project\nmodules/libraries :numpy, pandas, statsmodels, seaborn, matplotlib, sklearn\n'], 'url_profile': 'https://github.com/ozgeonec', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Linear-Regression---LSQ-method---Hypothesis-function-Python\nThis piece of code actually calculates the Hypothesis function - the optimal linear line which perfectly matches the all dataset by LSQ method\nBy using the Least Squered method(LSQ) the function perfectly find the values of the Hypothesis function(the Linear line) which passes closest to each data point, and by that we can predict the value of y by rely on the x value because these 2 variable are independent.\n'], 'url_profile': 'https://github.com/Raviv140', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'BENGALURU', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gaurav1210', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'algeria, biskra, sidi khaled, rue al akid amirouche', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lembarek', 'info_list': ['V', 'Updated Apr 14, 2020', 'R', 'Updated Jul 20, 2020', 'C#', 'Updated Apr 5, 2020', 'Haskell', 'GPL-3.0 license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Lagishetti-Venkatesh', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['Titanic Dataset Prediction from scratch using R\nWith Logistic Regression on the Survived Column using the formulas of cost,gradient and prediction.\n'], 'url_profile': 'https://github.com/tanmay8266', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['Supervised_Learning_Regression_Customer_Churn_Prediction\nContext: Customer behavior  prediction to retain customers\nEach cell description:\nCell 3: lib load\nCell 4: read data from csv file\nCell 5: helper function to find unique value in a feature\nCell 6: Apply label Encoder to convert into numerical value\nCell 7: Exploratory Data Analysis (EDA)\n\t\ta. Correlation matrix\n\t\t\n\t\tb. Correlation matrix visualization\n\t\t\n\t\tc. Pair plot analysis\n\t\t\n\t\td. Histogram analysis\n\t\t\n\t\te. Density analysis\n\t\t\n\t\tf. Scatter matrix analysis\n\t\t\n\t\tg. Pie chart of Churn\n\nCell 8: PCA analysis\nCell 9: Best parameter search for 4 models\nCell 10: KNN model\nCell 11: Random forests\nCell 12: Logistic Regression Model\nCell 13: Decision Tree Classifier Model\nCell 14: Single LSTM Model\nCell 15: LSTM Model Evaluation\nCell 16: ROC curves analysis\nCell 17: Precision recall curves analysis\nCell 18: Save Best model(LR)\nCell 19: Saved Model Execution\nDataset : Telco Customer Churn\nDataset Link: https://www.kaggle.com/blastchar/telco-customer-churn\nDataset includes 21 features:\ncustomerID\n\ngender\n\nSeniorCitizen\n\nPartner\n\nDependents\n\ntenure\n\nPhoneService\n\nMultipleLines\n\nInternetService\n\nOnlineSecurity\n\nOnlineBackup\n\nDeviceProtection\n\nTechSupport\n\nStreamingTV\n\nStreamingMovies\n\nContract\n\nPaperlessBilling\n\nPaymentMethod\n\nMonthlyCharges\n\nTotalCharges\n\nChurn\n\nWe have considered ""Churn"" as a label\n'], 'url_profile': 'https://github.com/munir-bd', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jovan-Petrovic', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Caracas, Venezuela', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/datacampero', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Aizawl, Mizoram', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['Brain-Weight-Head-Size-Linear-Regression\nA simple linear regression model to build a predictive analysis of the brain weight of individuals based on the size of their heads\n'], 'url_profile': 'https://github.com/ruvesh', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Essen, Germany', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['A python 3 workbook containing an exploratory analysis of the data scraped from a Brazillian real-estate website by kaggle user Rubens Junior (https://www.kaggle.com/rubenssjr/brasilian-houses-to-rent), as well as an attempt at fitting a regression model to the data in order to allow the prediction rental property prices.\nAfter visualising the data and trimming off the outliers, a multiple Log regression was performed, which could very accurately predict the rental prices of lower-cost properties, but failed to model the rental prices of more expensive properties (~25-30% error @ 50% percentile). I decided to try to split the data into clusters using KMeans clustering in order to create three separate models, hoping maybe to more accurately model the behaviour of more expensive apartments, but this returned worse results than the original model.\nThis suggests that while total floor space, the number of rooms, etc are good predictors for cheap apartments, there are some other major factors effecting the price of more expensive properties, perhaps the neighbourhood, type of apartment building, or accessibility of utilities and public transport\n'], 'url_profile': 'https://github.com/josephdennis93', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'HTML', 'MIT license', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': 'Udaipur', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['when our data is plotted , it is further splitted. Now for each split the average will be calculated. Thus it has higher accuracy than polyRegg and SVR\n'], 'url_profile': 'https://github.com/kaustavmitra26', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': ' Nuevo León, Mexico', 'stats_list': [], 'contributions': '564 contributions\n        in the last year', 'description': ['gradient-descent-for-simple-linear-regression\nAlgoritmo donde se usa el gradiente descendiente para una regresión lineal simple para la clase de Inteligencia Artificial impartida por el maestro Andres Hernandez G. para la Universidad de Monterrey.\n'], 'url_profile': 'https://github.com/gabri3l0', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Logistic-regression-using-SGD-without-scikit-learn\nThis file implements logistic regression with L2 regularization and SGD manually, giving in detail understanding of how the algorithm works.\n'], 'url_profile': 'https://github.com/skshiraj', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/meghanakolluri', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kaustavjisumaity', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '594 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zerkshaban', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/taimoor391', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Machine-Learning-Regression\nLinear Regression Polynomial Regression LASSO and Ridge\n'], 'url_profile': 'https://github.com/aakashkhanderao', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2021', 'Python', 'Updated Apr 5, 2020', 'R', 'Updated Mar 30, 2020', 'Julia', 'Updated Apr 4, 2020']}"
"{'location': 'Manipal', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TejasMIT', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/2471159N', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'Chicago, IL, US', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tarunkateja', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': ['\nremotePARTS\n\n\nremotePARTS is an R package that contains tools for running\nspatio-temporal auto regression analyses on large remotely-sensed data\nsets by partitioning data into manageable chunks.\nDescription\nThis package is based on the PARTS method for analyzing spatially\nautocorrelated time series (Ives et al., in prep).\nInstalation\nTo install the package and it’s dependencies, use the following R code:\ninstall.packages(""devtools"") # ensure you have the latest devtools\ndevtools::install_github(""morrowcj/remotePARTS"", build_vignettes = TRUE)\nThen, upon successful installation, load the package with\nlibrary(remotePARTS).\nThe latest version of\nRtools is required for\nWindows and C++11 is required for other systems.\nTo read documentation for any function use the ? operator in front of\nthe function in the R console. For example, to learn more about\nfitGLS() type ?fitGLS() and hit enter.\ninstallation notes/troubleshooting:\nIf vignettes won’t build, try installing without them and accessing the\nvignette online, for now:\ndevtools::install_github(""morrowcj/remotePARTS"", build_vignettes = FALSE)\nAlso, you may need to ensure that build tools are properly installed on\nyour machine: official Rstudio development\nprerequisites\nuse pkgbuild::has_build_tools(debug = TRUE) and\npkgbuild::check_build_tools(debug = TRUE) to unsure that your build\ntools are up to date.\nWindows\nThe above installation code worked on my Windows (10.0.19041 x86)\nmachine but:\nOn Windows 10 PC, you may need to change the permission settings for R\nin order for install_github() to work:\n\n\nright click on “C:\\Program Files\\R\\R-4.0.2\\library\\base”\n\n\nclick properties\n\n\nSelect “Security” Tab\n\n\nfind and select select “Users” in the “Group or user names” scroll\nmenu\n\n\ntick “Full control”\n\n\nLinux\nOn my linux machine (Ubuntu 20.04), The package would only install\nsuccessfully if build_vignettes = FALSE. Trying to build the vignettes\nduring the installation process made the package unusable (corrupt .rda\nfiles). If you can’t get the vignette to build, try installing without\nvignettes using the above instructions.\nOS X\nI was able to install on my partner’s macbook air after installing\nxcode.\nExample usage\nFor examples on how to use remotePARTS in it’s current state, see the\nAlaska vignette by using the following R code:\nvignette(""Alaska"")\nThe vignette is also available online:\nhttps://morrowcj.github.io/remotePARTS/Alaska.html.\nTesting\nremotePARTS is currently in early development. Stability and\nefficiency tests are ongoing and improvements occur incrementally.\nAutomated tests have exists for some, but not all, of the functions and,\nat present, tests have only been conducted in limited environments.\nTest Environments:\n\n1. Windows 10.0.19041 x64\n\n2. Linux:\nLSB Version:    core-11.1.0ubuntu2-noarch:security-11.1.0ubuntu2-noarch\nDescription:    Ubuntu 20.04.1 LTS\nRelease:    20.04\n\nPlease report any bugs or suggested features as git issues.\nData formats\nIn general, this package requires data to be contained in R matrices.\nData stored in other formats such as image files or rasters will need to\nbe converted. Furthermore, it is highly recommended that map pixels\nwithout data values (e.g.\xa0water in analyses of land patterns) be removed\nentirely. This is especially true when using the partitioned form of the\nanalysis because the matrix multiplication can’t handle missing data or\nunequal dimensions.\nThe following resource demonstrates how to manipulate raster files in R:\nGeospatial raster with R data\ncarpentry\nPlanned Features / To-do\nSince this package in developmental stages, there are many features that\nare currently unimplemented. This section will keep track of the\nfeatures and design implementations that I plan to include or change in\nthe next version as well.\n\n\n fix links in the @seealso documentation sections. The proper\nformat is \\code{\\link{functioname}} for internal functions,\n\\code{\\link[packagename]{functioname}} for external functions, and\n\\url{https://www.r-project.org} for web pages.\n\n\n add all remaining functions to the remotePARTS-package.R file\n(only fitGLS) is currently listed. Also, update ORCID information\nfor authors.\n\n\n Allow GLS functions to bypass the model-comparison step\n(i.e.\xa0t-test only) for possible performance gains when F-like tests\nare not necessary.\n\n\n allow users to, optionally, input parameters (e.g.\xa0r and a\nin the exponential-power function) instead of fitting ML parameters.\nThis should also provide an option to fit a spatial matrix by\nestimating all of nugget, r, and (optionally) a. Use optim()\nto do so. In the partitioned version, option to estimate for some\n(but maybe not all) partitions.\n\n\n add an option to fitGLS.partition() to break before\ncalculating any of the cross partition stats (this is broken down in\nthe vignette).\n\n\n add example for testing “is there an overall time trend” to the\nvignette - this can be done with a t-test for the intercept-only\nmodel. I’ve added an example in the “Alaska” vignette.\n\n\n make providing distance matrix optional instead of required\nfor fitGLS.partition_rcpp() and the partitioned method as a whole.\n\nnow fitGLS.partition() does this and the process is displayed in\nthe Alaska vignette. fitGLS.partition_rcpp() is deprecated.\n\n\n\n include parallelization and distributed comptuting options. If\nthese are not natively implemented (i.e.\xa0using openMP in C++), then\nexamples of how to make it work with other parallelization tools\nshould be provided.\n\n\n more explicit handling of missing data: “How should a constant\ntime series be treated?”; “What happends if there is a missing data\npoint within a single time series?”\n\n\n possibly change the CLS function so that it reads 1 line of data\nat a time to save memory. Also, using RcppEigen::fastLM() may be\nbetter than lm() in terms of speed. - Testing showed that\nfastLM() was actuallyl slower than lm() for all the problems I\ntested.\n\n\n Break up C++ functions into more than 1 file\n\n\n WRITE TESTS FOR EVERY FUNCTION!!! - most functions have\nat least some automated tests but more are needed.\n\n\n update documentation for every function to include output format\nand example usage. - most function have decent documentation but\nsome of the documentation is inconsistent across the package and\nsome functions are under-documented.\n\n\n replace fitGLS with fitGLS2 code and change the way all\nother C++ functions handle lists. The C++ code in fitGLS2 modifies\nan existing list made in R rather than building one within the C++\ncode.\n\n\n create S3 constructors and methods (i.e.\xa0print(), summary(),\netc.) for all relevant functions.\n\n\n remoteGLS() constructs a GLS object and\nprint.remoteGLS() prints a compact and summarized display.\n\n\n remoteCLS() constructor and methods for CLS objects. Both\nthe pixel-level fitCLS and the map-level fitCLS.map have S3\nmethods.\n\n\n remoteAR() constructor and methods for AR_REML objects.\nBoth pixel and map-level fitAR() have S3 methods.\n\n\n PARTmat(): partition matrix for the partitioned GLS\nmethod.\n\n\n\n\nIf there are any additional features that you would like to see\nimplemented, or bugs/issues that you ran into, please submit an issue on\ngithub.\n'], 'url_profile': 'https://github.com/morrowcj', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['machine-learning-for-salary-estimation\nto design a machine learning model to estimate the salary of a person based on  experience using simple linear regression\nthe project has been produced on Anaconda IDE (version 2019.10)\nthe major package ""sklearn"" thats been used on the project is compatible with version 0.22.2\ndata set has been acquried form the web site ""https://www.superdatascience.com/pages/machine-learning""\nthe project is part of an course in Udemy.\nfrom the authors Kirill Eremenko and Hadelin De Ponteves\nthe file ""python_simple_linear_regression.py"" has been developed by Nirav Reddi based on the above compatible versions\nwhere as the remaining file ""simple_linear_regression.py"" was supplied by the authors but isn\'t compatible with out some tweaks\n------------------------------------------------------------data_set-------------------------------------------------------------------\nthe file ""Salary_Data.csv"" t=is the file for supplied data set from the above provided web source.\n'], 'url_profile': 'https://github.com/NiravReddi', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'San Francisco, CA, USA', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Soccer-Science\nPredict English Premier League soccer match outcomes and beat the bookies with XGBoost, Random Forest, and Logistic Regression models.\nThis project was completed as part of the Insight Data Science Program. It was my first full end-to-end web application and live machine learning model.\nThe application was deployed on an AWS EC2 instance running Python Flask and provided predictions based on the home and away teams, and the latest odds on the future match from the bookies.\n'], 'url_profile': 'https://github.com/fcs30', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'Brussels', 'stats_list': [], 'contributions': '462 contributions\n        in the last year', 'description': ['Object-oriented interface for the implementation of Artificial Neural Networks in Keras for:\n\nclassification (MLP)\nregression (MLP)\ndimensionality reduction (Autoencoder)\n\nThe source code is in: /sourceCode, several examples to show the code functionalities are available.\n'], 'url_profile': 'https://github.com/gdalessi', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['brazilian_rent\nThis is a project that predicted the rent prices using multiple linear regression and artificial neural network\n'], 'url_profile': 'https://github.com/jimmy-lipko', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Classification_Problem_Prediction\nLogistic Regression Model to predict if an agent to be hired will be productive or not based on various parameters present.\nThe problem statement is given in the Jupyter Notebook.\n'], 'url_profile': 'https://github.com/spandanpal22', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}","{'location': 'Mumbai,India', 'stats_list': [], 'contributions': '369 contributions\n        in the last year', 'description': ['Best-Classifier\n'], 'url_profile': 'https://github.com/Aniket1313', 'info_list': ['Jupyter Notebook', 'Updated Apr 7, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', '1', 'R', 'Updated Mar 2, 2021', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'Updated Nov 23, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '257 contributions\n        in the last year', 'description': ['INT-417-Machine-Learning\nSoft Computing based techniques for Breast Cancer detection\n(i was not able to find the colorectal cancer dataset so with your permission i did the same on breast cancer dataset\n)\n'], 'url_profile': 'https://github.com/prateek0411999', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'SH', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Sound-Quality\nPrediction model of in-car sound quality in steady-state operating condition based on support vector regression\n'], 'url_profile': 'https://github.com/HitMasq', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'Aligarh', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mrperfectpandit', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Guangrui-best', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': [""ML to predict hospitalization risk at ED triage\nI build and query a postgreSQL database, then tune and ensemble Random Forest, Logistic Regression and SVM classifiers to predict hospitalization risk for ED patients at triage.\nData\nData originates from a retrospective study at the Yale New Haven Health System. It includes all adult ED visits between March 2014 and July 2017 from one academic and two community emergency rooms. The dataset is available on Kaggle.\nWhile a total of 972 variables were extracted per patient visit, I use Random Forest to assess each feature's information gain (based on Gini index) and subset to ~50 fields without a performance impact to the model.\nMethods\nPostgreSQL database management\n\nDatabase and table creation / manipulation through Python using the psycopg2 library\n\nHyperparameter tuning and feature importance assessment\n\nExhaustive search over specified Support Vector Machine parameter values for optimized estimator estimator\nRandom Forest feature information gain assessment to optimize feature sizing for model inclusion.\n\nMachine learning classification\n\nUse of Logistic Regression, Random Forest and Support Vector Machine classifiers to predict hospitalization outcome (admitted as inpatient or discharged from ED) and hospitalization risk.\n\nML model ensembling\n\nUse of ensembled, weighted voting model to provide a nuanced hospitalization prediction using multiple model outputs.\n\nData manipulation and analysis\nPostgreSQL\npsycopg2\nnumpy\npandas\nsklearn\n\nFront end and visualization\nTableau\nTabPy\nmatplotlib\nseaborn\n\n""], 'url_profile': 'https://github.com/dimitermilev', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['ML-Model-to-predict-Sale-Price (Regression Algorithm)\nOur project is based on the Kaggle Housing Prices Competition\nThe following steps were followed to access the predictive model using regression algorithm.\n-> Get Data\n-> Clean, Prepare & Manipulate Data\n-> Train Model\n-> Test Data\n-> Improve\nThanks to educative course, which helped me streamline my understanding towards step-wise split;\nThe following are the step-wise split of task\n\nExploratory Data Analysis #\nUnderstand the data structure\nDiscover and visualize the data to gain insights\nExplore numerical attributes\nLook for correlations among numerical attributes\nExplore categorical attributes\nPrepare the data for machine learning algorithms #\nDeal with missing values\nHandle outliers\nDeal with correlated attributes\nHandle text and categorical attributes\nFeature scaling\nTransformation Pipelines in Scikit-Learn #\nAssess Machine Learning Algorithms #\nTrain and evaluate multiple models on the training set\nComparative analysis of the models and their errors\nEvaluation Using Cross-Validation\nFine-Tune Your Model #\nPresent the Solution #\nLaunch, Monitor, and Maintain the System #\n\n'], 'url_profile': 'https://github.com/barani-bhoopalan', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Ml-project-sales-prediction\nIt is a machine learning project based on regression. It simply predict the sale. In this project I have used label encoder, linear regression and backword elimination.\n'], 'url_profile': 'https://github.com/SpyCop1998', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Atit-Bashyal', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': [""DataScience\nData Science practice code snippets with theory\nContents (data and python scripts) are sourced from the lectures of Udemy course - https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp\nLecture numbers in the Python scripts and indexes in data files are all referenced as is from the course to relate each and every file to the course's tutorial for better referencing.\n""], 'url_profile': 'https://github.com/madhavmahajan', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'R', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'MIT license', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated May 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Black-Friday-Sales-Prediction\nGiven are two datasets, train and test data consisting of black friday purchases by numerous buyers. We train the model using the train data and predict the purchase amount of buyers in test data. It is a regression problem and therefore we use Random Forest Regression and XGBoost Reggressor.\n'], 'url_profile': 'https://github.com/NavinJoshi19', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['covid-19 prediction\nPandemic is spreading all over the world; it becomes more important to understand about this spread. This NoteBook is an effort to analyze the cumulative data of confirmed, deaths, and recovered cases over time. In this notebook, the main focus is to analyze the spread trend of this virus all over the world. Its about the prediction of the covid-19 pandemic across of thye world and in india. Different graphs and visualization are used for the predicted data.I have used linear Regression with polynomial regression.\n'], 'url_profile': 'https://github.com/enggrobin', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rishab2903', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Coursera-Linear-Regression-to-predict-IMDb-movie-rating\nThis exercise is part of Linear Regression and Modeling certification with Coursera. It helps you predict the IMDb rating using movie runtime, popularity score, genre and sub-genre.\nSoftware requirement\n\nR Studio\n\nPackages Required\n\nggplot2\ndplyr\nstatsr\nGGally\nDAAG\ndevtools\n\n'], 'url_profile': 'https://github.com/sudeepl05', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/collins-emasi', 'info_list': ['Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Ottawa, Canada', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': [""Regression model for predicting Electricity-Usage-Data-science-Interview-challenge\nFor Data Science enthusiasts:\nThe Challenge :\nLeveraging the dataset located here: https://www.eia.gov/consumption/residential/data/2009/index.php?view=mi crodata, can you build a model that predicts consumption? The electric consumption is located in the KWH field. The goal of the exercise is to understand how you'd approach the problem, how you would choose a model/which model, etc.\n""], 'url_profile': 'https://github.com/rajshekharM', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Car-Sale-Data\nLinear regression model to predict the price of a used car depending on its specifications using Python, Pandas, NumPy, Statsmodels, sklearn and Jupyter Notebooks.\n'], 'url_profile': 'https://github.com/sujaysdesai', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/audrey-smith', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bchoudhury91', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'Lisbon', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['Drivers of Global CO2 Emissions\nType: Academic Project for the course of Statistics for Enterprise Data Analysis\nGroup: Francisco Costa, João Gouveia, Pedro Riveira, Nuno Rocha\nProgramming Language: R\nObjective: Through regression analysis and with the study of temporal data, this study aimed to infer the drivers of global CO2 emissions.\n'], 'url_profile': 'https://github.com/joaogouveia18', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Udemy--Machine-Learning-A-Z\nThis repository contains all the models, datasets and the templates used in the course such as Data pre-processin, Regression, Classification, Clustering and associate rule learning.\nThe code templates for each algorithm has been made in python as well as R. The datasets used are also attached in the respective directories.\n'], 'url_profile': 'https://github.com/sachandivyam', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'São Caetano do Sul, SP', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Machine Learning\n💻 Machine Learning Algorithms\nThis repository was created to store my machine learning algorithms based on the Machine Learning Course from Udemy:\nhttps://www.udemy.com/course/machinelearning/\n'], 'url_profile': 'https://github.com/jpedrotonello', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'London', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['Drivers of global CO2 emissions\nAcademic Project for the course of Statistics for Enterprise Data Analysis\nGroup: Francisco Costa, João Gouveia, Pedro Rivera, Nuno Rocha\nProgramming Language: R\nThrough regression analysis and with the study of temporal data, this study aimed to infer the drivers of global CO2 emissions.\n'], 'url_profile': 'https://github.com/FranciscoCosta1', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}","{'location': 'San Diego', 'stats_list': [], 'contributions': '380 contributions\n        in the last year', 'description': ['Stop-Sign Detection Redux\nStop-sign detection algorithm, using logistic regression for classification, as opposed to purely traditional image processing techniques (used in the other repository: https://github.com/roumenguha/Stop_Sign_Detection)\nConsult my Project Report for more background, and for the results of the bounding operation.\n\nBelow, we show the results of the color classification and segmentation.\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/roumenguha', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'HTML', 'Updated Apr 16, 2020', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Mar 31, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 30, 2020', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021']}"
"{'location': 'London', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['Drivers of global CO2 emissions\nAcademic Project for the course of Statistics for Enterprise Data Analysis\nGroup: Francisco Costa, João Gouveia, Pedro Rivera, Nuno Rocha\nProgramming Language: R\nThrough regression analysis and with the study of temporal data, this study aimed to infer the drivers of global CO2 emissions.\n'], 'url_profile': 'https://github.com/FranciscoCosta1', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'San Diego', 'stats_list': [], 'contributions': '380 contributions\n        in the last year', 'description': ['Stop-Sign Detection Redux\nStop-sign detection algorithm, using logistic regression for classification, as opposed to purely traditional image processing techniques (used in the other repository: https://github.com/roumenguha/Stop_Sign_Detection)\nConsult my Project Report for more background, and for the results of the bounding operation.\n\nBelow, we show the results of the color classification and segmentation.\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/roumenguha', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'San francisco', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['MovieRecommendationEngine\nConducted user-based collaborative filtering in R and supplemented with regularized regression to compute user/movie bias for targets with no history data\nSUMMARY\nOur team simulated the Netflix recommendation algorithm (not the most current) to predict movie ratings with information gathered from all the UCD MSBA students. After thorough comparison, we chose user-based collaborative filtering normalized by centered mean. However, for new users/movies that we have no existing information on, we supplement it with regularized regression which can calculate user/movie bias separately, and use them for estimation.\nApart from the specific scores we predicted for each user/movie, we got the benchmark score for the MSBA cohort, which suggests that the whole class are kind critics or are slightly selective, meaning only watching the movies in which they have confidence.\nThis recommender system can be used in many ways beyond movie prediction, such as in stocking prediction for white labeling companies, bulk buying quantities prediction for supply chain management and churn prediction in customer analysis.\nThere are several more ways to finetune the model.\nOn the algorithm side, we could mix the collaborative filtering with content-based filtering ;\nOn the evaluation side, apart from error rate, we could also use Mean Average Precision, coverage, personalization, intralist, customer happiness index to further improve the model.\nMAIN PACKAGE\nThe main package we use is \'recommenderlab\' of which the documentation is attached too.\nThe following is the common way of using it:\nmodel_train_scheme <- Realrating %>%\n  evaluationScheme(method = \'cross-validation\', \n                   train = train_proportion, # proportion of rows to train.\n                   given = 1, \n                   goodRating = NA, # for binary classifier analysis.\n                   k = 5)\n                   \nmodel_params1 <- list(method = ""cosine"",\n                     nn = 10, # find each user\'s 10 most similar users.\n                     sample = FALSE, \n                     normalize=NULL)\n\nmodel1 <- getData(model_train_scheme, ""train"") %>% #fit on the 75% training data.\n  Recommender(method = ""UBCF"", parameter = model_params1)\n\nMODEL SELECTION, EVALUATION, AND INTERPRETATIONS\nWe built several neighborhood models with different specifications on two approaches: user-based and item-based collaborative filtering, which predict ratings based on one user’s ratings of similar movies or a movie’s ratings by similar users respectively. And we chose the best model with the lowest error rate.\nAs a result, we found that user-based collaborative filtering normalized by centered mean (referred to as “UBCF_center”) has the lowest error rate. However, although such an approach is the best for predicting existing users, it can go far off for new users with no information. Under such circumstances, we will need user bias and movie bias.\nThe regularized regression is used here to compute all the user bias and movie bias, therefore generating an unbiased rating dataset to feed the UBCF_center model. But the error rate (RMSE) of the new version(referred to as “bias_remove model”) is still higher than the previous UBCF_center model. Therefore, we will use the more accurate, the UBCF_center model and only use our bias_remove model for new users/movie cases.\nWHAT\'S MORE\nThe value of our recommendation system extends beyond the movie sector. For example, our model could help online wholesale companies predict the bulk buying quantities based on the characteristics of each retailer. It may also help white labeling companies to predict user activities and avoid over-stocking.\nIt is possible to supplement collaborative filtering with content-based filtering, which compares the content of an item and its user profile . The method collects user information such as sex and age but not user history. Thus, it can solve the cold-start problems.\nA few ‘value’ functions could be useful when evaluating and refining the system. A popular metric called the Mean Average Precision appraises the positive predicted value of a recommendation system. Other metrics include coverage, personalization, intralist. In addition, we can use the churn rate or Customer Happiness Index (CHI) to further explore how the recommendation effects on the users.\nCustomer analytics and the recommendation system are closely intertwined. For example, when we are predicting the likelihood a customer is going to churn, we can use the Mean Average Precision to evaluate how precise our model is. The more precise the customer analytics is, the better the recommendation system is.\n'], 'url_profile': 'https://github.com/JacquelineHSH', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayshakhan2255', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '324 contributions\n        in the last year', 'description': [""From Scratch\nA collection of Machine Learning algorithms implemented in Python from scratch.\nMotivation\nTransforming an algorithm from math equations to code is a sometimes difficult but always fruitful journey every\nData Scientist should take at least once. The goal of this project is to go down to the last detail of each\nalgorithm, fill the gap between math and code, demystify ML models complexity and revisit their\nmathematical background.\nImplementations\n\n\nSupervised Learning\n\nMulti-layer Neural Networks [demo] [code]\nClassification Trees [demo] [code]\nAdaptive Boosting [demo] [code]\nSupport Vector Machine [demo] [code]\nLinear Regression [demo] [code]\n\n\n\nUnsupervised Learning\n\nK-means [demo] [code]\nDBSCAN [demo] [code]\nPrincipal Component Analysis (PCA) [demo] [code]\n\n\n\nReinforcement Learning\n\nN-armed Bandits [demo] [code]\nDynamic Programming [demo] [code]\nSARSA [demo] [code]\nQ-Learning [demo] [code]\n\n\n\nDependencies\n\nnumpy: Used in all implementations for vector/matrix operations and vectorized calculations\ncvxopt: Used in SVM for solving the quadratic programming problem\nscipy: Borrowed its KDTree implementation for fast nearest neighbours calculation\n\nDemos\nEach algorithm is accompanied with a notebook with\nmathematical background, application of the methodology\non toy datasets and visualizations.\nSupervised Learning\nMulti-layer Neural Networks\nFead-forward Neural Network\n---------------------------\n4 layers: [10, 10, 10, 1]\n\n       Inputs\n       \\ | /\n o o o o o o o o o o \n       \\ | /\n o o o o o o o o o o \n       \\ | /\n o o o o o o o o o o \n          | \n          o \n       Outputs\n\n\nClassification Trees\n\n{'expr': 'feature_2 <= 2.45',\n 'no': {'expr': 'feature_3 <= 1.75',\n        'no': {'expr': 'feature_2 <= 4.85',\n               'no': {'label': 2.0},\n               'yes': {'label': 2.0}},\n        'yes': {'expr': 'feature_2 <= 4.95',\n                'no': {'label': 2.0},\n                'yes': {'label': 1.0}}},\n 'yes': {'expr': 'feature_0 <= 4.35',\n         'no': {'expr': 'feature_0 <= 4.45',\n                'no': {'label': 0.0},\n                'yes': {'label': 0.0}},\n         'yes': {'label': 0.0}}}\n\nSupport Vector Machine\n\nLinear Regression\n\nUnsupervised Learning\nK-means\n\nDBSCAN\n\nPrincipal Component Analysis (PCA)\n \nReinforcement Learning\nN-armed Bandits\n\nDynamic Programming\n\nQ-Learning\n \nSARSA\n \n""], 'url_profile': 'https://github.com/tsitsimis', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': [""Machine-Learning\nA collection of Machine Learning Algorithms, starting from basic Linear Regression. Self Taught everything and everything is from scratch, from matplotlib to random number generation, all in Python.\n3/31/2020:\nThis repo is created. Machine Learning is incredibly interesting, and I want to document my journey through self learning this. Who knows where Machine Learning will take me one day?\nAdded the first python script to the repo. Here, I am learning to use Matplotlib.\n4/6/2020:\nI will now hold off in studying the principles of ML. I will now start to work in Python.\n4/12/2020:\nWorking through Andrew Ng's Machine Learning Course. Expected finish: a month, until May 12.\n""], 'url_profile': 'https://github.com/darenhua', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'Gujrat, Punjab, Pakistan', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/adeel085', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Clustering-cars-based-on-attributes\nAnalyzed cars dataset and performed exploratory data analysis and then categorized them using K means clustering. Used linear regression on the different clusters and estimated coefficients.\n'], 'url_profile': 'https://github.com/tanyagera', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Partisan Topics Text Analysis\nData Source\nThis data originally appeared in Gentzkow and Shapiro (GS, 2010) and considers text of the 2005 Congressional Record, containing all speeches in that year for members of the United States House and Senate. In particular, GS record the number times each of 529 legislators used terms in a list of 1000 phrases. It also includes the ideGentzkow, Matthew, Jesse M Shapiro, and Inter-university Consortium for Political Social Research. Political Slant of United States Daily Newspapers, 2005 (2009). Web.\nology of each speaker – Party: (Republican, Democrat, or Independent).\nAcknowledgements\nGentzkow, Matthew, Jesse M Shapiro, and Inter-university Consortium for Political Social Research. Political Slant of United States Daily Newspapers, 2005 (2009). Web. (https://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/26242)\nObjective\nImplement K-means methods and topic models to cluster text and interpret topics \nBuilt and compared multiple topic regression models to predict party membership from text clusters\n'], 'url_profile': 'https://github.com/boyasun', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}","{'location': 'Aizawl, Mizoram', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['Titanic-Data-Analysis\nA simple data analysis performed on the Titanic data set along with a logistic regression model to predict the survival of passengers based on certain factors.\n'], 'url_profile': 'https://github.com/ruvesh', 'info_list': ['Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 24, 2021', 'MIT license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Jun 30, 2020', '1', 'MATLAB', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020']}"
"{'location': 'Aizawl, Mizoram', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['Titanic-Data-Analysis\nA simple data analysis performed on the Titanic data set along with a logistic regression model to predict the survival of passengers based on certain factors.\n'], 'url_profile': 'https://github.com/ruvesh', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tanyagera', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Censoring-Simulation\nMonte Carlo Simulation that tests Bias and MSE of tobit and quantile regression models to deal with censoring. Working Paper is attached.\n'], 'url_profile': 'https://github.com/1carvercoleman', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cocassel', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '170 contributions\n        in the last year', 'description': ['The-Future-Value-of-Homes\nIncorporates machine learning and predicts the future values of homes for the next few decades based off of previous values for the state of Maryland. Implements the use of Bayesian regression due to our dataset not being tremendously large. A model is also created for visualization.\nFiles Included:\n\nAverageHomeValues.csv - Data table containing median price values for homes in Maryland from 1940-2020\nCode.py - Raw python code created to obtain our model\nDisplay of Model.png - Screenshot of model\n\nStep 1)\nImport the necessary libraries\n\nSTEP 2)\nNow I am grabbing the csv file attached within this repository. Due to me implementing more than just an integer for certain columns ($), I needed to then use pandas\' ""astype()"" function to essentially pass my columns as a float in order for the program to execute properly.\n\nSTEP 3)\nSet your size, titles, and fonts for the plot.\n\nSTEP 4)\nSet your training and test sets. In this case I am training the model on all median values from 1940 to 2020.\n\nSTEP 5)\nReshape and fit the data with the use of Bayesian Regression.\n\nSTEP 6)\nPredict up until the year 2050 and create print lines in order to display our training and test accuracy.\n\nSTEP 7)\nSet up plot and label the training data, test data, and prediction. Lastly, show the plot and accuracy.\n\n\n'], 'url_profile': 'https://github.com/ObiP1', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Catania', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Boros93', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kibesteve', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 5, 2020', 'Updated Apr 5, 2020', '1', 'R', 'MIT license', 'Updated Jan 14, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated May 20, 2020', 'Updated Apr 3, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Student-Performance-Prediction\nML tool that predict Student pointer using linear regression.also for UI i use flask.\n'], 'url_profile': 'https://github.com/programmerMayur', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Calendar regressors\nThe files contain calendar regressors used by the Central Statistical Bureau of Latvia for the seasonal and calendar adjustment of time series.\nRegressors are computed according to:\n\nthe Law on holiday, remembrance and celebratable days (in Latvian only),\nthe JDemetra+ Methodlogy.\n\nThe regressors are computed to be used by the software JDemetra+.\nFour types of regressors are available (see the folder data):\n\nTD_M - trading day and leap year regressors for monthly time series,\nTD_Q - trading day and leap year regressors for quarterly time series,\nWD_M - working day and leap year regressors for monthly time series,\nWD_M - working day and leap year regressors for quarterly time series.\n\nThe time span covered by the regressors is from 1995 to 2022.\nRegressors are available in the xlsx and csv formats.\nFor more information:\n\nSeasonal Adjustment\nJDemetra+ Documentation\n\nNumber of working days\nThe corresponding number of working days by month, quarter and year is available (see the folder data):\n\ncount_WD_long - number working days in long format,\ncount_WD_wide - number working days in wide format.\n\nThe time span covered by the regressors is from 1995 to 2022.\nData is imported from the data.gov.lv.\n'], 'url_profile': 'https://github.com/CSBLatvia', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': [""Facial-Keypoint-Detection-using-cnn\nIn this work we train a Convolutional Network to regress the facial keypoints of a person's face.\nIt is implemented using Pytorch.\n""], 'url_profile': 'https://github.com/ajayseeker', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arkpandey789', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['LazyProphet\npip install LazyProphet\n\nTime Series decomp via gradient boosting with a couple different estimators of trend:\n\nridge: approximates trend via a global fit from a polynomial ridge regression (don\'t really need ridge since we are boosting but oh well)\nlinear: approximates trend via a local linear changepoint model done using binary segmented regressions to minimize MAE or MSE\nmean: approximates trend via local mean change point model\n\nSeasonality can be naive averaging over freq number of time periods or \'harmonic\' which calculates seasonality similarly to Prophet using fourier series.\nNotes:\n\nNumber of gradient boosting rounds can be set to a max but once our cost function is minimized it will stop unless a minimum is set\nYou probably want to always have ols_constant = False for linear estimator\nWe can approximate where splits should occur for our local estimators (mean and linear) which speeds things up quite a bit\nThe regularization parameter effects the number of boosting rounds whereas l2 just effects the ridge regression regularization\n\nBasic flow of the algorithm:\n\nSome basic examples:\nimport quandl\nimport fbprophet\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport LazyProphet as lp\n\n#Get bitcoin data\ndata = quandl.get(""BITSTAMP/USD"")\ny = data[\'Low\']\ny = y[-730:]\ndf = pd.DataFrame(y)\ndf[\'ds\'] = y.index\n#adjust to make ready for Prophet\ndf.columns = [\'y\', \'ds\']\nmodel = fbprophet.Prophet()\nmodel.fit(df)\nforecast = model.predict(df)\n\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = 365, \n                               estimator = \'linear\',\n                               approximate_splits = True,\n                               regularization = 1.2,\n                               global_cost = \'maicc\',\n                               split_cost = \'mse\',\n                               seasonal_regularization = \'auto\',\n                               trend_dampening = 0,\n                               max_boosting_rounds = 50,\n                               exogenous = None\n                                    )\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\n#plot forecasts vs actual\ntsboosted_ = output[\'yhat\']\nproph = forecast[\'yhat\']\nplt.plot(tsboosted_, label = \'Lazy\', color = \'black\')\nproph.index = tsboosted_.index\nplt.plot(y, label = \'Actual\')\nplt.plot(proph, label = \'Prophet\')\nplt.legend()\nplt.show()\n#plot trend\nplt.plot(forecast[\'trend\'], label = \'Prophet\')\nplt.plot(output[\'trend\'].reset_index(drop = True), label = \'Lazy\')\nplt.plot(y.reset_index(drop = True))\nplt.legend()\nplt.show()\n#plot seasonality\nplt.plot(forecast[\'additive_terms\'], label = \'Prophet\')\nplt.plot(output[\'seasonality\'].reset_index(drop = True), label = \'Lazy\')\nplt.legend()\nplt.show()\n\n\n\nAn example using ridge and looking at the trend and seasonality decomp:\nimport quandl\nimport fbprophet\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport LazyProphet as lp\n\n#Get bitcoin data\ndata = quandl.get(""BITSTAMP/USD"")\ny = data[\'Low\']\ny = y[-730:]\ndf = pd.DataFrame(y)\ndf[\'ds\'] = y.index\n#adjust to make ready for Prophet\ndf.columns = [\'y\', \'ds\']\nmodel = fbprophet.Prophet()\nmodel.fit(df)\nforecast = model.predict(df)\n\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = 365, \n                               estimator = \'ridge\',\n                               approximate_splits = True,\n                               regularization = 1.2,\n                               global_cost = \'maicc\',\n                               split_cost = \'mse\',\n                               seasonal_regularization = \'auto\',\n                               trend_dampening = 0,\n                               max_boosting_rounds = 50,\n                               exogenous = None)\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\n#plot forecasts vs actual\ntsboosted_ = output[\'yhat\']\nproph = forecast[\'yhat\']\nplt.plot(tsboosted_, label = \'Lazy\', color = \'black\')\nproph.index = tsboosted_.index\nplt.plot(y, label = \'Actual\')\nplt.plot(proph, label = \'Prophet\')\nplt.legend()\nplt.show()\n#plot trend\nplt.plot(forecast[\'trend\'], label = \'Prophet\')\nplt.plot(output[\'trend\'].reset_index(drop = True), label = \'Lazy\')\nplt.plot(y.reset_index(drop = True))\nplt.legend()\nplt.show()\n#plot seasonality\nplt.plot(forecast[\'additive_terms\'], label = \'Prophet\')\nplt.plot(output[\'seasonality\'].reset_index(drop = True), label = \'Lazy\')\nplt.legend()\nplt.show()\n\n\n\nAn example using mean changepoints:\nimport quandl\nimport fbprophet\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport LazyProphet as lp\n\n#Get bitcoin data\ndata = quandl.get(""BITSTAMP/USD"")\ny = data[\'Low\']\ny = y[-730:]\ndf = pd.DataFrame(y)\ndf[\'ds\'] = y.index\n#adjust to make ready for Prophet\ndf.columns = [\'y\', \'ds\']\nmodel = fbprophet.Prophet()\nmodel.fit(df)\nforecast = model.predict(df)\n\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = 365, \n                            estimator = \'mean\', \n                            max_boosting_rounds = 50,\n                            approximate_splits = True,\n                            regularization = 1.2)\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\n#plot forecasts vs actual\ntsboosted_ = output[\'yhat\']\nproph = forecast[\'yhat\']\nplt.plot(tsboosted_, label = \'Lazy\', color = \'black\')\nproph.index = tsboosted_.index\nplt.plot(y, label = \'Actual\')\nplt.plot(proph, label = \'Prophet\')\nplt.legend()\nplt.show()\n#plot trend\nplt.plot(forecast[\'trend\'], label = \'Prophet\')\nplt.plot(output[\'trend\'].reset_index(drop = True), label = \'Lazy\')\nplt.plot(y.reset_index(drop = True))\nplt.legend()\nplt.show()\n#plot seasonality\nplt.plot(forecast[\'additive_terms\'], label = \'Prophet\')\nplt.plot(output[\'seasonality\'].reset_index(drop = True), label = \'Lazy\')\nplt.legend()\nplt.show()\n\n\n\nToy Example: What is the potential impact of the coronavirus?\nimport quandl\nimport fbprophet\nimport pandas as pd\nimport LazyProphet as lp\n\n#Get bitcoin data\ndata = quandl.get(""BITSTAMP/USD"")\ny = data[\'Low\']\ny = y[-730:]\n\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = None, \n                            estimator = \'mean\', \n                            approximate_splits = True)\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\n\n#Potential impact of coronavirus with a \'still normal\' date of Feb 1st\npct_change = output[\'trend\'].loc[(output[\'trend\'].index > \'2020-02-01\')].pct_change()\npct_change = pct_change.replace(to_replace=0, method=\'ffill\')\nimpact = np.mean(pct_change)\nprint(f\'Maybe like {int(impact*100)} percent?\')\nSome simulated data:\nimport quandl\nimport fbprophet\nimport pandas as pd\nimport LazyProphet as lp\n\nN = 730\nt = np.linspace(0, 4*np.pi, N)\nsine = 3.0*np.cos(t+0.001) + 0.5 + np.random.randn(N)\ny = pd.Series(sine)\n#some datetime index\ny.index = pd.date_range(start=None, end=\'2020-04-05\', periods=N)\ndf = pd.DataFrame(y, columns = [\'y\'])\ndf[\'ds\'] = y.index\n#fit prophet\nmodel = fbprophet.Prophet(yearly_seasonality = True)\nmodel.fit(df)\nforecast = model.predict(df)\n#%%\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = 365, \n                            approximate_splits = True,\n                            )\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\n#plot forecasts vs actual\ntsboosted_ = output[\'yhat\']\nproph = forecast[\'yhat\']\nplt.plot(tsboosted_, label = \'Lazy\', color = \'black\')\nproph.index = tsboosted_.index\nplt.plot(y, label = \'Actual\')\nplt.plot(proph, label = \'Prophet\')\nplt.legend()\nplt.show()\n\n#plot trend\nplt.plot(forecast[\'trend\'], label = \'Prophet\')\nplt.plot(output[\'trend\'].reset_index(drop = True), label = \'Lazy\')\nplt.plot(y.reset_index(drop = True))\nplt.legend()\nplt.show()\n#plot seasonality\nplt.plot(forecast[\'additive_terms\'], label = \'Prophet\')\nplt.plot(output[\'seasonality\'].reset_index(drop = True), label = \'Lazy\')\nplt.legend()\nplt.show()\n\n\n\nPlotting the components:\nimport quandl\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport LazyProphet as lp\n\n#Get bitcoin data\ndata = quandl.get(""BITSTAMP/USD"")\ny = data[\'Low\']\ny = y[-730:]\n\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = 365, \n                            estimator = \'linear\', \n                            max_boosting_rounds = 50,\n                            approximate_splits = True,\n                            regularization = 1.2)\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\nboosted_model.plot_components()\n\nDealing with Exogenous Variables\nNow let\'s take a look at exogenous variables which may have an effect on the BTC price. This is meant to be a demonstration using readily available information, the variables we use are just what comes with the Quandl request.\nExogenous variables are fit in the last step of the boosting loop and all coefficients and standard errors are updated using all boosting rounds so the coefficients most likely are regularized.\nAdding extra variables may also make the model want MORE boosting rounds, so we will increase the max_boosting_rounds.\nimport quandl\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport LazyProphet as lp\n\n#Get bitcoin data\ndata = quandl.get(""BITSTAMP/USD"")\n#let\'s get our X matrix with the new variables to use\nX = data.drop(\'Low\', axis = 1)\nX = X.iloc[-730:,:]\ny = data[\'Low\']\ny = y[-730:]\n\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = 365, \n                            estimator = \'linear\', \n                            max_boosting_rounds = 200,\n                            approximate_splits = True,\n                            regularization = 1.2,\n                            exogenous = X)\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\nboosted_model.summary()\nThe output is printed to the console, but all values also exist in the output dictionary from the fit() function.\n***************Exogenous Model Results***************\n\n        Coefficients  Standard Error  t-Stat  P-Value\nHigh           -0.27            0.37   -0.74    0.460\nLast            0.17           11.30    0.01    0.988\nBid             1.76           13.88    0.13    0.899\nAsk            -2.09           14.19   -0.15    0.883\nVolume         -0.02            0.01   -1.80    0.073\nVWAP            1.11            0.51    2.16    0.031\n\nForecasting\nIf you have no other variables and the problem is a simple Time Series setup then forecasting is just extrapolating the current measure of trend and seasonality utilizing the extrapolate(n_steps, future_X = None) method where n_steps is the number of steps to forecast and future_X is a dataframe/array for the future values of exogenous variables if you fit the model with any.  This just returns a numpy array not a series so beware!\nimport quandl\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport LazyProphet as lp\n\n#Get bitcoin data\ndata = quandl.get(""BITSTAMP/USD"")\n#let\'s get our X matrix with the new variables to use\nX = data.drop(\'Low\', axis = 1)\nX = X.iloc[-730:,:]\ny = data[\'Low\']\ny = y[-730:]\n\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = 365, \n                            estimator = \'linear\', \n                            max_boosting_rounds = 200,\n                            approximate_splits = True,\n                            regularization = 1.2,\n                            exogenous = X)\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\nforecast = boosted_model.extrapolate(30)\nMany times we are not sure if the current trend will hold and would like the trend to be dampened over the forecast horizon to have a 0 slope, this can be done with the trend_dampening argument when building the class.  For this metric- a .5 would mean that the trend hits roughly half the value of the unconstrained trend by the end of the forecast horizon.  A .1 would mean the trend would hit roughly 90% of it\'s unconstrained value.  The dampenening is achieved via exponential decay of the slope and is a smooth transition for all involved.\nimport quandl\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport LazyProphet as lp\n\n#Get bitcoin data\ndata = quandl.get(""BITSTAMP/USD"")\n#let\'s get our X matrix with the new variables to use\nX = data.drop(\'Low\', axis = 1)\nX = X.iloc[-730:,:]\ny = data[\'Low\']\ny = y[-730:]\n\n#create Lazy Prophet class\nboosted_model = lp.LazyProphet(freq = 365, \n                            estimator = \'linear\', \n                            max_boosting_rounds = 200,\n                            approximate_splits = True,\n                            regularization = 1.2,\n                            exogenous = X,\n                            trend_dampening = .5)\n#Fits on just the time series\n#returns a dictionary with the decomposition\noutput = boosted_model.fit(y)\nforecast = boosted_model.extrapolate(30)\n'], 'url_profile': 'https://github.com/tblume1992', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Final_projectREG\nFinal regression project\n'], 'url_profile': 'https://github.com/vicdetermont', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'HTML', 'Updated Apr 5, 2020', '1', 'R', 'EUPL-1.2 license', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Python', 'MIT license', 'Updated Aug 29, 2020', 'Updated Mar 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['nlpred\nProvides support for making predictions of binary outcomes based on natural language corpora.  Formally, creates predictive models using Logistic Regression with Elastic Net Regularization on topic models derived from Latent Dirichlet Allocation.  Supports multithreading for increased runtime efficiency on large data sets.\nA black box method, getModels(), is provided so that users who do not wish to manually tweak their models can create high-quality models by simply providing two vectors of documents (Strings): one vector of documents associated with true responses, and one vector of documents associated with false responses.\nQuick Start Example\n# Note that all of the below methods are contained in Model.py of nlpred; appropriate imports are required.\n\n# Declare a vector of Strings containing documents in the test/train set associated with TRUE reponses\ntrueDocs = [""true document 1 content"", ""true document 2 content"", ""true document 3 content""]\n\n# Declare a vector of Strings containing documents in the test/train set associated with FALSE reponses\nfalseDocs = [""false document 1 content"", ""false document 2 content"", ""false document 3 content""]\n\n# Call getModels() from Model.py to generate predictive models for the documents\nmodelDict = getModels(trueDocs, falseDocs, isRedditData = False)\n\n# modelDict is now a Dictionary containing five candidates for an optimal model, stored as Model objects:\n#   1) ""AUC"", the model with the highest AUC\n#   2) ""acc"", the model with the highest accuracy\n#   3) ""rec"", the model with the highest recall\n#   4) ""acc_1se"", the model with the highest accuracy among models whose AUC is within 1 standard error of the maximum\n#   5) ""rec_1se"", the model with the highest recall among models whose AUC is within 1 standard error of the maximum\nThe ""best"" model depends on the original corpus and purpose of the model, so it is left to the user to determine which of the five models returned by this method best suits their purposes.\nIf the user is unsure of which model to use, it would behoove them to simply choose the model with the highest AUC, stored as ""AUC"" in the return Dictionary.\nModel Object Functionality\nTo illustrate what can be done with a Model object, assume the model with the highest AUC was chosen as optimal:\noptimalModel = modelDict[\'AUC\']\nThe model can be saved to disk as a .pkl file in the current work directory:\noptimalModel.save(fileName = ""optimal_model.pkl"")\nModels may be manually loaded into the python environment from a .pkl file:\nloadedModel = loadModel(fileName = ""optimal_model.pkl"")\nThe model can be used to make predictions from new documents, passed as a vector of Strings:\nnewDocs = [""new document 1 content"", ""new document 2 content"", ""new document 3 content""]\n\npredictionDataFrame = optimalModel.predict(listOfDocStrings = newDocs)\n\n# predictionDataFrame is now  a DataFrame containing three columns:\n#   1)  \'paper_text\', the original input textual data\n#   2)  \'paper_text_processed\', the processed (tokenized with punctuation and capitalization removed) textual data\n#   3)  \'value\', the 1/0 (True/False) prediction for the \'paper_text\' entry in the same row\nAlternatively, predictions can be made directly from a .pkl file without having to load the model manually:\npredictionDataFrame = predictFromFile(fileName = ""optimal_model.pkl"", listOfDocStrings = newDocs)\nA summary of the model may be printed to stdout using the python standard print() function:\nprint(optimalModel)\nAlternatively, a summary of the model may be printed to stdout with the instance method print():\noptimalModel.print()\nInstance variables of the Model object may be obtained by the standard extensions, e.g.:\noptimalAUC = optimalModel.AUC\nThe instance variables accessible to the user are as follows:\n\n\n\nVariable\nDescription\n\n\n\n\nAUC\nA general measure of fit for the model as per the chosen predictive threshold for ""true"" vs ""false"" from logreg predictive output.\n\n\naccuracy\nThe classification rate, i.e. the percent of correct predictions; or, the number of correct predictions made divided by the total number of predictions made.\n\n\nrecall\na.k.a sensitivity; P(Y=1 | X=1), the proportion of true samples correctly identified as true.  I.e., the number of true samples correctly identified divided by the total number of true samples.  The recall is intuitively the ability of the classifier to find all the positive samples.\n\n\nconfusion_matrix\nThe confusion matrix of the fitted model.\n\n\ntopicQuantity\nThe number of topics used in .this model.\n\n\nfit\nThe fitted logistic regression model itself, of type sklearn.linear_model.LogisticRegressionCV\n\n\ntopicLDAModel\nThe LDA object for the fitted topic model.\n\n\nid\nA unique identifier for the model.  Note that this is a UUID object.  The actual id may be found in “id.hex”.  The UUID wrapper object is kept for aesthetic reasons with regard to printing, etc.\n\n\n\nFull Documentation\nFor full documentation, please see documentation.pdf in the repository.\n'], 'url_profile': 'https://github.com/JWLevesque', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'Nairobi', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Supervised-Learning\nThis repository contains notebooks of various supervised learning modules\n\n\n\nRegression,\n\n\n\n\nPerceptron Algorithm,\n\n\n\n\nDecision Trees ,\n\n\n\n\nNaive Bayes,\n\n\n\n\nSVM ,\n\n\n\n\nEnsemble Methods,\n\n\n\n\nModel Evaluation\n\n\n\n'], 'url_profile': 'https://github.com/maureenwaitherero', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'Bandung, Indonesia', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['solar-radiation-prediction\nThe 5th Bangkit assignment is to build a machine learning model based on knowledge from MLCC. The dataset obtained by NASA will be processed to make a prediction using regression.\n'], 'url_profile': 'https://github.com/marcellinusc', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'Lovely Professional University , Jalandar ,Punjab', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Constructive-Cost-Index-for-Predicting-Future-Predctions-\nCCI is a method with which we can estimate predictions of CCI in the future period. We are going to use Linear Regression, Logistic Regression and Clustering Techniques to implement CCI for future predictions\n'], 'url_profile': 'https://github.com/rpsnaik', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vishal-889', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Autistic Spectrum Disorder Screening with Ensemble Majority Voting and Weighted Average Voting\n\n\nEnsemble Combination Rules\nThe machine learning classifiers can be combined using different combination rules i.e., Majority Voting and Weighted Average Voting.\nMajority Voting\nEvery individual model makes a prediction for each test instances and the final output prediction is the one that receives the majority of votes.\nWeighted Average Voting\nIn weighted average voting we can increase the importance of one or more models by assigning weights to the models. The prediction of each model is multiplied by the weight and then their average is calculated.\nDataset - Autistic Spectrum Disorder\nAutistic Spectrum Disorder (ASD) is a neurodevelopment condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis. The rapid growth in the number of ASD cases worldwide necessitates datasets related to behavior traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilized for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioral features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behavior science.\nTask: Classification\nAttribute Type: Categorical, continuous and binary\nArea: Medical, health and social science\nFormat Type: Non-Matrix\nNumber of Instances (records in your data set): 704\nNumber of Attributes (fields within each record): 21\nAttribute Information:\nAttribute Type: Description\nAge: Number Age in years\nGender: String Male or Female\nEthnicity: String List of common ethnicities in text format\nBorn with jaundice Boolean (yes or no) Whether the case was born with jaundice\nFamily member with PDD Boolean (yes or no) Whether any immediate family member has a PDD\nWho is completing the test String Parent, self, caregiver, medical staff, clinician, etc.\nCountry of residence String List of countries in text format\nUsed the screening app before Boolean (yes or no) Whether the user has used a screening app\nScreening Method Type Integer (0,1,2,3) The type of screening methods chosen based on age category (0=toddler, 1=child, 2= adolescent, 3= adult)\nQuestion 1 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 2 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 3 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 4 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 5 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 6 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 7 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 8 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 9 Answer Binary (0, 1) The answer code of the question based on the screening method used\nQuestion 10 Answer Binary (0, 1) The answer code of the question based on the screening method used\nScreening Score Integer The final score obtained based on the scoring algorithm of the screening method used. This was computed in an automated manner.\n\n'], 'url_profile': 'https://github.com/Harshita9511', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Combating Employee Attrition in IT Industry\n• Applied data visualization techniques to explore distribution and correlation of employee demographics and churn rate. \n• Built random forest model and elastic net regression model to predict employee churn rate in R.\n'], 'url_profile': 'https://github.com/boyasun', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'Athens, Greece', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': [""Gene Contribution on Breast Cancer Survival\nThis repository is the implementation of an exercise for the Statistics for Big Data course for the MSc in Data Science of Athens University of Economics and Business.\nWe are provided with a dataframe that includes information about 78 deceased patients who were suffering from breast cancer. In particular, for each patient we have:\n\nTheir survival (in months)\ntheir age\nan indication about their ERP\npart of their genetic profile (~25.000 genes)\n\nThe aim of this analysis is to identify the genes that lead to the most significant contribution with\nregards to survival. To carry out this task we will be using LASSO and Logistic Regression.\nMore details to come.\nTO-DO\n\nglmnet with alpha=1 actually translates to Lasso. There is no need to create a logistic regression model afterwards. We can use the predict function with type='coefficients' to get the coefficient estimates. Then we can run predict again, this time with type='response' in order to get predictions for the test data.\nMoooore\n\n""], 'url_profile': 'https://github.com/gchoumos', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tatsuro-Ki', 'info_list': ['Python', 'GPL-3.0 license', 'Updated Jul 25, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'R', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['Coronavirus predicor developed in R, we have built a Predictor for Coronavirus which predicts how much total deaths can happen on a certain date.\nThis application works based on Linear Regression. And the dataset is fetched from WHO (reliable source).\n'], 'url_profile': 'https://github.com/rahulsavage12', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,850 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,850 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Cincinnati, Ohio', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/parthskuc', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['BCycle-Data-Project\nI ran different regressions on the BCycle dataset using Python libraries to see optimal locations for BCycle placements in terms of user\ntraffic for my MIS 304 class.\n'], 'url_profile': 'https://github.com/TanviVS', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['measurements\nA library that provides routines supporting calculations related to measurement activities.  The library contains routines in the following areas:\n\nStatistics\nLinear Regression\nData Smoothing\nFiltering\nFourier Transforms\nPeak Detection\nInterpolation\nDifferentiation\nIntegration\n\nThe library not only exposes a modern Fortran API, but also exposes a C API with equal functionallity.  The entire C API is contained within a single header file.\nDependencies\nThe measurements library depends upon the following libraries.\n\nFERROR\nLINALG\nNONLIN\n\nThis library also makes use of the modern_fftpack library, but embeds it as a submodule.\nPeak Detection Example\nThe following example highlights basic usage of the peak-detection functionallity.\nprogram main\n    use iso_fortran_env\n    use measurements_core\n    implicit none\n\n    ! Parameters\n    integer(int32), parameter :: npts = 1000\n    real(real64), parameter :: dt = 1.0d-3\n    real(real64), parameter :: threshold = 1.0d-2\n\n    ! Variables\n    real(real64) :: t(npts), x(npts)\n    type(peak_info) :: pks\n    integer(int32) :: i\n\n    ! Create a waveform\n    do i = 1, npts\n        if (i == 1) then\n            t(i) = 0.0d0\n        else\n            t(i) = t(i-1) + dt\n        end if\n        x(i) = exp(-2.0d0 * t(i)) * sin(15.0d0 * t(i))\n    end do\n\n    ! Locate the peaks\n    pks = peak_detect(x, threshold)\n\n    ! Print the peak and valley information\n    print \'(A)\', ""Peaks (indices, t, x)""\n    do i = 1, size(pks%max_values)\n        print \'(AI0AF7.5AF7.5)\', achar(9), &\n            pks%max_value_indices(i), achar(9), &\n            t(pks%max_value_indices(i)), achar(9), &\n            pks%max_values(i)\n    end do\n\n    print \'(A)\', ""Valleys (indices, t, x)""\n    do i = 1, size(pks%min_values)\n        print \'(AI0AF7.5AF8.5)\', achar(9), &\n            pks%min_value_indices(i), achar(9), &\n            t(pks%min_value_indices(i)), achar(9), &\n            pks%min_values(i)\n    end do\nend program\nThe above program produces the following output.\nPeaks (indices, t, x)\n        97      0.09600 0.81826\n        516     0.51500 0.35404\n        935     0.93400 0.15319\nValleys (indices, t, x)\n        306     0.30500 -0.53823\n        725     0.72400 -0.23288\n\nThe following plot illustrates the peak detection results.\n\nInterpolation Example\nThe following example illustrates the most basic use of the linear and spline interpolation routines.\nprogram main\n    use iso_fortran_env\n    use measurements_core\n    implicit none\n\n    ! Parameters\n    integer(int32), parameter :: knotpts = 9\n    integer(int32), parameter :: npts = 1000\n\n    ! Local Variables\n    integer(int32) :: i\n    real(real64) :: dx, x(knotpts), y(knotpts), xi(npts), yi(npts), yp(npts)\n    type(spline_interp) :: spline\n    type(linear_interp) :: linear\n\n    ! Define a data set:\n    x = [-4.0d0, -3.0d0, -2.0d0, -1.0d0, 0.0d0, 1.0d0, 2.0d0, 3.0d0, 4.0d0]\n    y = [0.0d0, 0.15d0, 1.12d0, 2.36d0, 2.36d0, 1.46d0, 0.49d0, 0.06d0, 0.0d0]\n\n    ! Define the interpolation points\n    dx = (maxval(x) - minval(x)) / (npts - 1.0d0)\n    do i = 1, npts\n        if (i == 1) then\n            xi(i) = minval(x)\n        else\n            xi(i) = xi(i-1) + dx\n        end if\n    end do\n\n    ! Compute the spline interpolation\n    call spline%initialize(x, y)\n    yi = spline%interpolate(xi)\n\n    ! Compute the linear interpolation\n    call linear%initialize(x, y)\n    yp = linear%interpolate(xi)\n\n    ! Print out the results\n    do i = 1, npts\n        print \'(F8.5AF8.5AF8.5)\', xi(i), achar(9), yi(i), achar(9), yp(i)\n    end do\nend program\nThe library also exposes a C API.  The following code is the C equivalent to the Fortran code shown above.\n#include <stdio.h>\n#include ""measurements.h""\n\n#define NINTERP 1000\n#define NPTS 9\n\nint main() {\n    // Local Variables\n    int i, flag;\n    double dx;\n    double x[NPTS] = { -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0 };\n    double y[NPTS] = { 0.0, 0.15, 1.12, 2.36, 2.36, 1.46, 0.49, 0.06, 0.0 };\n    double xi[NINTERP], yi[NINTERP], yp[NINTERP];\n\n    // Define the interpolation points\n    dx = (x[NPTS-1] - x[0]) / (NINTERP - 1.0);\n    for (i = 0; i < NINTERP; ++i) xi[i] = i == 0 ? x[0] : xi[i - 1] + dx;\n\n    // Compute the spline interpolation\n    flag = c_spline(NPTS, x, y, NINTERP, xi, yi, \n        SPLINE_QUADRATIC_OVER_INTERVAL, 0.0, \n        SPLINE_QUADRATIC_OVER_INTERVAL, 0.0);\n    \n    // Compute the linear interpolation\n    flag = c_interpolate(1, NPTS, x, y, NINTERP, xi, yp);\n\n    // Print out the results\n    for (i = 0; i < NINTERP; ++i) {\n        printf(""%f\\t%f\\t%f\\n"", xi[i], yi[i], yp[i]);\n    }\n\n    // End\n    return 0;\n}\nThe following plot illustrates the interpolation results.\n\n'], 'url_profile': 'https://github.com/jchristopherson', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Ottawa, ON', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': [""BlandAltmanPy\nPython package for performing statistical validation on the results of a regression model compared to the gold standard measurements.\nPython script to perform Bland-Altman statistical analysis on two vectors of data. Create a BlandAltman class with your data and call methods to automatically generate Bland-Altman statistics and graphs. Statistics and graphs are based on the gold standard Bland-Altman style statistical comparison preseented in [1] and [2].\nThe Bland-Altman method was introduced in a 1986 journal paper by those authors and presented methods for assessing validation of a new measure compared to a gold standard measure. These statistical methodologies have become the gold standard for comparing data from a novel medical device, with the original paper having over 47,000 citations.\nSteps\n\nPlace the BlandAltman.py file in the folder you are working in\nIn Python call 'import BlandAltmanPy'\nGet your two vectors of data into the Notebook\n\nSee example with example_data.csv file\n\nOne column should have the gold standard measurements\nAnother column should have the new measurements that you are comparing\nLabels should be in first row\n\n\n\n\nCreate the BlandAltman class for your data:\n\ncompare = BlandAltmanPy.BlandAltman(df.gold_standard,df.new_measure)\n\nNow you can call methods off of compare to get statistics and plots\n\nSee the Example_Jupyter_Notebook file for an example of using BlandAltmanPy within Jupyter Notebooks.\nDetails\nStatistics\nGet BlandAltman statistics by entering:\ncompare.print_stats()\n\n\n\nStatistic\nDescription\n\n\n\n\nMean error\nThe average of all the differences between the gold standard measure and the new measure\n\n\nMean absolute error\nThe average of all the absolute differences between the gold standard measure and the new measure\n\n\nMean squared error\nThe average of all the squared differences between the gold standard measure and the new measure. This metric does a better job than MAE at punishing outliers in the data, but has the disadvantage that the metric is no longer in the same units as the original inputs\n\n\nRoot mean squared error\nThe average of all the square root of the squared differences between the gold standard and the new measure. This metric is in the same units as the original inputs\n\n\nCorrelation\nPearson Product Correlation between the gold standard measure and the new measure\n\n\n95% Confidence Intervals\nBased on the provided data, there is a 95% chance that any new measure obtained falls within this range of the gold standard  measure\n\n\n\nAdjust number of decimals in print_stats output by setting round_amount:\ncompare.print_stats(round_amount = 3)\nReturn a python dictionary of the statistics:\nstats_dict = compare.return_stats()\nScatter Plot\nGet the BlandAltman scatter plot by:\ncompare.scatter_plot()\n\nDifference Plot\nGet the BlandAltman difference plot by:\ncompare.difference_plot()\n\n\nThe legend location looks a bit odd in the above graph. This is because it is set to auto-adjust to any location that will not cover the data points.\n\nAdvanced Settings\nSet custom labels for scatter plot\ncompare.scatter_plot(x_label='ECG HR [bpm]',y_label='PPG HR [bpm]')\nSet title in plots\ncompare.scatter_plot(x_label='ECG HR [bpm]',y_label='PPG HR [bpm]',the_title='HR Comparison')\ncompare.difference_plot(the_title='Bland-Altman Differnce Plot')\nToggle legend off in plots\nTurn legend off in plots by setting show_legend=False:\ncompare.scatter_plot(x_label='ECG HR [bpm]',y_label='PPG HR [bpm]',the_title='HR Comparison',show_legend=False)\ncompare.difference_plot(the_title='Bland-Altman Differnce Plot',show_legend=False)\nSave plots as images\nThe default behaviour is to save each plot image to a .pdf file in the output_images folder. You can adjust the name and extension of the saved image by using the file_name setting:\ncompare.scatter_plot(x_label='ECG HR [bpm]',y_label='PPG HR [bpm]',file_name='HR_Scatter_Compare.jpg')\ncompare.difference_plot(file_name='blood_pressure_diff_plot.pdf')\nSet figure size in plots\nSet figure size in inches. Useful for formatting images to fit journal paper size requirements. Default is figure_size=(4,4)\ncompare.scatter_plot(x_label='ECG HR [bpm]',y_label='PPG HR [bpm]',the_title='Heart Rate Comparison',figure_size=(8,8))\ncompare.difference_plot(the_title='Bland-Altman Differnce Plot',figure_size=(8,8))\nAdjust for multiple measurements on one subject\nEach measurement must come from an independent sample. This means that if you have multiple observations from one subject they must be averaged together to make one observation. This affects how the 95% confidence intervals are calculated. Set averaged to True in the initial set up of the BlandAltman class:\ncompare = BlandAltman(df.gold_standard,df.new_measure,averaged=True)\nGenerating plots for journal papers\nAvoid use of type 3 fonts for journal paper acceptance by setting the is_journal input to True:\ncompare.scatter_plot(x_label='ECG HR [bpm]',y_label='PPG HR [bpm]',is_journal=True)\nReferences\n[1] Bland, J.M. and Altman, D., 1986. Statistical methods for assessing\nagreement between two methods of clinical measurement. The lancet,\n327(8476), pp.307-310.\n[2] Giavarina, D., 2015. Understanding bland altman analysis. Biochemia\nmedica: Biochemia medica, 25(2), pp.141-151.\n""], 'url_profile': 'https://github.com/m-patterson-wearable', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['COVID_19_Analysis\nThe Purpose of this project to understand the insight of COVID 19 Data using Python, This project is divided into two following parts\n(1) EDA (Exploratory Data Analysis)\n(2) Apply Competency Questions (i.e Prediction using Regression, jaccard similarity and Locality Sensitive Hashing) on the data to get insight from data.\nif you have any query or question, you can ask me without any hesitation at khizersultan007@gmail.com\n'], 'url_profile': 'https://github.com/KhizarSultan', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/avina-p', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Breast-cancer-wisconsin-diagnostic-dataset-Classification-by-Tensorflow\nThis is a project using the Wisconsin Breast Cancer (Diagnostic) dataset from the UCI Machine Learning Repository. link: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) I have use Artificial Neural Network to see  best classification results in differentiating malignant tumors from benign tumors.\n'], 'url_profile': 'https://github.com/harshkarna', 'info_list': ['1', 'R', 'Updated Dec 25, 2020', 'Python', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', '2', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', '1', 'Fortran', 'GPL-3.0 license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Dec 2, 2020', '2', 'Jupyter Notebook', 'Updated Nov 9, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['DM_miniproject\n'], 'url_profile': 'https://github.com/harshvasa', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'Charlotte, NC', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gwerven', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Mercari-Price-Recommendation\nThis Case Study is based on a Kaggle dataset - https://www.kaggle.com/c/mercari-price-suggestion-challenge\nHere is the detailed blog - https://towardsdatascience.com/mercari-price-recommendation-for-online-retail-sellers-979c4d07f45c\nContents of the Code Files are given below :-\n\n\n\nCode File\nDescription\n\n\n\n\nkaggle_sub.py\nExecutable .py file for kaggle submission\n\n\nfinal(colab).ipynb\nFunction 1 - takes input X, returns prediction Y\n\n\nfinal(colab).ipynb\nFunction 2 - takes input (X,Y), returns evaluation metric (RMSLE)\n\n\n\nAll Experimentation and Models are in .ipynb files. Table of Contents and sections in .ipynb files as below :-\n\n\n\nS.No:\nSection\nNotebook(.ipynb)\n\n\n\n\n1.\nBusiness Problem\n1_eda.ipynb\n\n\n2.\nExploratory Data Analysis\n1_eda.ipynb\n\n\n3.\nData Processing\n2_process.ipynb\n\n\n4.\nFeature Engineering\n2_process.ipynb\n\n\n5.\nCorrelation heatmap\n2_process.ipynb\n\n\n6.\nFinal Data Preparation\n3_baseline_machine_learning_models.ipynb\n\n\n7.\nEvaluation Metiric\n3_baseline_machine_learning_models.ipynb\n\n\n8.\nBaseline Ridge Model\n3_baseline_machine_learning_models.ipynb\n\n\n9.\nBaseline LGBM Model\n3_baseline_machine_learning_models.ipynb\n\n\n10.\nBaseline Ensemble Model\n3_baseline_machine_learning_models.ipynb\n\n\n11.\nBaseline LSTM (Colab)\n4_baseline_lstm(colab).ipynb\n\n\n12.\nBaseline MLP (Colab)\n5_baseline_mlp(colab).ipynb\n\n\n13.\nFinal Models\n6_final_models.ipynb\n\n\n14.\nFinal Summary\n6_final_models.ipynb\n\n\n\n'], 'url_profile': 'https://github.com/debayanmitra1993-data', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'Johannesburg', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['In this assignment, the goal is to predict if someone has cancer or not. To do this, I will build a binary classifier with the logistic regression model.\n'], 'url_profile': 'https://github.com/masaimahapa', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kibesteve', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['BoardGameReviewPredictor\nGive a data set of over 80k board games. We need to develop an algorithm which is able to accurately predict the score that game will receive. In this project I have used Linear Regression, Decision Tree and K-neighbor algorithm to train the individual models  and then used ensemble technique to get the best accuracy.\n'], 'url_profile': 'https://github.com/dAttri97', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '341 contributions\n        in the last year', 'description': ['r-machine-learning-notebooks\nThis repository is a collection of various R notebooks and scripts under various machine learning topics in order to curate a learning module for machine learning enthusiasts. The repo contains modules on topics like regression, clustering etc. It also contains other various topics like web scraping.\n'], 'url_profile': 'https://github.com/ashide2729', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Credit-Risk-Modelling-with-Python\nBuilding an PD Model to predict the likelihood of loan getting default.\n'], 'url_profile': 'https://github.com/sujaytalesara', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': [""The Winton Stock Market Challenge\nWinton Stock Market Challenge was a competition hosted by Winton on Kaggle in 2016.\nThe main task of this competition was predict the interday and intraday return of a stock, given the history of the past few days.\n\nNOTE:\nTo view the final code with the interactive graphs, click here\ntl;dr\n\nDeveloped a data pre-processing pipeline\nTuned and Trained a Multi-Output Multi-Layer Perceptron Regression\nModel to predict stock returns based on returns from past two days\nand a set of features\n\nData\nIn this competition the challenge is to predict the return of a stock, given the history of the past few days.\nWe provide 5-day windows of time, days D-2, D-1, D, D+1, and D+2. You are given returns in days D-2, D-1, and part of day D, and you are asked to predict the returns in the rest of day D, and in days D+1 and D+2.\nDuring day D, there is intraday return data, which are the returns at different points in the day. We provide 180 minutes of data, from t=1 to t=180. In the training set you are given the full 180 minutes, in the test set just the first 120 minutes are provided.\nFor each 5-day window, we also provide 25 features, Feature_1 to Feature_25. These may or may not be useful in your prediction.\nEach row in the dataset is an arbitrary stock at an arbitrary 5 day time window.\n\n\n\n\nTechnologies Used\n\nPython\nPandas\nNumpy\nMatplotlib\nSeaborn\nPlotly\nScikit Learn\nPrinciple Componnent Analysis\nIterative Imputer\nRandom Forest Regressor\nMulti-layer Perceptron Regressor\nMulti Output Regressor\n\nExploratory Data Analysis\nExploratory Data Analysis  is performed to explore the structure of the data, identify categorical and continuos data feilds, missing values, and corelations amongst different data columns \n\n  Corelation Heatmap between diffent features:\n\n\nFeature Engineering\nAs observed in the corelation heatmap above, alot of features are strongly corelated to each other. This means that it is possibble to apply Dimentionality Reduction methods such as Principle Component Analysis. \nPrincipal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. \nThe optimum number of principle components can be found by observing the variance for different sets of components. The set with variance closest to one is concidered as the one with optimum number of principle components.\n\n\nHere we can observe that the optimum number of components is 12 \nTo simplify the problem, the intraday returns are aggregated as sum and standard deviation for both features (Ret_2 to Ret_120) and target labels (Ret_121 to Ret_180)\nStandard deviation of the interday returns is also considered to see how much the returns vary.\nModel Building\nAfter imputing missing values and executing Principle Component Analysis on the numerical data columns, the categorical data was transformed into dummy variable columns using Pandas' get_dummies() feature. \nThe data was split into training (70%) and testing (30%) data. \nI tried two different models:\n\n Random Forest Regressor: For baseline model\n   Multi Layer Perceptron Regressor (MLPReggresor): Since the data involved feature values of different ranges, I thought a Multi Layer Perceptron model will be resistent to those variations \n\nSince the problem statement dictates us to predict multiple values, MultiOutputRegressor is used.\ny_test (blue) vs. y_pred (orange) for first 500 data points in test data\n\nMLP RegressorRandom Forest Regressor\n\n\n\n\nHyperparameter Tuning\nAs seen in the graphs above, the prediction lined for Random Forest Regressors are mostly flat lines with a few sparse peaks. While on the contrary, Multi-level Perceptron Regressor shows way better results. Thus only Multi-level Perceptron Regressor underwent hyperparameter tuning.\nGrid Search Cross Validation method is used to fine tune the regression model.\nThe best model obtained after hyper parameter tuning is: \n\nModel Evaluation\nMean Absolute Error (MAE) is used the performance metric for evaluating the regression model. MAE is easy to interpret and provides a clear view of the performance of the model.\n\n\n\n The Mean Absolute Error of the model = 0.01366\n""], 'url_profile': 'https://github.com/chawla201', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '549 contributions\n        in the last year', 'description': ['The-Best-Classifier\nYou load a historical dataset from previous loan applications, clean the data, and apply different classification algorithm on the data. You are expected to use the following algorithms to build your models: k-Nearest Neighbour Decision Tree Support Vector Machine Logistic Regression The results is reported as the accuracy of each classifier, using the following metrics when these are applicable: Jaccard index F1-score LogLoass\n'], 'url_profile': 'https://github.com/sidhu1012', 'info_list': ['HTML', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'HTML', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Dec 23, 2020', 'Python', 'Updated Apr 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Movie-Revenue-Prediction\nA regression model is built to predict the box office revenue of a movie. The model could be used by the production studio to estimate the profitability of a movie project before green lighting it. The  movie revenue dataset is sourced from IMDB. The notebook file consists of the code to clean the data, visualize the data, select/ train the model and check the model predictions\n'], 'url_profile': 'https://github.com/Ravi-Rao26', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 13, 2020', 'C', 'MIT license', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Loan-Classifier\nBuild a classifier to predict whether a loan case will be paid off or not.\nWe load a historical dataset from previous loan applications, clean the data, and apply different classification algorithm on the data.\nWe are expected to use the following algorithms to build your models:\nk-Nearest Neighbour\nDecision Tree Support\nVector Machine\nLogistic Regression\nThe results is reported as the accuracy of each classifier, using the following metrics when these are applicable:\nJaccard index\nF1-score\nLogLoass\n'], 'url_profile': 'https://github.com/gauravapurv', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 13, 2020', 'C', 'MIT license', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['DATA_SCIENCE\nThe influence of temperature and monthly or seasonwise trends on the annual electricity consumption in IIT-Bombay has been investigated in order to develop a simple and data light electricity consumption forecasting model, to be used as part of more complex planning tools. The time period considered for the historical data is from January 2017 to December 2017. Multivariable Linear regression models are developed using historical electricity consumption, day, month, and weekday. Annual electricity consumption was strongly related to the selected variables.\n'], 'url_profile': 'https://github.com/AshaySingh2907', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 13, 2020', 'C', 'MIT license', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LRwArd04', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 13, 2020', 'C', 'MIT license', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NumericA2\nIn this assignment you will demonstrate your understanding of solving engineering problems using numerical computations and assessing particular algorithms. The objectives of this assignment are to program algorithms for root-ﬁnding, solving systems of linear algebraic equations, performing least-squares approximations and interpolations, regressing solutions and solving a differential equation using different integration and differentiation schemes.\n'], 'url_profile': 'https://github.com/LYX1231', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 13, 2020', 'C', 'MIT license', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['ML-classifier-algorithm\nNow that you have been equipped with the skills to use different Machine Learning algorithms, over the course of five weeks, you will have the opportunity to practice and apply it on a dataset. In this project, you will complete a notebook where you will build a classifier to predict whether a loan case will be paid off or not.  You load a historical dataset from previous loan applications, clean the data, and apply different classification algorithm on the data. You are expected to use the following algorithms to build your models:  k-Nearest Neighbour Decision Tree Support Vector Machine Logistic Regression The results is reported as the accuracy of each classifier, using the following metrics when these are applicable:  Jaccard index F1-score LogLoass\n'], 'url_profile': 'https://github.com/YuyingTan', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 13, 2020', 'C', 'MIT license', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}","{'location': 'lovely professional university', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': [""Classify-Song-Genres-from-Audio-Data\nUsing a dataset comprised of songs of two music genres (Hip-Hop and Rock), you will train a classifier to distinguish between the two genres based only on track information derived from Echonest (now part of Spotify). You will first make use of pandas and seaborn packages in Python for subsetting the data, aggregating information, and creating plots when exploring the data for obvious trends or factors you should be aware of when doing machine learning. Next, you will use the scikit-learn package to predict whether you can correctly classify a song's genre based on features such as danceability, energy, acousticness, tempo, etc. You will go over implementations of common algorithms such as PCA, logistic regression, decision trees, and so forth.\n""], 'url_profile': 'https://github.com/Rishabhtyagi999', 'info_list': ['Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 13, 2020', 'C', 'MIT license', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020']}",,,
