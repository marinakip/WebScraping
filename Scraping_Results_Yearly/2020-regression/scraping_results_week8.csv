"{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Deep symbolic regression\nDeep symbolic regression (DSR) is a deep learning algorithm for symbolic regression--the task of recovering tractable mathematical expressions from an input dataset. The package dsr contains the code for DSR, including a single-point, parallelized launch script (dsr/run.py), baseline genetic programming-based symbolic regression algorithm, and an sklearn-like interface for use with your own data.\nThis code supports the ICLR 2021 paper Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients.\nInstallation\nInstallation is straightforward in a Python 3 virtual environment using Pip. From the repository root:\npython3 -m venv venv3 # Create a Python 3 virtual environment\nsource venv3/bin/activate # Activate the virtual environmnet\npip install -r requirements.txt # Install Python dependencies\nexport CFLAGS=""-I $(python -c ""import numpy; print(numpy.get_include())"") $CFLAGS"" # Needed on Mac to prevent fatal error: \'numpy/arrayobject.h\' file not found\npip install -e ./dsr # Install DSR package\n\nTo perform experiments involving the GP baseline, you will need the additional package deap.\nExample usage\nTo try out DSR, use the following command from the repository root:\npython -m dsr.run ./dsr/dsr/config.json --b=Nguyen-6\n\nThis should solve in around 50 training steps (~30 seconds on a laptop).\nGetting started\nConfiguring runs\nDSR uses JSON files to configure training.\nTop-level key ""task"" specifies details of the benchmark expression for DSR or GP. See docs in regression.py for details.\nTop-level key ""training"" specifies the training hyperparameters for DSR. See docs in train.py for details.\nTop-level key ""controller"" specifies the RNN controller hyperparameters for DSR. See docs for in controller.py for details.\nTop-level key ""gp"" specifies the hyperparameters for GP if using the GP baseline. See docs for dsr.baselines.gspr.GP for details.\nLaunching runs\nAfter configuring a run, launching it is simple:\npython -m dsr.run [PATH_TO_CONFIG] [--OPTIONS]\n\nSklearn interface\nDSR also provides an sklearn-like regressor interface. Example usage:\nfrom dsr import DeepSymbolicRegressor\nimport numpy as np\n\n# Generate some data\nnp.random.seed(0)\nX = np.random.random((10, 2))\ny = np.sin(X[:,0]) + X[:,1] ** 2\n\n# Create the model\nmodel = DeepSymbolicRegressor(""config.json"")\n\n# Fit the model\nmodel.fit(X, y) # Should solve in ~10 seconds\n\n# View the best expression\nprint(model.program_.pretty())\n\n# Make predictions\nmodel.predict(2 * X)\n\nUsing an external dataset\nTo use your own dataset, simply provide the path to the ""dataset"" key in the config, and give your task an arbitary name.\n""task"": {\n    ""task_type"": ""regression"",\n    ""name"": ""my_task"",\n    ""dataset"": ""./path/to/my_dataset.csv"",\n    ...\n}\n\nThen run DSR:\npython -m dsr.run path/to/config.json\n\nNote the --b flag matches the name of the CSV file (-.csv ).\nCommand-line examples\nShow command-line help and quit\npython -m dsr.run --help\n\nTrain 2 indepdent runs of DSR on the Nguyen-1 benchmark using 2 cores\npython -m dsr.run config.json --b=Nguyen-1 --mc=2 --num_cores=2\n\nTrain DSR on all 12 Nguyen benchmarks using 12 cores\npython -m dsr.run config.json --b=Nguyen --num_cores=12\n\nTrain 2 independent runs of GP on Nguyen-1\npython -m dsr.run config.json --method=gp --b=Nguyen-1 --mc=2 --num_cores=2\n\nTrain DSR on Nguyen-1 and Nguyen-4\npython -m dsr.run config.json --b=Nguyen-1 --b=Nguyen-4\n\nRelease\nLLNL-CODE-647188\n'], 'url_profile': 'https://github.com/brendenpetersen', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '535 contributions\n        in the last year', 'description': ['Home |\nGitHub |\nSpeaking Engagements |\nTerms |\nE-mail\nPredicting Hotel Cancellations and ADR with Machine Learning\nThe purpose of this project is to predict hotel cancellations and ADR (average daily rate) values for two separate Portuguese hotels (H1 and H2). Included in the GitHub repository are the datasets and notebooks for all models run.\nThe original datasets and research by Antonio et al. can be found here: Hotel Booking Demand Datasets (2019). All other relevant references have been cited in the below articles.\nProject Stages\nStage 1: Data Manipulation and Feature Selection\n\n\nUsed pandas to collate over 115,000 individual cancellation and ADR entries into a weekly time series format.\n\n\nIdentified lead time, country of origin, market segment, deposit type, customer type, required car parking spaces, and week of arrival as the most important features in explaining the variation in hotel cancellations.\n\n\nStage 2: Classification\n\n\nTrained classification models on the H1 dataset and tested against the H2 dataset. Used boto3 and botocore to import data from AWS S3 bucket to SageMaker.\n\n\nUsed the Explainable Boosting Classifier by InterpretML, KNN, Naive Bayes, Support Vector Machines, and XGBoost to predict cancellations across the test set.\n\n\nSVM demonstrated the best performance overall with an f1-score accuracy of 71%, and 66% recall across the cancellation class.\n\n\nAn ANN model was also trained in conjunction with dice_ml to identify Diverse Counterfactual Explanations for hotel bookings, i.e. changes in feature parameters that would cause a non-canceling customer to cancel, and vice versa.\n\n\nStage 3: Regression\n\n\nUsed regression modelling to predict ADR (average daily rate) across each customer.\n\n\nTrained regression models on the H1 dataset and tested against the H2 dataset.\n\n\nRegression-based neural network with elu activation function showed the best performance, with a mean absolute error of 28 compared to the mean ADR of 105 across the test set.\n\n\nStage 4: Time Series\n\n\nUsed ARIMA and LSTM models to forecast weekly ADR trends.\n\n\nARIMA demonstrated best results in forecasting ADR for H1 (RMSE of 10 relative to mean ADR of 160) and H2 (RMSE of 8 relative to mean ADR of 131).\n\n\n'], 'url_profile': 'https://github.com/MGCodesandStats', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'Amsterdam', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['\nVisual Regression Testing with Jest\nAs developers, it is our job to ensure that our users receive the same experience that our UX colleagues design and trust us to implement. So, throughout the development life of a project, how can we make sure that the visuals are always correct? You could manually click through every possible user journey and check, but there has to be a better way.\nEnter automated visual regression testing. Keep an image of your component in its perfect state and compare that with the result of any code changes to ensure that no unexpected visual changes slip through unnoticed.\nThis is accomplished by combining jest, jest-image-snapshot, puppeteer, and docker to acheive one-to-one pixel-perfect comparisons in any environment. This repo serves as a demonstration of how to set this up for a real-life project.\nFull details can be found in this article.\n'], 'url_profile': 'https://github.com/tvthatsme', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\n\n\nDescription\nPython package for conditional density estimation. It either wraps or\nimplements diverse conditional density estimators.\nDensity estimation with normalizing flows\nThis package provides pass-through access to all the\nfunctionalities of nflows.\nSetup\nClone the repo and install all the dependencies using the\nenvironment.yml file to create a conda environment: conda env create -f environment.yml. If you already have a pyknos environment\nand want to refresh dependencies, just run conda env update -f environment.yml --prune.\nAlternatively, you can install via setup.py using pip install -e "".[dev]"" (the dev flag installs development and testing\ndependencies).\nExamples\nExamples are collected in notebooks in examples/.\nBinary files and Jupyter notebooks\nUsing\nWe use git lfs to store large binary files. Those files are not\ndownloaded by cloning the repository, but you have to pull them\nseparately. To do so follow installation instructions here\nhttps://git-lfs.github.com/. In\nparticular, in a freshly cloned repository on a new machine, you will\nneed to run both git-lfs install and git-lfs pull.\nContributing\nWe use a filename filter to identify large binary files. Once you\ninstalled and pulled git lfs you can add a file to git lfs by\nappending _gitlfs to the basename, e.g., oldbase_gitlfs.npy. Then\nadd the file to the index, commit, and it will be tracked by git lfs.\nAdditionally, to avoid large diffs due to Jupyter notebook outputs we\nare using nbstripout to remove output from notebooks before every\ncommit. The nbstripout package is downloaded automatically during\ninstallation of pyknos. However, please make sure to set up the\nfilter yourself, e.g., through nbstriout --install or with\ndifferent options as described\nhere.\nName\npyknós (πυκνός) is the transliterated Greek root for density\n(pyknótita) and also means sagacious.\nCopyright notice\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see https://www.gnu.org/licenses/.\nAcknowledgements\nThanks to Artur Bekasov, Conor Durkan and George Papamarkarios for\ntheir work on nflows.\nThe MDN implementation in this package is by Conor M. Durkan.\n'], 'url_profile': 'https://github.com/mackelab', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'Kolkata, India', 'stats_list': [], 'contributions': '814 contributions\n        in the last year', 'description': ['Regression\nData on two variables recorded simultaneously for a group of individuals are called bivariate data. Examples of bivariate data are heights and weights of the students in a class, the rainfall and the yield of paddy in a state for several consecutive years, etc.\nWhen we have bivariate data, we can, no doubt, consider the values of each variable separately to know the different measures like the mean and standard deviation of the variable; but here we are mainly concerned with two other problems.\nFirstly, we want to study the nature and extent of association, if any, between the variables. Secondly, if the variables are found to be associated we express one of them (regarded as the dependent variable) as a mathematical function of the other (considered as an independent variable), so that we can predict the value of the dependent variable when the value of the independent variable is known. The first problem is called correlation analysis and the second regression analysis. \n\n### What is Linear Regression?\nTo find the relationship between continuous correlated variables we use linear regression. Linear regression looks for a statistical  relationship between a set of correlated values. The representation is a linear equation that combines a specific set of input values (x) the solution to which is the predicted output for that set of input values (y). As such, both the input values (x) and the output value are numeric. When there is a single input variable (x), the method is referred to as simple linear regression. When there are multiple input variables,the method is referred to as multiple linear regression.\n\nIn contrast to the frequentist approach, Bayesian Linear Regression is also coded here.\nLink to my Medium Blog on ""Bayesian Linear Regression "".\nWhat is Polynomial Regression?\nLinear Regression works on data where the dependent and independent variable have a linear relationship. But in cases where the data do not have a linear relationship and instead possess a rather complex relationship, then Polynomial Regression is used.\nPolynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x).\nLink to my Medium Blog on ""Linear Regression Using Normal Equations and Polynomial Regression"".\nWhat is Logistic Regression?\nLogistic Regression is an example of a classification algorithm which is used to find a relationship between features and probability of a particular outcome. The term ""Logistic"" is taken from the Logit function that is used in this method of classification.\nLogistic Regression is tremendously useful in medical diagnosis given some specific symptoms and parameters. Like for example, logistic regression is used to detect early stages of breast cancer given certain parameters like age, past history, genetic factors, etc.\xa0\nThe name \'Logistic Regression\', comes from it\'s underlying technique which is quite the same as Linear Regression. Like all regression analysis, Logistic regression is an example of predictive analysis. However unlike regular regression, the outcome calculates the predicted probability of mutually exclusive event occurring based on multiple external factors.\nIt can thus be considered as a special case of linear regression where the target variable is categorical in nature. Linear regression has a considerable effect on outliers. To avoid this problem, log-odds function or logit function is used. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function.\nAs a small working example of Logistic Regression I have also worked on detecting malignancy of Breast Cancer based on the dataset of  UCI Wisconsin.\n\nLink to my Medium Blog on ""Logistic Regression"".\n'], 'url_profile': 'https://github.com/Rajwrita', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Regression algorithms\n'], 'url_profile': 'https://github.com/19PA1A0534', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'Taipei, Taiwan', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['\n\nreact-native-cli-plugin-benchmark\nReact Native CLI Plugin for Benchmark Regression\nInstallation\n$ yarn add react-native-cli-plugin-benchmark\nSupported Commands\nreact-native get-appsize-ios\nGet the generated IPA size from run-ios output\n$ react-native get-appsize-ios --configuration Release --sdk iphoneos \ninfo Found Xcode workspace ""RNApp.xcworkspace""\ninfo Building (using ""xcodebuild -workspace RNApp.xcworkspace -configuration Release -scheme RNApp -sdk iphoneos CODE_SIGN_IDENTITY="""" CODE_SIGNING_REQUIRED=NO CODE_SIGNING_ALLOWED=NO"")\ninfo Generated app size:\n{""/Users/kudo/Library/Developer/Xcode/DerivedData/RNApp-auhfmjezpdwqwhasqmpbigmosgfe/Build/Products/Release-iphoneos/RNApp.app"":37632}\nreact-native get-appsize-android\nGet the generated APK size from run-android output\n$ react-native get-appsize-android --variant release\ninfo Running jetifier to migrate libraries to AndroidX. You can disable it using ""--no-jetifier"" flag.\ninfo Building the app...\ninfo Generated app size:\n{""/Users/kudo/RNApp/android/app/build/outputs/apk/release/app-release.apk"":19603464}\nreact-native measure-ios\nMeasure from run-ios output\nNote that the command in fact to patch AppDelegate to log some information after 5 seconds from launch.\nThe information are from RCTPerformanceLogger and task memory.\n$ react-native measure-ios --configuration Release --no-packager\ninfo Found Xcode workspace ""RNApp.xcworkspace""\ninfo Launching iPhone X (iOS 12.4)\ninfo Building (using ""xcodebuild -workspace RNApp.xcworkspace -configuration Release -scheme RNApp -destination id=6FF18363-C213-4E59-9A83-3117EE7AE6FE"")\n▸ Compiling diy-fp.cc   \n▸ Compiling bignum.cc     \n▸ Compiling cached-powers.cc                                        \n▸ Compiling double-conversion.cc\n...\ninfo Launching ""org.reactjs.native.example.RNApp""\nsuccess Successfully launched the app on the simulator\ninfo Measurement result:\n{\n    ""duration.RCTPLScriptDownload"": ""2"",\n    ""duration.RCTPLScriptExecution"": ""181"",\n    ""duration.RCTPLRAMBundleLoad"": ""0"",\n    ""duration.RCTPLRAMStartupCodeSize"": ""0"",\n    ""duration.RCTPLRAMStartupNativeRequires"": ""0"",\n    ""duration.RCTPLRAMStartupNativeRequiresCount"": ""0"",\n    ""duration.RCTPLRAMNativeRequires"": ""0"",\n    ""duration.RCTPLRAMNativeRequiresCount"": ""0"",\n    ""duration.RCTPLNativeModuleInit"": ""2"",\n    ""duration.RCTPLNativeModuleMainThread"": ""56"",\n    ""duration.RCTPLNativeModulePrepareConfig"": ""0"",\n    ""duration.RCTPLNativeModuleMainThreadUsesCount"": ""4"",\n    ""duration.RCTPLNativeModuleSetup"": ""0"",\n    ""duration.RCTPLTurboModuleSetup"": ""0"",\n    ""duration.RCTPLJSCWrapperOpenLibrary"": ""0"",\n    ""duration.RCTPLBridgeStartup"": ""284"",\n    ""duration.RCTPLTTI"": ""474"",\n    ""duration.RCTPLBundleSize"": ""652466"",\n    ""memory"": ""55541760""\n}\n'], 'url_profile': 'https://github.com/Kudo', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'Melbourne, VIC, Australia', 'stats_list': [], 'contributions': '1,937 contributions\n        in the last year', 'description': [""Lightweight Machine Learning Classics with R\n(Draft v0.2.x of Machine Learning in R from Scratch)\n\nExplore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you're yet to be fluent with university-level linear algebra, calculus and probability theory or you've forgotten all the maths you've ever learned, and are seeking a gentle, albeit thorough, introduction to the topic.\n\nThis is a slightly older (distributed in the hope that it will be useful)\nversion of the forthcoming textbook (ETA 2022)\npreliminarily entitled Machine Learning in R from Scratch\nby Marek Gagolewski, which is now undergoing a major revision\n(when I am not busy with other projects). There will be not much work on-going\nin this repository anymore, as its sources have moved elsewhere;\nhowever, if you happen to find any bugs or typos, please drop me an\nemail. I will share a new draft once it is ripe. Stay tuned.\n\nAbout this Repository\nThis repository hosts the HTML version of the book.\nYou can read it at:\n\nTODO: https://lmlcr.gagolewski.com/ (a browser-friendly version)\nTODO: https://lmlcr.gagolewski.com/lmlcr.pdf (PDF)\n\nAbout the Author\nMarek Gagolewski (pronounced like Mark Gaggle-Eve-Ski 🙃)\nis currently a Senior Lecturer in Applied\nAI at Deakin University in Melbourne, VIC, Australia\nand an Associate Professor in Data Science (on long-term leave)\nat Faculty of Mathematics and Information Science, Warsaw University\nof Technology, Poland.\nHe is actively involved in developing usable free (libre) and open source\nsoftware, with particular focus on data science and machine learning.\nHe is the main author and maintainer of stringi – one of the most often\ndownloaded R packages (with over 33,000,000 downloads) that aims at natural\nlanguage and string processing as well as the Python and R package\ngenieclust implementing the fast and robust hierarchical clustering\nalgorithm Genie with noise point detection.\nHe's an author of more than 75 publications on machine learning and\noptimisation algorithms, data aggregation and clustering, statistical\nmodelling, and scientific computing. Moreover, Marek taught various courses\nrelated to R and Python programming, algorithms, data science,\nand machine learning in Australia, Poland, and Germany\n(e.g., at Data Science Retreat).\n\nCopyright (C) 2020-2021, Marek Gagolewski.\nThis material is licensed under the Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License\n(CC BY-NC-ND 4.0).\n""], 'url_profile': 'https://github.com/gagolews', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AkhilRautela', 'info_list': ['41', 'Python', 'BSD-3-Clause license', 'Updated Jan 14, 2021', '23', 'Jupyter Notebook', 'Updated Jan 26, 2021', '8', 'JavaScript', 'MIT license', 'Updated Jan 25, 2021', '9', 'Python', 'AGPL-3.0 license', 'Updated Dec 8, 2020', '6', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', '14', 'TypeScript', 'MIT license', 'Updated Sep 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '10', 'TeX', 'Updated Feb 14, 2021', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '225 contributions\n        in the last year', 'description': ['Regression\nRegression\n'], 'url_profile': 'https://github.com/guneetdeol', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['KTH Royal Institute of Technology: Course DD1327\nPackage Reg\nThis package provides a method for creating a predictive model using linear or logistic regression.\nThe package include two classes; one for linear regression and one for logistic regression, both of which uses a typical gradient descent method. If your prediction model should predict something continuous (for example pricing of property), the linear regression class is suggested. Logistic regression class is suggested for binary outcomes, such as when trying to classify images.\nPackage needs numpy\nClass lin_reg_layer:\ninit(self, num_in, num_out, bias_node = True)\nCreates a linear regression layer. Calls method ""new_coef"" to create matrix of random values aka the coefficients.\nnew_coef\nCreates a matrix with random float values between 0 and 1.\n_add_bias\nAdds a column of ones to matrix\nfeed_for\nDoes the feed forward process. Input some data and this function returns predicted value(s) using the coefficients.\ngrad_descent\nDoes a gradient descent step, changing the coefficients.\ncoef_ret\nreturns the coefficients\ncost\nCalculates and returns the cost\nClass log_reg_layer:\ninit\nCreates a logistic regression layer. Calls method ""new_coef"" to create matrix of random values aka the coefficients.\nnew_coef\nCreates a matrix with random float values between 0 and 1.\n_add_bias\nAdds a column of ones to matrix\nfeed_for\nDoes the feed forward process. Input some data and this function returns predicted value(s) using the coefficients.\ngrad_descent\nDoes a gradient descent step, changing the coefficients.\ncoef_ret\nreturns the coefficients\ncost\nCalculates and returns the cost\n'], 'url_profile': 'https://github.com/isakper', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ilyesnasraoui', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/thekalkulator', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression\nTest Automation Suite to be executed after every new build.\n'], 'url_profile': 'https://github.com/venkatramanareddy-sandi', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'Greece', 'stats_list': [], 'contributions': '346 contributions\n        in the last year', 'description': ['Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach\nThis course can be found at Coursera\nAbout this Course\nDo you have data and wonder what it can tell you?  Do you need a deeper understanding of the core ways in which machine learning can improve your business?  Do you want to be able to converse with specialists about anything from regression and classification to deep learning and recommender systems?\nIntroduction to the learning objectives\nIn this course, you will get hands-on experience with machine learning from a series of practical case-studies.  At the end of the first course you will have studied how to predict house prices based on house-level features, analyze sentiment from user reviews, retrieve documents of interest, recommend products, and search for images.  Through hands-on practice with these use cases, you will be able to apply machine learning methods in a wide range of domains.\nAbout the course\nThis first course treats the machine learning method as a black box.  Using this abstraction, you will focus on understanding tasks of interest, matching these tasks to machine learning tools, and assessing the quality of the output. In subsequent courses, you will delve into the components of this black box by examining models and algorithms.  Together, these pieces form the machine learning pipeline, which you will use in developing intelligent applications.\nLearning outcomes\nBy the end of this course, you will be able to:\n\n\nIdentify potential applications of machine learning in practice.\n\n\nDescribe the core differences in analyses enabled by regression, classification, and clustering.\n\n\nSelect the appropriate machine learning task for a potential application.\n\n\nApply regression, classification, clustering, retrieval, recommender systems, and deep learning.\n\n\nRepresent your data as features to serve as input to machine learning models.\n\n\nAssess the model quality in terms of relevant error metrics for each task.\n\n\nUtilize a dataset to fit a model to analyze new data.\n\n\nBuild an end-to-end application that uses machine learning at its core.\n\n\nImplement these techniques in Python.\n\n\nWeek 1 - Introduction\n\nWhy you should learn machine learning with us\nWho this specialization is for and what you will be able to do\nGetting started with Python, Jupyter Notebook & Turi Create\nGetting started with SFrames for data engineering and analysis\nWeek 1 Exercises:\n\nCourse examples\nGetting started with SFrames assignment\n\n\n\n\n\nWeek 2 - Regression\n\nLinear regression modeling\nEvaluating regression models\nSummary of regression\nWeek 2 Exercises:\n\nCourse examples - Predicting House Prices\nPredicting House Prices assignment\n\n\n\n\n\nWeek 3 Classification: Analyzing Sentiment\n\nClassification modeling\nEvaluating classification models\nSummary of classification\nWeek 3 Exercises:\n\nCourse examples - Analyzing sentiment\nAnalyzing sentiment assignment\n\n\n\n\n\nWeek 4 - Clustering and Similarity: Retrieving Documents\n\nAlgorithms for retrieval and measuring similarity of documents\nClustering models and algorithms\nSummary of clustering and similarity\nWeek 4 Exercises:\n\nCourse examples - Document retrieval\nRetrieving Wikipedia articles assignment\n\n\n\n\n\nWeek 5 - Recommender systems\n\nAbout Recommender systems\nCo-occurrence matrices for collaborative filtering\nMatrix factorization\nPerformance metrics for recommender systems\nSummary of recommender systems\nWeek 5 Exercises:\n\nCourse examples - Song recommender system\nRecommending Songs assignment\n\n\n\n\n\nWeek 6 - Deep Learning\n\nNeural networks: Learning very non-linear features\nDeep learning & deep features\nSummary of deep learning\nWeek 6 Exercises:\n\nCourse examples - Deep features for image classification\nDeep features for image retrieval assignment\n\n\n\n\n\n'], 'url_profile': 'https://github.com/PavlosIsaris', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'Kolkata,India', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': [""Industrial-Training-Project-Linear-Regression\nEmployee HR DATABASE:\n\n\n\n\nOriginal Dataset:\nAfter converting CSV file to Pandas Dataframe:\n\nUpdated Dataset:\nAdded columns Age,conv(10% of salary) & total(salary+hra+conv)\n\nReports/Data Visualization:\nSum, Mean & Standard Deviation of Total, Salary ,HRA & conv Based on Gender:\n\nState Wise Mean of Total , Salary ,HRA & conv:\n\nBar Plot with 'state' as X-axis and all mean values as Bars:\n\nState Wise Sum of Total , Salary ,HRA & conv:\n\nBar Plot with 'state' as X-axis and all sum values as Bars:\n\nScatter Plot- Experience Vs Salary of Employees:\n\nSalary Prediction Based On Experience:\n\n""], 'url_profile': 'https://github.com/disha2sinha', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'Indonesia', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kelenmarbun', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'Windsor', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Linear Regression, Multi variate Linear Regression and Polynomial Regression practice\n'], 'url_profile': 'https://github.com/Harnoorsingh5', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Linear Regression\nThe assets for the linear regression post are located here.\nSource of dataset: https://github.com/boosuro\n'], 'url_profile': 'https://github.com/neuralnetai', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated May 28, 2020', '2', 'Updated Feb 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/be-green', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AmritDadhiala', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'Arkansas', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/madandahal', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': [""Polynomial Regression Machine Learning\nThis example explains working with polynomail regression and concrete dataset.\nLink to the dataset - https://www.kaggle.com/maajdl/yeh-concret-data\nPolynomail regression add a degree of couve in the presiction to best fit the data instead of just a straight line.\nSteps:\n1) Load the Data:\n\nLoad the data in pandas.\nThis dataset will not get coloumns here but we can add that easily\nCheck the notebook for the code.\n\n2) Clean The Data:\n\nAs its the easiest problem the dataset doesn't have any Missing or duplicate values.\nAlthouth there's a scope of work everywhere so we can check for understanding.\n\n3) Feature Selections:\n\nWith basic method of correlection we will first try to judge which features we can drop.\nWe can also play with different method and get different predication scores.\nI did try the univariat feature selection.\nUltimately the score was same so, revert back....\n\n4) Split the data:\n\nWe need to split the data for training and testing.\nThe data is split into 70% training and 30% testing.\n\n5) Fit the model:\n\nwe use the liner regression model to get predicsitons.\nafter the fit, use predict method for prediction\n\n6) Prediction score:\n\nthere are many ways to check the goodness of model\nI have used r2_score to check and the model worked fine.\n\nCheck the code and use for more improvement and share...!!!!!\n""], 'url_profile': 'https://github.com/futureautomate', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Position-Salaries-Data-Set\nPolynomial Regression\n'], 'url_profile': 'https://github.com/omkarholmes', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'Tunisia', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Coronavirus-death-rate-prediction\nThis notebook analysis an open source data set of cocronavirus spread allover the world.  Keep the world safe!!!It predicts the daet and recovery rate through cleansing, visualizing data. A linear regression is performed for predicetion.Source:\nThe data set has been open sourced in Kaggle.\nThis code is for beginners.\nTry it and return feedbacks.\nWelcome\n'], 'url_profile': 'https://github.com/HaithemH', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['simple-linear-regression\nsimple linear regression in R and Python\n'], 'url_profile': 'https://github.com/mayosmjs', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Simple Linear Regression\nSimple linear regression is a type of regression analysis where the number of independent variables is one and there is a linear relationship between the independent(x) and dependent(y) variable. Based on the given data points, we try to plot a line that models the points the best. The line can be modelled based on the linear equation shown below.\ny=mx+b\nwhere m is the slope and b is the intercept.\nThe motive of the linear regression algorithm is to find the best values for m and b.\nIn this project, simple linear regression model is created by two  methods:\n\nUsing normal python code\n\n\nUsing library\n\nraw_code.py\nThis file contains the raw code for creating model for simple linear regression and its mean squared error is also found.\nusing_lib.py\nThis file contains code which uses sklearn library. LinearRegression class is imported from the sub module of sklearn, linear_model.\n\nsklearn is the library used for creating most of the machine learning models.\n\nplot_graph.py\nThis file will plot the line on the graph which is the best fit for the given dataset.\nDataset used\nkc_house_data.csv\n'], 'url_profile': 'https://github.com/philo-shoby', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': [""Polynomial Regression\nPolynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y\ny=b0 + x*b1 + x^2b2....\ncode.py\nThis file contains the code for creation of polynomial regression model using the 'sklearn' library. The sub module 'preprocessing' is imported for PolynomialFeatures class. This class is used to transform the feature data into interactive matrix form. The model is then created using the Linear Regression class from linear_model sub module.\nplot.py\nThis file presents the code for plotting the graph for polynomial regression.\n""], 'url_profile': 'https://github.com/philo-shoby', 'info_list': ['2', 'R', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '3', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['HeartPredictionModel_LogisticRegression\n'], 'url_profile': 'https://github.com/keshavbansal11', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manindra21', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Hyrax Regression Tests\nCurrently these tests depend on:\n\nA Hyrax server running on localhost:8080 (but see the --server opetion)\nThe programs getdap and getdap4 must be on the PATH\n\nBuild\nautoreconf -vif\n./configure \nmake check\n\nRunning make check builds testsuite and then runs it with the default\noptions. However, it's faster, if you want to test a remote server, build\nthe test program using make testsuite and then run them using\n./testsuite --server=<name> --jobs=8.\nRun ./testsuite --help to see a full set of options.\nWorth knowing:\n\nThe --jobs=N will speed up the tests quite a bit, especially with a remote\nserver.\nThe --server= option will switch from the default localhost:8080 to\na different server. The server must have the stock data and handlers.\nThe --besdev=[yes|no] option toggles a set of error code tests that only work\nwith a server compiled using --enable-developer but are useful tests, all the same.\nThere are a number of keywords (-k ) that can be used to select groups of\nrelated tests. For example, ./testsuite -j9 -k html will run all of the tests that\nget HTML responses. The keywords supported are html, test, error, dods, dap, and header.\n\n""], 'url_profile': 'https://github.com/OPENDAP', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['MLHW4\nLogistic Regression and Perceptron\n'], 'url_profile': 'https://github.com/arjun-natarajan99', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\n'], 'url_profile': 'https://github.com/sarojadevi', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '801 contributions\n        in the last year', 'description': ['Analytic Bayesian Linear Regression\nimplementation with hyperpriors: https://github.com/asherbender/bayesian-linear-model\nlog evidence discussion: http://www.utstat.utoronto.ca/~radford/sta414.S11/week4a.pdf\n'], 'url_profile': 'https://github.com/glandfried', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Airfoil_Noise_Regression_Analysis\n'], 'url_profile': 'https://github.com/Joheun-Kang', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': [""CarPrice_LinearRegression\nLinear Regression, Prediction Model\nIn this project, we have build a multiple linear regression model for the prediction of car prices.\nProblem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Preparation\nThere is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. You need to consider only company name as the independent variable for model building.\nModel Evaluation:\nWhen you're done with model building and residual analysis, and have made predictions on the test set, just make sure you use the following two lines of code to calculate the R-squared score on the test set.\n""], 'url_profile': 'https://github.com/TanmayaKharyal', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'Trinidad and Tobago', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Regression and Classification Tutorial\n'], 'url_profile': 'https://github.com/RafaelArturo', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/matthewstokeley', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 19, 2020', 'Shell', 'Updated Apr 16, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 20, 2020', 'Python', 'Updated Jul 27, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Feb 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': ['lr\nsimple linear regression test\n\nthis is just to demonstrate sklearn linear_model  and  gradient descent method get similar result.\nTODO\n\nadd jax demo\n\n'], 'url_profile': 'https://github.com/ggsonic', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Startup-Data-Set\nMultiple Linear Regression\n'], 'url_profile': 'https://github.com/omkarholmes', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Jamshedpur', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mukherjeetejas', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['LinearRegression\nCode For Linear Regression\n'], 'url_profile': 'https://github.com/Pruthwik', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['SVR\nSupport Vector Regression\n'], 'url_profile': 'https://github.com/dhirajk100', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Machine-Learning\nSingle Variable Linear Regression\n'], 'url_profile': 'https://github.com/Akshay-Kumar-Mittal', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guyas', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Logistic Regression Classifier built from scratch\nThese project is about logistic regression.\nIt is a great example of machine learning for beginners and everything is implemented from scratch.\nA model is trained to predict whether an email is ""spam"" or ""ham"".\nThe email data are inside the email_list file.\nThe dataset used in this project is called ""pu_corpora_public"".\n'], 'url_profile': 'https://github.com/kourloskostas', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['linear-regression\nlinear regression in R\n'], 'url_profile': 'https://github.com/gilbertkibet', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Multivariate-Regression\nMultivariate-linear-regression\n'], 'url_profile': 'https://github.com/LeboSeribe', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Updated May 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}"
"{'location': 'SA', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['Heart-Disease -Prediction\nLogistic regression classifier\nHeart\tdisease\tdescribes\ta\trange\tof\tconditions\tthat\taffect\theart.\tDiseases\tunder\tthe\theart\tdisease\tumbrella\tinclude\tblood\tvessel\tdiseases,\tsuch\tas\tcoronary\tartery\tdisease,\theart\trhythm\tproblems\t(arrhythmias)\tand\theart\tdefects\t(congenital\theart\tdefects),\tamong\tothers.\tHeart\tdisease\tis\tone\tof\tthe\tbiggest\tcause\tfor\tmorbidity\tand\tmortality\tamong\tthe\tpopulation\tof\tthe\tworld.\tPrediction\tof\tcardiovascular\tdisease\tis\tregarded\tas\tone\tof\tthe\tmost\timportant\tsubject\tin\tthe\tsection\tof\tclinical\tdata\tanalysis.\nPipeline\n1\\ partioning (75% training 25% teting) \n2\\ Normalization \n3\\ calculate sigmoid\n4\\ compute logistic recreation Cost\n5\\ learn logistic recreation Theta :\nto get the best theta parameters for\tthe\tgiven\tlambda\tvalue\n6\\ train logistic recreation Model:\napplies\t10-fold\tcross\tvalidation\tto\tchoose\tthe\tbest\tlambda\tvalue\n7\\  predict the Class.\nTest Performance\ncalculates accuracy,\trecall,\tprecision,\tand\tf-score.\n'], 'url_profile': 'https://github.com/HebahAlshamlan', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dvyushin', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'delhi', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ajaysinraj', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['AppEx 09: Multiple Linear Regression\nTask 1\nFit a multiple linear regression model with the following main effects: lrgfont, Height_in, Width_in, and relig.\nInterpret each of the estimated model coefficients.\nTask 2\nFit a multiple linear regression model with main effects for Height_in and lrgfont, as well as their interaction.\nWhat is the model equation for paintings where the dealer devotes an additional paragraph? How about for paintings where the dealer does NOT devote an additional paragraph?\n'], 'url_profile': 'https://github.com/sta199-sp20-002', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yevonnaelandrew', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '419 contributions\n        in the last year', 'description': ['MATH533_Regression_and_ANOVA\nRegression and analysis of variance\n'], 'url_profile': 'https://github.com/JairParra', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""Regression_House-price-prediction\nHouse Prices: Advanced Regression Techniques\nData fields\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale\n\n""], 'url_profile': 'https://github.com/Erandani', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/stuti2886', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iamarpan', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Predicting Medical Cost using Regression Algorithms\n'], 'url_profile': 'https://github.com/alyssamelody', 'info_list': ['1', 'MATLAB', 'Updated Jul 18, 2020', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 26, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/priyankagarwal1', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manindra21', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/trispangrib', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '340 contributions\n        in the last year', 'description': ['truckvotes\nUNDER CONSTRUCTION\nExamples of generalized linear regression in a Jupyter notebook.\n\nBased on an original project from August 2015.\ndata\n2008\n\nFEC votes per state\nUS Census population estimates per state\nUS DoT truck registrations per state\n\n2012\n\nFEC votes per state\nUS Census population estimates per state\nUS DoT truck registrations per state\n\n'], 'url_profile': 'https://github.com/samkennerly', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'Bandung, Indonesia', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nSimple Linear Regression using Python.\nImplement linear regression using the sklearn library and visualized the result.\nDataset can be found in ""Salary_Data.csv"".\n'], 'url_profile': 'https://github.com/angelamarpaung99', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cgibbons87', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tomimester', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'Brunswick', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['RegressionScript\n'], 'url_profile': 'https://github.com/florianbeyer', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'Goa', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sahamath', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lucaspada894', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 20, 2020', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Multiple-regression-using-random-forest\n'], 'url_profile': 'https://github.com/biswajitdikun', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['Machine-Learning-with-Spark\nImplementation of  ML Algorithms in Spark using MLlib\n\nLinear regression to predit sales value\nLogistic regression to classify cryotherapy data\nK-mean clustering data (Cluster customer base on their purchased products)\n\n'], 'url_profile': 'https://github.com/akshaykokane', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'New York,NY', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': [""Stock Prediction with Linear Regression\nUsed Features\n\nAdj. Close: Close price of the day\nHL_PCT: High to low percentage change (High - Low) / Low\nPCT_change: Daily percentage change (Close - Open) / Open\nAdj. Volume: Daily traded volume\n\nCleaning\n\nAll the NaNs are filled with -9999 to count as an outlier by the sklearn LinearRegression object's fit function.\n\nAdjustable Parameter\n\nTicker is currently set for GOOGL, you can pick another as you please.\nYou can determine how much of historical data you want to incorparate in your predictions by changing the forecast_out variable\n\n""], 'url_profile': 'https://github.com/tunino91', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '627 contributions\n        in the last year', 'description': ['dotstatistics\n\n\nDescription\nDotStatistics is an open online multi-user platform for statistical analysis. Created using .NET Core technologies.\nImplemented alogrithms\n\nMatrix operations\n\nBasic arithmetic operations\nConversion to built-in types\n\n\nLinear Algebra\n\nGauss-Jordan method\n\n\nOptimization\n\nLeast squares linear regression\n\n\n\nTechnologies stack\n\nASP.NET CORE\nANGULAR\n\n'], 'url_profile': 'https://github.com/antonAce', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Cape Town, South Africa', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['ML-Handbook\nDescriptive statistics\nData exploration\nHistograms\nFeature selection\nDimensionality reduction\nClustering\nLinear Regression\nRandom Forest\n'], 'url_profile': 'https://github.com/ChadGouws2307', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Kolkata ', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Housing-price-prediction-using-Regularised-linear-regression\nconfig\n\n\nupload the ipynb file to Google colab\n\n\nupload the csv file to Google colab\n\n\nrun all cells\n\n\ngraph of error vs lambda in gradient descent\n\ngraph of error vs lambda in Normal equation\n\n'], 'url_profile': 'https://github.com/SouravG', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['rentalhouse\nLinear Regression of house rental price\n'], 'url_profile': 'https://github.com/ykishimotoy', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DuoEmery124', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Bay Area ', 'stats_list': [], 'contributions': '3,792 contributions\n        in the last year', 'description': ['regression-analysis-with-time-series-data\nRegression analysis with time series data\n\nPublished in Political Research Quarterly: https://journals.sagepub.com/doi/10.1177/1065912920983456\n\nMotivation\nThe importance of the War on Poverty on the community-building efforts among Asian Americans and Latinos has been mentioned by prior research in sociology, history, and ethnic studies. However, the evidence for this importance has mostly come from anecdotal examples. The present study provides the first systematic evidence for the influence of the War on Poverty programs on Asian American and Latino civic mobilization in the 1960s and 1970s.\nDatasets\nUpdate: I scanned and digitized all of the newsletters issued by the National Council of La Raza from 1973 to 1981.\nI have collected a wide range of original data for this project.\n\n\nOrganizational data: An original dataset traces the founding of Asian American and Latino CBOs over the last century. The dataset includes about 299 Asian American and 519 Latino CBOs. Each observation includes the organization\'s title, the year of founding, the physical address, and whether the CBO’s purpose was social (i.e., charitable, educational, cultural or some other non-political function), advocacy (engaged in political or legal lobbying), or hybrid (active in both types of work). I made the dataset publicly available at Harvard Dataverse and created a dashbaord for easy exploration of the data.\n\n\nAdministrative data:\n\n\n\nFederal budget statistics: The federal budget data come from the Budget of the U.S. Government, Fiscal Year 2017 Historical Tables.\nPopulation statistics: Asian American and Latino population data come from the U.S. census.\nParty control: Party control of the presidency and Congress data come from Russell D. Renka\'s website\n\n\n\nFoundation data: Ford Foundation information on grants comes from the catalog of the Rockefeller Archive Center.\n\n\nCommunity newspaper data: The transcribed records of The International Examiner (1976 - 1987) are provided by the Ethnic Newswatch database.\n\n\nWorkflow\n\nData cleaning, wrangling, and merging\nDescriptive data analysis\nStatistical modeling of time series data (interrupted time series design)\n\n1. Data cleaning, wrangling, and merging [Code]\nThe original organization dataset that I collected contains a founding year variable, but it is not time series data. Time series data, by definition, has a series of temporally varying observations. The founding year variable could miss some years if no organizations were founded in those years. For this reason, filling in these years is important. The following code is my custom function to perform that task.\n2. Descriptive data analysis [Code]\n\nFigure 1. Organizational trend\nUpdate: Used a simpler descriptive plot for easy interpretation.\nFigure 1 shows that, when the organizational founding trend is measured density (number of organizations per 1 milliion population), the founding rate of community-based organizations in both Asian American and Latino communities increased before the budget cut and decreased after the budget cut. We could not find a similar trend in advocacy or hybrid organizations (organizations active both in advocacy and service delivery). This evidence is consistent with the theory that CBOs are service-oriented, and thus most dependent, and thus more dependent on outside financial support than the other two types of organizations. However, the evidence is only suggestive as the change could also have been influenced by other factors. Also, the data includes noise as well as signals.\n3. Statistical modeling of time series data [Code]\n3.1. Checking on independent and dependent variables\n\nFigure 2. Budget trend\n\nFigure 3. Dictionary-based methods analysis\n\nUpdate: I scanned, digitized, and analyzed all of the newsltters issued by the National Council of La Raza from 1973 to 1981.\n\nBefore moving into a more serious statistical analysis, I checked some assumptions I made about the research design. Figure 2 shows that the percentage of the federal budget for education, employment, and social service plunged after the Reagan cuts. (This amount shows a slight decrease during the Carter administration, as he was forced to reduce social programs due to budget constraints.) Reagan made these reductions more dramatic and consistent throughout the 1980s. A more specific analysis of the budget change shows that programs empowering minority communities, such as the Comprehensive Employment Training Act, were critically hurt by the budget cuts. I did not include a more detailed analysis of these policies owing to spatial constraints. This evidence is important for seeing the budget cut as a major intervention.\nFigure 3 shows how minority communities reacted to the budget crisis. The International Examiner (IE), a community newspaper circulating among Asian American activists in Seattle, did not mention Reagan until 1981. I created a custom dictionary and analyzed the frequency of budget-related terms that appeared in Reagan-related articles from this newspaper source. The figure shows that when the newspaper first mentioned Reagan, the budget crisis received serious attention. This evidence is crucial for seeing the year 1981 as a critical juncture for Asian American and Latino community organizers.\n\nFigure 4. Outlier detection\nFinally, I checked the presence of outliers (an observation with a large residual). Outliers are particularly influential in small data analysis and can change the results. I construed a multivariate regression model and detected any DV values that are unusual given the predicted values of the model using Cook\'s distance (Cook\'s D). I then removed these outliers and imputed new values using the k-nearest neighbors (KNN) algorithm.\n3.2. Interrupted time-series design\nDid the Reagan cuts alter the founding rate of Asian American and Latino CBOs and advocacy organizations? This question is difficult to answer because other factors could have influenced the outcome. To account for these other factors, I built a multivariate statistical model. However, we don\'t know which model would fit the data best. For that reason, I constructed various statistical models, fitted each of them to the data, and compared their model fit using the Akaike Information Criterion (AIC). The ordinary least squares (OLS) regression model is a base model. OLS with a logged dependent variable fits better for skewed data. The Poisson model assumes that the residuals follow a Poisson, not a normal, distribution. It also models the natural log of the response variable. The negative binomial model is similar to the Poisson model except it does not assume that the conditional means and conditional variances are equal.\n\nFigure 5. Interrupted time series design analysis\nFigure 5 illustrates how these different models fitted to the data. The blue plotted line indicates predicted values. The grey ribbons, around the line plot, display two standard errors, which are approximate to 95% confidence intervals. The impacts of the intervention could be detected in two ways in an interrupted time series design: level and slope. The level of the predicted values is almost identical between the pre- and post-intervention periods. In contrast, the slope change is detected in all four models.\nI then checked the performances of these four models using AIC. The AIC score measures the difference between model accuracy and complexity. A lower AIC score indicates a less overfitting model. However, checking AIC scores of these models at one point might not be sufficient for a comprehensive test. Another concern is that these models perform differently depending on the period under investigation. Some models may fit for a short-term period and others do better for a long term. To address this concern, I created a ""for loop"" function and checked how the AIC scores of these four models varied as I extended the data from the year 1970 to 2017. Figure 6 indicates that the OLS model with logged dependent variable consistently outperformed the OLS, Poisson, and negative binomial models.\n# Initilization vars\nlm.AIC <-NA\nlm_log.AIC <-NA\npoisson.AIC <- NA\nnb.AIC <- NA\nyear <-NA\n\n# For loop\nfor (i in c(0:38)){\n\n# Models\nlm.out <- lm(Freq ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i))\n\nlm_log.out <- lm(log(Freq) ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i))\n\npoisson.out <- glm(Freq ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i), family = ""poisson"")\n\nnb.out <- glm.nb(Freq ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i))\n\n# AIC scores of the models\nlm.AIC[i] <- AIC(lm.out)\nlm_log.AIC[i] <- AIC(lm_log.out)\npoisson.AIC[i] <- AIC(poisson.out)\nnb.AIC[i] <- AIC(nb.out)\n\n# Year var\nyear[i] <- 1970 + i\n}\n\n\n*Figure 6. Model performance comparisons\nUpdate: Limited the data for the bootstrapping analysis up to year 2004 to avoid too much extrapolation. The breakpoint was determined by the strucchange::breakpoints() function`. This decision applies to all the model outcomes presented below.\nFrom this point on, I used the OLS with logged DV for the analysis and limited the data to CBOs. We saw the slope change. Given the research question, it is important to know to what extent the federal funding contributed to the slope change as opposed to other factors. To do so, I examined how the coefficients of federal funding changed as we extended the data from 1970 to 2004. For instance, the 1970 data is the subset of the original data which includes observations up to the year 1970. I created a point plot using a for loop. The point plot in Figure 7 shows that the coefficients of the federal funding were positive up to the cutpoint. Then they became almost zero after the cutpoint. An opposite trend was found from the changes in the coefficients of population growth. They were either negative or zero before the cutpoint. Then they became positive in the 1980s and then again became almost zero or negative afterward.\nTo measure the certainty of the coefficient change, I added confidence intervals using bootstrapping. Bootstrapping is resampling with replacement. For each regression model at a time point, I resampled the data and ran the same analysis for 1,000 times, and created confidence intervals based on the sampling variability of regression coefficients. This information shows that the coefficient change around the cutpoint is statistically significant.\n# Initilization vars\nfed <- NA\npop <- NA\nyear <- NA\n\n# For loop\nfor (i in c(0:38)){\n\n# Model\nmodel <- lm(log(Freq) ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i))\n\n# Save iterations\nfed[i] <- model$coefficients[3] %>% as.numeric()\npop[i] <- model$coefficients[4] %>% as.numeric()\nyear[i] <- 1970 + i}\n\n\nFigure 7. Changes in coefficients with bootstrapped CIs\n3.3. Correcting standard errors\nSo far, we have examined how the reduced treatment (reduced budget) influenced the DV. In this subsection, I focus on the first half of the previous analysis: the relationship between the increasing budget and the increasing organizational founding rate. We saw that they are correlated, but is the relationship statistically significant? In Figure 7, the regression coefficients are all over zero before the intervention and the confidence intervals are wide. To check robustness, I ran the model with the subset of the data that included observations up to the year 1980. Then, I employed various methods to calculate correct standard errors.\n\nHeteroskedasticity: I ran the studentized Breusch-Pagan test to check the presence of heteroskedasticity. This is a problem because OLS assumes constant variance (homoscedasticity). Otherwise, p-values become unstable. The null hypothesis of the test is that the variance of residuals is constant (homoscedasticity). The p-value of the test result is 0.04. Thus, we reject the null hypothesis.\nAutocorrelation: The autocorrelation test (ACF) (Figure 8) shows the correlation between the time series and its lagged values. It appears that none of the correlation coefficients are statistically significant. I also ran the Durbin-Watson test, which checks the autocorrelation of residuals. The test fails to reject the null hypothesis. Therefore, we worry about heteroskedasticity but not autocorrelation.\n\n\nFigure 8. Autocorrelation test\n\nTable 1. Standard errors corrected for heteroskedasticity and outliers\nIn Table 1, the first model is a simple OLS. The second model is also an OLS but it uses the robust Newey-West variance estimator to account for heteroskedasticity. In this case, we expect the standard errors to be different from the base model. The third model is based on a robust regression model to account for unusual observations. In this case, we expect that both regression coefficients and standard errors could be different from the base model. As expected, they are different but only marginally. The statistical significance of federal funding (p value < 0.01) does not change across the three models. In model 3, the coefficient increased by 0.002. Note that the dependent variable is logged. Thus, the regression coefficient of 0.3 is close to 2. In substantive terms, we can interpret a one percent increase in federal funding to be associated with the increase of the two Asian American and Latino CBOs in the 1960s and 1970s.\n3.4. Sensitivity analysis\nHowever, we cannot take the regression coefficients at their face value. There could still be confounders. If a model is misspecified, then the regression coefficient is not an unbiased estimator. I ran a sensitivity test using the sensemakr package in R. The result shows that ""unobserved confounders (orthogonal to covariates) that do not explain more than 34.65% of the residual variance of both the treatment and the outcome are not strong enough to reduce the absolute value of the effect size by 100%.""\nI suspect that philanthropic giving could be one of those unobserved confounders. It is very difficult to find systematic data on philanthropic giving, especially in a historical context. I collected the Ford Foundation grant data and found that Ford allocated grants selectively. Only a few Asian American and Latino CBOs received support from the Ford Foundation. Yet, as this evidence is only partial information of philanthropic giving, I suggest that one should take the above regression coefficient with a grain of salt.\n'], 'url_profile': 'https://github.com/jaeyk', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Brampton , Canada', 'stats_list': [], 'contributions': '1,196 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mohd-ahsan-mirza', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Java', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 22, 2020', 'C#', 'MIT license', 'Updated Oct 30, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 26, 2020', '1', 'JavaScript', 'Updated Dec 29, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sham10', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'Farmington, CT', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/thezross', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jiinson95', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kamranrahnama', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Predicting Wine Quality using Regression\n'], 'url_profile': 'https://github.com/alyssamelody', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahulshukla31', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['Simple Linear Regression 1 - Mathematical Theory\nLinear regression is a statistical method for estimating how a variable Y can depend linearly on other variables X1, X2, ..., Xn. We will explain what ""linear"" means in the next section.\nWhen our model consists of a dependent variable Y, and a single independent variable X, we say that the linear regression is simple. This article will briefly discuss the mathmatics of simple linear regression, before exploring two real world applications.\nMathematical Model\nConsider the two variables Y and X mentioned above. For example, Y could be the annual income of a working adult, and X could be the years of education received by that adult. Then, the ith adult provides a data point consisting of the pair (xi,yi).\nStrictly speaking, if we want to use simple linear regression to predict or estimate Y from X, we would need to believe that the yi in the data points are generated according to this equation.\n\nyi = a + bxi + ei\n\nThe ei are random error terms that represent noise, measurement errors, as well as other variables affecting the value of yi. Ideally, these error terms should be independent and identically distributed (IID) from the same normal distribution, with a mean of zero.\nAlthough these ideal conditions on the random error terms are not strictly necessary, they are required for a powerful result known as the ""Guass-Markov Theorem"", which we will briefly touch on later. In practice, the conditions specified above are usually not completely satisfied, but linear regression is often still used as a rough approximation or estimate.\nFitting Model to Data\nSuppose our data consists of points (x1,y1), (x2/sub>,y2), ..., (xm,ym). We can try using these points to estimate the true parameters a and b. This is known as ""fitting"" a linear model to the data.\nTechnically, Y = a + bX is not a linear function. Linear functions have to pass through the origin, and so the intercept term a must be zero. Mathematically, if the intercept term a is not zero, then Y is an affine function of X.\nHowever, we will stick to the established convention, and refer to Y = a + bX as linear.\nThe Method of Least Squares\nWe work backwards to fit a linear model to the data. Let\'s use a ""^"" superscript to denote variables that are estimated. First, we start with a linear model with estimated coefficients a^ and b^. Then, given any single data point xi, we can estimate the corresponding yi with the equation below.\n\ny^i = a^ + b^ xi\n\nThere is a ""residual"" error e^i = yi - y^i, between our estimated/fitted value and the true/actual value. By squaring this residual error, and summ over all data points i, we get the sum of squared errors.\n\nΣi (yi - y^i)2\n\nThe method of least square refers to picking coefficient a^ and b^ such that this sum of squared errors is minimized. This method was made famous by Carl Friedrich Gauss\'s calculation of celestial orbits. For more information on its history, check out this Wikipedia link.\nLeast Squares Solutions\nThe two usual ways to get the least squares estimates are either to solve for them explicitly or use a numerical method that searches for a usually approximate solution. These numerical methods try different values of the coefficients, using clues like the gradient/derivatives, to lower the residual error at each step.\nIn the case of simple linear regression, we can easily get the least squares solutions with simple calculus. These are known as the normal equations. If there are more X variables, we can still get an explicit solution using matrix calculus. However, it is usually faster to use a numerical method.\nGuass-Markov Theorem\nThe famous Guass-Markov Theorem says that if the random error terms ei satisfy all the conditions/assumptions we mentioned, and that Y truly depend on X linearly, then the method of least squares will produce estimates of the coefficients a and b that are ""BLUE"". The acronym BLUE stands for ""Best Linear Unbiased Estimator"".\nWe will elaborate on the mathematical meaning of BLUE, as well as the proof of the Guass-Markov Theorem, in a later article on multivariate regression. For now, a simplified explanation is that our estimator for each of the cofficient is constructed from a linear combination of the Xi variables in our data (Linear), and will agree with the real coefficient as the number of data points becomes large (Unbiased). On top of that, our estimators are ""best"" among all linear and unbiased estimators, because they will have the lowest variance among all such estimators.\nCoefficient of Determination\nHow good is our linear model is at estimating or ""predicting"" points in our data? This is also known as the ""goodness of fit"". One way to determine goodness of fit is to look at the coefficient of determination. It is usually denoted r2 or R2. This is sometimes call multiple correlation. For simplicity, we will just stick to the coefficient of determination terminology.\nTo explain what the coefficient of determination is, we first have to state some definitions. Let ymean be the mean value of all yi values in our data.\n\n  Total Sum Of Squares  = Σi (yi - ymean)2\n\n\n  Explained Sum Of Squares  = Σi y^i - ymean)2\n\n\n  Residual Sum of Squares = Σi (yi - y^i)2\n\nIt can be shown that: Total Sum of Squares  =  Explained Sum of Squares + Residual Sum of Squares. This can be interpreted as: the squared variation of yi values from their mean consists of a part that is explained by our model, and a residual error that our model failed to capture.\nThen, the coefficient of determination R2 =  (Explained Sum of Squares)/(Total Sum of Squares).\nThis is a number between 0 and 1 that gives us a rough idea of how good our fit is. For example, if our model is a perfect fit with no errors, then the residuals will all be zero. Then, we will have (Explained Sum of Squares)/(Total Sum of Squares).\nOne thing is to note is that in order to get R2 = 0, our predicted y^i value will need to always be equal to ymean.\nResidual Analysis\nAnother way to judge the goodness of fit of our model is to look at its residual errors. As we mentioned before, there are strong conditions on the true error terms, in the true underlying model. In practice, we will almost surely deviate from these conditions.\nWe can ""eyeball"" how severe the deviations are by plotting the residuals. If our model is a good fit, and the conditions are satisfied, we should not see any patterns in the residual errors e^i for each data point i. They should be approximately normally distributed with mean zero.\nPart Two\nPlease proceed to part 2 of this article for an application of simple linear regression to a set of real world data: https://github.com/tommyzakhoo/simple_regression2\n'], 'url_profile': 'https://github.com/tommyzakhoo', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Predicting UK Employment using Polynomial Regression\n'], 'url_profile': 'https://github.com/alyssamelody', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Chaitanya-Kumar\nLinear Regression in jupyter for Data science\n'], 'url_profile': 'https://github.com/KALACHAITANYAKUAMR', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}","{'location': 'Roma', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['house_price_prediciton: Kaggle competition\nHouse price regression using lasso method\nData manipulation:\n\nRemoved variables with more than 90% of Nas\nImputation of Nas with 0  or None (for categorical) in the case it stays for the absence of the feature in the corresponding observation\nImputation of NAs with most frequent terms (mode function) for some features\nImputation Lot frontage Nas with median grouped by Neighborhood\nRemoved variable Utilities\nlog trasform of SalePrice and other skewed numeric features\nuse dummyVars caret libray to encode categorical variables\n\nModel selection:\n-Lasso with cross-validation (10 kfold) glmnet library\nKaggle score: 0.12229\nData manipulation\n\nperform a  linear regression over all data to detect outliers with function OutliersTest()\nperform a  linear regression over the important feature GrLivArea\nModel selection\nLasso with cross-validation (10 kfold)  glmnet library\n\nKaggle score: 0.11777\nData manipulation and Features engeneering\n\nnew feature: TotalFeet (TotalBsmtSF+X1stFlrSF+X2ndFlrSF)\nnew feature:  Grg binary variable to represent presence/absence\nnew features:  squared-root of the age of the garage and of the house\ntransform   numeric feature “OverallQual”  in to categorical\n\nModel selection\nStacked Lasso cross-validation (using function “replicate” (20 times) over cv.glmnet), to extract different lambda due to random partitions of data in cross-validation (like suggested by the library).  With  “n-lambda” we make a prediction and a new lasso regression to have the final price.\nKaggle score: 0.11411\n'], 'url_profile': 'https://github.com/bikocat', 'info_list': ['Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 19, 2020', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hallepuchalski', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BIGBOSS-FOX', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'Cairo, Egypt', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': [""\nI was enrolled in Andrew Ng's course Machine Learning and this is one of the assignments that was required in Matlab.\nI decided to do it also in Python for reference.\nProvided the dataset, i run some visuals and predict new instance.\n\n""], 'url_profile': 'https://github.com/Shymaa-Alsayed', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'Raleigh', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Predict-Housing-Prices\nProject to predict the housing prices using the supervised learning technique - learn Linear Regression\n'], 'url_profile': 'https://github.com/Arunprakash1990', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['ML_Kaggle_Competition_House_Prices\nThe goal of this project was to compete in an introductory Kaggle competition to predict the price of houses using the Ames Housing data set.\nThe Kaggle Competition can be found here:\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/bbotzheim', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Predicting House Prices using Simple Linear Regression\n'], 'url_profile': 'https://github.com/alyssamelody', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jonsh96', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anazamarripaz', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '1,543 contributions\n        in the last year', 'description': ['voila-asv\nasv-powered performance regression for Voila.\nIn order to benchmark all commits since e.g. the first release:\nasv run 0.0.1..master --quick --show-stderr\n\n'], 'url_profile': 'https://github.com/davidbrochart', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Heroku-Demo\n'], 'url_profile': 'https://github.com/rblzaman', 'info_list': ['Python', 'Updated Feb 25, 2020', 'HTML', 'Updated Feb 22, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Python', 'Updated May 13, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 23, 2020']}"
"{'location': 'France', 'stats_list': [], 'contributions': '1,543 contributions\n        in the last year', 'description': ['voila-asv\nasv-powered performance regression for Voila.\nIn order to benchmark all commits since e.g. the first release:\nasv run 0.0.1..master --quick --show-stderr\n\n'], 'url_profile': 'https://github.com/davidbrochart', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/subitchaselvasekar', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'Victoria, BC', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Fashion-MNIST-LR\nLogistic Regression for Fashion-MNIST dataset\n'], 'url_profile': 'https://github.com/kgautam14', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '240 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/misrapk', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['predicting student research logistic regression\nlearning to use logistic regression in sklearn\n'], 'url_profile': 'https://github.com/rishisankhla', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Predicting Advertising Sales using Multiple Linear Regression\n'], 'url_profile': 'https://github.com/alyssamelody', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Prediction of Airfares using various regression techniques\nManoj Bhandari\n\nLoad the required packages to workspace\nif(!require(""pacman"")) install.packages(""pacman"")\npacman::p_load(data.table, forecast, leaps, dplyr, corrplot, tinytex, \n               ggplot2, tidyr, stats)\n\nCreate a correlation table and scatterplots between FARE and the predictors to check what seems to be the best single predictor of FARE.\nlibrary(data.table, corrplot)\nraw.data <- fread(""Airfares.csv"")\nd <- setDF(raw.data[, !1:4])\nnum.ind <- unlist(lapply(d, is.numeric))\nnumeric.data <- d[,num.ind]\nDisplayed below is the correlation table between FARE and other\npredictors\ncorrplot(cor(numeric.data)[10, -10,drop = FALSE], method = ""pie"", bg = ""grey"", \n        title = ""Fares vs Predictors"")\n\nDisplayed below is the scatter plot matrix between various\npredictors\nlibrary(stats)\n\nplot(d$COUPON,d$FARE, xlab = ""COUPON"", ylab = ""FARE"", main = ""COUPON vs FARE"")\nabline(lm(d$FARE~d$COUPON))\n\nplot(d$NEW,d$FARE, xlab = ""NEW"", ylab = ""FARE"", main = ""NEW vs FARE"")\nabline(lm(d$FARE~d$NEW))\n\nplot(d$HI,d$FARE, xlab = ""HI"", ylab = ""FARE"", main = ""HI vs FARE"")\nabline(lm(d$FARE~d$HI))\n\nplot(d$S_INCOME,d$FARE, xlab = ""S_INCOME"", ylab = ""FARE"", main = ""S_INCOME vs FARE"")\nabline(lm(d$FARE~d$S_INCOME))\n\nplot(d$E_INCOME,d$FARE, xlab = ""E_INCOME"", ylab = ""FARE"", main = ""E_INCOME vs FARE"")\nabline(lm(d$FARE~d$E_INCOME))\n\nplot(d$S_POP,d$FARE, xlab = ""S_POP"", ylab = ""FARE"", main = ""S_POP vs FARE"")\nabline(lm(d$FARE~d$S_POP))\n\nplot(d$E_POP,d$FARE, xlab = ""E_POP"", ylab = ""FARE"", main = ""E_POP vs FARE"")\nabline(lm(d$FARE~d$E_POP))\n\nplot(d$DISTANCE,d$FARE, xlab = ""DISTANCE"", ylab = ""FARE"", main = ""DISTANCE vs FARE"")\nabline(lm(d$FARE~d$DISTANCE))\n\nplot(d$PAX,d$FARE, xlab = ""PAX"", ylab = ""FARE"", main = ""PAX vs FARE"")\nabline(lm(d$FARE~d$PAX))\n\nDISTANCE seems to be the single best predictor because of the\nfollowing reasons:\n\nFrom the correlation table, we observe that the correltion between\nFARE and DISTANCE has a strong positive relationship when compared\nto other predictors.\nEven logically, as the DISTANCE increases, the FARE usually\nincreases to adjust the fuel price and other\nfactors.\n\n\n\nExplore the categorical predictors by computing the percentage of flights in each category and create a pivot table with the average fare in each category to check which categorical predictor seems best for predicting FARE.\nCalculate the average mean of the FARE from the dataset to be compared\nwith various categorical predictors\nlibrary(dplyr,tidyr)\navg <- mean(d$FARE)\ncount <- nrow(d)\nPivot Table of SW vs FARE\npivot1 <- d %>%\nselect(SW,FARE) %>%\ngroup_by(SW) %>%\nsummarise(Percentage_Of_Flights = (length(SW)/count) * 100,\nAverage_Fares_SW = mean(FARE), \nVariation_From_Average = mean(FARE) - avg)\nknitr::kable(pivot1, caption = ""SW vs FARE"")\n\n\n\nSW\nPercentage_Of_Flights\nAverage_Fares_SW\nVariation_From_Average\n\n\n\n\nNo\n69.59248\n188.18279\n27.30612\n\n\nYes\n30.40752\n98.38227\n-62.49441\n\n\n\nSW vs FARE\nPivot Table of VACATION vs FARE\npivot2 <- d %>%\n  select(VACATION, FARE) %>%\n  group_by(VACATION) %>%\n  summarise(Percentage_Of_Flights = (length(VACATION)/count) * 100,\n            Average_Fares_VACATION = mean(FARE),\n            Variation_From_Average = mean(FARE) - avg)\nknitr::kable(pivot2, caption = ""VACATION vs FARE"")\n\n\n\nVACATION\nPercentage_Of_Flights\nAverage_Fares_VACATION\nVariation_From_Average\n\n\n\n\nNo\n73.35423\n173.5525\n12.67582\n\n\nYes\n26.64577\n125.9809\n-34.89579\n\n\n\nVACATION vs FARE\nPivot Table of GATE vs FARE\npivot3 <- d %>%\n  select(GATE, FARE) %>%\n  group_by(GATE) %>%\n  summarise(Percentage_Of_Flights = (length(GATE)/count) * 100,\n            Average_Fares_GATE = mean(FARE),\n            Variation_From_Average = mean(FARE) - avg)\nknitr::kable(pivot3, caption = ""GATE vs FARE"")\n\n\n\nGATE\nPercentage_Of_Flights\nAverage_Fares_GATE\nVariation_From_Average\n\n\n\n\nConstrained\n19.43574\n193.129\n32.252355\n\n\nFree\n80.56426\n153.096\n-7.780724\n\n\n\nGATE vs FARE\nPivot Table of SLOT vs FARE\npivot4 <- d %>%\n  select(SLOT, FARE) %>%\n  group_by(SLOT) %>%\n  summarise(Percentage_Of_Flights = (length(SLOT)/count) * 100,\n            Average_Fares_SLOT = mean(FARE),\n            Variation_From_Average = mean(FARE) - avg)\nknitr::kable(pivot4, caption = ""SLOT vs FARE"")\n\n\n\nSLOT\nPercentage_Of_Flights\nAverage_Fares_SLOT\nVariation_From_Average\n\n\n\n\nControlled\n28.52665\n186.0594\n25.18272\n\n\nFree\n71.47335\n150.8257\n-10.05100\n\n\n\nSLOT vs FARE\n**The avaerage FARE of the data set is ** From all the pivot tables,\nwe observe the following:\n\nWhen SW = Yes, i.e., when Southwest airlines serves the route, we\ncan see a great variation in the FARE with respect to average FARE.\nThe average FARES decreases significantly when Southwest serves that\nroute.\nWhen SW = No, i.e., when Sothwest airlines does not serve that\nroute, the FARES are usually higher than the usual average FARE\n\nWe can infer that operation of Southwest airlines in a particular\nroute serves in predicting the FARES.\n\n\nSplitting 80% of data into training dataset and 20% of remaining data\ninto test/validation dataset\nlibrary(stats)\nset.seed(42)\nsample <- sample.int(n = nrow(d), size = round(.80*nrow(d)))\ntrain <- d[sample, ]\ntest  <- d[-sample, ]\nRunning a linear regression on the training dataset and printing the\nsummary of Linear Regression\nlin.model <- lm(FARE ~., data = train)\noptions(scipen = 999)\nsummary(lin.model)\n## \n## Call:\n## lm(formula = FARE ~ ., data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -99.282 -23.384  -2.476  22.156 106.501 \n## \n## Coefficients:\n##                   Estimate     Std. Error t value             Pr(>|t|)    \n## (Intercept)  13.8781441835  30.7076946550   0.452             0.651507    \n## COUPON       11.6744988371  13.6949175687   0.852             0.394365    \n## NEW          -2.2468005921   2.0827213457  -1.079             0.281210    \n## VACATIONYes -37.8385127965   3.9788129464  -9.510 < 0.0000000000000002 ***\n## SWYes       -38.9566477546   4.2526101838  -9.161 < 0.0000000000000002 ***\n## HI            0.0085414832   0.0010936608   7.810   0.0000000000000343 ***\n## S_INCOME      0.0006160967   0.0005709965   1.079             0.281119    \n## E_INCOME      0.0015472928   0.0004141497   3.736             0.000209 ***\n## S_POP         0.0000040087   0.0000007411   5.409   0.0000000987167149 ***\n## E_POP         0.0000039572   0.0000008329   4.751   0.0000026562530825 ***\n## SLOTFree    -16.4322948237   4.3647846605  -3.765             0.000187 ***\n## GATEFree    -21.1634823059   4.4093579183  -4.800   0.0000021065804690 ***\n## DISTANCE      0.0715673994   0.0039223121  18.246 < 0.0000000000000002 ***\n## PAX          -0.0007340587   0.0001662490  -4.415   0.0000123830100844 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 35.41 on 496 degrees of freedom\n## Multiple R-squared:  0.7817, Adjusted R-squared:  0.7759 \n## F-statistic: 136.6 on 13 and 496 DF,  p-value: < 0.00000000000000022\n\n\n\nUsing leaps package, run stepwise regression to reduce the number of predictors.\nfare.stepwise.reg <- step(lin.model, direction = ""both"")\n## Start:  AIC=3652.06\n## FARE ~ COUPON + NEW + VACATION + SW + HI + S_INCOME + E_INCOME + \n##     S_POP + E_POP + SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - COUPON    1       911  622732 3650.8\n## - NEW       1      1459  623280 3651.3\n## - S_INCOME  1      1460  623281 3651.3\n## <none>                   621821 3652.1\n## - E_INCOME  1     17499  639320 3664.2\n## - SLOT      1     17769  639590 3664.4\n## - PAX       1     24441  646263 3669.7\n## - E_POP     1     28296  650118 3672.8\n## - GATE      1     28881  650702 3673.2\n## - S_POP     1     36680  658501 3679.3\n## - HI        1     76469  698290 3709.2\n## - SW        1    105205  727026 3729.8\n## - VACATION  1    113382  735204 3735.5\n## - DISTANCE  1    417379 1039200 3912.0\n## \n## Step:  AIC=3650.81\n## FARE ~ NEW + VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + \n##     E_POP + SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - S_INCOME  1      1261  623994 3649.8\n## - NEW       1      1678  624410 3650.2\n## <none>                   622732 3650.8\n## + COUPON    1       911  621821 3652.1\n## - E_INCOME  1     17126  639859 3662.6\n## - SLOT      1     18407  641139 3663.7\n## - GATE      1     29285  652018 3672.2\n## - E_POP     1     29484  652217 3672.4\n## - PAX       1     34128  656860 3676.0\n## - S_POP     1     36089  658821 3677.5\n## - HI        1     78594  701326 3709.4\n## - SW        1    107735  730468 3730.2\n## - VACATION  1    114276  737009 3734.7\n## - DISTANCE  1    824468 1447200 4078.9\n## \n## Step:  AIC=3649.84\n## FARE ~ NEW + VACATION + SW + HI + E_INCOME + S_POP + E_POP + \n##     SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - NEW       1      1697  625690 3649.2\n## <none>                   623994 3649.8\n## + S_INCOME  1      1261  622732 3650.8\n## + COUPON    1       713  623281 3651.3\n## - E_INCOME  1     16167  640161 3660.9\n## - SLOT      1     20012  644006 3663.9\n## - E_POP     1     28559  652552 3670.7\n## - GATE      1     29766  653759 3671.6\n## - PAX       1     32869  656863 3674.0\n## - S_POP     1     41722  665715 3680.8\n## - HI        1     79501  703495 3709.0\n## - SW        1    126837  750831 3742.2\n## - VACATION  1    128080  752073 3743.1\n## - DISTANCE  1    826967 1450960 4078.2\n## \n## Step:  AIC=3649.22\n## FARE ~ VACATION + SW + HI + E_INCOME + S_POP + E_POP + SLOT + \n##     GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## <none>                   625690 3649.2\n## + NEW       1      1697  623994 3649.8\n## + S_INCOME  1      1280  624410 3650.2\n## + COUPON    1       907  624783 3650.5\n## - E_INCOME  1     15649  641339 3659.8\n## - SLOT      1     19217  644907 3662.6\n## - E_POP     1     28766  654456 3670.1\n## - GATE      1     29165  654856 3670.5\n## - PAX       1     32706  658396 3673.2\n## - S_POP     1     42648  668338 3680.9\n## - HI        1     78891  704581 3707.8\n## - SW        1    126577  752267 3741.2\n## - VACATION  1    127066  752756 3741.5\n## - DISTANCE  1    825966 1451656 4076.4\n\nsummary(fare.stepwise.reg)\n## \n## Call:\n## lm(formula = FARE ~ VACATION + SW + HI + E_INCOME + S_POP + E_POP + \n##     SLOT + GATE + DISTANCE + PAX, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -99.148 -22.077  -2.028  21.491 107.744 \n## \n## Coefficients:\n##                   Estimate     Std. Error t value             Pr(>|t|)    \n## (Intercept)  42.0764345686  14.7566725244   2.851             0.004534 ** \n## VACATIONYes -38.7574569132   3.8500841929 -10.067 < 0.0000000000000002 ***\n## SWYes       -40.5282166043   4.0337560764 -10.047 < 0.0000000000000002 ***\n## HI            0.0082681499   0.0010423739   7.932   0.0000000000000143 ***\n## E_INCOME      0.0014446281   0.0004089281   3.533             0.000450 ***\n## S_POP         0.0000041850   0.0000007176   5.832   0.0000000098509604 ***\n## E_POP         0.0000037791   0.0000007890   4.790   0.0000022053722984 ***\n## SLOTFree    -16.8515659965   4.3045728245  -3.915             0.000103 ***\n## GATEFree    -21.2165142735   4.3991611435  -4.823   0.0000018824635124 ***\n## DISTANCE      0.0736714582   0.0028704349  25.666 < 0.0000000000000002 ***\n## PAX          -0.0007619280   0.0001491869  -5.107   0.0000004660838631 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 35.41 on 499 degrees of freedom\n## Multiple R-squared:  0.7803, Adjusted R-squared:  0.7759 \n## F-statistic: 177.2 on 10 and 499 DF,  p-value: < 0.00000000000000022\n\nfare.stepwise.pred <- predict(fare.stepwise.reg, test)\nAnalysis of the summary:\nThe observations of the stepwise regressions are as below:\n\nInitial number of predictors = 13\nFinal number of predictors after running Stepwise linear regression\n= 10\nThree of the predictors, NEW, S_INCOME and COUPON were\ndiscarded by the\nregression\n\n\n\nRepeat the process in (4) using exhaustive search instead of stepwise regression to compare the resulting best model to the one you obtained in (4) in terms of the predictors included in the final model.\nRunning the exhaustive search and storing the summary of serahc results\nin variable\nsum\nsearch <- regsubsets(FARE ~ ., data = train, nbest = 1, nvmax = dim(train)[2],\n                     method = ""exhaustive"")\nsum <- summary(search)\nres <- data.frame(\n  Adjusted.Rsquare = which.max(sum$adjr2),\n  CP = which.min(sum$cp),\n  BIC = which.min(sum$bic)\n)\nind <- min(res)\nsum$which\n##    (Intercept) COUPON   NEW VACATIONYes SWYes    HI S_INCOME E_INCOME\n## 1         TRUE  FALSE FALSE       FALSE FALSE FALSE    FALSE    FALSE\n## 2         TRUE  FALSE FALSE       FALSE  TRUE FALSE    FALSE    FALSE\n## 3         TRUE  FALSE FALSE        TRUE  TRUE FALSE    FALSE    FALSE\n## 4         TRUE  FALSE FALSE        TRUE  TRUE  TRUE    FALSE    FALSE\n## 5         TRUE  FALSE FALSE        TRUE  TRUE  TRUE    FALSE    FALSE\n## 6         TRUE  FALSE FALSE        TRUE  TRUE  TRUE    FALSE    FALSE\n## 7         TRUE  FALSE FALSE        TRUE  TRUE  TRUE    FALSE    FALSE\n## 8         TRUE  FALSE FALSE        TRUE  TRUE  TRUE    FALSE     TRUE\n## 9         TRUE  FALSE FALSE        TRUE  TRUE  TRUE    FALSE    FALSE\n## 10        TRUE  FALSE FALSE        TRUE  TRUE  TRUE    FALSE     TRUE\n## 11        TRUE  FALSE  TRUE        TRUE  TRUE  TRUE    FALSE     TRUE\n## 12        TRUE  FALSE  TRUE        TRUE  TRUE  TRUE     TRUE     TRUE\n## 13        TRUE   TRUE  TRUE        TRUE  TRUE  TRUE     TRUE     TRUE\n##    S_POP E_POP SLOTFree GATEFree DISTANCE   PAX\n## 1  FALSE FALSE    FALSE    FALSE     TRUE FALSE\n## 2  FALSE FALSE    FALSE    FALSE     TRUE FALSE\n## 3  FALSE FALSE    FALSE    FALSE     TRUE FALSE\n## 4  FALSE FALSE    FALSE    FALSE     TRUE FALSE\n## 5  FALSE FALSE     TRUE    FALSE     TRUE FALSE\n## 6  FALSE FALSE     TRUE     TRUE     TRUE FALSE\n## 7   TRUE  TRUE    FALSE    FALSE     TRUE  TRUE\n## 8   TRUE  TRUE    FALSE    FALSE     TRUE  TRUE\n## 9   TRUE  TRUE     TRUE     TRUE     TRUE  TRUE\n## 10  TRUE  TRUE     TRUE     TRUE     TRUE  TRUE\n## 11  TRUE  TRUE     TRUE     TRUE     TRUE  TRUE\n## 12  TRUE  TRUE     TRUE     TRUE     TRUE  TRUE\n## 13  TRUE  TRUE     TRUE     TRUE     TRUE  TRUE\n\nsum$rsq\n##  [1] 0.4168069 0.5793894 0.6966218 0.7232479 0.7366555 0.7565835 0.7607777\n##  [8] 0.7674947 0.7748171 0.7803115 0.7809073 0.7813501 0.7816700\n\nsum$adjr2\n##  [1] 0.4156589 0.5777302 0.6948231 0.7210558 0.7340429 0.7536799 0.7574419\n##  [8] 0.7637820 0.7707638 0.7759090 0.7760679 0.7760708 0.7759476\n\nsum$cp\n##  [1] 818.89220 451.53899 187.21153 128.72255 100.26346  56.99127  49.46286\n##  [8]  36.20326  21.56831  11.08605  11.73270  12.72670  14.00000\n\nFrom the values of R-Squared, Adjusted R-Squared and CP, we can see\nthat CP gives us the best model as after 10 variables, other variables\nstart being insignificant. So we will consider the model with 10\nvariables as our best model\nmyres <- as.vector(sum$which[ind,2:ncol(train)])\nbest_variables <- train[,myres]\nbest_model <- cbind(best_variables, FARE = train$FARE)\nblm <- lm(FARE ~. , data = best_model)\nsummary(blm)\n## \n## Call:\n## lm(formula = FARE ~ ., data = best_model)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -99.148 -22.077  -2.028  21.491 107.744 \n## \n## Coefficients:\n##                   Estimate     Std. Error t value             Pr(>|t|)    \n## (Intercept)  42.0764345686  14.7566725244   2.851             0.004534 ** \n## VACATIONYes -38.7574569132   3.8500841929 -10.067 < 0.0000000000000002 ***\n## SWYes       -40.5282166043   4.0337560764 -10.047 < 0.0000000000000002 ***\n## HI            0.0082681499   0.0010423739   7.932   0.0000000000000143 ***\n## E_INCOME      0.0014446281   0.0004089281   3.533             0.000450 ***\n## S_POP         0.0000041850   0.0000007176   5.832   0.0000000098509604 ***\n## E_POP         0.0000037791   0.0000007890   4.790   0.0000022053722984 ***\n## SLOTFree    -16.8515659965   4.3045728245  -3.915             0.000103 ***\n## GATEFree    -21.2165142735   4.3991611435  -4.823   0.0000018824635124 ***\n## DISTANCE      0.0736714582   0.0028704349  25.666 < 0.0000000000000002 ***\n## PAX          -0.0007619280   0.0001491869  -5.107   0.0000004660838631 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 35.41 on 499 degrees of freedom\n## Multiple R-squared:  0.7803, Adjusted R-squared:  0.7759 \n## F-statistic: 177.2 on 10 and 499 DF,  p-value: < 0.00000000000000022\n\nfare.best.pred <- predict(blm, test)\naccuracy(fare.best.pred, test$FARE)\n##               ME    RMSE      MAE       MPE     MAPE\n## Test set 3.06081 36.8617 27.70568 -5.938062 21.62142\n\nWhen we compare the resulting best model of exhaustive search with the\nmodel generated by the stepwise regression, we can observe that the\npredictive accuracy of both the models are identical. ***\n\nCompare the predictive accuracy of both models—stepwise regression and exhaustive search—using measures such as RMSE.\naccuracy(fare.stepwise.pred, test$FARE)\n##               ME    RMSE      MAE       MPE     MAPE\n## Test set 3.06081 36.8617 27.70568 -5.938062 21.62142\n\naccuracy(fare.best.pred, test$FARE)\n##               ME    RMSE      MAE       MPE     MAPE\n## Test set 3.06081 36.8617 27.70568 -5.938062 21.62142\n\nThe predictive accuracy of both Stepwise regression and Exhaustive\nsearch is\nidentical\n\n\nUsing the exhaustive search model, let us predict the average fare on a route with the following characteristics: COUPON = 1.202, NEW = 3, VACATION = No, SW = No, HI = 4442.141, S_INCOME = $28,760, E_INCOME = $27,664, S_POP = 4,557,004, E_POP = 3,195,503, SLOT = Free, GATE = Free, PAX = 12,782, DISTANCE = 1976 miles.\ndata.pred <- data.frame(COUPON = 1.202, NEW = 3, VACATION = \'No\', SW = \'No\', HI = 4442.141,\n                        S_INCOME = 28760, E_INCOME = 27664, S_POP = 4557004, E_POP = 3195503,\n                        SLOT = \'Free\', GATE = \'Free\', PAX = 12782, DISTANCE = 1976)\nprediction <- predict(blm, data.pred)\nThe predicted value with ‘SW = No’ is -> 247.6839836 ***\n\nLet us predict the reduction in average fare on the route in previous step, if Southwest decides to cover this route [using the exhaustive search model above].\ndata.pred1 <- data.frame(COUPON = 1.202, NEW = 3, VACATION = \'No\', SW = \'Yes\', HI = 4442.141,\n                        S_INCOME = 28760, E_INCOME = 27664, S_POP = 4557004, E_POP = 3195503,\n                        SLOT = \'Free\', GATE = \'Free\', PAX = 12782, DISTANCE = 1976)\nprediction1 <- predict(blm, data.pred1)\nThe predicted value with ‘SW = Yes’ is ->\n207.155767\n\n\nUsing leaps package to run backward selection regression to reduce the number of predictors.\nfare.backward <- step(lin.model, direction = ""backward"")\n## Start:  AIC=3652.06\n## FARE ~ COUPON + NEW + VACATION + SW + HI + S_INCOME + E_INCOME + \n##     S_POP + E_POP + SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - COUPON    1       911  622732 3650.8\n## - NEW       1      1459  623280 3651.3\n## - S_INCOME  1      1460  623281 3651.3\n## <none>                   621821 3652.1\n## - E_INCOME  1     17499  639320 3664.2\n## - SLOT      1     17769  639590 3664.4\n## - PAX       1     24441  646263 3669.7\n## - E_POP     1     28296  650118 3672.8\n## - GATE      1     28881  650702 3673.2\n## - S_POP     1     36680  658501 3679.3\n## - HI        1     76469  698290 3709.2\n## - SW        1    105205  727026 3729.8\n## - VACATION  1    113382  735204 3735.5\n## - DISTANCE  1    417379 1039200 3912.0\n## \n## Step:  AIC=3650.81\n## FARE ~ NEW + VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + \n##     E_POP + SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - S_INCOME  1      1261  623994 3649.8\n## - NEW       1      1678  624410 3650.2\n## <none>                   622732 3650.8\n## - E_INCOME  1     17126  639859 3662.6\n## - SLOT      1     18407  641139 3663.7\n## - GATE      1     29285  652018 3672.2\n## - E_POP     1     29484  652217 3672.4\n## - PAX       1     34128  656860 3676.0\n## - S_POP     1     36089  658821 3677.5\n## - HI        1     78594  701326 3709.4\n## - SW        1    107735  730468 3730.2\n## - VACATION  1    114276  737009 3734.7\n## - DISTANCE  1    824468 1447200 4078.9\n## \n## Step:  AIC=3649.84\n## FARE ~ NEW + VACATION + SW + HI + E_INCOME + S_POP + E_POP + \n##     SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - NEW       1      1697  625690 3649.2\n## <none>                   623994 3649.8\n## - E_INCOME  1     16167  640161 3660.9\n## - SLOT      1     20012  644006 3663.9\n## - E_POP     1     28559  652552 3670.7\n## - GATE      1     29766  653759 3671.6\n## - PAX       1     32869  656863 3674.0\n## - S_POP     1     41722  665715 3680.8\n## - HI        1     79501  703495 3709.0\n## - SW        1    126837  750831 3742.2\n## - VACATION  1    128080  752073 3743.1\n## - DISTANCE  1    826967 1450960 4078.2\n## \n## Step:  AIC=3649.22\n## FARE ~ VACATION + SW + HI + E_INCOME + S_POP + E_POP + SLOT + \n##     GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## <none>                   625690 3649.2\n## - E_INCOME  1     15649  641339 3659.8\n## - SLOT      1     19217  644907 3662.6\n## - E_POP     1     28766  654456 3670.1\n## - GATE      1     29165  654856 3670.5\n## - PAX       1     32706  658396 3673.2\n## - S_POP     1     42648  668338 3680.9\n## - HI        1     78891  704581 3707.8\n## - SW        1    126577  752267 3741.2\n## - VACATION  1    127066  752756 3741.5\n## - DISTANCE  1    825966 1451656 4076.4\n\nsummary(fare.backward)\n## \n## Call:\n## lm(formula = FARE ~ VACATION + SW + HI + E_INCOME + S_POP + E_POP + \n##     SLOT + GATE + DISTANCE + PAX, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -99.148 -22.077  -2.028  21.491 107.744 \n## \n## Coefficients:\n##                   Estimate     Std. Error t value             Pr(>|t|)    \n## (Intercept)  42.0764345686  14.7566725244   2.851             0.004534 ** \n## VACATIONYes -38.7574569132   3.8500841929 -10.067 < 0.0000000000000002 ***\n## SWYes       -40.5282166043   4.0337560764 -10.047 < 0.0000000000000002 ***\n## HI            0.0082681499   0.0010423739   7.932   0.0000000000000143 ***\n## E_INCOME      0.0014446281   0.0004089281   3.533             0.000450 ***\n## S_POP         0.0000041850   0.0000007176   5.832   0.0000000098509604 ***\n## E_POP         0.0000037791   0.0000007890   4.790   0.0000022053722984 ***\n## SLOTFree    -16.8515659965   4.3045728245  -3.915             0.000103 ***\n## GATEFree    -21.2165142735   4.3991611435  -4.823   0.0000018824635124 ***\n## DISTANCE      0.0736714582   0.0028704349  25.666 < 0.0000000000000002 ***\n## PAX          -0.0007619280   0.0001491869  -5.107   0.0000004660838631 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 35.41 on 499 degrees of freedom\n## Multiple R-squared:  0.7803, Adjusted R-squared:  0.7759 \n## F-statistic: 177.2 on 10 and 499 DF,  p-value: < 0.00000000000000022\n\nfare.backward.pred <- predict(fare.backward, test)\naccuracy(fare.backward.pred, test$FARE)\n##               ME    RMSE      MAE       MPE     MAPE\n## Test set 3.06081 36.8617 27.70568 -5.938062 21.62142\n\n\nThe initial number of predictors are 13 before running backward\nselection regression\nThe final number of predictors which are significant to the model\nare 10\nThe variables that were removed are : COUPON, NEW and\nS_INCOME\nCOUPON, S_INCOME and NEW were removed in Step 2,3 and 4\nrespectively\n\n\n\nLet us run a backward selection model using stepAIC() function.\nif(!require(MASS)) install.packages(""MASS"")\nfare.backwardAIC <- stepAIC(lin.model, direction = ""backward"")\n## Start:  AIC=3652.06\n## FARE ~ COUPON + NEW + VACATION + SW + HI + S_INCOME + E_INCOME + \n##     S_POP + E_POP + SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - COUPON    1       911  622732 3650.8\n## - NEW       1      1459  623280 3651.3\n## - S_INCOME  1      1460  623281 3651.3\n## <none>                   621821 3652.1\n## - E_INCOME  1     17499  639320 3664.2\n## - SLOT      1     17769  639590 3664.4\n## - PAX       1     24441  646263 3669.7\n## - E_POP     1     28296  650118 3672.8\n## - GATE      1     28881  650702 3673.2\n## - S_POP     1     36680  658501 3679.3\n## - HI        1     76469  698290 3709.2\n## - SW        1    105205  727026 3729.8\n## - VACATION  1    113382  735204 3735.5\n## - DISTANCE  1    417379 1039200 3912.0\n## \n## Step:  AIC=3650.81\n## FARE ~ NEW + VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + \n##     E_POP + SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - S_INCOME  1      1261  623994 3649.8\n## - NEW       1      1678  624410 3650.2\n## <none>                   622732 3650.8\n## - E_INCOME  1     17126  639859 3662.6\n## - SLOT      1     18407  641139 3663.7\n## - GATE      1     29285  652018 3672.2\n## - E_POP     1     29484  652217 3672.4\n## - PAX       1     34128  656860 3676.0\n## - S_POP     1     36089  658821 3677.5\n## - HI        1     78594  701326 3709.4\n## - SW        1    107735  730468 3730.2\n## - VACATION  1    114276  737009 3734.7\n## - DISTANCE  1    824468 1447200 4078.9\n## \n## Step:  AIC=3649.84\n## FARE ~ NEW + VACATION + SW + HI + E_INCOME + S_POP + E_POP + \n##     SLOT + GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## - NEW       1      1697  625690 3649.2\n## <none>                   623994 3649.8\n## - E_INCOME  1     16167  640161 3660.9\n## - SLOT      1     20012  644006 3663.9\n## - E_POP     1     28559  652552 3670.7\n## - GATE      1     29766  653759 3671.6\n## - PAX       1     32869  656863 3674.0\n## - S_POP     1     41722  665715 3680.8\n## - HI        1     79501  703495 3709.0\n## - SW        1    126837  750831 3742.2\n## - VACATION  1    128080  752073 3743.1\n## - DISTANCE  1    826967 1450960 4078.2\n## \n## Step:  AIC=3649.22\n## FARE ~ VACATION + SW + HI + E_INCOME + S_POP + E_POP + SLOT + \n##     GATE + DISTANCE + PAX\n## \n##            Df Sum of Sq     RSS    AIC\n## <none>                   625690 3649.2\n## - E_INCOME  1     15649  641339 3659.8\n## - SLOT      1     19217  644907 3662.6\n## - E_POP     1     28766  654456 3670.1\n## - GATE      1     29165  654856 3670.5\n## - PAX       1     32706  658396 3673.2\n## - S_POP     1     42648  668338 3680.9\n## - HI        1     78891  704581 3707.8\n## - SW        1    126577  752267 3741.2\n## - VACATION  1    127066  752756 3741.5\n## - DISTANCE  1    825966 1451656 4076.4\n\nsummary(fare.backwardAIC)\n## \n## Call:\n## lm(formula = FARE ~ VACATION + SW + HI + E_INCOME + S_POP + E_POP + \n##     SLOT + GATE + DISTANCE + PAX, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -99.148 -22.077  -2.028  21.491 107.744 \n## \n## Coefficients:\n##                   Estimate     Std. Error t value             Pr(>|t|)    \n## (Intercept)  42.0764345686  14.7566725244   2.851             0.004534 ** \n## VACATIONYes -38.7574569132   3.8500841929 -10.067 < 0.0000000000000002 ***\n## SWYes       -40.5282166043   4.0337560764 -10.047 < 0.0000000000000002 ***\n## HI            0.0082681499   0.0010423739   7.932   0.0000000000000143 ***\n## E_INCOME      0.0014446281   0.0004089281   3.533             0.000450 ***\n## S_POP         0.0000041850   0.0000007176   5.832   0.0000000098509604 ***\n## E_POP         0.0000037791   0.0000007890   4.790   0.0000022053722984 ***\n## SLOTFree    -16.8515659965   4.3045728245  -3.915             0.000103 ***\n## GATEFree    -21.2165142735   4.3991611435  -4.823   0.0000018824635124 ***\n## DISTANCE      0.0736714582   0.0028704349  25.666 < 0.0000000000000002 ***\n## PAX          -0.0007619280   0.0001491869  -5.107   0.0000004660838631 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 35.41 on 499 degrees of freedom\n## Multiple R-squared:  0.7803, Adjusted R-squared:  0.7759 \n## F-statistic: 177.2 on 10 and 499 DF,  p-value: < 0.00000000000000022\n\nfare.backward.predAIC <- predict(fare.backwardAIC, test)\naccuracy(fare.backward.predAIC, test$FARE)\n##               ME    RMSE      MAE       MPE     MAPE\n## Test set 3.06081 36.8617 27.70568 -5.938062 21.62142\n\n\nInitially all the values are considered in a model with Start AIC =\n3652.06\nIn the second step, as AIC of COUPON = 3650.8 < Start AIC of\n3652.06, COUPON is moved from the model\nIn next step, AIC=3650.81, so the variable S_INCOME whose AIC =\n3649.8 < 3650.81 will be removed.\nIn the next step, AIC = 3649.84, so the variable NEW with AIC =\n3649.2 < 3649.84 will be removed\nIn the next step, AIC = 3649.22 and no variable has AIC less than\nthis value. So the remaining variables are considered for the final\nmodel\n\n'], 'url_profile': 'https://github.com/manoj95b', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kamranrahnama', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Singleregressionline\nUses data to plot a regression line\n'], 'url_profile': 'https://github.com/jkrasusky12', 'info_list': ['Python', 'Updated May 13, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 21, 2020']}"
"{'location': 'Tokyo, Japan', 'stats_list': [], 'contributions': '155 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/knok', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Philadelphia', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['R\nCourses Material:\n\nMSSP 630: Quantitative Reasoning and Social Statistics\nMSSP 897: Applied Linear Modeling\n\n'], 'url_profile': 'https://github.com/zzhao19', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['simple-linear-regression1\n'], 'url_profile': 'https://github.com/utkarsh027', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': [""2020 Academy Award Winners (oscarDataAnalysisAndModel)\noscarDataAnalysisAndModel is an application used for the extraction, structured representation, and future predictions of historical and future films, cast, directors, and other associated attributes of Oscar Award winners. It does this by scraping .CSVs, calling upon Third-Party APIs, and parsing through with Python and JavaScript.\nInstall\ngit clone https://github.com/chrisestrada007/oscarDataAnalysisAndModel.git\n\nUsage\nTo deploy locally with Flask on localhost:\npython app.py\n\nTo visit the hosted version:\n### http://oscaranalysis.herokuapp.com\n\nHow to run the files:\nTo Run the best picture prediction model, go to chrisTestingGrounds >> Run the Best_Movie_prediction_Chris.ipynb\nHere's the Direct link - https://github.com/chrisestrada007/oscarDataAnalysisAndModel/tree/master/chrisTestingGrounds/Best_Movie_prediction_Chris.ipynb\nTo Run the scrapper script we used to script tmdb, films101, go to https://github.com/chrisestrada007/oscarDataAnalysisAndModel/tree/master/chrisTestingGrounds >> Run\ntmdb scrapper : tmdb_scraper.ipynb\nfilm101 : scrapingFilms101.ipynb\nTo Run the Bechdel Test Run >>\nhttps://github.com/chrisestrada007/oscarDataAnalysisAndModel/blob/master/Spaar/final_shiz/user_input_predictions____many_seaborn_visualizations_from_my_dataset.ipynb\nThe vizualizations from Tableau are incorportaed in the website itself.\nRequirements\nhttps://github.com/chrisestrada007/oscarDataAnalysisAndModel/blob/master/requirements.txt\n\nCredits\n2020 Update and prediction:\n\nChris: https://github.com/chrisestrada007\n\nOriginal project was created for the UCBX Data Analytics Bootcamp Final by:\n\nChris: https://github.com/chrisestrada007\nKayti: https://github.com/kaytipotgieter\nPriya: https://github.com/Priyarag\nJoe: https://github.com/hone1er\nSpaar: https://github.com/galadREAL\n\n""], 'url_profile': 'https://github.com/chrisestrada007', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['linear_regression_model\nSimple linear regression model created from equations.\nEquatio  of the geometry line\ny = mx + c\nhere y is the dependent variable.the variable to be predicted\nx is the independent variable i.e. the variable that is controllable. It is the input.\nm is the slope.\nc is the intercept.\n'], 'url_profile': 'https://github.com/krishnapm642', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alyssamelody', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ka1shi-medium', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jayvachhani77', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['eCommerse-RegressionModel\nPlease use the python notebook file eCommerce_LinearRegressionModel.ipynb to view the project work.\n'], 'url_profile': 'https://github.com/rauts', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Connecticut', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['MLR-House-Price\nMultiple Linear Regression ~ Finding House Price\n""Ask a home buyer to describe their dream house, and they probably won\'t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition\'s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.""\nKaggle competition:House Prices~ Predict sales prices.\n'], 'url_profile': 'https://github.com/MohammedMahmud', 'info_list': ['Python', 'Updated May 11, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}"
"{'location': 'Farmington, CT', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Introduction to ML Math\nAn introduction to the math behind Machine Learning.\nUse single input linear regression to plot data in the main array.\nFor this assignment, students should input their own independent and dependent variables.\nCitations are required in the multi-line comment at the top, along with name and date.\nAdjust other settings, like X and Y variables or graph colors. \nComments or suggestions welcome for other assignments related to Artificial Intelligence.\n'], 'url_profile': 'https://github.com/thezross', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'Tampa', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gokul1794', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NumpyProjects\nThis repository shows my experience with data exploration, linear regression, logistic regression, decision trees, and clustering using Numpy.\n'], 'url_profile': 'https://github.com/timhakobian', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['tensorflow_models\ntensorFlow models (Linear Classifier ,DNN Classifier ,Decision Tree Classifier ,Linear Regression ,DNN Regressor)\n'], 'url_profile': 'https://github.com/AhmedElsayed201097', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['Board-Game-Review-Prediction-using-Machine-Learning\nPredicting board game review using Linear Regression and Random Forest Regressor\n'], 'url_profile': 'https://github.com/sharanya02', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': [""regressionAssignmentPredictingCO2Emissions\nPredicting CO2 Emissions Using Other Variables (for a given year)\nLet's now try to build a regression model to predict CO2 emissions using other variables (for any given year). This will help us in understanding which variables affect CO2 emissions. This understanding can then be used by, for example, organisations/authorities such as the UN, governments etc. to create regulatory policies etc.\n""], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['RegressionForCarCO2Emission\n'], 'url_profile': 'https://github.com/EvilSurya7', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'Compostela', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': [""regressions\n\nLinear-regression\n\nMiles Per Gallon = m * (Car horsepower) + b\nML with regressions.\nMatrix A Shape (4,2)\nMatrix B Shape (2,3)\n(4,2) <----> (2,3) Inner shape value are the same. Elegible for matrix multiplication.\n(4,2) x (2,3) = (4,3)\nSlope of MSE with respect to B\nSlope of MSE with respect to M\nApply matrix multiplication\nResult:\nSlope of MSE with respect to M and B\n\n\nLogistic-regression\n\nGiven a vehicle's weight, horsepower, and engine displacement, will it PASS or NOT PASS a smog passedemissions check?\n\n\nMultinominal-logistic-regression.\n\nGiven the horsepower, weight and displacement of a vehicle, will it have high, medium, or low\nfuel efficiency?\n""], 'url_profile': 'https://github.com/josecho', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'Minneapolis, Minnesota', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Regressions\nPrototype on how statistics and related narative can be displayed\n'], 'url_profile': 'https://github.com/usa85', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'México', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['regressioncourse\n'], 'url_profile': 'https://github.com/adfmb', 'info_list': ['Python', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Python', 'Updated Feb 18, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}"
"{'location': 'Tokyo, Japan', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['predicting-car-prices\nThis is the guided project of ""Linear Regression For Machine Learning"" at Dataquest.\nIn this project, we build the linear regression model worked and learn some techniques for cleaning, transforming, and selecting features.\nReferences\n\n""Linear Regression For Machine Learning"" Dataquest (2019) [link].\n\n'], 'url_profile': 'https://github.com/naokiwifruit', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Kei Fukutani\nUSF email address: kfukutani@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'São Paulo, BRAZIL', 'stats_list': [], 'contributions': '224 contributions\n        in the last year', 'description': ['project1-data-prediction\nproject about prediction using linear regression, supervised learning\n'], 'url_profile': 'https://github.com/higoramorim', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nura-keasri', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Lanzhou, China', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Last update: 2020-2-22\nfiles:\nrkc.R: the collection of all R-functions required for estimating the model proposed by Hansen(2017,JBES)\nrkt.R: the collection of all R-functions required for estimating the model proposed by Yang and Su(2018,JIMF)\nYang and Su(2018) empirical applicaiton: replicate the empirical results in the paper\nHansen (2017) empirical applicaiton: replication the empirical results in the paper\nReference\n[1] Hansen, B.E., 2017. Regression kink with an unknown threshold. J. Bus. Econ. Stat. 35 (2), 228–240.\n[2] Lixiong Yang, Jen-Je Su. Debt and growth: Is there a constant tipping point?[J].  Journal of International Money and Finance(SSCI, ABS Grade 3), 2018, 87:133-143.\n'], 'url_profile': 'https://github.com/lixiongyang', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Bloomington', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Logistic-Regression-Python\nImplementing Logistic Regression from scratch in Python using numpy\n'], 'url_profile': 'https://github.com/singhvis29', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kprasertchoang', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/venkatachalam13', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Module 5 -  Online Buyer Intention\nData and Introduction\nThe dataset contains information of around ~ 12000 user sessions on a e-commerce website with 10 continuous features and 7 categorical features. The challenge is to predict whether the user will end up purchasing a product on the website. It is a classification problem. The dataset has a class imbalance. It can be found at https://www.kaggle.com/roshansharma/online-shoppers-intention\nConclusion\nWe do a NAIVE Bayes, Logistic Regression, SVM and Random Forests for our predictions. Random Forests performs the best with accuracy of 91.5%\nBlog\nhttps://aghalsasi-datascience.blogspot.com/2020/02/online-purchasen-intent.html\n'], 'url_profile': 'https://github.com/aghalsas', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PadmaPriyaJayaraj', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'MIT license', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'R', 'Updated Feb 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mr-juice', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['Check\nSimple web-app that uses a Sci-kit Learn model to determine whether a user is human or a bot.\nDocker\nTo build the image from the Dockerfile  just run:\ndocker build .\nTo run the image:\ndocker run -d -p 5000:5000 IMAGE\nApp\nTo run the app alone:\npython3 app.py\nYou may need to install the required packages:\npip3 install -r requirements.txt\nSending data with curl post requests\nTo send user activity to be checked by the SKLearn model, structure your curl command like the following one:\ncurl 0.0.0.0:5000/ -d \'{""country"":""US"", ""region"":""CA"", ""visitortype"":""ANONYMOUS"", ""referrer"":""1""}\' -H \'Content-Type: application/json\'\nKeys:\n\ncountry: User\'s country\nregion: User\'s region\nvisitortype: Whether the user is browsing anonymously or logged in.\nreferrer: Either 1 or 0\n\n'], 'url_profile': 'https://github.com/andcarnivorous', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Zun Yang\nUSF email address:  zyang65@dons.usfca.edu\nThe PDF version of this assignment spec is available under Files on Canvas.\nAs seen during class, there are many requirements on the data we feed to simple linear regression (SLR). In this lab, your task is to recreate the visual work in class to determine whether a dataset is appropriate for SLR and show how you are able to apply the technique to a previously-unseen dataset.\nData\nThere are two datasets for this lab, listed in Table 1. Use (only) the Predictors listed.\n\n\n\nDataset\nTarget\nPredictors\n\n\n\n\nToluca (Links to an external site.) dataset (courtesy of Paul Intrevado)\nlotSize\nworkHours\n\n\nCredit (Links to an external site.) dataset (from Introduction to Statistical Learning with Applications in R)\nLimit\nIncome, Rating, Cards, Age, Education\n\n\n\nTable 1: Datasets, Targets and Predictors\nProcess\nUse the starter code here (Links to an external site.) and fill in your name and USF email in the readme.\nFor each predictor, your implementation must:\n\nPlot the predictor against the target\nPlot the residual against the target\nDetermine the coefficients (slope, intercept) against the target\n\n… and based on the above, you must determine whether the predictor is suitable for SLR.\nGrading\nGrades for this assignment will be determined by the grader as follows:\n\n100% = Code functions, is well-documented and clearly shows the relationships of all predictors to targets and to residuals.\n75% = Code functions but is not well-documented or does not clearly show the relationships of all predictors to targets and to residuals.\n50% = Code functions but is not well-documented -AND- does not clearly show the relationships of all predictors to targets and to residuals.\n0% = No submission / code does not function.\n\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'india', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Project Overview\nThe ""spam"" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography…\nOur collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word \'george\' and the area code \'650\' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\nNumber of Instances: 4601 (1813 Spam = 39.4%)\nNumber of Attributes: 58 (57 continuous, 1 nominal class label)\nAttribute Information:\nThe last column of \'spambase.data\' denotes whether the e-mail was considered spam (1) or not (0)\n48 attributes are continuous real [0,100] numbers of type word freq WORD i.e. percentage of words in the e-mail that match WORD\n6 attributes are continuous real [0,100] numbers of type char freq CHAR i.e. percentage of characters in the e-mail that match CHAR\n1 attribute is continuous real [1,…] numbers of type capital run length average i.e. average length of uninterrupted sequences of capital letters\n1 attribute is continuous integer [1,…] numbers of type capital run length longest i.e. length of longest uninterrupted sequence of capital letters\n1 attribute is continuous integer [1,…] numbers of type capital run length total i.e. sum of length of uninterrupted sequences of capital letters in the email\n1 attribute is nominal {0,1} class of type spam i.e denotes whether the e-mail was considered spam (1) or not (0),\nMissing Attribute Values: None\n'], 'url_profile': 'https://github.com/ashweta81', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Moscow, Russia', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AndreyGorbatov1', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Glass-classification-type\nPredict the glass type using Linear Regression algorithm\n'], 'url_profile': 'https://github.com/RajathiSelvan', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Jaya Lakshmi\nUSF email address:jlakshmi@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '709 contributions\n        in the last year', 'description': ['USA pricing prediction model by linear regression\n'], 'url_profile': 'https://github.com/Dharana23', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/davidgodinez', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Eric Lieu\nUSF email address: elieu@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'HTML', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}"
"{'location': 'Paris', 'stats_list': [], 'contributions': '284 contributions\n        in the last year', 'description': ['ft_linear_regression (subject)\nIntroduction\nMachine learning is a growing field of computer science that may seem a bit complicated and reserved only to mathematicians. You may have heard of neural networks or\nk-means clustering and don’t undersdand how they work or how to code these kinds of\nalgorithms...\nBut don’t worry, we are actually going to start with a simple, basic machine learning\nalgorithm.\nObjective\nThe aim of this project is to introduce you to the basic concept behind machine learning. For this project, you will have to create a program that predicts the price of a car by using a linear function train with a gradient descent algorithm.\nWe will work on a precise example for the project, but once you’re done you will be\nable to use the algorithm with any other dataset.\nFormulas\n\n\n\nInstall\ngit clone https://github.com/ChokMania/ft_linear_regression.git\npython -m pip install -r lib.txt\nUsage\nTo train our model\npython train.py\nThis will save the weights trained in `./data/theta.txt`\n\n\n\nTo predict a value\npython main.py\nEnter a mileage: n\nWith: n km, the estimated price is : x\n\n\n'], 'url_profile': 'https://github.com/ChokMania', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jami-Kate', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Chou Sung Wang(Sam)\nUSF email address:  cwang115@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Linjing Shao\nUSF email address:  lshao@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Kedar Khetia\nUSF email address: kmkhetia@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Data-Mining\nSimple Linear Regression for a Salary-Experience Dataset\nIts a simple Linear Regression Script to learn the basics of the algo by performing it over a salary vs Experience dataset.\nSteps:-\nInstall Anaconda Navigator(Best)\nLaunch Spyder\nopen the given files and run it using run buttonpresent at the top toolbar.\nComments have been present in the file to understand the significance of the code.\n'], 'url_profile': 'https://github.com/goyalakshay12', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Divya Vijayan\nUSF email address:  dvijayan@usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Na Lu\nUSF email address:nlu5@usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Xianan Li\nUSF email address:  xli68@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': [""Introduction\nThis is a very basic natural language processing system using binary logistic regression. In data that I have tested it with I was able to get test errors as low as 0.15. It runs was sripted using python 3.6 and has very specific data formatting requirements. Once understood however, it is very accessible.\nThis program is split into two parts and is dependent on data formatted a certain way. There should be three different data sets: a training set, a validation set, and a test set. You could theoretically make them all the same, or make any of them just empty files if you do not wish to train, validate, or test respecively. However, staying true to the problem of machine learning, it is recommended that the user has three different datasets. This file will explain how this data is to be formatted and how the program runs.\nData Format\nThe data (training, validation, and testing) must all be formatted in separate files as follows:\n<class>\\t<word1> <word2> ... <wordN>\\n\n\nwhere the class is either 0 or 1, and the words are any tokens without spaces.\nExample:\n1 these are the words which will be read\n0 these are words to be read from the next sample\n4 th i s is  an invalid    sample \\n\\n\\n\n\n\nThis program also relies on having an indexed dictionary which defines which words should be considered features. This should be formatted as follows:\n\n<word> <index>\n\nwhere the word is any token with a space and the index is a non-negative integer value.\nExample:\nthis 0\nis 1\nthe 2\ndictionary 3\nand 4\nthese 5\nwords 6\nwill 7\nbe 8\nconsidered 9\nfeatures 10\n\nRunning\n\n\nThis program is split into two parts but for convenience there is a single python script (run.py) that will run both. The first part (feature.py) reads in and formats/pre-processes the data, and the second part (lr.py) performs the learning on the formatted data. Both rely on dictTrie.py. The expected arguments for feature.py, lr.py, and run.py are given below:\n\n\nfeature.py expects these arguments:\n\n\n<train_input> the file which holds the training data\n<validation_input> the file which holds the validation data\n<test_input> the file which holds the test data\n<dict_input> the file which holds the dictionary\n<formatted_train_out> the file to which will be written the pre-processed training data\n<formatted_validation_out> the file to which will be written the pre-processed validation data\n<formatted_test_out> the file to which will be written the pre-processed validation data\n<feature_flag> a flag which should either be set to 1 (which will keep all occuring words in the data) or 2 (which will remove words which occur more than 4 times)\n\n\nlr.py expects these arguments:\n\n<formatted_train_input> the file which holds the pre-processed training data\n<formatted_validation_input> the file which holds the pre-processed validation data\n<formatted_test_input> the file which holds the pre-processed test data\n<dict_input> the file which holds the dictionary\n<train_out> the file to which will be written the classifications for the training data after training has occurred\n<test_out> the file to which will be written the classifications for the test data after training has occurred\n<metrics_out> the file to which test error and train error will be written after training has occurred\n<num_epoch> the number of epochs (traversals of training data) to train for. 30-60 is recommended.\n\n\nrun.py expects these arguments:\n\n<data_name> when run, run.py will look for data based on the following concatenation: 'input/<data_name>_<train|validation|test>.tsv'\n<dict_input> when run, run.py will look for the dictionary based on the following concatenation: 'input/<dict_input>.txt'\n<feature_flag> used in feature.py as described above\n<num_epoch> when run, run.py will save its output based on the following concatenation: 'output/<data_name>_<num_epoch>_<train|test|metrics>.<labels|txt>\n\n""], 'url_profile': 'https://github.com/JacobOaks', 'info_list': ['Python', 'Updated Oct 25, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Pontakorn Pakavaleetorn\nUSF email address:  ppakavaleetorn@usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Chicago, IL ', 'stats_list': [], 'contributions': '274 contributions\n        in the last year', 'description': ['Logistic Regression E-mail Classification\n\u200b\tThe python program implements logistic regression for e-mail classification. This is part of a supervised learning problem. The input files are in the csv format. The last column denotes the class label of the data set. Most of the attributes indicate whether a particular word or character was frequently occurring in the e-mail.\nRequirement\n\nPython 3\nspambasetrain.csv\nspambasetest.csv\n\nPrerequisites\n\n\nLogistic Regression\n\n\nGradient Descent\n\n\nRunning the tests\n\u200b\tIt contains two different programs for different lambda values. The  program will parse the input csv file.\nThe program will output the following :\n\nAccuracy\n\nTesting Accuracy\nTraining Accuracy\n\n\nChanges to accuracy for different number of iterations.\n\n'], 'url_profile': 'https://github.com/yashchitre03', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Dallas ', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Introduction:\nIn computing, a compute kernel is a routine compiled for high throughput accelerators such as graphics processing units (GPUs),  separate from but used by a main program (typically running on a central processing unit). They are sometimes called compute shaders, sharing execution units with vertex shaders and pixel shaders on GPUs, but are not limited to execution on one class of device, or graphics\nThe project is to demonstrate linear and logistic regression to predict the GPU run time which is done based on 14 different configurations of processor such as local work group size, local memory shape etc.\nDataset:\nDataset can be downloaded from: https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance\nThis data set measures the running time of a matrix-matrix product AB = C, where all matrices have size 2048 x 2048, using a parameterizable SGEMM GPU kernel with 241600 possible parameter combinations. For each tested combination, 4 runs were performed and their results are reported as the 4 last columns. All times are measured in milliseconds.\nThere are 14 parameter, the first 10 are ordinal and can only take up to 4 different powers of two values, and the 4 last variables are binary. Out of 1327104 total parameter combinations, only 241600 are feasible (due to various kernel constraints). This data set contains the results for all these feasible combinations.\nTask:\nBy using packages like Numpy, Pandas and various plotting packages, a gradient decent algorithm to calculate the cost function and to find the global minima by tuning the value of learning rate. By using this algorithm we estimate the optimal Beta values to construct the regression equation to predict the GPU computation time.\nExperimentation:\nFirst experiment is to implement gradient descent algorithm in the dataset. I initially scaled the dataset using standard scalar from sklearn. Then the dataset has been split into test and train split with split percentage of 70 as training data and 30 as testing data.\nGPU RUNTIME =𝛽0+𝛽1∗x1+𝛽2∗x2+𝛽3∗x3+𝛽4∗x4+𝛽5∗x5+ 𝛽6∗x6+𝛽7∗x7+𝛽8∗x8+𝛽9∗x9+𝛽10∗x10+𝛽11∗x11+𝛽12∗x12+𝛽13∗x13+𝛽14∗x14\nI selected the following learning rates to run the algorithm 0.001,0.003,0.006,0.1. And the results were plotted as functions of alpha and cost of test and train of the samples taken from the dataset.\nPart B\nNow the gradient decent algorithm is implemented with the logistic regression model. By using gradient decent optimum cost is calculated. The sigmoid function was calculated by the formula 1 / (1 + np.exp(-x)) to calculate the gradient decent as to obtain the output between 0 or 1. Our current prediction function returns a probability score between 0 and 1. In order to map this to a discrete class, we select a threshold value or tipping point as the median of the runtime above which we will classify values into 1 or 0.\n'], 'url_profile': 'https://github.com/Murugan08', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['RiboViz regression test data 21/02/2020\nProvenance:\n\nURL: https://github.com/RiboViz/RiboViz\nDate: Fri Feb 21 13:40:30 2020 +0000\nBranch: develop\nCommit: ce57fc9f618b79dd08498bb473842fca7fcbacfa\nEnvironment: see environment.txt\n\nUsage:\n$ git clone https://github.com/riboviz/regression-test-data-20200221\n$ cd riboviz\n$ pytest riboviz/test/regression/test_vignette.py --expected=$HOME/regression-test-data-20200221\nFor full instructions see the RiboViz developer guide in the RiboViz repository.\n'], 'url_profile': 'https://github.com/riboviz', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Tyler Iams\nUSF email address:  taiams@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Linear-regression\nFitting a multiple linear regression model in julia\nIn this case I tried to fit a multiple linear regression in Julia using housing data\n'], 'url_profile': 'https://github.com/kshitijbaloothiya', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Edmonton, Alberta, Canada', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YingnanMa', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Avocado-price\nTo predict the avocado price ising Linear Regression algorithm\n'], 'url_profile': 'https://github.com/vinoraju99', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Minneapolis, MN', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Penalized weighted sum regression for chemical mixtures\nThis repository contains the code to fit a modified form of Weighted Quantile Sum regression that does not require data to be split into training and test sets for weight estimation and inference. This method was introduced in ""A permutation-based approach to inference for weighted sum regression with correlated chemical mixtures"" by Lyden, et al.\nThe contents of this repository are as follows:\nfunctions.R includes functions that are helpful in generating correlated data.\nmethod-functions.R includes functions to fit modified WQS, i.e., estimate mixture component weights using penalized constrained regressions and run a permutation test for the overall mixture effect.\nexample.R applies method-functions.R to estimate weights and overall mixture effects in a simulated dataset.\nSimulations/ contains code to run all simulation studies described in Lyden, et al.\n'], 'url_profile': 'https://github.com/glyden', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'White Plains,New York', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['lasso_ridge_elasticnet\nComparison between Linear Regression, Lasso , Ridge and ElasticNet\n'], 'url_profile': 'https://github.com/lithathampan', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Julia', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 23, 2020', 'HTML', 'Updated Feb 23, 2020', 'R', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Paraphrase Identification using string similarity\nThis project examines string similarity metrics for paraphrase identification.\nIt converts semantic textual similarity data to paraphrase identification data using threshholds.\nThough semantics go beyond the surface representations seen in strings, some of these\nmetrics constitute a good benchmark system for detecting paraphrase.\nData is from the STS benchmark.\nHomework: pi_logreg.py\n\nTrain a logistic regression for PI on the training data using three features:\n\nBLEU\nWord Error Rate\nCosine Similarity of TFIDF vectors\n\n\nUse the logistic regression implementation in sklearn.\nUpdate the readme:\n\n1-sentence description of the dataset\nReport your model's accuracy, precision, recall and f1-measure on the dev set.\nComment on what these metrics tell you about your model's performance and\ncompare to the baseline in lab.py (~5 sentences)\n\n\n\npython pi_logreg.py --sts_dev_file stsbenchmark/sts-dev.csv --sts_train_file stsbenchmark/sts-train.csv\nlab.py\nlab.py converts a STS dataset to PI and reports the number of remaining examples\nand the distribution of paraphrase/nonparaphrase.\nThen, it evaluates TFIDF vector similarity with a threshold of 0.7 as a model of paraphrase\nusing precision and recall.\nExample usage:\npython lab.py --sts_data stsbenchmark/sts-dev.csv\nResults\nTODO: write up your results for the baseline in the lab and the logistic regression model in the homework.\n""], 'url_profile': 'https://github.com/ANLY521', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'Johannesburg', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/masaimahapa', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'Houston TX', 'stats_list': [], 'contributions': '447 contributions\n        in the last year', 'description': ['HousePricing_R\nUse R to predict the house price by Linear Regression\nA final project of R for business Analytics\nThe client wants to better understand the sales prices of the real estate properties. Your main job is to help her explain and eventually predict the sales prices of some properties as close to reality as possible.\nFinal\nhttps://htmlpreview.github.io/?https://github.com/Xiaoyang-Rebecca/HousePricing_R/blob/master/Final_Project_Report_Tips_and_Outline.html\n'], 'url_profile': 'https://github.com/Xiaoyang-Rebecca', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'Lubbock, TX ', 'stats_list': [], 'contributions': '305 contributions\n        in the last year', 'description': ['Regression-Class\nHw solutions and projects for Linear Regression Class\n'], 'url_profile': 'https://github.com/tung2921', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['regtwo\nCode for project II in SF2930 Regression Analysis\n'], 'url_profile': 'https://github.com/itslwg', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Danielle Hu\nUSF email address:   lhu@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'Bloomington', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Linear-Regression-Python\nImplementing Linear Regression from scratch in Python using numpy\n'], 'url_profile': 'https://github.com/singhvis29', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'Waterford, Ireland', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['LinearRegression\nImplementation of Linear Regression from scratch in Python\nResults:\nBelow are the results for implemented logistic regression after repeating the process 10 times with a random division of 2/3 train data and 1/3 test data.\nAverage Accuracy: 94.47761194029852 %\nSimilarly, below are the results for scikit-learn logistic regression after repeating the process for 10 times:\nAverage Accuracy: 93.10902255639097 %\n'], 'url_profile': 'https://github.com/PreetiSajjan', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'Gurgaon, Haryana, India', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karansehgal1988', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['linear_regression\nThe goal of this project is to find the closest y = ax + b function that matches given coordinate points.\nThis solution uses gradient descent and prints the computed coefficients in the theta_values file when running train.py. Train.py also displays a graph containing points showing the training data as well as the computed function. There is also a representation of the lowering error over the training iterations.\nOnce training is done you can run predict.py in order to predict a new value.\n'], 'url_profile': 'https://github.com/Kelias-42', 'info_list': ['Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'HTML', 'Updated Feb 22, 2020', 'R', 'Updated Mar 9, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', '1', 'Python', 'Updated Feb 17, 2020', 'R', 'Updated Mar 15, 2020', 'Python', 'Updated Oct 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vimalgshettigar', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'Dehradun, Uttrakhand, India', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Titanic-Survival-Machine-learning-project-using-Logistic-Regression\n'], 'url_profile': 'https://github.com/himan005', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Suryadeep Guha\nUSF email address: sguha2@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Soo Jung Kim\nUSF email address: skim104@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Aarthi Parthipan \nUSF email address:  aparthipan@dons.usfca.edu \n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: S.Masoud Hosseini\nUSF email address: shosseini@usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Real-Estate-Price-Prediction\nReal Estate Price Prediction - using Advanced Linear Regression Techniques\nOverview\nThere are several factors that influence the price a buyer is willing to pay for a house. Some are apparent and obvious and some are not. Nevertheless, a rational approach facilitated by machine learning can be very useful in predicting the house price. A large data set with 79 different features (like living area, number of rooms, location etc) along with their prices are provided for residential homes in Ames, Iowa. The challenge is to learn a relationship between the important features and the price and use it to predict the prices of a new set of houses.\n'], 'url_profile': 'https://github.com/karthik0036', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Machine Learning-Internship Project\nworldhappinessreport2019\nlinear regression to solve world-happiness-report-2019\n'], 'url_profile': 'https://github.com/kavyashreekp12', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Machine-Learning-Regression\nThe repository solves the regression problem of Machine Learning. It was created from the book Hands-on Machine Learning with Scikit-Learn, Keras & Tenserflow by Aurelien Geron. (GitHub)\nIt consists of:\n\nData Collection\nVisualization of Data\nPreparing the data for ML Algorithm\nSelecting and Training the Model\nFine Tuning the Model\nEvaluation of the System on Test Set\n\n'], 'url_profile': 'https://github.com/megam5', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Time_Series_and_Regression\nFINTECH Homework on Time Series analysis and Regression analysis.\n'], 'url_profile': 'https://github.com/exposuretherapy', 'info_list': ['Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:\nUSF email address:\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Armen Khachatrian\nUSF email address:  akhachatrian@dons.usfca.edu\nNote: There are two approaches presented; Statistical way shows how data and model should been analysed\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Lab 05 Regression\nName: Domingo Huang\nUSF email address: yhuang158@dons.usfa.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'Gurgaon, Haryana, India', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karansehgal1988', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['Real-or-Not-NLP-with-Disaster-Tweets\nProject Name: NLP DISASTER TWEETS: EDA, NLP, TENSORFLOW, KERAS\nProblem Description:\nSentiment Analysis of the dataset of twitter disaster tweets and predicting\n\xa0 Actual Disaster\n\xa0 Metaphorically Disaster\nTable of Contents:\nIntroduction\nLibraries\nLoading Data\nExploratory Data Analysis\n\u2003\xa0 Analyzing Labels\n\u2003 \xa0Analyzing Features\n\u2003 \u2003 \xa0\xa0Sentence Length Analysis\nData Cleaning\n\u2003 \xa0 Remove URL\n\u2003\xa0 Handle Tags\n\u2003\xa0 Handle Emoji\n\xa0\u2003 Remove HTML Tags\n\xa0\u2003 Remove Stopwords and Stemming\n\xa0\u2003 Remove Useless Characters\n\xa0\u2003 WORLDCLOUD\nFinal Pre-Processing Data\nMachine Learning\n\u2003\xa0 Logistic Regression\n\u2003\xa0 Navie Bayes\n\xa0\xa0\u2003 \u2003 Gaussian Naive Bayes\n\u2003\xa0\xa0 \u2003 Bernoulli Naive Bayes\n\u2003\xa0\xa0 \u2003 Complement Naive Bayes\n\u2003\xa0\xa0 \u2003 Multinomial Naive Bayes\n\u2003\xa0 Support Vector Machine (SVM)\n\u2003 \xa0\xa0\u2003 RBF kernel SVM\n\u2003\xa0\xa0 \u2003 Linear Kernel SVM\n\u2003\xa0 Random Forest\nDeep Learning\n\xa0\u2003 Single Layer Perceptron\n\xa0\u2003 Multi Layer Perceptron\n\xa0\xa0\u2003 \u2003 Model 1 : SIGMOID + ADAM\n\xa0\xa0\u2003 \u2003 Model 2 : SIGMOID + SGD\n\xa0\xa0\u2003 \u2003 Model 3 : RELU + ADAM\n\xa0\xa0\u2003 \u2003 Model 4 : RELU + SGD\n\xa0\xa0\u2003 \u2003 Model 5 : SIGMOID + BATCH NORMALIZATION + ADAM\n\xa0\xa0\u2003 \u2003 Model 6 : SIGMOID + BATCH NORMALIZATION + SGD\n\xa0\xa0\u2003 \u2003 Model 7 : RELU + DROPOUT + ADAM\n\xa0\xa0\u2003 \u2003 Model 8 : RELU + DROPOUT + SGD\nPre-requisites and Installation:\nThis project requires Python and the following Python libraries installed:\n\xa0\xa0\u2003NumPy\n\xa0\xa0\u2003Pandas\n\xa0\xa0\u2003Matplotlib\n\xa0\xa0\u2003scikit-learn\n\xa0\xa0\u2003Tensorflow\n\xa0\xa0\u2003Keras\n\nData Overview\nSize of tweets.csv - 1.53MB\nNumber of rows in tweets.csv = 11369\nFeatures:\n\xa0\xa0\xa0\xa0 id - a unique identifier for each tweet\n\xa0\xa0\xa0\xa0 text - the text of the tweet\n\xa0\xa0\xa0\xa0 location - the location the tweet was sent from (may be blank)\n\xa0\xa0\xa0\xa0 keyword - a particular keyword from the tweet (may be blank)\n\xa0\xa0\xa0\xa0 target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\nWordCloud\nWord Clouds are a visual representation of the frequency of words within a given tweets.\n\nResults\nKey Performance Index:\nMicro f1 score: Calculate metrics globally by counting the total true positives, false negatives and false positives. This is a better metric when we have class imbalance.\nMacro f1 score: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\nMicro-Averaged F1-Score (Mean F Score): The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\nF1 = 2 (precision recall) / (precision + recall)\nAll the models are compared on the basis of Accuracy, Precision, Recall, F1-Score, Time. \n\nBest Performing Models are: - Support Vector Machine, Deep Learning(Relu + Adam), Deep Learning(Relu + Adam + Dropouts)\nConclusion\nDeep Learning Models are easy to overfit and underfit.\nDo not underestimate the power of Machine Learning techniques.\nRelu and Adam with Dropout proved to best as expected.\nSVM is still the best as far as accuracy and training time is concerned.\nReferences:\n\nhttps://www.kaggle.com/vbmokin/nlp-with-disaster-tweets-cleaning-data\nhttps://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\nhttps://machinelearningmastery.com/\n\n'], 'url_profile': 'https://github.com/erYash15', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['linear-regression\ncalculate the area\n'], 'url_profile': 'https://github.com/venkatachalam13', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hjkl3692008', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/orbenishay', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AkhilRautela', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['LinearRegression\n简单实现了最小二乘法，对随机生成的平面二维点作线性回归\n运行参数\n\n环境:Spyder（Python3.7）\n依赖库：Numpy、Matplotlib\n操作系统：Windows7旗舰版，64位\nCPU：Intel Corei7-5500U 2.4GHz\n\n\n对随机生成的100个点，运行效果如下：\n\n'], 'url_profile': 'https://github.com/DerrickChiu', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'R', 'Updated Mar 15, 2020', '2', 'Jupyter Notebook', 'Updated Jan 31, 2021', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020']}"
"{'location': 'Gainesville', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amitbhadra', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CarolynMassa', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/thuylinh3061998', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['BDA-LogisticReg-LowBirthWeight\n'], 'url_profile': 'https://github.com/rutula30', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sangajala', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cjancsar', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LinearRegression\nRegressâo Linear\n'], 'url_profile': 'https://github.com/jaimeberg', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Analysis\nUnivariate Analysis on frahmingham data using SAS\nAssess the relationship between Systolic Blood PRessure and Age of the people by performing simple linear regression model using SAS\nDevelop Regression ,odel from scratch using Proc IML in SAS\nPerform Multiple Regression model....\n'], 'url_profile': 'https://github.com/nenduru1', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/acouturi', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Advanced_regression\nOneHotEncoding,Standard_scaler,PCA, Imputer(mean), Lasso_regression is used.\n'], 'url_profile': 'https://github.com/Rishabhsingh715', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Aug 21, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Analysis\nUnivariate Analysis on frahmingham data using SAS\nAssess the relationship between Systolic Blood PRessure and Age of the people by performing simple linear regression model using SAS\nDevelop Regression ,odel from scratch using Proc IML in SAS\nPerform Multiple Regression model....\n'], 'url_profile': 'https://github.com/nenduru1', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/acouturi', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/roesenerm', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'Çankaya / ANKARA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ferdiakdogan', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['Linear-Regression\nBasic Functions on Linear Model\nTrain Set and Test Set taken from kaggle.\nProject employs various Plots deccribing their various plots.\nSteps involved\n\nImport data set\nClean Data set\nEmploy Linear REgression Model\nFind F statistics\n\n'], 'url_profile': 'https://github.com/ananyasrivastav', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear-Regression\nHere you get the idea about how to work with case study on Linear Regression Problems.\n'], 'url_profile': 'https://github.com/akki999', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['regression_exercise\nThis exercise is split into two parts. We will work on part one today. Part 2 is dedicated to regularization and gradient descent which we can keep for the next day.\nDay 1\n\npolynomial_regression_exercise.ipynb\n\nDay 2\n\ngradient_descent_challenge.ipynb\n\n'], 'url_profile': 'https://github.com/lighthouse-labs', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'Gothenburg, Sweden', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Waldemar-DataScience', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['randomforest_regression\n基于随机森林的回归模型\n'], 'url_profile': 'https://github.com/zishengwu', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['House Price Regression Model\nThis R script uses the classic Ames Housing dataset to illustrate a full\nregression modeling project.\nCode Sections\n\nCreate Target Population\nTreat Missing Values\nCorrelation\nOutlier Treatment\nDeep Dive into 3 Highly Correlated Variables\nTrain/Test Split\nAutomated Feature Selection (Forward, Backward, Stepwise)\nModel Results (VIF, R-squared, AIC, BIC, MSE, RMSE, MAE)\nValidation\nReduce Model Complexity\nDiagnostic Analysis\n\n'], 'url_profile': 'https://github.com/ryanjclark', 'info_list': ['Updated Feb 22, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'R', 'Updated Mar 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mar477', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Regression linéaire simple\n'], 'url_profile': 'https://github.com/Chamroukhi', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'Mandi, India', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Regression Analysis on the Boston Housing Prices Dataset\nA google colab Notebook for the regression analysis of the Boston Housing prices dataset using the Keras API in python.\n1. The Dataset\nThe Boston housing prices is a small dataset containing 506 cases with 13 training attributes describing the houses and the median house price in thousand dollars as the target attribute.\n\n2. The Model\nFor this dataset a Shallow Neural network gave better results as compared to a deep network due to its small size. On training the network the mean absolute error on the test data was ~2.5k dollars which  is an acceptable error in the case of housing prices.\n\n'], 'url_profile': 'https://github.com/Sarthakj0805', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'Hillsboro, Oregon', 'stats_list': [], 'contributions': '577 contributions\n        in the last year', 'description': ['linear_regression\nImplementation of a  linear regression algorithm in various languages.\n'], 'url_profile': 'https://github.com/rahulunair', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wangxubuaa', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Linear Regression\nIn this notebook, we learn how to use scikit-learn to implement simple linear regression. We have an open-source IBM dataset that is related to fuel consumption and Carbon dioxide emission of cars. We split our data into training and test sets, create a model using training set, evaluate your model using test set, and finally use model to predict unknown values.\n'], 'url_profile': 'https://github.com/sourabhamancha', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/stefen78', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': ['Linear-Regression\nSimple linear regression is useful for finding relationship between two continuous variables. One is predictor or independent variable and other is response or dependent variable. It looks for statistical relationship but not deterministic relationship. Relationship between two variables is said to be deterministic if one variable can be accurately expressed by the other. For example, using temperature in degree Celsius it is possible to accurately predict Fahrenheit. Statistical relationship is not accurate in determining relationship between two variables. For example, relationship between height and weight.\n'], 'url_profile': 'https://github.com/Satnam00', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prateekkohli1992', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Zig', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'HTML', 'Updated Mar 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['multilinear_regression\n'], 'url_profile': 'https://github.com/kansusha', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Memphis, TN', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['Logistic-Regression\nThis is a sample notebook to practice creating a logistic regression model to predict customer churn using hypothetical customer data\n'], 'url_profile': 'https://github.com/zachcornelison', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Logistic-Regression\nProject from graduate computer science course in Machine Learning\n'], 'url_profile': 'https://github.com/bill12m', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Montréal', 'stats_list': [], 'contributions': '245 contributions\n        in the last year', 'description': [""Regression_polynomiale\nDataset salary en fonction du nombre d'année d'experience\n""], 'url_profile': 'https://github.com/mbiombani', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '271 contributions\n        in the last year', 'description': [""Least Squares and Maximum Likelihood for Linear Regression\nSolving the linear regression problem using various techniques.\n\nAbove image depicts the different results produced from the different techniques.\nWarning: Minimizing the Euclidean Distance(TLS) method can take pretty long.\n\n\nOrdinary Least Squares\n\n\nMinimizing the square differences between values of estimated Y and observed Y. Solved using gradient descent, update rule is as following.\n\nCode accepts a dataset matrix(which can be multidimensional), a targets matrix(only supports single dimensional targets) and returns a vector of weights and a matrix of predicted Y's.\nfrom LRmodule import errorGradient\nregression = errorGradient.ErrorGradient(Dataset,Targets,verbose=[True/False])\nweights, predicted_y = regression.run()\n\n\nMinimizing Euclidean Distances | Total Least Squares\n\n\nMinimizing the square of Euclidean Distances between the data-points and the hyperplane, a form of Total Least Squares. Solved using gradient descent, update rule is as following.\n\nCode has the same properties as before.\nfrom LRmodule import euclideanGradient\nregression = euclideanGradient.EuclideanGradient(Dataset,Targets,verbose=[True/False])\nweights, predicted_y = regression.run()\n\n\nMaximum Likelihood\n\n\nForming a closed form solution by calculating maximum likelihood from the log-likelihood of the sum of squared errors. Weights are be calculated according to the formula below.\n\nCode has the same properties as before.\nfrom LRmodule import wML\nregression = wML.WML(Dataset,Targets,verbose=[True/False])\nweights, predicted_y = regression.run()\n\n\nCalculating R-Squared\n\n\nRepository also contains a module to calculate r-squared with.\nfrom rsquare import r2\nr = r2.r2()\nvalue = r.calculate(Predicted_Y, Original_Y)\nmain.py contains demo code which creates regression using sklearn make_regression to test the above programs with. Also compares the r-squared values and plots graphs depicting and comparing the predicted linear equations(Only if using 1 feature).\n""], 'url_profile': 'https://github.com/mnk400', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Europe', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['MLProject-Week1\nMachine Learning MGI Project for week 1. Estimation of the chlorophyll content from a hyperspectral image using linear regression and K-NN regression\n'], 'url_profile': 'https://github.com/Calyppe', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/godblesszhouzhou', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Winter Springs, FL', 'stats_list': [], 'contributions': '486 contributions\n        in the last year', 'description': ['LogisticRegression\nThis directory is used for assignment 03 of data mining and covers logistic regression.\n'], 'url_profile': 'https://github.com/darkhark', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression-analysis\nMultivariate Regression on Excess US Stock Returns\n*For Illustrative Purposes only\nBased on Level II of the CFA Curriculum(Volume 1, Quantitative Methods, page 389)\nAdditional Credit to the following links: https://towardsdatascience.com/simple-and-multiple-linear-regression-with-python-c9ab422ec29c\nhttps://www.statsmodels.org/stable/regression.html\nDependent Variable(y): Excess Nasdaq 100 One-Month Stock Returns = One-Month Nasdaq 100 Stock Returns minus One-Month T-Bill\nIndependent Variables(x):\nDefault Spread(monthly), t-1 = Previous Months Yield on AA Bonds minus yield on AAA Bonds\nTerm Spread(monthly), t-1 = Previous Months Yield on 10 Yr US Treasury minus Yield on 1 YR US Treasury\nPresidential Party Dummy Variable(monthly), t-1: 1 if President is Democrat, 0 if Republican\n'], 'url_profile': 'https://github.com/brettboles', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jayanthd26', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'Python', 'MIT license', 'Updated Sep 11, 2020', '1', 'R', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rosairis98', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['factbook_regressionmodel\nDesigned a machine learning regression model that determines overall performance of countries and  categorized them by developed, underdeveloped and developing countries by predicting the GDP per capita.\n'], 'url_profile': 'https://github.com/Yasaswini2105', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'Viet Nam', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['LinearRegression\nBasic about LinearRegression\n'], 'url_profile': 'https://github.com/phamthephuc', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'Stockholm', 'stats_list': [], 'contributions': '496 contributions\n        in the last year', 'description': ['BFM-Regression\nAccording to the World Health Organization, obesity is a major risk factor for several chronic diseases, including cardiovascular diseases and cancer. In order to operationalize obesity, one can use the body fat mass (BFM). The problem with this is that the highly accurate methods of doing this is expensive. Some simpler measurements are known to be related to the BFM, such as waist circumference and waist-to-hip-ratio. We have access to a dataset with BMF measurements (or more precisely, body density measurements) on males and corresponding measurements such as height, weight and age. Our main goal is to develop and validate our own regression model for prediction of the BFM. This includes through residual analysis, handling outliers, analyzing occurrences of multicolinearity and variable selection methods using different performance criteria.\nCode\nThe code of our analysis can be found in project1.R. The help functions and data sets used are located in the same directory.\n'], 'url_profile': 'https://github.com/Jos3f', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CarolynMassa', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/slumfy', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'Europe', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['MLProject-Week1\nMachine Learning MGI Project for week 1. Estimation of the chlorophyll content from a hyperspectral image using linear regression and K-NN regression\n'], 'url_profile': 'https://github.com/Calyppe', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'Baltimore, MD', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Running a Genome Wide Exact Logistic Regression\nThis repository include several scripts used on the article ""The genetic architecture of the susceptibility to Congenital Zika syndrome in Brazilians"".\nThe Running_ELR_ver5 code is an updated version to run a genome-wide exact logistic regression.\nCurrently is running for just one covariate.\nTo run the code use the following command line\nRun using: ""R < ELRrun.R raw.plink.filename chromosome-number plinkbimfile output.filename total_number_of_MCMC Burnin covar_file --no-save > screen_output""\nTo run this program you need al least six parameters:\n-raw plink file       : Raw plink file obtaned with --recode A on plink, include genotype information\n-chromosome number    : [0,..,22] a number that select a subset of genotypes\n-plink bim file       : Bim file of the PLINK binary format of the same dataset\n-output.filename      : String corresponding to output name\n-total number of MCMC : Total of iterations to run including burnin\n-Burnin               : Total number of iterations for burnin iterations\n-cov file             : Covariate file (with header and TAB delimited), if included, the script will automatically adjust the regression\n-IMPORTANT 1          : YOU NEED TO INSTALL THE ELRM PACKAGE!!\n-IMPORTANT 2          : Currently, this version can adjust just for one covariate !!\n-IMPORTANT 3          : The covariate needs to be a discrete variable!!\n'], 'url_profile': 'https://github.com/vicbp1', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['tea-sales-regression\nAnalysed associations and variations that drive sales, created Multivariable Regression model in R, applied K-Fold Cross Validation and evaluated robustness of model and regression assumptions.\nDescription:\nA manufacturer of a consumer goods brand wants to know how much additional sales they created that can be at least associated or explained by their three marketing activities:\nTV advertisements, online banner advertisements, and direct-mail sales promotions.\nData used: 3 years of monthly sales data for their product, as well as the average retail prices and the necessary advertising data, split separately into their 5 regional markets.\n'], 'url_profile': 'https://github.com/rframadanti', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kalutika', 'info_list': ['Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'R', 'MIT license', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Feb 4, 2021', '1', 'R', 'Updated Feb 21, 2020', 'R', 'Updated Feb 25, 2021', 'R', 'Updated May 22, 2020', 'JavaScript', 'Updated Feb 18, 2020']}"
"{'location': 'London, Ontario, Canada', 'stats_list': [], 'contributions': '38,113 contributions\n        in the last year', 'description': ['dli_image_regression\nNVIDIA IoT Deep Learning Institute image regression at the edge (IoT) project for Jetson Nano.\nThis project is meant to be run at the edge on an NVIDIA Jetson Nano (or similar ARM IoT architecture computer).\n\nRequirements\n\nJupyter Notebooks (AKA IPython)\nPytorch\nJetpack SDK or DLI Jetson Nano image (preferably the latter for headless mode compatibility)\nJetcam\nJupyter Clickable Image Widget\n\nRepository structure\ndata\nContains 360 images times 3 classes (total images = 1080) taken and used for training. The images were encrypted for privacy purposes.\nmodels\nContains different model iterations and are named after the total training images and epochs. The best performing model is the xy_360dpx3_45ep.pth model which was trained with 360 images per class and for a total of 45 epochs.\nnotebooks\nContains the Jupyter notebook used to interactively train and evaluate an emotion prediction (up or down) model using deep learning and PyTorch.\nsrc\nStandard src scripts and utilities plus a folder structure generator script.\nUsage\nUse the interactive notebook from the notebooks folder and train your own model or load one already made from the models folder and evaluate how effective it is using your camera. The model will output xy coordinates (in the form of a circle) where it thinks it found the selected face feature class (only one at a time).\nLicense\nMIT License\n'], 'url_profile': 'https://github.com/socd06', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['ml-workshop-regression-task\n'], 'url_profile': 'https://github.com/lbuklya', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '183 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arparker5', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Abdoulaye01', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Linear-Regression-GellyAuto\nTo predict which variables are significant in predicting the price of a car and well those variables describe the price of a car\n'], 'url_profile': 'https://github.com/RR93-user', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['SVR-Linear-Regression-Model\n'], 'url_profile': 'https://github.com/n8slayer', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Logistic-Regression-practice-examples\nLogistic regression using logit as the model for a dataset full of categorical data\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/baptistemokas', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Alexcei', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 5, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}"
"{'location': 'Dhaka,Bangladesh', 'stats_list': [], 'contributions': '1,021 contributions\n        in the last year', 'description': ['RandomForestRegression-AI\n'], 'url_profile': 'https://github.com/MdShahadatHossainbd', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anoopreddy524', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['SL_Regression_Test-Release-\n'], 'url_profile': 'https://github.com/wq8231', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manoj2306', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieljohnson18', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'Johannesburg,ZA', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/boitshepo97', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Learning-Forest-Fires\nA project to learn about the causes of forest fires as part of Artificial Intelligence course.\n'], 'url_profile': 'https://github.com/Harshdeeprana', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'LA', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexandertrinh', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CarlsonZhuo', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ntokozomfene', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Java', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '225 contributions\n        in the last year', 'description': ['Logistic-Regression-with-Python\nIn this notebook, we create a model for a telecommunication company, to predict when its customers will leave for a competitor, so that they can take some action to retain the customers.\n'], 'url_profile': 'https://github.com/guneetdeol', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/berkaydoner', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yash956', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Salary-Analysis-Linear-Regression\nSalary-Multilinear-Regression\n'], 'url_profile': 'https://github.com/Wonder13oy', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '449 contributions\n        in the last year', 'description': [""baseball_HR_linear_regression\nProject Goals\n- The goal of this project is to use linear regression models to predict total home runs for 2020. \nIn order to account for possible player injuries that may limit their plate appearances, \nwe engineered our dependent variable, Home Runs per Plate Appearance. \n\nData Sources\n- BaseballSavant.mlb.com\n- Baseball-Reference.com\n\nData Summary\n- Using datasources mentioned above, we collected player statistics from the 2015 season through 2018. \nThen we concatenated all four datasets into one master training set that we would use to train our regression model. \nAfter cleaning the data, we had a total of 854 observations and 54 possible independent variables. \n\nTools\n- Pandas\n- SKLearn\n- Ridge, Lasso, Linear Regression Models\n- Standard Scaler\n- Polynomial Features\n\nModeling Process\n- Selecting the most appropriate and impactful feature variables for our model consisted of several key processes.\n    - A Correlation Heatmap to avoid using variables that are highly correlated with each other\n    - Using scatter plots to show the relationship between potential independent variables and Home Runs\n    - Using Ridge and Lasso with multiple lambda values\n    - Transforming our training set into polynomial features\n    - Linear Regression\n    - StandardScaler\n    - Trial and Error\n\nFinal Model\n- Our best fitted model ended up being a basic linear regression with a lambda value of zero. \nThe primary metric that was used to determine the model with the best fit is RMSE (Root Mean Squared Error), which had a value of .6723 . \n\nResults\n- After merging our final predictions with our 2019 dataset, we conducted a spot check of random players to compare our predicted Home Run values with predictions conducted on FanGraphs.com. We noticed that the better the player, the further our predictions were off by. These findings coincide with our residual error plot which shows a funnel-shape distribution; as Home Runs increase, so does our variance in residuals. \n\nWays to Improve Our Model\n- more player observations\n- Need to take into account difference in ballpark size and the Juice Ball of 2019\n- Because we only used data from 2019 to predict results for 2020, a player having a 'bad' year by chance could skew our results.\n\n""], 'url_profile': 'https://github.com/jacksonbull87', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'Bangalore, Karnataka', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Linear-regression-on-datasets\nLinear Regression is a machine learning algorithm based on supervised learning. ... Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output).\n'], 'url_profile': 'https://github.com/BhargavBollineni', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'indonesia', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/muhammadfarrel', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bmagnificent86', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Here you will find files, Source code and resource links to all things regression mod related.\n'], 'url_profile': 'https://github.com/regressmods', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression\nIndividual project on predicting house price using the dataset from Kaggle\n'], 'url_profile': 'https://github.com/jaypark9524', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Linear-Regression-part-1\nThis is a part 1 of the simple linear regression of machine learning course.\n'], 'url_profile': 'https://github.com/DeepModiDev', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['EDA-and-Regression\nAnalyzing attrition rate using IBM data set in R\nDataset link: https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/data#WA_Fn-UseC_-HR-Employee-Attrition.csv\nScope: How does Attrition affect companies? and how does HR Analytics help in analyzing attrition? A major problem in high employee attrition is its cost to an organization. Job postings, hiring processes, paperwork and new hire training are some of the common expenses of losing employees and replacing them.\nObjective: To investigate how the company objective factors influence in attrition of employees, and what kind of working environment is most likely to cause attrition\nBusiness Problem Statement:Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.\nAnalytics Approach I shall be looking at all variables through some plots and infer about it in my exploratory analysis. And through my exploration I shall try to identify the Variables that tend to have an impact in the attrition of the most experienced and talented employees and try to fit a classification models and use it to test hypotheses and draw inferences.\nThe problem will be explored in following stages:\nData Exploration (EDA) – looking at categorical and continuous variables summaries and making inferences about the data.\nData Cleaning – imputing missing values in the data and checking for outliers and eliminating insignificant Variables form data set, scaling.\nData Splitting – Randomly splitting the data set into train (80%) and test (20%) data sets.\nFeature Engineering – modifying existing variables and creating new ones for analysis.\nModel Building – making predictive models on the train data and testing it.\n'], 'url_profile': 'https://github.com/akshaygidh', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abdennouraissaoui', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['Titanic-Logistic-Regression\nPredict a classification- survival or deceased\nI used Pandas for data analysis and Matplotlib/seaborn for data visualization.\nCleaned data and fitted a logistic regression model and used it to classify survival. Finally, I evaluated the results using an evaluation matrix.\nUdemy course - Python for Data Science and Machine Learning Bootcamp\n'], 'url_profile': 'https://github.com/nrepesh', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Machine-Learning---Linear-Regression\nContains Week 2 solutions for Andrew Ng's Linear Regression Assignment from Coursera. Solutions are implemented in python.\n""], 'url_profile': 'https://github.com/chex2chex', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '326 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Iffigues', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/at17556', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/c0dewithMAK', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Regression Based Analysis\nBuilding a model to predict how much revenue to expect from each customer within their first year of orders.\nTo view Python code:\nhttps://juanduranc.github.io/sites/Python%20RegressionAnalysis.html\nBusiness Insights pdf file:\nhttps://github.com/juanduranc/Python-Regression-Based-Analysis/blob/master/Business%20Insights%20Regression%20Analysis.pdf\n'], 'url_profile': 'https://github.com/juanduranc', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Go', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 20, 2020']}"
"{'location': 'Johannesburg', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['MULTIVARIATE-LINEAR-REGRESSION\n'], 'url_profile': 'https://github.com/codeART96', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manoj2306', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieljohnson18', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PadmaPriyaJayaraj', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/universe1987', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['MultipleLinearRegression\n'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': [""Advertisement-Logistic-Regression\nPredict a classification- internet user clicked on an Advertisement or not\nI used Pandas for data analysis and Matplotlib/seaborn for data visualization.\nCleaned data and fitted a logistic regression model and used it to classify survival. Finally, I evaluated the results using an evaluation matrix.\nUdemy course - Python for Data Science and Machine Learning Bootcamp\nThis data set contains the following features:\n'Daily Time Spent on Site': consumer time on site in minutes\n'Age': cutomer age in years\n'Area Income': Avg. Income of geographical area of consumer\n'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n'Ad Topic Line': Headline of the advertisement\n'City': City of consumer\n'Male': Whether or not consumer was male\n'Country': Country of consumer\n'Timestamp': Time at which consumer clicked on Ad or closed window\n'Clicked on Ad': 0 or 1 indicated clicking on Ad\n""], 'url_profile': 'https://github.com/nrepesh', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['predicting through polynomial regression\nlearn in python\n'], 'url_profile': 'https://github.com/rishisankhla', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/darkcon3000', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/abdennouraissaoui', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Logistic-regression_1\nMy 1st project on Logit - Number classification\n'], 'url_profile': 'https://github.com/ganguly-cloud', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Espoo, Finland', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Logistic-Regression-in-Advertisement\nPredicting if the customer will click on the advertisement displayed on Facebook or not\n'], 'url_profile': 'https://github.com/mesushan', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Non-negative-linear-regression\nthe definition of data\nFor the input query and returned document pairs (we denote (q,d)), we calculated 14 physically relevant correlation values. The larger the value of each of the 14 features, the higher the correlation of (query, document) pair, that is, the dependent variable(""label2"" in data/EngineRelevance.csv) have positive correlation with all independent variables. Therefore, this question is Non-Negative Linear Regression Problems.\nindependent variables name is:\nqhit_term_count:    hits number for recall documentn in recall query\n\nqhit_term_ratio:    hits ratio for recall document segmentation in query\n\nqhit_term_weight:   hits weight for recall document segmentation in query\n\nqhit_term_weight2:  hits weight for recall document segmentation in query\n\nqhit_term_weight_core: hits weight for core recall document segmentation in query\n\ndhit_term_ratio:    hits ratio for query segmentation in document\n\ndhit_term_weight:   hits weight for query segmentation in document\n\ndhit_term_weight2:  hits weight for query segmentation in document\n\ndhit_term_weight_core: hits weight for core query document segmentation in document\n\njaccard: jaccard similarity between query and document\n\njaccard_qweight: similarity between query and document using query segmentation weight\n\njaccard_dweight: similarity between query and document using document segmentation weight\n\ntfidf: tfidf value for query and dicument\n\ntfidf_norm: normalized tfidf value for query and dicument\n\nbm25： bm25 value between query and dicument\n\nbm25_norm: normalized tfidf value between query and dicument\n\ndependent variables name is:\nlabel2: whether relevant or not?\n\nIn the search sorting scene, in order to facilitate comparison, we chose AUC and RMSE for comparison.\nBy running the code ""Non_negative_regression_demo.py"". The readers will get the following results.\n [least square (LS)], the coefficients are:\n   [ 0.01245128 -0.40390261 -0.0191926  -0.52963571  0.20836933  0.21884406\n   -0.05320568 -0.02270426  0.11965694 -0.57252524  0.48290015  0.00831857\n   -0.00658889 -0.20222227  0.020237    1.11454091]\n\n [non-negative least square (NLS)], the coefficients are:\n    [0.01245128 0.         0.         0.         0.20836933 0.21884406\n     0.         0.         0.11965694 0.         0.48290015 0.00831857\n     0.         0.         0.020237   1.11454091]\n\n [non-negative linear regression (NNLR)], the coefficients are:\n    [0.00090505 0.05289951 0.09156436 0.05211574 0.10416422 0.00429807\n     0.00641059 0.00409428 0.00968512 0.00114789 0.07732478 0.00118315\n     0.0019383  0.06258468 0.0103732  0.13788792]\n\n [gradient truncation (GT)], the coefficients are:\n    [0.04793231 0.08881607 0.08788822 0.08778685 0.08748704 0.09209439\n     0.09177271 0.09185865 0.09140391 0.09384025 0.08980766 0.09344165\n     0.         0.08790583 0.         0.08829649]\n\n method    auc       rmse\n  LS     0.851463  0.200439\n  NLS    0.841941  0.529146\n  NNLR   0.843245  0.204014\n  GT     0.823391  0.217370\n\n'], 'url_profile': 'https://github.com/andrew-pengjj', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Johannesburg', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ThembaMbulwana', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['.csv file contains one year of transactional data.\nA linear model was developed to predict a continuous variable of daily sales.\nMean absolute error was used to determine the best performing predictor,which turned out to be daily prospects (mean absolute error= 225)\n'], 'url_profile': 'https://github.com/DCPhD', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Banglore,Karnataka,India.', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Supervised-Learning---Regression\n'], 'url_profile': 'https://github.com/Ajaygorkar', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Week5--Regression\nThis repositoy contains the input files for the regression problem in class.\n'], 'url_profile': 'https://github.com/PM520-Spring-2020', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mstrick7', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Logistic-Regression-Project\nThis project is a simple analysis of the different factors affecting advertisement effectiveness.\n'], 'url_profile': 'https://github.com/ZakariaPZ', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}"
"{'location': 'Espoo, Finland', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Logistic-Regression-in-Advertisement\nPredicting if the customer will click on the advertisement displayed on Facebook or not\n'], 'url_profile': 'https://github.com/mesushan', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Non-negative-linear-regression\nthe definition of data\nFor the input query and returned document pairs (we denote (q,d)), we calculated 14 physically relevant correlation values. The larger the value of each of the 14 features, the higher the correlation of (query, document) pair, that is, the dependent variable(""label2"" in data/EngineRelevance.csv) have positive correlation with all independent variables. Therefore, this question is Non-Negative Linear Regression Problems.\nindependent variables name is:\nqhit_term_count:    hits number for recall documentn in recall query\n\nqhit_term_ratio:    hits ratio for recall document segmentation in query\n\nqhit_term_weight:   hits weight for recall document segmentation in query\n\nqhit_term_weight2:  hits weight for recall document segmentation in query\n\nqhit_term_weight_core: hits weight for core recall document segmentation in query\n\ndhit_term_ratio:    hits ratio for query segmentation in document\n\ndhit_term_weight:   hits weight for query segmentation in document\n\ndhit_term_weight2:  hits weight for query segmentation in document\n\ndhit_term_weight_core: hits weight for core query document segmentation in document\n\njaccard: jaccard similarity between query and document\n\njaccard_qweight: similarity between query and document using query segmentation weight\n\njaccard_dweight: similarity between query and document using document segmentation weight\n\ntfidf: tfidf value for query and dicument\n\ntfidf_norm: normalized tfidf value for query and dicument\n\nbm25： bm25 value between query and dicument\n\nbm25_norm: normalized tfidf value between query and dicument\n\ndependent variables name is:\nlabel2: whether relevant or not?\n\nIn the search sorting scene, in order to facilitate comparison, we chose AUC and RMSE for comparison.\nBy running the code ""Non_negative_regression_demo.py"". The readers will get the following results.\n [least square (LS)], the coefficients are:\n   [ 0.01245128 -0.40390261 -0.0191926  -0.52963571  0.20836933  0.21884406\n   -0.05320568 -0.02270426  0.11965694 -0.57252524  0.48290015  0.00831857\n   -0.00658889 -0.20222227  0.020237    1.11454091]\n\n [non-negative least square (NLS)], the coefficients are:\n    [0.01245128 0.         0.         0.         0.20836933 0.21884406\n     0.         0.         0.11965694 0.         0.48290015 0.00831857\n     0.         0.         0.020237   1.11454091]\n\n [non-negative linear regression (NNLR)], the coefficients are:\n    [0.00090505 0.05289951 0.09156436 0.05211574 0.10416422 0.00429807\n     0.00641059 0.00409428 0.00968512 0.00114789 0.07732478 0.00118315\n     0.0019383  0.06258468 0.0103732  0.13788792]\n\n [gradient truncation (GT)], the coefficients are:\n    [0.04793231 0.08881607 0.08788822 0.08778685 0.08748704 0.09209439\n     0.09177271 0.09185865 0.09140391 0.09384025 0.08980766 0.09344165\n     0.         0.08790583 0.         0.08829649]\n\n method    auc       rmse\n  LS     0.851463  0.200439\n  NLS    0.841941  0.529146\n  NNLR   0.843245  0.204014\n  GT     0.823391  0.217370\n\n'], 'url_profile': 'https://github.com/andrew-pengjj', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': [""\n\n\nA progressive Node.js framework for building efficient and scalable server-side applications, heavily inspired by Angular.\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescription\nNest framework TypeScript starter repository.\nInstallation\n$ npm install\nRunning the app\n# development\n$ npm run start\n\n# watch mode\n$ npm run start:dev\n\n# production mode\n$ npm run start:prod\nTest\n# unit tests\n$ npm run test\n\n# e2e tests\n$ npm run test:e2e\n\n# test coverage\n$ npm run test:cov\nSupport\nNest is an MIT-licensed open source project. It can grow thanks to the sponsors and support by the amazing backers. If you'd like to join them, please read more here.\nStay in touch\n\nAuthor - Kamil Myśliwiec\nWebsite - https://nestjs.com\nTwitter - @nestframework\n\nLicense\nNest is MIT licensed.\n""], 'url_profile': 'https://github.com/cjancsar', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['simple-linear-regression\n'], 'url_profile': 'https://github.com/kansusha', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'Italy', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SimoneDut', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '149 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sanketughadmathe', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/WartimeHotTot', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Logistic-regression_2\nMy second project on titanic dataset\n'], 'url_profile': 'https://github.com/ganguly-cloud', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/utkarsh027', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}","{'location': 'Iselin', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yashvyas95', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 6, 2020', 'TypeScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/schrodinger007', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Maldives', 'stats_list': [], 'contributions': '305 contributions\n        in the last year', 'description': ['Coursework on Generalized Linear models for the module SF2930 Regression Analysis @ KTH Royal Institute of Technology\nImplemented in R.\nOnline notebook demo available on binder here where all the package dependencies are handled via the install.R file.\n'], 'url_profile': 'https://github.com/mnazaal', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Contrastive_regression_analysis\n'], 'url_profile': 'https://github.com/vivianchen98', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/siddharthabingi', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Gainesville', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amitbhadra', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xdong22', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '2,346 contributions\n        in the last year', 'description': ['array-proxy-regression\nThis README outlines the details of collaborating on this Ember application.\nA short introduction of this app could easily go here.\nPrerequisites\nYou will need the following things properly installed on your computer.\n\nGit\nNode.js\nYarn\nEmber CLI\nGoogle Chrome\n\nInstallation\n\ngit clone <repository-url> this repository\ncd array-proxy-regression\nyarn install\n\nRunning / Development\n\nember serve\nVisit your app at http://localhost:4200.\nVisit your tests at http://localhost:4200/tests.\n\nCode Generators\nMake use of the many generators for code, try ember help generate for more details\nRunning Tests\n\nember test\nember test --server\n\nLinting\n\nyarn lint:hbs\nyarn lint:js\nyarn lint:js --fix\n\nBuilding\n\nember build (development)\nember build --environment production (production)\n\nDeploying\nSpecify what it takes to deploy your app.\nFurther Reading / Useful Links\n\nember.js\nember-cli\nDevelopment Browser Extensions\n\nember inspector for chrome\nember inspector for firefox\n\n\n\n'], 'url_profile': 'https://github.com/patocallaghan', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Saarbrücken, Germany', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': ['ANOVA and Regression\nThis repository contains the exercises in SAS language I carried out in the subject ANOVA and Regression. Every exercise and assignment has been obtained from one of the following books:\n\nFREUND, R. J. - WILSON, W. J. - MOHR, D. Statistical methods. 3ª Ed. (2010)\nFREUND, R. J. - WILSON, W. J. - SA, P. Regression analysis. Statistical modeling of\na response variable. 2ª Ed. (2006)\nMONTGOMERY, D. C. - PECK, E. A. - VINING, G. G. Introduction to linear regression\nanalysis. 4ª Ed. (2006).\nMONTGOMERY, D. C. - PECK, E. A. - VINING, G. G. Introducción al análisis de\nregresión lineal. 3ª Ed (2006).\nNETER, J., KUTNER, M.H., NACHTSHEIM, C.J. y WASSERMAN, W. (2005). Applied\nLinear Statistical Models, McGrawHill.\nVILAR, J.M. (2006). Modelos estadísticos Aplicados. Univ. da Coruña\n\nThe comments explaining the followed procedure in the different exercises are in Spanish.\n'], 'url_profile': 'https://github.com/pabvald', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['Regression-Data-sets\nRegression is a statistical measurement used in finance, investing, and other disciplines that attempts to determine the strength of the relationship between one dependent variable (usually denoted by Y) and a series of other changing variables (known as independent variables)\n'], 'url_profile': 'https://github.com/shaik4182', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Tuticorin', 'stats_list': [], 'contributions': '501 contributions\n        in the last year', 'description': [""Linear_Regression_Fitting\nIt's has Linear Regression model fitting and shows the relationship between the Independent and Dependent feature.\n""], 'url_profile': 'https://github.com/Nivitus', 'info_list': ['Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Dec 26, 2020', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 21, 2020', 'JavaScript', 'Updated Feb 20, 2020', 'SAS', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'GPL-3.0 license', 'Updated Apr 11, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['advancedRegressionAssignment\nSurprise Housing House Price Prediction\nProblem Statement:\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below. The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know: Which variables are significant in predicting the price of a house, and How well those variables describe the price of a house.\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal\nYou are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for the management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Akshita1303', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'Peoria, IL , USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HJ-mhatre', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ktarungoud', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': [""House-Price-Prediction-regression\nThis is a Kaggle's competition which I have solved using few regression models.\nThe files contains:\n\nImporting data\nData cleaning and preprocessing\nData visualization\nModel Building\n\nThe link of this competition : https://www.kaggle.com/c/house-prices-advanced-regression-techniques\nIf there are any corrections, suggestions, improvements or any recommendations, please feel free to DM me or mail me at n.khadagade711@gmail.com\nThank you! :)\n""], 'url_profile': 'https://github.com/NeerajKhadagade29', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['R_linear-regression\nBasic concepts about linear regression.\n'], 'url_profile': 'https://github.com/Yimei120', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'Syracuse, NY', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ezair', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/paras-yadav1', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganguly-cloud', 'info_list': ['Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ZakariaPZ', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['DataAS\n'], 'url_profile': 'https://github.com/MajidAzmi', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': ['TFIDF_Logistic-Regression\nBuilt Logistic Regression model to predict the sentiment of the tweet as good or bad\nUsed the libraries pandas dataframes to handle the dataet.\nFound out the best theta value using the cross validation.\nUsed the theta values which generated best accuracy to predict the sentiment.\n'], 'url_profile': 'https://github.com/NavyaShiva', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/paras-yadav1', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganguly-cloud', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['crossValidationwithLinearRegression\nThis notebook demonstrates how to do cross-validation (CV) with linear regression as an example (it is heavily used in almost all modelling techniques such as decision trees, SVM etc.). We will mainly use sklearn to do cross-validation.\nThis notebook is divided into the following parts:\nExperiments to understand overfitting\nBuilding a linear regression model without cross-validation\nProblems in the current approach\nCross-validation: A quick recap\nCross-validation in sklearn:\n4.1 K-fold CV\n4.2 Hyperparameter tuning using CV\n4.3 Other CV schemes\n'], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['Simple_linear_regression\n'], 'url_profile': 'https://github.com/pawarharshit', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['SL_Regression_Test-Release\n'], 'url_profile': 'https://github.com/wq8231', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}","{'location': 'Varanasi', 'stats_list': [], 'contributions': '308 contributions\n        in the last year', 'description': ['Prediction of Forest Fires\n""Helping out my room-mate Harshdeep with his final year project!""\n\nDocumentation is covered in Thesis only.\n\nPRESENTATION\nTHESIS\n'], 'url_profile': 'https://github.com/aryanc55', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 18, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/siddharthabingi', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'Peoria, IL , USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HJ-mhatre', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Ship Valuation using Multiple Linear Regression\nUsed R for valuation of a ship. The estimated price came out to be ~130 million USD as per the model.\nInitial data exploration done using correlation analysis. Feature Extraction done to remove multi collinearity. BIC metric used for model validation. R squared ~90%.\nHypothesis\nOn eyeballing the ship data, we found that some variables had a greater impact on its price while some could be ignored for calculat-ing the ship price.\nSome initial findings:\n\n\nIn general, there was a downward trend of average price as we increased the age, but there were few pockets (Ships aged 18-19, 14-15) that were off. On an average, ships having a sale price equal to or more than 100 million are aged less than 10 years with a few exceptions.\n\n\nAnalyzed the relationship between deadweight ton with the price. We assumed that more DWT (i.e. more capacity valued if they weigh in the range of 170-175 DWT (Ex-hibit B). This might be because of better fuel efficiency of this deadweight class or because of some other variables affecting the price.\n\n\nBaltic-dry capesize index values in-creased by approximately by 169% in the given time frame. Although we knew that this cannot be the sole criteria determin-ing the ship price, it had to be one of the key factors. We considered two sets of ships to verify the relation of capesize index with ship price keeping other vari-ables like age and DWT almost similar.  Analysis indicated that similar ships were sold for more if the capesize index value was more.\n\n\n'], 'url_profile': 'https://github.com/saniya-k', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '585 contributions\n        in the last year', 'description': ['linear_regression 🚀 \nMachine learning introduction\nHypothesis 🚕\nRo = b + m * o\nGradient descent 🚠\ntmp0 = p0 - lrate * (d * sum(hyp - y))\ntmp1 = p1 - lrate * (d * sum((hyp - y) * X))\np0 = tmp0\np1 = tmp1\n\nImplementation ✈️\nA trainer which train a model given an datasets\n\nThe dataset must follow this pattern:\ncolumns: [km, price]\n\n\nA reader which predict price based on a model\n\nThe model must follow this pattern:\ncolumns: [theta0, theta1, Xmin, Xmax, ymin, ymax]\n\n\nUsage ⛵\npython train.py\nusage: python train.py [-h] [--path path] [--out path] [--plot PLOT]\n                [--epochs EPOCHS] [--lrate LRATE]\n\nTrain a model given a dataset\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --path path      must be a valid dataset path\n  --out path       must be a valid output path\n  --plot PLOT      plot training\n  --epochs EPOCHS  number of iteration\n  --lrate LRATE    number of iteration\n\npython predict.py\nusage: predict.py [-h] [--path path] [-p path] <mileage>\n\nPredict the price given an mileage\n\npositional arguments:\n  <mileage>    must be a valid integer\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --path path  must be a valid model.csv path\n  -p path      must be a valid model.csv path\n\n'], 'url_profile': 'https://github.com/alngo', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['predict hand written number image used logistic regression\nhere we are building full logistic regression model to predict hand written number\nand also showed how to perform regularization and dimensionality reduction - PCA\n'], 'url_profile': 'https://github.com/rishisankhla', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Analysis of Mental Health Indicators in the USA (2019)\nCollaborators: Mate Pocs & Song Ying\nExecutive Summary\nAnalyzing the most influential variables to predict the prevalence of mental health issues in USA.\nUsing the dataset collected by the University of Wisconsin, we achieved the following:\n\nPerformed Data Cleaning & Feature Engineering\nCreated Baseline Model for Comparison\nAnalyzed Model Performance with addition of Interaction Features\nAnalyzed Model Performance with addition of Polynomial Features\nAnalyzed Model Performance of an Overfitted Model\nAnalyzed Model Performance with different regularization techniques:\n\nLasso\nRidge\n\n\nVerified Model with Test Data\nInterpretation of Results\nActionable Insights with Visualizations\n\nMethodology\nThis project uses Python 3, documented with Jupyter Notebook. We used a combination of Numpy, Pandas and StatsModels for data cleaning, filtering and feature engineering. ScikitLearn was used for preprocessing, model selection and model performance analysis. Matplotlib & Seaborn were both used for Visualizations. 1000 data samples were allocated for testing. K-fold (5-fold) cross-validation technique was used for our train and validation data during individual model analysis.\nHigh Level Overview\nThe project started by identifying relevant metrics to our target variable, namely Poor Mental Health Days, which describes the average number of poor mental health days per month. The relevant metrics are then filtered by checking its multicollinearity with Variance Inflation Factor. After data cleaning, a baseline model was created for comparison with future adjusted models. Interactions and Polynomials Features were added to improve model performance. A high complexity model was created by including both interaction features and high degree of polynomial features, which resulted in an overfitted model.\nWe then used regularization techniques like Lasso & Ridge method to reduce overfitting of the model while improving respective model performance during cross-validation. Residual Homoscedasticity & Normality Tests were conducted on the final model, which was then applied onto the test data. Finally, we interpretted the test results and formulated actionable insights from the analysis.\nFiles:\n\nindex.ipynb : main Jupyter Notebook\nanalytic_data_2019.csv : input data\ndata_linreg.csv : data cleaned and ready for linear regression\nus_county.py : contains dictionary for column renaming\n\nLimitations\n\nDuring our preliminary feature selection, we removed several variables with high levels of multicollinearity but remaining variables still possess relatively high VIF figures.\nOur final model consists of many variables after few rounds of screening and filtering, currently at around 100 predictive variables, including interaction and polynomial features, which we aim to cut down for future work.\nScaling issues encountered with the addition of multiple polynomial features, as different model performance acquired before and after normalization of the train data.\n\nFuture Work (Technical)\n\nAdditional Feature Selection to be conducted by detailed analysis of variable coefficients.\nScaling Issues to be investigated.\nPrediction Function (Reverse Scaling) to be improved\nColumns with too many missing values to be removed (Currently at a threshold of 10% of total data points)\nNon-Linear Data Transformations like Log Transformation could be conducted to a few variables during Feature Engineering\nQuantifying test results for homoscedasticity and normality\n\nFuture Work (External)\n\nMore relevant data like political and weather information to be added\nOutsource missing data points from several states from external sources\n\nOutcomes\nOne has to be careful when interpreting the outcome of a model like this. We can say that certain variables are a good predictor for other variables, but this does not necessarily mean that artificially changing them would also change the target variable. The causation could be reverse, or it could be tied to a third, perhaps external, unknown variable. When we say ""improve"" or ""decrease"" below, it is with understanding that we talk about the predictive powers of the variables, not necessarily the real-life implications.\n\nWe identified couple of variables, like insufficient sleep, physical inactivity, smoking, uninsured children, which have a high deteriorating effect on mental health.\nOn the positive side there are variables like going to college and having a large amount of social associations, these improve mental health.\nA number of variables were interactions, like excessive drinking, which in itself didn\'t have a large coefficient, but combined with other variables, it does.\nThe single most relevant variable (in the dataset that we kept, because remember, we excluded health-related variables) seems to be insufficient sleep. This variable shows up in many interactions, and has a very interesting polynomial shape. Higher insufficient sleep decreases mental health, but not in a linear shape, the effect is decreasing.\n\nActionable insights\nWith keeping the precautions outlined in the paragraph above when it comes to interpreting results of regression models, we believe can still draw some insights from the model.\n\nInsufficient sleep is a very good indicator of mental health. It can be used as a good predictor for future mental health issues. If we assume there is some causation from sleep quality to mental health, mental health can be improved via awareness of this factor. Sleep quality can be improved to some extent via external economic factors, improving work-life balance, etc.\nSmoking also highly correlates with mental health issues, which is a fact that could improve the effects of anti-smoking advertisements.\n\n'], 'url_profile': 'https://github.com/MatePocs', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AbdulHazari', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['Diabetes-prediction-system\nDiabetes is a chronic disease characterized by hyperglycemia. It may cause many complications. According to the growing morbidity in recent years, in 2040, the world’s diabetic patients will reach 642 million, which means that one of the ten adults in the future is suffering from diabetes. There is no doubt that this alarming figure needs great attention. With the rapid development of machine learning, machine learning has been applied to many aspects of medical health. In this study, I used a simple logistic regression model to predict diabetes mellitus.\nThis is a end to end framework for. This repo has the codes for the both front end and backend implementation of the system which is deployed using flask api\n'], 'url_profile': 'https://github.com/sethuramanio', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['safaricomNoExpiryBundles\nIntroduction\nThis shows how safaricom calculates the amount of bundles to give on a specific amount\nGetting Started\n\nmake sure you have Java-8 and above and intalled on your machine or vm.\nclone this project using the command git clone https://github.com/paulobiero/safaricomNoExpiryBundles.git\nBuild the maven pom.xml file to istall dependancies using the mvn install at the projects directory.\nRun the application using your favourite ide or using the commandline by using the command javac -d . Main.java in the src/main/java/sample directory\n\nResult\nif you successfully build your project you will see the following result,to use the commandline function to key in your amount close the graph window\n\n'], 'url_profile': 'https://github.com/paulobiero', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'Python', 'Updated Mar 2, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', '1', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 2, 2020', '1', 'Java', 'Updated Feb 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kat-young13', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Yelp Rating Prediction - model_features_with_graphs.py is what you probably want to check out\nIn this project, I have taken data from Yelp in order to use multiple linear regression to predict the rating of a restuarant given other features about it\nyelpRatingCode.py was my initial script where I created a model based on a small subset of the dataframe.\nHowever in model_features_with_graphs.py I create multiple models based on different subsets of data to create the best fitting model. In this file I also have written the code to print the graphs but to see the graphs, you would need to run the program on your computer.\nI also:\n\nUsed the pandas python library to create dataframes which held the data from the .csv files\nNormalized the data and plotted the independent variables against the dependent\nCompared different coefficient values to see which variables held the most weight on the dependent variable\nSplit the data into training (80%) and testing (20%) sets\nEvaluated the accuracy of the model by calculating the coefficient of the determinant of R2\n\nP.S. Data files are over 100mb, figuring out how to upload them, will be done shortly\n'], 'url_profile': 'https://github.com/zaydalameddine', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'Gainesville', 'stats_list': [], 'contributions': '172 contributions\n        in the last year', 'description': ['WineData\nRegression with Least Squares Loss and Huber Loss on Wine quality dataset\nThis is a classification problem where the outcome is a score between 0 and 10 depending on various attributes such as.\nThe dataset can be found at: http://archive.ics.uci.edu/ml/datasets/Wine+Quality\nI performed regression on the data with the following loss functions and achieved the following results:\n\nDeadzone Linear Loss:  Achieved accuracy of 72%\nHuber Loss: Achieved accuracy of 74%\nLeast Squares Loss:  Achieved accuracy of 75%\n\n'], 'url_profile': 'https://github.com/geethakamath18', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Cost_of_staying_healthy\nHealth insurance cost analysis and prediction by linear regression. Written in R programming language. Enjoy\nYou can find r scripting file or HTML file. Leave your comments. Thanks\n'], 'url_profile': 'https://github.com/IMuminov', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Kaggle-competition\nPredict Airbnb prices through supervised learning (Linear Regression, Boost, and Random Forest)\nPerformed extensive feature selection (forward, stepwise and lasso) to indenity essential variables to\navoid noise and overfitting as well as to reduce computational requirements\n'], 'url_profile': 'https://github.com/gg2762', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '340 contributions\n        in the last year', 'description': [""It's my Kaggle test\nTitanic-Data-Science-Solutions\nI use Logistic Regression,SVC,Random Forest,GaussianNB,K-Means ,DecisionTreeClassifier to solve the problem\nHourse saleprice prediction\nThe dataset has 80 features !!!,clean data is very important whatever models you use ,i use xgboost,it is very powerful!\n""], 'url_profile': 'https://github.com/guodalongplus', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'Salem', 'stats_list': [], 'contributions': '208 contributions\n        in the last year', 'description': [""\nMalicious-URLs-Detection-using-Machine-Learning\nShreekanth J S\nURL's were Classified into two categories:\n\nMalicious Website.\nSafe Website.\n\nTF - IDF Vectorizer was used for Feature Extraction.\nLogistic Regression Classifier was used to classify URLs.\nPerformance:\nAccuracy : 98.27%\nConfusion Matrix :\n\nScreenshots:\n\n\nDataset:\nhttps://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs\n""], 'url_profile': 'https://github.com/shreekanthsenthil', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Himanip06', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Network-Intrusion-Detector\nThis project aims to build a network intrusion detector, a predictive model capable of distinguishing between bad connections, called intrusions or attacks, and good normal connections. \nModelled as a binary classification problem. The below models are used to detect the bad intrusions \n• Logistic Regression \n• Nearest Neighbor \n• Support Vector Machine \n• Fully-Connected Neural Networks \nThe following python libraries have been used:\nScikit-learn \nNumPy \nPandas \nMatplotLib \nDataset Used\nUNSW-NB15: https://www.unsw.adfa.edu.au/australian-centre-for-cyber-security/cybersecurity/ADFA-NB15-Datasets/\nApproach\n\n\nData that is captured is generally dirty and is unfit for statistical analysis, we performed data cleaning by removing null values followed by performing one hot encoding for the categorical values and normalization for the numeric values. \n\n\nIn order to reduce the complexity of a model and enable the algorithm to train faster, we performed feature selection to select the top 20 features out of which 15 features are taken. \n\n\nWe split the data and trained each of our models, We compared our models with accuracy, F1 score, precision and recall after plotting a cofusion matrix. We analysed the ROC curves for each model.\n\n\nThe neural networks were tested with different hyperparameters In order to understand how a model behaves when each of them are changed.\n\n\n'], 'url_profile': 'https://github.com/Dharmapuri31', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['About this Dataset\nThis dataset has data collected from New York, California and Florida about 50 business Start-ups. The variables used in the dataset are Profit, R&D spending, Administration Spending, and Marketing Spending. As part of this analysis, we will figure out the attribute which makes the company more profitable.\nBelow are the first few columns of the dataset.\n\n\n\nR&D Spend\nAdministration\nMarketing Spend\nState\nProfit\n\n\n\n\n165349.2\n136897.8\n471784.1\nNew York\n192261.83\n\n\n162597.7\n151377.59\n443898.53\nCalifornia\n191792.06\n\n\n153441.51\n101145.55\n407934.54\nFlorida\n191050.39\n\n\n144372.41\n118671.85\n383199.62\nNew York\n182901.99\n\n\n142107.34\n91391.77\n366168.42\nFlorida\n166187.94\n\n\n131876.9\n99814.71\n362861.36\nNew York\n156991.12\n\n\n134615.46\n147198.87\n127716.82\nCalifornia\n156122.51\n\n\n130298.13\n145530.06\n323876.68\nFlorida\n155752.6\n\n\n120542.52\n148718.95\n311613.29\nNew York\n152211.77\n\n\n123334.88\n108679.17\n304981.62\nCalifornia\n149759.96\n\n\n101913.08\n110594.11\n229160.95\nFlorida\n146121.95\n\n\n100671.96\n91790.61\n249744.55\nCalifornia\n144259.4\n\n\n93863.75\n127320.38\n249839.44\nFlorida\n141585.52\n\n\n91992.39\n135495.07\n252664.93\nCalifornia\n134307.35\n\n\n119943.24\n156547.42\n256512.92\nFlorida\n132602.65\n\n\n\n'], 'url_profile': 'https://github.com/Prasanna-Mohanty', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Jun 28, 2020', 'HTML', 'Updated Feb 19, 2020', '1', 'R', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}"
"{'location': 'Salem', 'stats_list': [], 'contributions': '208 contributions\n        in the last year', 'description': [""\nMalicious-URLs-Detection-using-Machine-Learning\nShreekanth J S\nURL's were Classified into two categories:\n\nMalicious Website.\nSafe Website.\n\nTF - IDF Vectorizer was used for Feature Extraction.\nLogistic Regression Classifier was used to classify URLs.\nPerformance:\nAccuracy : 98.27%\nConfusion Matrix :\n\nScreenshots:\n\n\nDataset:\nhttps://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs\n""], 'url_profile': 'https://github.com/shreekanthsenthil', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Himanip06', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Network-Intrusion-Detector\nThis project aims to build a network intrusion detector, a predictive model capable of distinguishing between bad connections, called intrusions or attacks, and good normal connections. \nModelled as a binary classification problem. The below models are used to detect the bad intrusions \n• Logistic Regression \n• Nearest Neighbor \n• Support Vector Machine \n• Fully-Connected Neural Networks \nThe following python libraries have been used:\nScikit-learn \nNumPy \nPandas \nMatplotLib \nDataset Used\nUNSW-NB15: https://www.unsw.adfa.edu.au/australian-centre-for-cyber-security/cybersecurity/ADFA-NB15-Datasets/\nApproach\n\n\nData that is captured is generally dirty and is unfit for statistical analysis, we performed data cleaning by removing null values followed by performing one hot encoding for the categorical values and normalization for the numeric values. \n\n\nIn order to reduce the complexity of a model and enable the algorithm to train faster, we performed feature selection to select the top 20 features out of which 15 features are taken. \n\n\nWe split the data and trained each of our models, We compared our models with accuracy, F1 score, precision and recall after plotting a cofusion matrix. We analysed the ROC curves for each model.\n\n\nThe neural networks were tested with different hyperparameters In order to understand how a model behaves when each of them are changed.\n\n\n'], 'url_profile': 'https://github.com/Dharmapuri31', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PranavChittella', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['yield fee correlation\n'], 'url_profile': 'https://github.com/shplishka', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['PacktPublishing/Start Machine Learning Here Linear Regression model in R\nStart Machine Learning Here Linear Regression model in R by Packt Publishing\n'], 'url_profile': 'https://github.com/sanjanapackt', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Predict Sales Volume\nGiven historical sales data of a variety of products, forecast sales volume for new products. Data sets of existing products and new products have selected product information.\n\nexistingproductattributes2017.csv: set of 80 products with 17 features\nnewproductattributes2017.csv: set of 24 new products with the same 17 features\n\n'], 'url_profile': 'https://github.com/cejecj', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'Saint Paul, MN', 'stats_list': [], 'contributions': '254 contributions\n        in the last year', 'description': ['temperature-decay-rate\nAnalysis of a temperature decay rate data set with Linear Regression and Generalization.\n'], 'url_profile': 'https://github.com/bradylange', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chiyahn', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}","{'location': 'Clemson, SC', 'stats_list': [], 'contributions': '899 contributions\n        in the last year', 'description': ['sss-nepc-review\nSchool Choice Advocacy Report Plays Regression Modeling Games to Make Its Case Against Public Schools\n'], 'url_profile': 'https://github.com/svmiller', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'R', 'MIT license', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Oct 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020']}"
"{'location': 'Saint Paul, MN', 'stats_list': [], 'contributions': '254 contributions\n        in the last year', 'description': ['temperature-decay-rate\nAnalysis of a temperature decay rate data set with Linear Regression and Generalization.\n'], 'url_profile': 'https://github.com/bradylange', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chiyahn', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'Clemson, SC', 'stats_list': [], 'contributions': '899 contributions\n        in the last year', 'description': ['sss-nepc-review\nSchool Choice Advocacy Report Plays Regression Modeling Games to Make Its Case Against Public Schools\n'], 'url_profile': 'https://github.com/svmiller', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'bengaluru ', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['benchmarks\nTechnologies Involved: Python, Pandas, Numpy, Seaborn, Matplotlib, Logistic regression, Decision Tree Classifier\n'], 'url_profile': 'https://github.com/ketkib', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['linearRegressionCarPricePredictionAssignment\nTo build a multiple linear regression model for the prediction of car prices.\nProblem Statement:\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and\nproducing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends.\nSpecifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be\nvery different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car.\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the American market.\nBusiness Goal :\nYou are required to model the price of cars with the available independent variables. It will be used by the management to\nunderstand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars,\nthe business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand\nthe pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'Bethlehem/State College, PA', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['STAT-462\nAssignments Completed in STAT 462 - Applied Regression Analysis in Spring 2020. Used R code for completion of all assignments.\n'], 'url_profile': 'https://github.com/tjschaeffer', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Sparse principal component regression via singular value decomposition approach.\nThis is an R source code for performing sparse principal component regression via singular value decomposition approach (spcrSVD). The directory R consists of four files as follows.\n\nspcrSVD.R provides spcrSVD via an ADMM algorithm.\ncv.spcrSVD.R computes cross-validation of spcrSVD.\nspcrSVD_linearized.R provides spcrSVD via a linearized ADMM algorithm.\ncv.spcrSVD_linearized.R computes cross-validation of spcrSVD_linearized.\n\nspcrSVD is introduced in the paper:\nKawano, S. (2020) Sparse principal component regression via singular value decomposition approach. arXiv:2002.09188 URL.\nUsage example\nRead source files.\nsource(""R/spcrSVD.R"")\nsource(""R/cv.spcrSVD.R"")\nsource(""R/spcrSVD_linearized.R"")\nsource(""R/cv.spcrSVD_linearized.R"")\n\nSetting of simulation.\nlibrary(MASS)\nn <- 50; ErrorVariance <- 1; k <- 1; np <- 2; dummy_np <- 8\nw <- 1e-1; rho1 <- 1; rho2 <- 1; rho3 <- 1; rho_V <- 1; rho_beta <- 1; tol <- 1e-5\nSigma <- diag( rep(1,np) )\nfor(i in 1:nrow(Sigma)) for(j in 1:ncol(Sigma)) Sigma[i ,j] = 0; Sigma[1,1] = 1; Sigma[2,2] = 1\nnu0 <- c(2,1)\nx_ori <- mvrnorm(n, rep(0, nrow(Sigma)), Sigma)\nx_dummy <- matrix( rnorm(dummy_np*n), n, dummy_np )\nx_ori <- cbind(x_ori, x_dummy)\ny_ori <- nu0[1]*x_ori[ ,1] + nu0[2]*x_ori[ ,2] + rnorm(n, 0, ErrorVariance)\ny <- y_ori - mean(y_ori)\nx <- sweep(x_ori, 2, apply(x_ori,2,mean))\n\nPerform spcrSVD\n# Perform spcrSVD in the file spcrSVD.R\nspcrSVD(x=x, y=y, k=k, w=w, lambda_V=1e-1, lambda_beta=1e-1, rho1=rho1, rho2=rho2, rho3=rho3, tol=tol)\n\n# Perform cv.spcrSVD with five-fold in the file cv.spcrSVD.R\ncv.spcrSVD(x=x, y=y, k=k, w=w, fold=5, rho1=rho1, rho2=rho2, rho3=rho3, tol=tol)\n\n# Perform spcrSVD_linearized in the file spcrSVD_linearized.R\nspcrSVD_linearized(x=x, y=y, k=k, w=w, lambda_V=1e-1, lambda_beta=1e-1, rho_V=rho_V, rho_beta=rho_beta, tol=tol)\n\n# Perform cv.spcrSVD_linearized with five-fold in the file cv.spcrSVD_linearized.R\ncv.spcrSVD_linearized(x=x, y=y, k=k, w=w, fold=5, rho_V=rho_V, rho_beta=rho_beta, tol=tol)\n\n'], 'url_profile': 'https://github.com/ShuichiKawano', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'Elblag, Poland', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': [""predict_cereal_ml\nA simple project to familiarize with knn and RF classification and regression models in sklearn.\nDataset:\nhttps://www.kaggle.com/crawford/80-cereals\nMain objectives:\n\nWe are predicting the impact of breakfast cereal features on the classification of chosen shelf in shop.\nWe are predicting the impact of breakfast cereal features on the consumers' rating of chosen cereal.\n\n""], 'url_profile': 'https://github.com/maciej-zieniewicz', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Cross-Validation\nProject from graduate computer science course in Machine Learning. Includes examples of ridge and lasso regression.\n'], 'url_profile': 'https://github.com/bill12m', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 23, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'HTML', 'Updated May 2, 2020', 'R', 'Updated Oct 7, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'TeX', 'Updated Feb 17, 2020']}"
"{'location': 'Shen Zhen, China', 'stats_list': [], 'contributions': '287 contributions\n        in the last year', 'description': ['Radial Basis Function C++ Implementation\nThis is a c++ implementation of RBF neural network using OpenCV as 3rdparty library\nUsuage\nyou can use this code by adding the source code in your project, and link with opencv library.\n'], 'url_profile': 'https://github.com/yuecideng', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HieuQN', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': [""Multiple Linear Regression Machine Learning\nThis is the simplest example of multiple linear regression with Boston housing data.\nSteps:\n1) Load the Data:\n\nLoad the data in pandas.\nThis dataset will not get coloumns here but we can add that easily\nCheck the notebook for the code.\n\n2) Clean The Data:\n\nAs its the easiest problem the dataset doesn't have any Missing or duplicate values.\nAlthouth there's a scope of work everywhere so we can check for understanding.\n\n3) Feature Selections:\n\nWith basic method of correlection we will first try to judge which features we can drop.\nWe can also play with different method and get different predication scores.\n\n4) Split the data:\n\nWe need to split the data for training and testing.\nThe data is split into 70% training and 30% testing.\n\n5) Fit the model:\n\nwe use the liner regression model to get predicsitons.\nafter the fit, use predict method for prediction\n\n6) Prediction score:\n\nthere are many ways to check the goodness of model\nI have used r2_score to check and the model worked fine.\n\nCheck the code and use for more improvement and share...!!!!!\n""], 'url_profile': 'https://github.com/futureautomate', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['red-wine-quality\nThis repository deals with Red Wine Quality dataset from Kaggle and implementation of Linear Regression algorithm\n'], 'url_profile': 'https://github.com/BalaKowsalya', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['predicting car purchased-simple linear regression\nlearning how perform simple linear regression with numpy and seaborn python library\n'], 'url_profile': 'https://github.com/rishisankhla', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Sentiment-Analysis-on-Amazon-Product-Reviews\nThe goal is to perform sentiment analysis to determine whether a review is positive or negative using a classifier in python for sentiment analysis on Amazon reviews.\nPerformed Logistic Regression and Random Forest on both Bagofwords and RF-IDF Models\n'], 'url_profile': 'https://github.com/DineshKarnati', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': [""advanceRegressionHousePricePrediction\nThis is for kaggle competition https://www.kaggle.com/c/house-prices-advanced-regression-techniques/ .\nCompetition Description:\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity\nto an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of\nbedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to\npredict the final price of each home.\nPractice Skills\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting\nAcknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists\nlooking for a modernized and expanded version of the often cited Boston Housing dataset.\n""], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'Texas, United States', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Ames_housing\nPredicting the housing prices in Ames Iowa using advanced regression techniques:\nThis problem is part of a kaggle competition.\nData:\nThe dataset contains features of houses in Ames such as Lot frontage, Pool, Fireplace, etc. with Sale Price as the output variable.\nCode:\nThe following operations have been performed in the code\na) The null values have been replaced by the mean in the numeric columns. For non numeric values , the null values have been replaced by the category with the highest frequency.\nb)Next the appropriate features have been picked out using Recursive Feature Elimination (RFECV) for Linear regression.\nc) After applying Linear regression, we get an RMSE of 0.18715\nd)Next we follow the same steps to apply Random Forest regressor. This gives us an RMSE of around 0.16\ne) Finally we use XGBoost because its performance capability and ability to handle null values. This gives us an RMSE of 0.13010\n'], 'url_profile': 'https://github.com/aqueelj', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '242 contributions\n        in the last year', 'description': ['LaVie Insurance\nThe primary goal of this project was to utilise machine learning methods including regression and regularisation to find the best predictive model to understand the mulitvariate predictors of premature death rate, our proxy for life insurance premiums.\nTable of Contents\n\n File Descriptions \n Strucuture \n Executive Summary \n\n Data Cleaning and Feature Engineering \n Preprocessing \n Modelling \n Conclusion \n\n\n\n\nFile Description\n\nImages: folder containing images of some tables and plots used in README.md\nindex.ipynb: notebook with data exploration, preprocessing, and modelling.\ninit.py: contains classes for linear regression, polynomials, and regularisation.\nanalytic_data2019.csv: data used for modelling.\nfinal_model.pkl: final model saved using pickle.\nRegression and Insurance.pdf: presentation summarising project process and findings\n\n\nStructure of Notebook:\n\n\nImports\n\nImporting Libraries\nImporting Dataset\n\n\n\nEDA\n\nEarly Exploration and Cleaning\nTesting Homoscedasticity\nCorrelation Heatmap\n\n\n\nModelling\n\nTrain Test Split\nLinear\nQuadratic\nCubic\nRegularisation Using Regression Models\n\n\n\nValidation\n\nRegularisation Using Regression Models\nModel Selection\nCoefficients of Best Models\n\n\n\nTesting Final Model\n\nLasso Model Test Data\nCoefficient Weighting\n\n\n\n\nExecutive Summary\nAs a struggling US life insurance company, our goal is to increase revenues by 2% using a risk premium based on premature deaths per state. We aim to hike insurance contract prices across the states that pause the highest future risk of premature death rates. This should in turn take our net premium growth rate above annual inflation which we have been on par with for the last five years and move to a more risk-adjusted business model which is key in our industry.\n\nData Cleaning and Feature Engineering:\nThe dataset we used contained a wide ranging set of health ranking features per county within each state. This included data points such as premature deaths, low birthweight, adult smoking etc. As an intial step, we selected the premature death as our dependent variable, what we aim to predict. This variable is categorized as Years Potential Life Loff (YPLL) - estimate of the average years a person would have lived if he or she had not died prematurely. It is a measure of premature mortality.\nWe then decided to exclude columns with confidence interval and quantile to simplify our selection process and model application. In addition, we tested for multicollinearity for the independent variables using a VIF (variance inflation factor) test. It yield some highly collinear relationships across variables that we then excluded.\nWe subsequently ran a correlation matrix across all remaining variables and selected 5 variables with some form of positive or negative correlation and plausible causal relationsip with premature deaths. We purposefully kept a wide range of correlation selection as to avoid missing possible related interactions which could help us predict our dependent variable.\nSome of our independent variables selected included: Diabetes prevalance raw value, Poor or fair health raw value, and median household income raw value\nCorrelation Heatmap of Remaining Variables\n\n\n\n\nPreprocessing\nThe initial step included setting up a structured framework to the data. We initially setup a train-test split at a 2/3 // 1/3 respective split to include at least 1000 observations in our testing test. For the regularisation models we used StandardScaler to standardise our data.\n\nModelling:\nWe then performed 5-fold cross validation across our models for our training data to validate the completeness and quality of our data-model interaction. This also allowed us to select the highest performing models within each model group/type.\nThe initial baseline model was a multivariate linear regression using the Ordinary Least Squares (OLS) method. With 5 different variables the model yielded an R^2 score of 0.5223 which was an encouraging sign as an first instance model.\nWe then proceeded to perform a polynomial transformation to our variables to factor interactions of independent variables and their explanatory power of premature deaths. We ran three levels of polynomial regression: quadratic, qubic, quartic and quintic. The best results came out of the quadratic transformation with an improved R^2 score of 0.5554. We elected to keep this transformation.\nUsing a z-score standard scaler method we took our poly-transformed data and scaled it before applying the regularisation methods.\nTo improve our coefficients and simplify our model we decided to run a series of regularisation technqiues using lasso, ridge and elastic net regressions with several alpha levels. In total we had 60 models with regularisation (20 Lasso, 20 Ridge, and 20 Elastic Net). We once again applied the 5-fold validation process to find the best R^2 value across model types with the optimal level.\nTable Comparing Best Models\n\n\n\nBar Chart Comparing Best Models\n\n\n\nOur rationale at this point is to come back to our stakeholders and make sure the regression is as simple as possible to apply it in a business context. We selected a lasso model yielding an R^2 of 0.5241 on the training set with an alpha level of 47.373684.\nFinally, we ran the model on the test set and got an R^2 of 0.5626 illustrating some accuracy in our model and a possibility to deploy it for our business application.\nFinal Model Coefficients\n\n\n\nFinal Model Residuals Plot\n\n\n\n\nConclusion\nWe believe our final model provides us enough prediction accuracy to implement the life insurance premium increase. We are aiming to perform this increase as a function of YPLL predictions scaled to the population size. This model should allow us not only to increase our revenues but to shift towards a more risk-adjusted revenue approach which we can replicate in the future.\n'], 'url_profile': 'https://github.com/awesomeahi95', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['hierScale\nHussein Hazimeh and Rahul Mazumder\nMassachusetts Institute of Technology\nIntroduction\nhierScale is scalable toolkit for fitting sparse linear regression models with pairwise feature interactions. The optimization is done under the strong hierarchy (SH) constraint: an interaction coefficient is non-zero only if its associated main feature coefficients are non-zero. This constraint can enhance the interpretability of sparse interaction models and also reduce the future data collection costs; see the discussion in (Hazimeh and Mazumder, 2020).\nMore formally, given a data matrix X of main features and a response vector y, the toolkit fits a convex relaxation of the following model:\n\nwhere X_i denotes the ith column (feature) of X and * refers to element-wise multiplication. The L0 norms impose sparsity on the coefficients and the constraints enforce SH. See (Hazimeh and Mazumder, 2020) for details on how the convex relaxation of the above problem is derived. The optimization is done for a regularization path (i.e., over a grid of lambda_1\'s and lambda_2\'s). We use proximal gradient descent (PGD) for optimization, along with novel proximal screening and gradient screening rules, which speed up PGD by over 4900x.\nInstallation\nhierScale is written in Python 3. It uses Gurobi internally (for solving the LPs required for checking the optimality conditions). Before installing hierScale, please make sure that  Gurobi and its Python interface (gurobipy) are installed.\nTo install hierScale, run the following command:\npip install hierScale\nQuick Start\nIn Python, assuming you have the data X and y stored as numpy arrays, run the following to fit a regularization path:\nfrom hierScale import hier_fit, hier_predict\n\n# Set the parameters of the algorithm.\nparams = {\n    ""nLambda"": 100, # Number of lambda_1\'s in the path.\n    ""maxSuppSize"": 500, # Max support size to terminate the path at.\n}\n\n# Fit a path.\nsolutions, lambdas = hier_fit(X, y, params)\n\n# solutions is a list of all the solutions in the path.\n# To access the ith solution, say i=10, use the following:\ncurrent_solution = solutions[10]\ncurrent_solution.B # A dictionary of the non-zero coefficients in beta.\ncurrent_solution.T # A dictionary of the non-zero coefficients in theta.\ncurrent_solution.intercept # The intercept term.\n\n# To predict the response given a matrix X, run the following:\nhier_predict(current_solution, X)\n\n# For more advanced usage and parameters, please check the documentation:\nprint(hier_fit.__doc__)\nReferences\nLearning Hierarchical Interactions at Scale: A Convex Optimization Approach.\nBibtex citation below:\n@InProceedings{pmlr-v108-hazimeh20a, \ntitle = {Learning Hierarchical Interactions at Scale: A Convex Optimization Approach},\nauthor = {Hazimeh, Hussein and Mazumder, Rahul},\nbooktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},\npages = {1833--1843},\nyear = {2020},\neditor = {Silvia Chiappa and Roberto Calandra},\nvolume = {108},\nseries = {Proceedings of Machine Learning Research},\naddress = {Online},\nmonth = {26--28 Aug},\npublisher = {PMLR},\npdf = {http://proceedings.mlr.press/v108/hazimeh20a/hazimeh20a.pdf},\nurl = {http://proceedings.mlr.press/v108/hazimeh20a.html}}\n\n'], 'url_profile': 'https://github.com/hazimehh', 'info_list': ['C++', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', '1', 'Jupyter Notebook', 'Updated Oct 4, 2020', '1', 'Python', 'MIT license', 'Updated Nov 17, 2020']}"
"{'location': 'Amaravati, Andhra Pradesh, India', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Linear-Regression-using-mathematical-formula\nLinear regression is a way to explain the linear relation between a dependent variable and one or more explanatory variables. It is a common Statistical Data Analysis Technique. I used mathematical formula here which is very important for the machine learning and AI researcher to have knowledge in basic statistical mathematics.\n'], 'url_profile': 'https://github.com/palak-ag', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Ridge-and-LAsso-Regression-implementation\n'], 'url_profile': 'https://github.com/SachinBorgave09', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Classificaiton-and-Regression-Trees-CARTs-\nVisualization with graphize\n'], 'url_profile': 'https://github.com/mnasirizadeh', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Columbia, SC', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yizenglistat', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Jaipur', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\nKaggle competition to predict house price\n'], 'url_profile': 'https://github.com/sudendroid', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['GA\nSTA 243 Final Project (Genetic Algorithm for variable selection in linear models)\n'], 'url_profile': 'https://github.com/lalalaeat', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kprasertchoang', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Linear-Regression-Model--Geely-Auto Assignment\nProblem Statement:\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car \nHow well those variables describe the price of a car \nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the American Market.\n\n'], 'url_profile': 'https://github.com/ThejasviniVunnam', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'R', 'Updated Jan 26, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 18, 2020', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}"
"{'location': 'Germany', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['advertisingData_LogisticRegressionModel\n'], 'url_profile': 'https://github.com/rauts', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '209 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\n'], 'url_profile': 'https://github.com/phuongvnguyen', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbalajiai', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['logisticRegressionIndianDiabetesSolution\nData Science Project : To determine the factors of diabetes in Indians.\n'], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'San Juan, Puerto Rico', 'stats_list': [], 'contributions': '492 contributions\n        in the last year', 'description': ['This project was bootstrapped with Create React App.\nAvailable Scripts\nIn the project directory, you can run:\nyarn start\nRuns the app in the development mode.\nOpen http://localhost:3000 to view it in the browser.\nThe page will reload if you make edits.\nYou will also see any lint errors in the console.\nyarn test\nLaunches the test runner in the interactive watch mode.\nSee the section about running tests for more information.\nyarn build\nBuilds the app for production to the build folder.\nIt correctly bundles React in production mode and optimizes the build for the best performance.\nThe build is minified and the filenames include the hashes.\nYour app is ready to be deployed!\nSee the section about deployment for more information.\nyarn eject\nNote: this is a one-way operation. Once you eject, you can’t go back!\nIf you aren’t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.\nYou don’t have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.\nLearn More\nYou can learn more in the Create React App documentation.\nTo learn React, check out the React documentation.\nCode Splitting\nThis section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting\nAnalyzing the Bundle Size\nThis section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\nMaking a Progressive Web App\nThis section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\nAdvanced Configuration\nThis section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration\nDeployment\nThis section has moved here: https://facebook.github.io/create-react-app/docs/deployment\nyarn build fails to minify\nThis section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify\n'], 'url_profile': 'https://github.com/yadielar', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '295 contributions\n        in the last year', 'description': ['Denoising Noisy Documents\n\n\n\n\n\n\n\nNumerous scientific papers, historical documentaries/artifacts, recipes, books are stored as papers be it handwritten/typewritten. With time, the paper/notes tend to accumulate noise/dirt through fingerprints, weakening of paper fibers, dirt, coffee/tea stains, abrasions, wrinkling, etc. There are several surface cleaning methods used for both preserving and cleaning, but they have certain limits, the major one being: that the original document might get altered during the process. The purpose of this project is to do a comparative study of traditional computer vision techniques vs deep learning networks when denoising dirty documents.\nCheck out the Medium post for the complete analysis published in Towards Data Science here:\nhttps://towardsdatascience.com/denoising-noisy-documents-6807c34730c4\nAutoencoder architecture\n\nThe network is composed of 5 convolutional layers to extract meaningful features from images. In the first four convolutions, we use 64 kernels. Each kernel has different weights, perform different convolutions on the input layer, and produce a different feature map. Each output of the convolution, therefore, is composed of 64 channels.\nThe encoder uses max-pooling for compression. A sliding filter runs over the input image, to construct a smaller image where each pixel is the max of a region represented by the filter in the original image. The decoder uses up-sampling to restore the image to its original dimensions, by simply repeating the rows and columns of the layer input before feeding it to a convolutional layer.\nBatch normalization reduces covariance shift, that is the difference in the distribution of the activations between layers, and allows each layer of the model to learn more independently of other layers.\nModel: ""model_1""\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nimage_input (InputLayer)     (None, 420, 540, 1)       0         \n_________________________________________________________________\nConv1 (Conv2D)               (None, 420, 540, 32)      320       \n_________________________________________________________________\npool1 (MaxPooling2D)         (None, 210, 270, 32)      0         \n_________________________________________________________________\nConv2 (Conv2D)               (None, 210, 270, 64)      18496     \n_________________________________________________________________\npool2 (MaxPooling2D)         (None, 105, 135, 64)      0         \n_________________________________________________________________\nConv3 (Conv2D)               (None, 105, 135, 64)      36928     \n_________________________________________________________________\nupsample1 (UpSampling2D)     (None, 210, 270, 64)      0         \n_________________________________________________________________\nConv4 (Conv2D)               (None, 210, 270, 32)      18464     \n_________________________________________________________________\nupsample2 (UpSampling2D)     (None, 420, 540, 32)      0         \n_________________________________________________________________\nConv5 (Conv2D)               (None, 420, 540, 1)       289       \n=================================================================\nTotal params: 74,497\nTrainable params: 74,497\nNon-trainable params: 0\n_________________________________________________________________\n\nRegression\nAlong with autoencoder, another machine learning technique we have used is Linear Regression. Instead of modelling the entire image at once, we tried predicting the cleaned-up intensity for each pixel within the image, and constructed a cleaned image by combining together a set of predicted pixel intensities using linear regression. Except at the extremes, there is a linear relationship between the brightness of the dirty images and the cleaned images. There is a broad spread of x values as y approaches 1, and these pixels probably represent stains that need to be removed.\n\nAWS architecture\n\nAnalysis Approach\n\nUse median filter to get a “background” of the image, with the text being “foreground” (due to the fact that the noise takes more space than the text in large localities). Next subtract this “background” from the original image.\nApply canny edge detection to extract edges. Perform dilation (i.e. make text/lines thicker, and noise/lines thinner) then erosion while preserving thicker lines and removing thinner ones (i.e. noisy edges)\nUse adaptive thresholding. (works really well since often text is darker than noise). Thus, preserve pixels that are darkest “locally” and threshold rest to 0 (i.e. foreground)\nCNN Autoencoder: The network is composed of 5 convolutional layers to extract meaningful features from images.\n\n\nDuring convolutions, same padding mode will be used. We pad with zeros around the input matrix, to preserve the same image dimensions after convolution.\nThe encoder uses max-pooling for compression. A sliding filter runs over the input image, to construct a smaller image where each pixel is the max of a region represented by the filter in the original image.\nThe decoder uses up-sampling to restore the image to its original dimensions, by simply repeating the rows and columns of the layer input before feeding it to a convolutional layer.\nPerform batch-normalization as required. For the output, we use sigmoid activation to predict pixel intensities between 0 and 1.\n\n\nCompare results from {1, 2, 3, 4} using the following metrics: RMSE, PSNR, SSIM, UQI\n\nResults:\nMedian Filtering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdaptive Thresholding\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanny Edge Detection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoencoder\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScreens\nLogin Screen\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogin Screen\nAutoencoder\n\n\n\n\n\n\nMedian Filtering\nMedian Filtering Results\n\n\n\nProject Schema\nDirectory structure for Denoizer repository:\n|-- dataset\n |-- test.zip → test image dataset\n\t|-- train.zip → training image dataset containing noisy dataset\n\t|-- train_cleaned.zip → cleaned images for respective noisy images in train.zip\n|-- frontend\n\t|-- static → CSS, JS for flask web app \n\t|-- templates → HTML pages for flask web app\n|-- reports → collection of reports submitted on this project\n|-- results → resultant images for each technique \n\t|-- adaptive-results\n\t|-- autoencoder-results\n\t|-- edge-detection-results\n\t|-- median-results\n\t|-- regression-results\n|-- screens → screenshot/snippets  for each tab/page on webapp\n\nData:\nOur data is collected from UCI\'s machine learning repository. The dataset comprises of train and test images of Noisy documents which contain noise from various sources like accidental spills, creases, ink spots and so on.\n[1] https://archive.ics.uci.edu/ml/datasets/NoisyOffice\n[2] https://www.kaggle.com/sthabile/noisy-and-rotated-scanned-documents\nTeam\n\nKartikeya Shukla\nChinmay Wyawahare\nMichael Lally\n\n'], 'url_profile': 'https://github.com/gandalf1819', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'Cambridge, UK', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['typescript-is\nTypeScript transformer that generates run-time type-checks.\n\n\n\n\n\n\n\n💿 Installation\nnpm install --save typescript-is\n\n# Ensure you have the required dependencies at compile time:\nnpm install --save-dev typescript\n\n# If you want to use the decorators, ensure you have reflect-metadata in your depdendencies:\nnpm install --save reflect-metadata\n💼 Use cases\nIf you\'ve worked with TypeScript for a while, you know that sometimes you obtain any or unknown data that is not type-safe.\nYou\'d then have to write your own function with type predicates that checks the foreign object, and makes sure it is the type that you need.\nThis library automates writing the type predicate function for you.\nAt compile time, it inspects the type you want to have checked, and generates a function that can check the type of a wild object at run-time.\nWhen the function is invoked, it checks in detail if the given wild object complies with your favorite type.\nIn particular, you may obtain wild, untyped object, in the following situations:\n\nYou\'re doing a fetch call, which returns some JSON object.\nYou don\'t know if the JSON object is of the shape you expect.\nYour users are uploading a file, which is then read by your application and converted to an object.\nYou don\'t know if this object is really the type you expect.\nYou\'re reading a JSON string from localStorage that you\'ve stored earlier.\nPerhaps in the meantime the string has been manipulated and is no longer giving you the object you expect.\nAny other case where you lose compile time type information...\n\nIn these situations typescript-is can come to your rescue.\nNOTE this package aims to generate type predicates for any serializable JavaScript object.\nPlease check What it won\'t do for details.\n🎛️ Configuration\nThis package exposes a TypeScript transformer factory at typescript-is/lib/transformer-inline/transformer\nAs there currently is no way to configure the TypeScript compiler to use a transformer without using it programatically, the recommended way is to compile with ttypescript.\nThis is basically a wrapper around the TypeScript compiler that injects transformers configured in your tsconfig.json.\n(please vote here to support transformers out-of-the-box: https://github.com/Microsoft/TypeScript/issues/14419)\nUsing ttypescript\nFirst install ttypescript:\nnpm install --save-dev ttypescript\nThen make sure your tsconfig.json is configured to use the typescript-is transformer:\n{\n    ""compilerOptions"": {\n        ""plugins"": [\n            { ""transform"": ""typescript-is/lib/transform-inline/transformer"" }\n        ]\n    }\n}\nNow compile using ttypescript:\nnpx ttsc\nUsing with ts-node, webpack, Rollup\nPlease check the README of ttypescript for information on how to use it in combination with ts-node, webpack, and Rollup.\nOptions\nThere are some options to configure the transformer.\n\n\n\nProperty\nDescription\n\n\n\n\nshortCircuit\nBoolean (default false). If true, all type guards will return true, i.e. no validation takes place. Can be used for example in production deployments where doing a lot of validation can cost too much CPU.\n\n\nignoreClasses\nBoolean (default: false). If true, when the transformer encounters a class, it will ignore it and simply return true. If false, an error is generated at compile time.\n\n\nignoreMethods\nBoolean (default: false). If true, when the transformer encounters a method, it will ignore it and simply return true. If false, an error is generated at compile time.\n\n\ndisallowSuperfluousObjectProperties\nBoolean (default: false). If true, objects are checked for having superfluous properties and will cause the validation to fail if they do. If false, no check for superfluous properties is made.\n\n\n\nIf you are using ttypescript, you can include the options in your tsconfig.json:\n{\n    ""compilerOptions"": {\n        ""plugins"": [\n            {\n                ""transform"": ""typescript-is/lib/transform-inline/transformer"",\n                ""shortCircuit"": true,\n                ""ignoreClasses"": true,\n                ""ignoreMethods"": true,\n                ""disallowSuperfluousObjectProperties"": true\n            }\n        ]\n    }\n}\n⭐ How to use\nBefore using, please make sure you\'ve completed configuring the transformer.\nIn your TypeScript code, you can now import and use the type-check function is (or createIs), or the type assertion function assertType (or createAssertType).\nValidation (is and createIs)\nFor example, you can check if something is a string or number and use it as such, without the compiler complaining:\nimport { is } from \'typescript-is\';\n\nconst wildString: any = \'a string, but nobody knows at compile time, because it is cast to `any`\';\n\nif (is<string>(wildString)) { // returns true\n    // wildString can be used as string!\n} else {\n    // never gets to this branch\n}\n\nif (is<number>(wildString)) { // returns false\n    // never gets to this branch\n} else {\n    // Now you know that wildString is not a number!\n}\nYou can also check your own interfaces:\nimport { is } from \'typescript-is\';\n\ninterface MyInterface {\n    someObject: string;\n    without: string;\n}\n\nconst foreignObject: any = { someObject: \'obtained from the wild\', without: \'type safety\' };\n\nif (is<MyInterface>(foreignObject)) { // returns true\n    const someObject = foreignObject.someObject; // type: string\n    const without = foreignObject.without; // type: string\n}\nAssertions (assertType and createAssertType)\nOr use the assertType function to directly use the object:\nimport { assertType } from \'typescript-is\';\n\nconst object: any = 42;\nassertType<number>(object).toFixed(2); // ""42.00""\n\ntry {\n    const asString = assertType<string>(object); // throws error: object is not a string\n    asString.toUpperCasse(); // never gets here\n} catch (error) {\n    // ...\n}\nDecorators (ValidateClass and AssertType)\nYou can also use the decorators to automate validation in class methods.\nTo enable this functionality, you should make sure that experimental decorators are enabled for your TypeScript project.\n{\n    ""compilerOptions"": {\n        ""experimentalDecorators"": true\n    }\n}\nYou should also make sure the peer dependency reflect-metadata is installed.\nnpm install --save reflect-metadata\nYou can then use the decorators:\nimport { ValidateClass, AssertType } from \'typescript-is\';\n\n@ValidateClass()\nclass A {\n    method(@AssertType() value: number) {\n        // You can safely use value as a number\n        return value;\n    }\n}\n\nnew A().method(42) === 42; // true\nnew A().method(\'42\' as any); // will throw error\nStrict equality (equals, createEquals, assertEquals, createAssertEquals)\nThis family of functions check not only whether the passed object is assignable to the specified type, but also checks that the passed object does not contain any more than is necessary. In other words: the type is also ""assignable"" to the object. This functionality is equivalent to specifying disallowSuperfluousObjectProperties in the options, the difference is that this will apply only to the specific function call. For example:\nimport { equals } from \'typescript-is\';\n\ninterface X {\n    x: string;\n}\n\nequals<X>({}); // false, because `x` is missing\nequals<X>({ x: \'value\' }); // true\nequals<X>({ x: \'value\', y: \'another value\' }); // false, because `y` is superfluous\nTo see the declarations of the functions and more examples, please check out index.d.ts.\nFor many more examples, please check out the files in the test/ folder.\nThere you can find all the different types that are tested for.\n⛔ What it won\'t do\n\nThis library aims to be able to check any serializable data.\nThis library will not check functions. Function signatures are impossible to check at run-time.\nThis library will not check classes. Instead, you are encouraged to use the native instanceof operator. For example:\n\nimport { is } from \'typescript-is\';\n\nclass MyClass {\n    // ...\n}\n\nconst instance: any = new MyClass();\nis<MyClass>(instance); // error -> classes are not supported.\n\n// Instead, use instanceof:\nif (instance instanceof MyClass) {\n    // ...\n}\n\nThis library will not magically check unbound type parameters. Instead, make sure all type parameters are bound to a well-defined type when invoking the is function. For example:\n\nimport { is } from \'typescript-is\';\n\nfunction magicalTypeChecker<T>(object: any): object is T {\n    return is<T>(object); // error -> type `T` is not bound.\n}\nIf you stumble upon anything else that is not yet supported, please open an issue or submit a PR. 😉\n🗺️ Road map\nFeatures that are planned:\n\nPromise support. Something like assertOrReject<Type>(object) will either resolve(object) or reject(error).\nOptimize the generated conditions. Things like false || ""key"" === ""key"" can be simplified. Might be more interesting to publish a different library that can transform a TypeScript AST, and then use it here, or use an existing one. Might be out of scope, as there are plenty of minifiers/uglifiers/manglers out there already.\n\n🔨 Building and testing\ngit clone https://github.com/woutervh-/typescript-is.git\ncd typescript-is/\nnpm install\n\n# Building\nnpm run build\n\n# Testing\nnpm run test\n'], 'url_profile': 'https://github.com/ChrisKitching', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'Türkiye/İstanbul', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xlarchs', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['corona-prediction\nsimpel polynomial regressing applied on corona virus current data\n'], 'url_profile': 'https://github.com/chhatrapal-digiqt', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 22, 2020', 'Python', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'JavaScript', 'Updated Feb 19, 2020', '6', 'Jupyter Notebook', 'CC-BY-SA-4.0 license', 'Updated Nov 10, 2020', 'TypeScript', 'MIT license', 'Updated Sep 8, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020']}"
"{'location': 'Nagpur,Maharastra,India', 'stats_list': [], 'contributions': '224 contributions\n        in the last year', 'description': ['Supervised-machine-learning-Project\nIn that I am putting all are project of Supervised Machine Learning i.e.Linear Regression,Logistics Regression,KNN Classification,Decision Tree Classification which have done previously during practice.\n'], 'url_profile': 'https://github.com/roshanappa', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PawanSran', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Baltimore-Fire-Department-Salary-Simple-Linear-Regression-Analysis\nOverview - Background and Findings\nIn China, probably anywhere in the world, the image of firefighters is often related to altruism and great. Thus, I want to concentrate my analysis on the income of employees in the fire department in Baltimore. Data is open-source data from Open Baltimore, which contains all the government employees\' salaries in fiscal year 2019.\nThe analysis focuses on three aspects:\n\nWhat is the linear regression model of employee\'s contractual income in fire department compares to their tenure.\nWhat is the linear regression model of employee\'s actual earning in the fire department compared to their tenure.\nWhat is the linear regression model of employee\'s difference in earning in fire department compared to their tenure.\n\nInsight One\nWe can see that regarding the question of how the contracted income changes compare to the employment time, the simple linear regression gives us a result with an R2 value of 49%. It means that this linear model can predict contracted income growth for nearly 50% of employees in the fire department.\n\nInsight Two\nWhen I add the data of the actual earnings on the chart and create another linear model. We can see that regarding the question of how the actual earning changes compare to the employment time, the simple linear regression gives us a result with an R2 value of 41.36%. It means that this linear model can predict actual earnings growth for about 40% of employees in the fire department. The standard error of residual is $21810.27, which means that the contracted income for 68% of employees are within $21810.27 from the predicted income. Interestingly, two models start around the same intercept. Then the gap gets bigger with the years go on.\n\nInsight Three\nThen it brings me to the third model, which is the linear model for the difference between actual earning and contracted salary. We can see that regarding the question of how the difference in earning v.s contracted income changes compare to the employment time, the simple linear regression that gives us a result with an R2 value of 9.89%. It means that this linear model can only predict the difference in earning less than 10% of employees in the fire department. However, the standard error of residual for the linear model is just $14804.61, which means that the income difference for 68% of employees are within $14804.61 from the predicted income. Also, we can conclude that the longer an employee stays in the fire department, he/she would have a higher possibility of earning more than the contracted income.\n\nOutline of Analysis Process\nThe original file was exported from the Open Baltimore Data Base and is available in this repository as a CSV document. The final excel document with the analysis and charts covered is also available in this repository as an Excel document.\nThe research question is: How will the contracted income change in the fire department, and what is the reality? Is the fact matches perfectly with the contract?\nThen I manipulated the excel document to create the first two charts presented above:\n\nBaltimore_Fire_Department_Employee\'s_Contracted_Salary_Compared_to_Their_Tenure\nBaltimore_Fire_Department_Employee\'s_Annual_Actual_Earning_Compared_to_Their_Tenure\n\nTwo models seem to fit because they have the same trend with a similar intercept. However, when I examined it closer, I found that the slope for actual earning is over $500 higher than the contracted income. According to the linear models, if you stayed in the fire department for 30 years, your actual income of that year might be $15190 higher than what is stated in the contract. How Can that be?\nSo it came to the next question, what is the linear model for the difference between actual earning and contracted income? I create the third chart presented above:\n\nBaltimore_Fire_Department_Employee\'s_Income_Difference_from_Contract_Compared_to_Their_Tenure\n\nThis model help gave some explanations. Although the R2 value is only 9.89%, which means it can only precisely predict 9.89% of the employee, the standard error of residual is much lower than the model for actual earning. Although it can not accurately predict the situation for many people, most people are closely related to the same trend.\nWe can see from the third model that the difference between actual earning and contracted income is generally increasing in years. Earning more can be influenced by many factors, such as overtime-earning. However, it\'s not likely that the longer an employee stays in the fire department, the more overtime he/she would have. Considering the reality, I assume that the increasing difference could be caused by benefits such as compensation for number of years in work, etc.\nWork Cited\nData Source: Open Baltimore Data Base\nMore than You would Want to Know(Step by Step Description of How I Manipulate the Excel)\nFirst, we need to take a look at the data and understand the meaning for each label for all non-contract Baltimore City government employees in Fiscal Year 2019:\n\nNAME: First and last name\nJOBTITLE: Civil service or non-civil service job title in\nDEPTID: Baltimore City government department ID number\nDESCR: Baltimore City government department name and subsection number\nHIRE_DT: Date employee was hired in Baltimore City government\nANNUAL: Employee\'s annual salary as noted in their contract\nGROSS: Employee\'s actual earned income from Baltimore City government\nHere is a preview of the original data from the website\n\n\nI will demonstrate the steps for making the first chart and analysis below.\nStep 1 - Data Cleaning\nDESCR Column\nFirst, I cleaned the DESCR column to remove the department subcategory numbers so that we can filter departments by name only. I used Flash Fill Function in Excel 2016 to auto separate the department names.\nTo do this, I:\n\nAdd a column after DESCR and name it as ""Department""\nManually type in the department for the first row\n\nSelect all the cells in that column\nUse short-cut ctrl + E\n\n\nHIRE_DT Column\nWe want to analyze factors comparing with the tenure of employees, so we need to have the data of how long they have been employed in Baltimore City. Since there are only the starting date of the employment, I used Today() formula to achieve the goal.While the TODAY() function counts the days the employee has been employed. I divided it by 365 to get the years of his/her employment\n\nInsert a new column after the HIRE_DT column and name it time_on_job\nType in =TODAY()- in the first cell and then click on the cell in that row under the HIRE_DT column.\nDivide the days by 365 to get the years on the job\nDouble click the bottom-right corner of the cell with the formula to fill in all the cells below\n\nFiltering Data for fire Department (Department Column)\nFor it to be easier when further creating the linear regression model and analyzing, I filter the data for the fire department. Then copy-paste it into a new worksheet.\n\nSelect the cell with ""Department"", click on Data -> Filter\nUse the filter to filter just the information for the fire department\nCreate a new worksheet and copy-paste the information into the new worksheet\n\nSimple Linear Regression\nNow that I have clean up the data, I can create a linear model to analyze the data.\n\nSelect both column of time_on_jobs and Gross, click on Insert -> Chart -> Scatter\nLabel the X-axis, Y-axis, and chart title so that we remember what this chart about is. Click on the chart, then click on Chart Design -> Add Chart Element -> Chart Title. Do the same for the X, Y-axis.\nAdd the linear model to the chart. Click on the chart, then click on Chart Design -> Add Chart Element -> Trendline ->Linear\nDisplay the equation and R2 for the linear model. Double click on the trendline, then from the Format Trendline on the left-hand side, click on Trendline Options -> Display Equation on chart. Do the same for R2.\n\nCalculate Errors\nWe can calculate the predicted contracted income for employees in the fire department with the linear model we create. Then we can see how many and how much the predicted values deviate from the data. Together with the standard error of residual that I am going to calculate, we can find out/ decide if an employee is an outlier.\nCalculate Predicted Value\n\nName a new column as Predicted_difference\nIn the second cell of that column, calculate the predicted value with the formula time_on_jobs * slope + intercept\nDouble click the bottom-right of that cell to auto-fill the predicted value for all the employees\n\nCalculate Error Value\n\nName another new column as Difference_Error\nIn the second cell of that column, calculate the errors with the formula ANNUAL_RT - Predicted_difference\nDouble click the bottom-right of that cell to auto-fill the error value for all the employees\n\nStandard Error of Residual and Outliers\nWhile the R2 value is accurate for analyzing how the linear model can precisely predict much percentage of the data, it\'s useful to introduce another matric, the standard error of residual. Standard error of residual can help to interpret the distribution of the data. About 68% of the data is within one standard error of residual, while about 98% of the data is within two standard errors of residual. By using this factor, we can decide whether data is an outlier (whether it is within two standard errors of residual). Thus we can further analyze what could be the situations for the outliers and if it\'s reasonable to be included in the model.\nCalculate Standard Error of Residual\n\nName a cell under the chart as Standard Error of Residual\nIn the next cell, calculate the errors with the function formula =STEYX(ANNUAL_RT:time_on_jobs)\n\nFind Out the Outlier\n\nName a new column as Outliers\nIn the second cell of that column, find out outliers with the formula =IF(ABS(Difference_error)>2*Standard Error of Residual,1,0) 1 means it\'s an outlier, 0 represents the opposite.\n\n'], 'url_profile': 'https://github.com/LTLUTUO', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['linearRegressionMediaCompanyCaseStudy\nData Science Project : A digital media company (similar to Voot, Hotstar, Netflix, etc.) had launched a show. Initially, the show got a good response, but then witnessed a decline in viewership. The company wants to figure out what went wrong.\n'], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['multipleLinearRegressionHousingCaseStudy\nTo use the data to optimize the sale prices of the properties based on important factors such as area, bedrooms, parking, etc.\nProblem Statement:\nConsider a real estate company that has a dataset containing the prices of properties in the Delhi region. It wishes to use the data to optimise the sale prices of the properties based on important factors such as area, bedrooms, parking, etc.\nEssentially, the company wants —\n\nTo identify the variables affecting house prices, e.g. area, number of rooms, bathrooms, etc.\nTo create a linear model that quantitatively relates house prices with variables such as number of rooms, area, number of bathrooms, etc.\nTo know the accuracy of the model, i.e. how well these variables can predict house prices.\n\nSo interpretation is important!\n'], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020']}"
"{'location': 'Toronto, ON, Canada', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Build-a-Regression-Model-in-Keras\nsee the above notebook on nbviewer in the following link: here.\n\nthis is the final assignment of coursera course Introduction to Deep Learning and Neural Networks with Keras which can be accessed here.\n'], 'url_profile': 'https://github.com/bobchengyang', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Serbia', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Belgrade Apartments Price Prediction ML Regression\nProgram consists of 2 parts:\n\n\nGetting data about Belgrade Apartments from website: halooglasi.com with python using BeautifulSoup\nlibrary and creating CSV file. Web scraping script and csv file can be found on GitHub link above. Web\nscraping script: webScrappingRealEstate.py | csv file: RealEstateBelgrade.csv\n\n\nPutting data from csv file into Pandas(python library) dataframe, modifying dataframe and traning ML\nmodel using algorithms from scikit-learn python library.\n\n\n'], 'url_profile': 'https://github.com/NenadM42', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': [""leadScoringCaseStudy\nLead conversion prediction for online education company\nProblem Statement\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. \nWhen these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals.\nOnce these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%.\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. \nIf they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\nLead Conversion Process - Demonstrated as a funnel\nAs you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom.\nIn the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. \nThe company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.\nThe CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.\nData\nYou have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column ‘Converted’ which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn’t converted.\nAnother thing that you also need to check out for are the levels present in the categorical variables.\nMany of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value.\nGoal\n\nBuild a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\n\n""], 'url_profile': 'https://github.com/Ashutosh27ind', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/linhong801', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Nagpur, India', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Titanic-Survival-Classifier-Using-Logistic-Regression\nPredicting survival on the Titanic based on:\nSKLearn Logistic Regression Classifier\nLogistic Regression Classifier built without any library\nClassification additionally involved:\ni) Handling missing data\nii) Handling string categorical values\niii) Feature Scaling\niv) Hyperparameter tuning\nDetailed explanation is provided in the notebook.\n'], 'url_profile': 'https://github.com/rajats', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Linear-Regression-From-Scratch-in-Python\n'], 'url_profile': 'https://github.com/yeswanth544', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}"
"{'location': 'Stillwater, Oklahoma', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HarishPatlolla', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NarimanElsamadony', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Regression-Model-for-predicting-House-Sale-Prices\nThis report aims to investigate how well the prices of houses in King County can be predicted from the area of the living spaces. The dataset used for this analysis contains sample house sale prices for King County, areas of living spaces and basements, number of bedrooms and bathrooms and fifteen other house related features. A simple linear regression model would be used to predict the prices of houses in King County from the areas of living spaces. The results of this analysis can provide real estate dealers with a reliable estimate for the value of properties in King County.\nBefore the simple linear regression model is used, we would determine, using a scatterplot, if there is a linear association (correlation) between the variables of interest. This linear association will be quantified using the Pearson’s correlation coefficient r. Afterwards, a least square regression line would be used to produce a mathematical equation that describes the relationship between area of living spaces and the prices of houses which will be used to predict the prices of houses in King County. Again, the coefficient of determination R2 (r2 for simple linear regression) is computed to measure how well the model predicts the prices of houses. Furthermore, a confidence interval for the slope of the regression equation is computed and interpreted. Finally, a p-value for the regression slope is computed and interpreted to evaluate the statistical significance of this slope.\n'], 'url_profile': 'https://github.com/Chika-Jinanwa', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'Kolkata, India', 'stats_list': [], 'contributions': '410 contributions\n        in the last year', 'description': ['Data-Analysis-with-ML-and-Regression\nData Analysis projects I worked on during a Hands-on Workshop on Data Science and Machine Learning conducted by NIVT, at Jadavpur University. \nTechnologies used:  Jupyter Notebook , numpy , pandas , matplotlib , scikit-learn.\n'], 'url_profile': 'https://github.com/soumitri2001', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Prateek2690', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Prateek2690', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project - Regression Modeling with the Ames Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Ames Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'ames.csv\' as a pandas dataframe\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\'seaborn\')\names = pd.read_csv(\'ames.csv\')\n\nsubset = [\'YrSold\', \'MoSold\', \'Fireplaces\', \'TotRmsAbvGrd\', \'GrLivArea\',\n          \'FullBath\', \'YearRemodAdd\', \'YearBuilt\', \'OverallCond\', \'OverallQual\', \'LotArea\', \'SalePrice\']\n\ndata = ames.loc[:, subset]\nThe columns in the Ames housing data represent the dependent and independent variables. We have taken a subset of all columns available to focus on feature interpretation rather than preprocessing steps. The dependent variable here is the sale price of a house SalePrice. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n# You observations here \nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nClearly, the results are not very reliable. The best R-Squared is witnessed with OverallQual, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Ames dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['predicting rating with full linear regression model\nhere we uses sklearn library\n'], 'url_profile': 'https://github.com/rishisankhla', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'R', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Predingting-house-prices-using-linear-regression\n'], 'url_profile': 'https://github.com/Code-tanisha', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['predicting car purchase logistic regression logit function\nhere we will just learn the execution of logit function in python\n'], 'url_profile': 'https://github.com/rishisankhla', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['BayesGPfit\nAn R package for Bayesian Gaussian process regression on regular grid points based on modified exponential sqaured kernel\n\nInstall and load the package\n\ndevtools::install_github(""kangjian2016/BayesGPfit"")\nlibrary(BayesGPfit)\n\n\nSimulate curve on d-dimensional Euclidean space\n\nlibrary(lattice)\nset.seed(1224)\ndat = list()\ndat$x = GP.generate.grids(d=2,num_grids = 100)\ncurve = GP.simulate.curve.fast(dat$x,a=0.01,b=0.5,poly_degree=20L)\nGP.plot.curve(curve,main=""Simulated Curve"")\n\n\nBayesian model fitting based on two methods\n\ndat$f = curve$f + rnorm(length(curve$f),sd=1)\nfast_fit = GP.fast.Bayes.fit(dat$f,dat$x,a=0.01,b=0.5,poly_degree=20L,progress_bar = TRUE)\nreg_fit = GP.Bayes.fit(dat$f,dat$x,a=0.01,b=0.5,poly_degree=20L,progress_bar = TRUE)\nmse = c(reg = mean((reg_fit$f - curve$f)^2),\n       fast = mean((fast_fit$f - curve$f)^2))\nprint(mse)\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '443 contributions\n        in the last year', 'description': ['Data Modelling on US Election Data\nThis project was done as a course assignment for CS418: Introduction to Data Science course at the University of Illinois at Chicago during the Fall 2019 term along with teammates Yushenli1996 and nathanhe789.\n\nThe dataset was partly provided to us by the Professor. There were 2 CSV files: one contained a merged data file of demographic data and election data of counties of certain US states from the 2016 US Senate Elections(generated in this project), and another data file containing only the demographic data of some US counties.\nThe merged data file was meant to be used for training machine learning classification/predictive models to predict winning political party for a particular county, while the demographic data file was to be used as the testing set for the models.\nThe merged data file was partitioned into training and validation sets using Holdout method. 75% of data was allocated for training the models and rest 25% for validation of the models.\nAdditionally, the numeric attributes in the training and validation sets were standardized to have a mean of 0 and variance of 1.\n\nThe main purpose of the assignment was to perform Data Modelling on the merged demographic-election data. The data modelling tasks performed on the dataset are:\n\n\nBuild Linear Regression Model\n\nUsing all attributes\nBy selecting different attributes to find the best set of attributes\nUsing LASSO regression\n\n\n\nBuild Classification Models and select 2 best performing models\n\nUsing all attributes\nBy selecting different attributes to find the best set of attributes\n\n\n\nBuild Clustering Models and select 2 best performing models\n\nUsing all attributes\nBy selecting different attributes to find the best set of attributes\n\n\n\nPredict the Democratic and Republican party votes of each county using the best performing regression model using the testing set of demographic data\n\n\nPredict winning political party in each county using the best performing classification model using the testing set of demographic data\n\n\nCreate choropleth map to visualize the majority political party of each county as predicted by the best performing classification model\n\nUsing political party attribute in the dataset\n\nUsing political party predicted by SVM\n\n\n\n\nCheck out the Jupyter Notebook or the project report to see the data science flow implemented.\n'], 'url_profile': 'https://github.com/samujjwaal', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Telecom Churn Case Study\nBusiness Problem Overview\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\nUnderstanding and Defining Churn\nThere are two main models of payment in the telecom industry - postpaid (customers pay a monthly/annual bill after using the services) and prepaid (customers pay/recharge with a certain amount in advance and then use the services).\nIn the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\nHowever, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\nThus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term ‘churn’ should be defined carefully.  Also, prepaid is the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\nThis project is based on the Indian and Southeast Asian market.\nDefinitions of Churn\nThere are various ways to define churn, such as:\nRevenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as ‘customers who have generated less than INR 4 per month in total/average/median revenue’.\nThe main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don’t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\nUsage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\nA potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‘two-months zero usage’ period, predicting churn could be useless since by that time the customer would have already switched to another operator.\nIn this project, you will use the usage-based definition to define churn.\nHigh-value Churn\nIn the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\nIn this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.\nUnderstanding the Business Objective and the Data\nThe dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively.\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\nUnderstanding Customer Behaviour During Churn\nCustomers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\n\nThe ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.\n\nThe ‘action’ phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n\nThe ‘churn’ phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase.\n\nIn this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase.\nData Preparation\nThe following data preparation steps are crucial for this problem:\n\nDerive new features\n\nThis is one of the most important parts of data preparation since good features are often the differentiators between good and bad models. Use your business understanding to derive features you think could be important indicators of churn.\n\nFilter high-value customers\n\nAs mentioned above, you need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\nAfter filtering the high-value customers, you should get about 29.9k rows.\n\nTag churners and remove attributes of the churn phase\n\nNow tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\n\n\ntotal_ic_mou_9\n\n\ntotal_og_mou_9\n\n\nvol_2g_mb_9\n\n\nvol_3g_mb_9\n\n\nAfter tagging churners, remove all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names).\nModelling\nBuild models to predict churn. The predictive model that you’re going to build will serve two purposes:\n\n\nIt will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase). By knowing this, the company can take action steps such as providing special plans, discounts on recharge etc.\n\n\nIt will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks.\n\n\nIn some cases, both of the above-stated goals can be achieved by a single machine learning model. But here, you have a large number of attributes, and thus you should try using a dimensionality reduction technique such as PCA and then build a predictive model. After PCA, you can use any classification model.\nAlso, since the rate of churn is typically low (about 5-10%, this is called class-imbalance) - try using techniques to handle class imbalance.\nYou can take the following suggestive steps to build the model:\n\n\nPreprocess data (convert columns to appropriate formats, handle missing values, etc.)\n\n\nConduct appropriate exploratory analysis to extract useful insights (whether directly useful for business or for eventual modelling/feature engineering).\n\n\nDerive new features.\n\n\nReduce the number of variables using PCA.\n\n\nTrain a variety of models, tune model hyperparameters, etc. (handle class imbalance using appropriate techniques).\n\n\nEvaluate the models using appropriate evaluation metrics. Note that is is more important to identify churners than the non-churners accurately - choose an appropriate evaluation metric which reflects this business goal.\n\n\nFinally, choose a model based on some evaluation metric.\n\n\nThe above model will only be able to achieve one of the two goals - to predict customers who will churn. You can’t use the above model to identify the important features for churn. That’s because PCA usually creates components which are not easy to interpret.\nTherefore, build another model with the main objective of identifying important predictor attributes which help the business understand indicators of churn. A good choice to identify important variables is a logistic regression model or a model from the tree family. In case of logistic regression, make sure to handle multi-collinearity.\nAfter identifying important predictors, display them visually - you can use plots, summary tables etc. - whatever you think best conveys the importance of features.\nFinally, recommend strategies to manage customer churn based on your observations.\n'], 'url_profile': 'https://github.com/apoorvbh', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '287 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ZubairQazi', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'Auburn, Alabama, USA', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': [""Gelman Hill Exercises\nFinally getting around to uploading my solutions to the chapter problems from Andrew Gelman and Jennifer Hill's Data Analysis Using Regression and Multilevel/Hierarchical Models (first ed). I will try to keep this updated as I have time to work through the book. Feedback is welcome!\n\nChapter 3 (last updated: 2019-12)\n\n\nR Script\nR Markdown (use to render html)\n\n\nChapter 4 (last updated: up next)\n\n""], 'url_profile': 'https://github.com/kww-22', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Capstone\nThe Git repo where the code for creating a linear regression model for my capstone will be added!\n'], 'url_profile': 'https://github.com/laniepreston', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}","{'location': 'IIIT Naya Raipur', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': [""Deep Neural Networks from Scratch using NumPy\nThis project implements a Python Class to define, train and evaluate Deep Neural Network models for classification and regression tasks.\nIt uses the Backpropagation algorithm with various Activation functions, Optimizers and Regularizers for training the model objects.\nThis project is completely built from scratch using the NumPy library. No Deep Learning framework is utilized.\nDetailed description of this project along with results can be found here.\nGetting Started\nPrerequisites\nRunning this project on your local system requires the following packages to be installed :\n\nnumpy\nmatplotlib\n\nThey can be installed from the Python Package Index using pip as follows :\n pip install numpy\n pip install matplotlib\n\nYou can also use Google Colab in a Web Browser without needing to install the mentioned packages.\nUsage\nThis project is implemented as an interactive Jupyter Notebook. You just need to open the notebook on your local system or on Google Colab and execute the code cells in sequential order. The function of each code cell is properly explained with the help of comments.\nTools Used\n\nNumPy : Used for storing and manipulating high dimensional arrays, and performing large scale mathematical computations on them.\nMatplotlib : Used for plotting the Learning Curves.\nGoogle Colab : Used as the development environment for executing high-end computations on its backend GPUs/TPUs and for editing the Jupyter Notebook.\n\nContributing\nYou are welcome to contribute :\n\nFork it (https://github.com/rohanrao619/Deep_Neural_Networks_from_Scratch/fork)\nCreate new branch : git checkout -b new_feature\nCommit your changes : git commit -am 'Added new_feature'\nPush to the branch : git push origin new_feature\nSubmit a pull request !\n\nLicense\nThis Project is licensed under the MIT License, see the LICENSE file for details.\nProject Description and Results\nClass Methods\nA Python class named Deep_Neural_Network is defined with the following major methods :\n\n\ncreate() : The method to define the architecture of Deep Neural Network and initialize weights.\ncreate(self,input_size,output_size,hidden_dims,output_type,initializer='random',\n       seed=None,activation='relu',leaky_relu_slope=0.1)\n           \nParameters:\n\ninput_size(int)       :   No. of neurons in input layer.\n\noutput_size(int)      :   No. of classes in classification task (2 in case of binary classification,\n                          modify the dataset accordingly !)\n                          (or) No. of Target variables in case of regression task.\n\nhidden_dims(int list) :   No. of neurons in hidden layers.\n\noutput_type(string)   :   Type of task :\n                          'classification'  :  Classification (discrete target).\n                          'regression'      :  Regression (continuous target).\n\ninitializer(string)   :   Weight initializer :\n                          'random'  : Random initialization.\n                          'xavier'  : Xavier initialization (preferred for tanh activation).\n                          'he'      : He initialization (preferred for ReLU activation).\n\nseed(int)             :   NumPy seed for random initialization.\n\nactivation(string)    :   Activation function for hidden layers. One of the following :\n                          'linear'  : Linear activation.\n                          'sigmoid' : Sigmoid activation.\n                          'tanh'    : Hyperbolic tangent activation.\n                          'relu'    : Rectified Linear Unit activation.\n                          'lrelu'   : Leaky Rectified Linear Unit activation.\n\n                          Activation function at the output layer would be SoftMax for classification\n                          and Linear for regression.\n\nleaky_relu_slope(int) :   Slope for Leaky ReLU activation.\n\n\n\ntrain() : The method to train the weights and biases of each layer for the provided training data with ground truths.\ntrain(self,X_train,Y_train,X_val,Y_val,optimizer='vanilla',regularizer=None,regularizer_lambda=0.02,\n      keep_probs=[],mini_batch_size=32,epochs=100,learning_rate=0.01,beta=0.9,beta1=0.9,beta2=0.99,\n      print_loss_freq=100,plot_loss=True)\n\nParameters :\n\nX_train(NumPy 2D array of shape(input_size,m))   :  Input data(for batch of size m) for training.\n\nY_train(NumPy 2D array of shape(output_size,m))  :  Ground truths(for batch of size m) for training.\n\nX_val(NumPy 2D array of shape(input_size,m))     :  Input data(for batch of size m) for validation.\n\nY_val(NumPy 2D array of shape(output_size,m))    :  Ground truths(for batch of size m) for validation.\n\noptimizer(string)             :   Optimizer for training process, one of the following :\n                                  'vanilla'     : Original gradient decsent.\n                                  'momentum'    : Gradient descent with momentum.\n                                  'rmsprop'     : Root mean square propagation.\n                                  'adam'        : Adaptive moments estimation.\n\nregularizer(string)           :   Regularizer for weights of network, one of the following :\n                                  'l1'      : L1 regularization.\n                                  'l2'      : L2 regularization.\n                                  'dropout' : Dropout regularization.\n                                  None      : No regularizer.\n\nregularizer_lambda(float)     :   Regularization parameter lambda for L1 or L2 regularization.\n\nkeep_probs(float[0,1] list)   :   Keeping probabilities for hidden layers in Dropout regularization.\n\nmini_batch_size(int)          :   Mini Batch size (1 for Stochastic gradient descent).\n\nepochs(int)                   :   No. of iterations over the training set.\n\nlearning_rate(float)          :   Learning rate aplha.\n\nbeta(float)                   :   Optimizer parameter beta for 'momentum' and 'rmsprop' optimizers.\n\nbeta1(float)                  :   Optimizer parameter beta2 for 'adam' optimizer.\n\nbeta2(float)                  :   Optimizer parameter beta2 for 'adam' optimizer.\n\nprint_loss_freq(int)          :   Frequency of printing metrics.\n\nplot_loss(boolean)            :   Plot learning curves or not.\n\nReturns :\n\nMetrics_history(tuple)        :   History of metrics in form of lists\n\n\n\npredict() : The method to predict outputs for given unknown input data.\npredict(self,X)\n\nParameters :\n\nX(NumPy 2D array of shape (input_size,m))  : Input data for batch of size m.\n\nReturns :\n\nY_pred(NumPy 2D array of shape (output_size_size,m))  : Predicted output for batch of size m.\n\n\n\nOther methods implemented for back-end computations are :\n\nMethods computing various Activations functions and their gradients.\nforward_propagation() : The method to forward propagate input data through the network, and calculate activations of each layer in the network.\nbackward_propagation() : The method to compute the gradient of cost with respect to weights and biases of each layer in the network.\ncompute_cost() : The method to compute the cost for the current forward propagated batch.\naccuracy() : The method to calculate classification accuracy.\nto_one_hot() : The method to convert SoftMax probabilities to labels in one hot form.\nsave_weights() : The method to save model weights.\nload_weights() : The method to load model weights.\n\nTheir detailed description can be found here.\nMNIST Handwritten Digits Classification\nA Deep_Neural_Network object was created and trained with the following configuration :\nmodel = Deep_Neural_Network()\n\nmodel.create(784,10,[800],output_type='classification',activation='lrelu',initializer='he',\n             leaky_relu_slope=0.1)\n             \ncosts = model.train(X_train,Y_train,X_val,Y_val,optimizer='adam',regularizer='dropout',keep_probs=[0.75],\n                    mini_batch_size=128,epochs=20,print_loss_freq=5,learning_rate=0.0002)\n\nFollowing results were obtained :\n\nTest accuracy of 98.4 % was achieved using the same model.\nBoston Housing Prices Prediction\nA Deep_Neural_Network object was created and trained with the following configuration :\nmodel = Deep_Neural_Network()\n\nmodel.create(2,1,[10,20],output_type='regression',activation='relu',initializer='he')\n\ncosts=model.train(X_train.T,Y_train,X_val.T,Y_val,optimizer='rmsprop',regularizer='l2',\n                  regularizer_lambda=0.1,mini_batch_size=32,epochs=50,print_loss_freq=10,\n                  learning_rate=0.002)\n\nFollowing results were obtained :\n\nThe model produced L2 Loss of 12.64 on Test data.\nFinal Notes\nThanks for going through this Repository! Have a nice day.\nGot any Queries? Feel free to contact me.\nSaini Rohan Rao\n\n\n\n\n\n\n""], 'url_profile': 'https://github.com/rohanrao619', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'C++', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Predicting the salary of Hitters\nManoj Bhandari\nif(!require(""pacman"")) install.packages(""pacman"")\n## Loading required package: pacman\n\npacman::p_load(ISLR, caret, ggplot2, rpart, rpart.plot, gbm, leaps, randomForest)\n# Loading the Library and Data Set\ndata(""Hitters"")\n# Converting the data set into a data frame\nhitters.df <- Hitters\nRemove the observations with unknown salary information.\n#Creating a new data frame after removing all the missing salary observations\nhitters.new.df <- as.data.frame(hitters.df[complete.cases(hitters.df$Salary),])\n\n#Number of observations removed\nremoved <- sum(is.na(hitters.df$Salary))\n59 records were removed\nGenerate log-transform the salaries.\nlog.transform <- log(hitters.new.df$Salary)\nhist(Hitters$Salary)\n\nhist(log.transform)\n\nHere, log-transformation is used to convert a highly skewed salary data into a relatively lesser skewed (normalized) data.\nCreate a scatterplot with Hits on the y-axis and Years on the x-axis using all the observations. Color code the observations using the log Salary variable.\nggplot(hitters.new.df,aes(Years,Hits)) +\n  geom_point(aes(colour = log.transform))\n\nAccording to the scatterplot and the colour scale, the log salary is higher for players with ~5+ years of experience and ~100+ hits.\nRun a linear regression model of Log Salary on all the predictors using the entire dataset. Use regsubsets() function to perform best subset selection from the regression model. Identify the best model using BIC. Which predictor variables are included in this (best) model?\n#removing salary column \nhitters.new.df$Salary <- NULL\n\n#Adding the log.transform column to the main data frame\nhitters.new.df$LogSalary <- log.transform\n\n#running regression\nhitters.lm <- lm(LogSalary~.,data = hitters.new.df)\n\nsummary(hitters.lm)\n## \n## Call:\n## lm(formula = LogSalary ~ ., data = hitters.new.df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.22870 -0.45350  0.09424  0.40474  2.77223 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  4.618e+00  1.765e-01  26.171  < 2e-16 ***\n## AtBat       -2.984e-03  1.232e-03  -2.421  0.01620 *  \n## Hits         1.308e-02  4.622e-03   2.831  0.00503 ** \n## HmRun        1.179e-02  1.205e-02   0.978  0.32889    \n## Runs        -1.419e-03  5.794e-03  -0.245  0.80670    \n## RBI         -1.675e-03  5.056e-03  -0.331  0.74063    \n## Walks        1.096e-02  3.554e-03   3.082  0.00229 ** \n## Years        5.696e-02  2.413e-02   2.361  0.01902 *  \n## CAtBat       1.283e-04  2.629e-04   0.488  0.62596    \n## CHits       -4.414e-04  1.311e-03  -0.337  0.73670    \n## CHmRun      -7.809e-05  3.144e-03  -0.025  0.98020    \n## CRuns        1.513e-03  1.459e-03   1.037  0.30072    \n## CRBI         1.312e-04  1.346e-03   0.097  0.92246    \n## CWalks      -1.466e-03  6.377e-04  -2.298  0.02239 *  \n## LeagueN      2.825e-01  1.541e-01   1.833  0.06797 .  \n## DivisionW   -1.656e-01  7.847e-02  -2.111  0.03580 *  \n## PutOuts      3.389e-04  1.505e-04   2.251  0.02526 *  \n## Assists      6.214e-04  4.300e-04   1.445  0.14970    \n## Errors      -1.197e-02  8.537e-03  -1.402  0.16225    \n## NewLeagueN  -1.742e-01  1.536e-01  -1.134  0.25788    \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 0.6135 on 243 degrees of freedom\n## Multiple R-squared:  0.5586, Adjusted R-squared:  0.524 \n## F-statistic: 16.18 on 19 and 243 DF,  p-value: < 2.2e-16\n\n#subset selection\nsearch <- regsubsets(LogSalary ~ ., data = hitters.new.df, nbest = 1, nvmax = dim(hitters.new.df)[2],\n                     method = ""exhaustive"")\nsum <- summary(search)\n\nsum$bic\n##  [1] -117.0304 -156.4291 -159.2777 -159.2182 -159.0885 -157.9207 -157.1229\n##  [8] -156.1954 -152.7649 -148.8061 -144.5962 -140.6541 -136.5480 -131.0939\n## [15] -125.7112 -120.1995 -114.7125 -109.1859 -103.6145\n\nwhich.min(sum$bic)\n## [1] 3\n\nsum$which[(which.min(sum$bic)),]\n## (Intercept)       AtBat        Hits       HmRun        Runs         RBI \n##        TRUE       FALSE        TRUE       FALSE       FALSE       FALSE \n##       Walks       Years      CAtBat       CHits      CHmRun       CRuns \n##        TRUE        TRUE       FALSE       FALSE       FALSE       FALSE \n##        CRBI      CWalks     LeagueN   DivisionW     PutOuts     Assists \n##       FALSE       FALSE       FALSE       FALSE       FALSE       FALSE \n##      Errors  NewLeagueN \n##       FALSE       FALSE\n\nWe can see that Hits, Walks and Years are the three predictor variables included in the best model.\nNow create a training data set consisting of 80 percent of the observations, and a test data set consisting of the remaining observations.\nset.seed(42) \n# Create data partition\npartition <- createDataPartition(hitters.new.df$LogSalary, p=0.8, list = FALSE)\n\n# Create data frame with training data\ntraining <- hitters.new.df[partition,]\n# Create data frame with test/validation data\nvalidation <- hitters.new.df[-partition,]\nGenerate a regression tree of log Salary using only Years and Hits variables from the training data set. Which players are likely to receive highest salaries according to this model?\n# Generate regression tree using only Years and Hits as predictors\nsalary.tree <- rpart(LogSalary~ Years + Hits, training)\nsummary(salary.tree)\n## Call:\n## rpart(formula = LogSalary ~ Years + Hits, data = training)\n##   n= 212 \n## \n##           CP nsplit rel error    xerror       xstd\n## 1 0.42633247      0 1.0000000 1.0140639 0.07340074\n## 2 0.11655032      1 0.5736675 0.5865620 0.06788478\n## 3 0.03751440      2 0.4571172 0.4965007 0.06716200\n## 4 0.02450593      3 0.4196028 0.5011562 0.07317841\n## 5 0.01471508      4 0.3950969 0.4822315 0.07325505\n## 6 0.01419923      5 0.3803818 0.4934601 0.07854788\n## 7 0.01272645      7 0.3519833 0.4899783 0.07894188\n## 8 0.01000000      8 0.3392569 0.4816827 0.07909190\n## \n## Variable importance\n## Years  Hits \n##    70    30 \n## \n## Node number 1: 212 observations,    complexity param=0.4263325\n##   mean=5.923586, MSE=0.7900778 \n##   left son=2 (69 obs) right son=3 (143 obs)\n##   Primary splits:\n##       Years < 4.5   to the left,  improve=0.4263325, (0 missing)\n##       Hits  < 117.5 to the left,  improve=0.1965698, (0 missing)\n##   Surrogate splits:\n##       Hits < 29.5  to the left,  agree=0.689, adj=0.043, (0 split)\n## \n## Node number 2: 69 observations,    complexity param=0.0375144\n##   mean=5.088074, MSE=0.4701937 \n##   left son=4 (42 obs) right son=5 (27 obs)\n##   Primary splits:\n##       Hits  < 113   to the left,  improve=0.1936769, (0 missing)\n##       Years < 3.5   to the left,  improve=0.1556438, (0 missing)\n## \n## Node number 3: 143 observations,    complexity param=0.1165503\n##   mean=6.326735, MSE=0.4450625 \n##   left son=6 (78 obs) right son=7 (65 obs)\n##   Primary splits:\n##       Hits  < 117.5 to the left,  improve=0.30673420, (0 missing)\n##       Years < 6.5   to the left,  improve=0.05862589, (0 missing)\n## \n## Node number 4: 42 observations,    complexity param=0.01419923\n##   mean=4.846119, MSE=0.4591726 \n##   left son=8 (35 obs) right son=9 (7 obs)\n##   Primary splits:\n##       Hits  < 42    to the right, improve=0.1046318, (0 missing)\n##       Years < 3.5   to the left,  improve=0.0981606, (0 missing)\n## \n## Node number 5: 27 observations,    complexity param=0.01272645\n##   mean=5.464449, MSE=0.2546142 \n##   left son=10 (10 obs) right son=11 (17 obs)\n##   Primary splits:\n##       Years < 2.5   to the left,  improve=0.3100747, (0 missing)\n##       Hits  < 154.5 to the left,  improve=0.2753220, (0 missing)\n## \n## Node number 6: 78 observations,    complexity param=0.02450593\n##   mean=5.989447, MSE=0.3493758 \n##   left son=12 (23 obs) right son=13 (55 obs)\n##   Primary splits:\n##       Years < 6.5   to the left,  improve=0.15062240, (0 missing)\n##       Hits  < 72.5  to the left,  improve=0.08789166, (0 missing)\n##   Surrogate splits:\n##       Hits < 112.5 to the right, agree=0.731, adj=0.087, (0 split)\n## \n## Node number 7: 65 observations\n##   mean=6.731481, MSE=0.2595516 \n## \n## Node number 8: 35 observations,    complexity param=0.01419923\n##   mean=4.748095, MSE=0.174288 \n##   left son=16 (26 obs) right son=17 (9 obs)\n##   Primary splits:\n##       Years < 3.5   to the left,  improve=0.4489766, (0 missing)\n##       Hits  < 68.5  to the left,  improve=0.1256046, (0 missing)\n## \n## Node number 9: 7 observations\n##   mean=5.336242, MSE=1.595331 \n## \n## Node number 10: 10 observations\n##   mean=5.098096, MSE=0.02167841 \n## \n## Node number 11: 17 observations\n##   mean=5.67995, MSE=0.266245 \n## \n## Node number 12: 23 observations\n##   mean=5.634708, MSE=0.2731994 \n## \n## Node number 13: 55 observations,    complexity param=0.01471508\n##   mean=6.137792, MSE=0.3066013 \n##   left son=26 (10 obs) right son=27 (45 obs)\n##   Primary splits:\n##       Hits  < 50.5  to the left,  improve=0.14616100, (0 missing)\n##       Years < 7.5   to the right, improve=0.03966411, (0 missing)\n## \n## Node number 16: 26 observations\n##   mean=4.583513, MSE=0.07930576 \n## \n## Node number 17: 9 observations\n##   mean=5.223551, MSE=0.1443708 \n## \n## Node number 26: 10 observations\n##   mean=5.688727, MSE=0.2432422 \n## \n## Node number 27: 45 observations\n##   mean=6.237584, MSE=0.2659094\n\n# Plot the regression tree\nrpart.plot(salary.tree)\n\nAccording to the regression tree using only years and hits, the players with more than 5 years of experience and more than 118 hits are likely to receive highest salaries. The branch with the logSalary value of 6.7 follow this rule and shows 31% of the data.\nNow create a regression tree using all the variables in the training data set. Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter lambda. Produce a plot with different shrinkage values on the x- axis and the corresponding training set MSE on the y-axis.\n# Generate regression tree using all the predictors\nsalary.tree.1 <- rpart(LogSalary~ ., training)\nsummary(salary.tree.1)\n## Call:\n## rpart(formula = LogSalary ~ ., data = training)\n##   n= 212 \n## \n##           CP nsplit rel error    xerror       xstd\n## 1 0.55135671      0 1.0000000 1.0066497 0.07305976\n## 2 0.05834488      1 0.4486433 0.4804035 0.06344758\n## 3 0.04189138      2 0.3902984 0.4572746 0.06691197\n## 4 0.02919914      3 0.3484070 0.4383191 0.07325143\n## 5 0.02838394      4 0.3192079 0.4225515 0.07188270\n## 6 0.01938340      5 0.2908240 0.4071116 0.07147896\n## 7 0.01793890      6 0.2714406 0.3994112 0.07163687\n## 8 0.01000000      7 0.2535017 0.3988895 0.07105675\n## \n## Variable importance\n##  CAtBat   CHits   CRuns    CRBI  CWalks   Years    Runs    Hits   AtBat \n##      17      17      17      16      14      10       2       2       2 \n##  CHmRun   Walks     RBI PutOuts \n##       2       1       1       1 \n## \n## Node number 1: 212 observations,    complexity param=0.5513567\n##   mean=5.923586, MSE=0.7900778 \n##   left son=2 (74 obs) right son=3 (138 obs)\n##   Primary splits:\n##       CAtBat < 1329.5 to the left,  improve=0.5513567, (0 missing)\n##       CHits  < 419    to the left,  improve=0.5442920, (0 missing)\n##       CRuns  < 208.5  to the left,  improve=0.5356312, (0 missing)\n##       CRBI   < 121    to the left,  improve=0.5149909, (0 missing)\n##       CWalks < 120    to the left,  improve=0.5119708, (0 missing)\n##   Surrogate splits:\n##       CHits  < 324    to the left,  agree=0.986, adj=0.959, (0 split)\n##       CRuns  < 158    to the left,  agree=0.976, adj=0.932, (0 split)\n##       CRBI   < 119.5  to the left,  agree=0.953, adj=0.865, (0 split)\n##       CWalks < 120    to the left,  agree=0.934, adj=0.811, (0 split)\n##       Years  < 4.5    to the left,  agree=0.873, adj=0.635, (0 split)\n## \n## Node number 2: 74 observations,    complexity param=0.04189138\n##   mean=5.022274, MSE=0.349524 \n##   left son=4 (45 obs) right son=5 (29 obs)\n##   Primary splits:\n##       CRuns  < 91     to the left,  improve=0.2712824, (0 missing)\n##       CHits  < 182    to the left,  improve=0.2650167, (0 missing)\n##       CAtBat < 689    to the left,  improve=0.2484637, (0 missing)\n##       CRBI   < 55.5   to the left,  improve=0.2461947, (0 missing)\n##       CWalks < 63.5   to the left,  improve=0.1685103, (0 missing)\n##   Surrogate splits:\n##       CHits  < 172.5  to the left,  agree=0.973, adj=0.931, (0 split)\n##       CAtBat < 689    to the left,  agree=0.959, adj=0.897, (0 split)\n##       CRBI   < 60     to the left,  agree=0.878, adj=0.690, (0 split)\n##       CWalks < 63.5   to the left,  agree=0.851, adj=0.621, (0 split)\n##       Years  < 3.5    to the left,  agree=0.743, adj=0.345, (0 split)\n## \n## Node number 3: 138 observations,    complexity param=0.05834488\n##   mean=6.406898, MSE=0.3571116 \n##   left son=6 (94 obs) right son=7 (44 obs)\n##   Primary splits:\n##       CHmRun < 101    to the left,  improve=0.1983012, (0 missing)\n##       Walks  < 59.5   to the left,  improve=0.1947176, (0 missing)\n##       Hits   < 117.5  to the left,  improve=0.1936239, (0 missing)\n##       CRBI   < 324.5  to the left,  improve=0.1862495, (0 missing)\n##       AtBat  < 369    to the left,  improve=0.1862096, (0 missing)\n##   Surrogate splits:\n##       CRBI   < 525    to the left,  agree=0.913, adj=0.727, (0 split)\n##       CRuns  < 707.5  to the left,  agree=0.877, adj=0.614, (0 split)\n##       CAtBat < 4975   to the left,  agree=0.862, adj=0.568, (0 split)\n##       CHits  < 1331   to the left,  agree=0.855, adj=0.545, (0 split)\n##       CWalks < 472    to the left,  agree=0.841, adj=0.500, (0 split)\n## \n## Node number 4: 45 observations,    complexity param=0.02838394\n##   mean=4.775078, MSE=0.379853 \n##   left son=8 (38 obs) right son=9 (7 obs)\n##   Primary splits:\n##       Runs  < 18.5   to the right, improve=0.2781316, (0 missing)\n##       AtBat < 173    to the right, improve=0.2727753, (0 missing)\n##       Hits  < 39.5   to the right, improve=0.2419642, (0 missing)\n##       RBI   < 12.5   to the right, improve=0.2039077, (0 missing)\n##       Walks < 13     to the right, improve=0.1354265, (0 missing)\n##   Surrogate splits:\n##       AtBat < 147    to the right, agree=0.956, adj=0.714, (0 split)\n##       Hits  < 39.5   to the right, agree=0.956, adj=0.714, (0 split)\n##       CRuns < 18.5   to the right, agree=0.933, adj=0.571, (0 split)\n##       Walks < 6      to the right, agree=0.911, adj=0.429, (0 split)\n##       CHits < 50     to the right, agree=0.911, adj=0.429, (0 split)\n## \n## Node number 5: 29 observations\n##   mean=5.405855, MSE=0.06050789 \n## \n## Node number 6: 94 observations,    complexity param=0.02919914\n##   mean=6.224833, MSE=0.3066681 \n##   left son=12 (48 obs) right son=13 (46 obs)\n##   Primary splits:\n##       Hits    < 118    to the left,  improve=0.1696599, (0 missing)\n##       PutOuts < 223    to the left,  improve=0.1599369, (0 missing)\n##       AtBat   < 358    to the left,  improve=0.1443333, (0 missing)\n##       Walks   < 50     to the left,  improve=0.1294664, (0 missing)\n##       CHits   < 450.5  to the left,  improve=0.1169089, (0 missing)\n##   Surrogate splits:\n##       AtBat   < 472.5  to the left,  agree=0.936, adj=0.870, (0 split)\n##       Runs    < 50.5   to the left,  agree=0.883, adj=0.761, (0 split)\n##       RBI     < 45.5   to the left,  agree=0.809, adj=0.609, (0 split)\n##       PutOuts < 223    to the left,  agree=0.755, adj=0.500, (0 split)\n##       Walks   < 30.5   to the left,  agree=0.713, adj=0.413, (0 split)\n## \n## Node number 7: 44 observations,    complexity param=0.0179389\n##   mean=6.795856, MSE=0.2427736 \n##   left son=14 (23 obs) right son=15 (21 obs)\n##   Primary splits:\n##       Hits  < 135    to the left,  improve=0.2812856, (0 missing)\n##       AtBat < 377.5  to the left,  improve=0.2567908, (0 missing)\n##       Walks < 61     to the left,  improve=0.2548501, (0 missing)\n##       Runs  < 55.5   to the left,  improve=0.2449233, (0 missing)\n##       RBI   < 62.5   to the left,  improve=0.2348766, (0 missing)\n##   Surrogate splits:\n##       AtBat   < 487    to the left,  agree=0.955, adj=0.905, (0 split)\n##       Runs    < 55.5   to the left,  agree=0.864, adj=0.714, (0 split)\n##       RBI     < 71.5   to the left,  agree=0.841, adj=0.667, (0 split)\n##       Walks   < 61     to the left,  agree=0.727, adj=0.429, (0 split)\n##       PutOuts < 238.5  to the left,  agree=0.705, adj=0.381, (0 split)\n## \n## Node number 8: 38 observations\n##   mean=4.635573, MSE=0.08385507 \n## \n## Node number 9: 7 observations\n##   mean=5.532392, MSE=1.307526 \n## \n## Node number 12: 48 observations\n##   mean=6.001536, MSE=0.2307175 \n## \n## Node number 13: 46 observations,    complexity param=0.0193834\n##   mean=6.457838, MSE=0.2796002 \n##   left son=26 (14 obs) right son=27 (32 obs)\n##   Primary splits:\n##       CRBI   < 241    to the left,  improve=0.2524297, (0 missing)\n##       CHits  < 466    to the left,  improve=0.1545274, (0 missing)\n##       CAtBat < 1947   to the left,  improve=0.1474714, (0 missing)\n##       CRuns  < 228.5  to the left,  improve=0.1472570, (0 missing)\n##       Years  < 4.5    to the left,  improve=0.1353644, (0 missing)\n##   Surrogate splits:\n##       CAtBat < 1782   to the left,  agree=0.870, adj=0.571, (0 split)\n##       CHits  < 545    to the left,  agree=0.870, adj=0.571, (0 split)\n##       CRuns  < 221.5  to the left,  agree=0.848, adj=0.500, (0 split)\n##       CWalks < 101.5  to the left,  agree=0.826, adj=0.429, (0 split)\n##       Runs   < 81     to the right, agree=0.804, adj=0.357, (0 split)\n## \n## Node number 14: 23 observations\n##   mean=6.546155, MSE=0.2043678 \n## \n## Node number 15: 21 observations\n##   mean=7.069338, MSE=0.1417559 \n## \n## Node number 26: 14 observations\n##   mean=6.056186, MSE=0.3327176 \n## \n## Node number 27: 32 observations\n##   mean=6.633561, MSE=0.1549035\n\n# Plot the regression tree\nrpart.plot(salary.tree.1)\n\nset.seed(42)\n\n# Generate a range of shrinkage values\nshrink.values <- seq(0.001, 0.102, by = 0.002)\nMSE = rep(NA, length(shrink.values))\nfor(i in 1:length(shrink.values)){\n  boost.salary <- gbm(LogSalary~., data = training, \n                      distribution = ""gaussian"", n.trees = 1000,\n                      shrinkage = shrink.values[i])\n  predictions = predict(boost.salary, training, n.trees = 1000)\n  MSE[i] = mean((predictions - training$LogSalary)^2)\n}\n\nplot(shrink.values, MSE, xlab = ""Shrinkage values"", ylab = ""Mean Square Errors"",\n     main = ""Plot of Shrinkage values vs MSE for Training Dataset"")\n\nProduce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.\nset.seed(42)\n\n# Generate a range of shrinkage values\nshrink.values <- seq(0.01, 0.2, by = 0.005)\nMSE = rep(NA, length(shrink.values))\nfor(i in 1:length(shrink.values)){\n  boost.salary <- gbm(LogSalary~., data = validation, \n                      distribution = ""gaussian"", n.trees = 1000,\n                      shrinkage = shrink.values[i])\n  predictions = predict(boost.salary, validation, n.trees = 1000)\n  MSE[i] = mean((predictions - validation$LogSalary)^2)\n}\n\nplot(shrink.values, MSE, xlab = ""Shrinkage values"", ylab = ""Mean Square Errors"",\n     main = ""Plot of Shrinkage values vs MSE for Test Dataset"")\n\nWhich variables appear to be the most important predictors in the boosted model?\nsummary(boost.salary)\n\n##                 var    rel.inf\n## CAtBat       CAtBat 18.9879086\n## Assists     Assists  8.2434252\n## CWalks       CWalks  7.4775879\n## PutOuts     PutOuts  6.3349233\n## CHits         CHits  6.3049375\n## Walks         Walks  6.1415334\n## CHmRun       CHmRun  6.0724061\n## Errors       Errors  5.8812600\n## CRBI           CRBI  5.7208945\n## RBI             RBI  5.6009837\n## HmRun         HmRun  4.7524529\n## Years         Years  4.6489486\n## Runs           Runs  3.5343610\n## AtBat         AtBat  2.4158783\n## Hits           Hits  2.2039921\n## Division   Division  1.8750940\n## League       League  1.8277845\n## CRuns         CRuns  1.6889002\n## NewLeague NewLeague  0.2867281\n\nThe most important predictor in the boosted model is CAtBat followed by Assists and then CWalks.\nNow apply bagging to the training set. What is the test set MSE for this approach?\nset.seed(42)\nbag.salary <- randomForest(LogSalary~., data=training,\n                            importance = TRUE)\n\nbag.salary\n## \n## Call:\n##  randomForest(formula = LogSalary ~ ., data = training, importance = TRUE) \n##                Type of random forest: regression\n##                      Number of trees: 500\n## No. of variables tried at each split: 6\n## \n##           Mean of squared residuals: 0.2033667\n##                     % Var explained: 74.26\n\ntest.bag <- predict(bag.salary, newdata=validation)\nplot(test.bag, validation$LogSalary)\nabline(0,1)\n\nmean((test.bag-validation$LogSalary)^2)\n## [1] 0.1178609\n\n'], 'url_profile': 'https://github.com/manoj95b', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Probabilities-and-Statistics-Project\nBuilding, interpreting and comparing few linear regression models, both simple and multiple, on trees data set from R.\n\nFeatures\n\ndescriptive statistic operations: mean, median, variance, quantiles and Laplace distribution\naddition of new data to the data set for creating better predictive models\ninterpretations of the results analyzing predictive ability\n\nR Packages\n\ndata sets\nggplot2\nGGally\nscatterplot3d\n\nProject done in collaboration with:\n\nAna Puiu\nNatasa Cirstea\n\n'], 'url_profile': 'https://github.com/madalina-cirstea', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'Peking', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Analysis-Using-R\nThis is a directory of group project which implemented MLR, Covariance Analysis, and Logistic Regression Using R.\nContents\n9/2019-10/2019\nInfluencing Factors to Labor Supply -- Multiple Linear Regression\n\nDiamand Pricing                     -- Covariance Analysis\n\nLogout Warning                      -- Logistic Regression\n\n2/2020-3/2020\nHow Do Trending YouTube Videos Receive More Likes? -- Text Mining\n\n'], 'url_profile': 'https://github.com/yitong14', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'Gainesville', 'stats_list': [], 'contributions': '172 contributions\n        in the last year', 'description': [""IonosphereData\nRegression with Least Squares loss, Hinge loss and Logistic Loss on the Ionosphere data with CVX package\nThis is a binary classification problem where the outcome is either 'good' or 'bad' depending on various attributes.\nThe dataset can be found at: https://archive.ics.uci.edu/ml/datasets/ionosphere\nI performed regression on the data with the following loss functions and achieved the following results:\n\nHinge Loss:  Achieved accuracy of 97%\nLogistic Loss: Achieved accuracy of 96%\nLeast Squares Loss:  Achieved accuracy of 94%\n\n""], 'url_profile': 'https://github.com/geethakamath18', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'indonesia', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['Readme.md\n'], 'url_profile': 'https://github.com/ikhsanrahman', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prateekpr', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['aml-data-discovery\nThis repository contains code that analyses a data set using different models. (Regression, Clustering, SVN, Ensemble, Neural Networks)\npython code for every technique can be found in the /code directory\ndocumentation of results can be found in the /doc directory\n'], 'url_profile': 'https://github.com/DanielsHappyWorks', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'Leicester, UK', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': [""Airbnb Seattle Analysis Blog Post\nUdacity DataScience NanoDegree Project 1\nThis repo contains the code used to produce the data analysis and interactive visualistions for a blog post on Medium.\nBusiness Understanding\nAirbnb has become the go-to marketplace when trying to find a place to stay. However, it can sometimes be overwhelming when faced with a mass of choices, and numerous factors to filter against. In the article, we take a look at Seattle's Airbnb ecosystem. Looking at how location can affect price, how prices change throughout the year, as well as the type of listings available these neighbourhoods.\nQuestions:\n\nHow does distance to the city centre affect listings?\nWhat is the best time of year to\xa0go?\nWhat type of listings are available in\xa0Seattle?\nHow are the listings managed in\xa0Seattle?\nCan we predict the Airbnb prices using factors assessed?\n\nData Understanding\nThe data was provided from the Kaggle project (https://www.kaggle.com/airbnb/seattle/data) and the opensource datasets provided by airbnb (http://insideairbnb.com/get-the-data.html)\nThe data made available was:\n\nCalendar.csv - listing prices across the year\nListings.csv - property listings available through Airbnb in Seattle in 2016\nReviews.csv - reviews left for listings\n\nModelling and Preparation\nData cleaning was performed, including:\n\nCleaning strings of unwanted characters\nOnehot encoding to handle categorical variables\nScaling for ML model\nGPS distance calculation using the Haversine Formula\n\nRef: https://www.kite.com/python/answers/how-to-find-the-distance-between-two-lat-long-coordinates-in-python\n\n\nReviewing data skew\n\nML Features:\n\nneighbourhood_group_cleansed\nhost_is_superhost\nroom_type\nbathrooms\nbedrooms\nbeds\nreview_scores_rating\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\nreviews_per_month\n\nTarget:\n\nprice\n\nModelling\nAs part of the evaluation a simple Linear Regression Machine Learning Model was created using the sklearn library to assess the usage of reviews to predict the price associated with the listing.\nR^2 score of 54%\nEvaluation\nTo review the output please look at the medium article here.\nDeployment\n\nTools\n\nJupyter Notebooks: used as datascience analysis\npandas: data manipulation\nPlotly: Interactive plots deployed to Medium article\n\nRef: https://towardsdatascience.com/how-to-create-a-plotly-visualization-and-embed-it-on-websites-517c1a78568b\n\n\nSklearn learn: Linear regression Model\n\n\n\ntest\nAcknowledgement\nThis was completed as Project 1 of Udacity's Datascience Nanodegree\n""], 'url_profile': 'https://github.com/jhmarlow', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '132 contributions\n        in the last year', 'description': ['Assignmnet-1\nThis assignment is about predicting housing prices using Linear and Polynomial Regression, with Lasso and Ridge Regularisation.\nAssignment-2\nThis assignment is about predicting flight status (i.e. ontime or delayed) using Logistic Regression.\n'], 'url_profile': 'https://github.com/hiteshK03', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yimenghummmm', 'info_list': ['Updated Feb 23, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Mar 27, 2020', 'Python', 'Updated Jun 28, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 9, 2020', 'Python', 'Updated Apr 7, 2020', 'Python', 'Updated Feb 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Cologne, Germany', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Cat-Detection-System-with-simple-Logistic-Regression-Classifier-\n'], 'url_profile': 'https://github.com/yunihafsarii', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Copenhagen ', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['PySpark-and-logistic-regression-for-loan-prediction-\nAim:\nThe project aims to make use of Python and Spark to extract insights from the data.\nSecondly, to learn how to use ML Pipeline which provides a uniform set of high-level APIs on top of DataFrames.\nAnd in the end, to predict whether the loan applicant can replay the loan or not using logistic regression.\nAttributes in the dataset:\nLoan id, Gender, Married, Dependents, Education, Self Employed, Applicant income, Coapplicant income, Loan Amount,Credit History, Property_Area, Loan_Status\n'], 'url_profile': 'https://github.com/Meghdad-DTU', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models. Moreover, you also got a brief introduction to data ethics. Remember that throughout your data work it is essential to consider personal privacy and the potential impacts of the data you have access to.\nRegression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preperation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\nEthics\nAside from regression, you also took a look at data privacy and ethics. You probably had already heard some of these ideas, but may have not been familiar with GDPR or privacy advocacy groups like the Electornic Frontier Foundation. The digital age has brought a slew of political and philosophical questions to the arena, and there are always fascinating (and disturbing) conversations to be had. Be sure to keep these and other issues at the forefront of your thought process, and not simply be dazzled by the power of machine learning algorithms. Ask yourself questions like, ""What is the algorithm being used for?"" or ""What are the ramifications or impact of this analysis/program/algorithm?"".\nWhen Einstein released his theory of relativity, its impact had tremendous benefit in advancing the field of physics yet the subsequent development of the Manhattan project was arguably a great detriment of humanity. To a similar vain, be thoughtful of which planes of thought you are operating on, and always be sure to include an ethical and philosophical perspective of the potential ramifications of your work.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'U.S.A.', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DSman123', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nIntroduce Statsmodels for multiple regression\nPresent alternatives for running regression in Scikit Learn\n\nStatsmodels for multiple linear regression\nThis lecture will be more of a code-along, where we will walk through a multiple linear regression model using both Statsmodels and Scikit-Learn.\nRemember that we introduced single linear regression before, which is known as ordinary least squares. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(""auto-mpg.csv"") \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[""acceleration""]\nlogdisp = np.log(data[""displacement""])\nloghorse = np.log(data[""horsepower""])\nlogweight= np.log(data[""weight""])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[""acc""]= scaled_acc\ndata_fin[""disp""]= scaled_disp\ndata_fin[""horse""] = scaled_horse\ndata_fin[""weight""] = scaled_weight\ncyl_dummies = pd.get_dummies(data[""cylinders""], prefix=""cyl"")\nyr_dummies = pd.get_dummies(data[""model year""], prefix=""yr"")\norig_dummies = pd.get_dummies(data[""origin""], prefix=""orig"")\nmpg = data[""mpg""]\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 26 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_3     392 non-null uint8\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_70     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_1    392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(21)\nmemory usage: 23.4 KB\n\nThis was the data we had until now. As we want to focus on model interpretation and still don\'t want to have a massive model for now, let\'s only inlude ""acc"", ""horse"" and the three ""orig"" categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis= 1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_1\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n1\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n1\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n1\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n1\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n1\n0\n0\n\n\n\n\nA linear model using Statsmodels\nNow, let\'s use the statsmodels.api to run our ols on all our data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$, where, with $n$ predictors, X is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = ""mpg ~ acceleration+weight+orig_1+orig_2+orig_3""\nmodel = ols(formula= formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable ""mpg"" out of your data frame, and use the a ""+"".join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nOr even easier, simply use the .OLS-method from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors dataframe so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nInterpretation\nJust like for single multiple regression, the coefficients for our model should be interpreted as ""how does Y change for each additional unit X""? Do note that the fact that we transformed X, interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed X, the actual relationship is ""how does Y change for each additional unit X\'"", where X\' is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit learn\nYou can also repeat this process using Scikit-Learn. The code to do this can be found below. The Scikit-learn is generally known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit learn compared to Statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of Scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -0.71140721, -0.29903267,  1.01043987])\n\nThe intercept of the model is stored in the .intercept_-attribute.\n# intercept\nlinreg.intercept_\n21.472164286075383\n\nWhy are the coefficients different in scikit learn vs Statsmodels?\nYou might have noticed that running our regression in Scikit-learn and Statsmodels returned (partially) different parameter estimates. Let\'s put them side to side:\n\n\n\n\nStatsmodels\nScikit-learn\n\n\n\n\nintercept\n16.1041\n21.4722\n\n\nacceleration\n5.0494\n5.0494\n\n\nweight\n-5.8764\n-5.8764\n\n\norig_1\n4.6566\n-0.7114\n\n\norig_2\n5.0690\n-0.2990\n\n\norig_3\n6.3785\n1.0104\n\n\n\nThese models return equivalent results!\nWe\'ll use an example to illustrate this. Remember that minmax-scaling was used on acceleration, and standardization on log(weight).\nLet\'s assume a particular observation with a value of 0.5 for both acceleration and weight after transformation, and let\'s assume that the origin of the car = orig_3. The predicted value for mpg for this particular value will then be equal to:\n\n16.1041 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 6.3785 = 22.0691 according to the Statsmodels\n21.4722 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 1.0104 = 22.0691 according to the Scikit-learn model\n\nThe eventual result is the same. The extimates for the categorical variables are the same ""up to a constant"", the difference between the categorical variables, in this case 5.3681, is added in the intercept!\nYou can make sure to get the same result in both Statsmodels and Scikit-learn, by dropping out one of the orig_-levels. This way, you\'re essentially forcing the coefficient of this level to be equal to zero, and the intercepts and the other coefficients will be the same.\nThis is how you do it in Scikit-learn:\npredictors = predictors.drop(""orig_3"",axis=1)\nlinreg.fit(predictors, y)\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -1.72184708, -1.30947254])\n\nlinreg.intercept_\n22.482604160455665\n\nAnd Statsmodels:\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    22.4826     0.789    28.504  0.000    20.932    24.033\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1    -1.7218     0.653    -2.638  0.009    -3.005    -0.438\n\n\norig_2    -1.3095     0.688    -1.903  0.058    -2.662     0.043\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               9.59\n\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in both Scikit-Learn and Statsmodels. Before we discuss the model metrics in detail, let\'s go ahead and try out this model on the Boston Housing Data Set!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['Text-Mining-and-Logistic-Regression-on-IMdb-dataset\nPredict if the movie production will make profit or not with sentimental analysis by text mining IMdb user reviews from each movies. From randomly sampled 60 movies from 2011 to 2016, each of 100 user reviews are extracted, pre-processed and given sentimental analysis scores indicating positive or negative response.\n'], 'url_profile': 'https://github.com/yoonkim313', 'info_list': ['Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '1', 'R', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Updated Feb 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nCovariance and Correlation\nWe start the section by covering covariance and correlation, both of which relate to how likely two variables are to change together. For example, with houses, it wouldn\'t be too surprising if the number of rooms and the price of a house was correlated (in general, more rooms == more expensive).\nStatistical Learning Theory\nWe then explore statistical learning theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shenghao001', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/salmacmpeg', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets. Key takeaways include:\n\nThe Pearson Correlation (range: -1 -> 1) is a standard way to describe the correlation between two variables\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models. Moreover, you also got a brief introduction to data ethics. Remember that throughout your data work it is essential to consider personal privacy and the potential impacts of the data you have access to.\nRegression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preperation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\nEthics\nAside from regression, you also took a look at data privacy and ethics. You probably had already heard some of these ideas, but may have not been familiar with GDPR or privacy advocacy groups like the Electornic Frontier Foundation. The digital age has brought a slew of political and philosophical questions to the arena, and there are always fascinating (and disturbing) conversations to be had. Be sure to keep these and other issues at the forefront of your thought process, and not simply be dazzled by the power of machine learning algorithms. Ask yourself questions like, ""What is the algorithm being used for?"" or ""What are the ramifications or impact of this analysis/program/algorithm?"".\nWhen Einstein released his theory of relativity, its impact had tremendous benefit in advancing the field of physics yet the subsequent development of the Manhattan project was arguably a great detriment of humanity. To a similar vain, be thoughtful of which planes of thought you are operating on, and always be sure to include an ethical and philosophical perspective of the potential ramifications of your work.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Predicting-prices-of-used-cars-using-Linear-Regression\nA raw dataset with used cars and their details is transformed and the prices of the cars are predicted using Ordinary Least Square, a machine learning technique coded in python 3\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'Haryana, India', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arnavobero1', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aakshroy20', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'Abuja, Nigeria', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Smith3dx', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/malvikajadhav', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['Sentiment Analysis of Twitter Amazon Reviews\nTable of contents\n\nIntroduction\nTechnologies\nAlgorithms\nApproach\n\nIntroduction\nSentiment Analysis file has the code for the analysis\nTechnologies\nProject is created with:\n\nPython\nPackages (scikit-learn, Pandas, Numpy, NLTK)\nTwitter Amazon Reviews Dataset ( 100k Tweets-->90k Train, 10k Test )\n\nModel\n\nTF-IDF Vectorization\nLogistic Regression\n\nApproach:\n\nData Preprocessing\nTF-IDF vectorization\nLogistic Regression\nK-fold CV\nMetrics- accuracy, precision, recall\n\n'], 'url_profile': 'https://github.com/sruthi1014', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'Algeria, Algiers ', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['The-best-Classifer\nusing historical dataset from previous loan applications, clean the data, and apply different classification  algorithm on the data such as (KNN , Decision Tree ,Logistic Regression , SVM).\n'], 'url_profile': 'https://github.com/Bouguedra-Adem', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Black Friday Sales Prediction\nThis Repository contains a Python Notebook which provides a step by step approach on how to solve the following\nproblem posted by AnalyticsVidya.\nProblem Statement\nA retail company “ABC Private Limited” wants to understand the customer purchase behavior\n(specifically, purchase amount) against various products of different categories.\nThey have shared a purchase summary of various customers for selected high volume products from last month.\nThe data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city),\nproduct details (product_id and product category) and Total purchase_amount from last month. Now, they want to build a model to predict the purchase amount of customers against various products which will help them to create a personalized offers for customers against different products.\nFor more explanation refer to my\nBlog on Medium.com where I have documented the entire process.\nResults:\nI used XGBoost Regressor to achieve a Mean Square Error of 2700.\n\n'], 'url_profile': 'https://github.com/omigirish', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Python', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}"
"{'location': 'Denver, Colorado', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Loan-Classifier\nAnalysis of the probability of a loan being paid late applying KNN analysis, decision tree analysis, log regression analysis, jaccard scores, f1 scores, and log-loss\n'], 'url_profile': 'https://github.com/meri-matilda', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""Predicting-ad-click-based-on-user-features\nIn this repository is used a fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement. We create a Logistic Regression model that will predict whether or not they will click on an ad based on the features of that user.\nThis data set contains the following features:\n\n'Daily Time Spent on Site': consumer time on site in minutes\n'Age': cutomer age in years\n'Area Income': Avg. Income of geographical area of consumer\n'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n'Ad Topic Line': Headline of the advertisement\n'City': City of consumer\n'Male': Whether or not consumer was male\n'Country': Country of consumer\n'Timestamp': Time at which consumer clicked on Ad or closed window\n'Clicked on Ad': 0 or 1 indicated clicking on Ad\n\nThe data is explored visually using seaborn by finding general relationships between the Area Income and Age, the Daily Time Spent on Site and Age, the Daily Time Spent on Site and Daily Internet Usage. Also, plot all the variables against each other using pairplot with value colors defined by the column 'Clicked on Ad' to check the impact on clicks with each changing parameter.\nA logistic regression model is created with 'Daily Time Spent on Site','Age','Area Income','Daily Internet Usage','Male' and then predictions are made for the test data.\nThen the model performance is evaluated by calculating the Classification Report and Confusion matrix.\nTo conclude, the logistic regression model performs quite well i.e. approx. 91% accuracy with few mislabelled points which is not bad given the size of our artificially generated dataset.\n""], 'url_profile': 'https://github.com/prachij94', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Classifications Algorithms\nComparing several classifications algorithms. For this test I will use banknote authentication data to fit logistics regression model and evaluate performance of LDA, QDA and KNN classifiers.\n'], 'url_profile': 'https://github.com/michelbenites', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NLPModelComparison\nI made logistic regression (Machine Learning), recurrent neural network (Deep Learning), and Lexicon (Rule-Based) models. Compared them for accuracy and efficiency.\nHow were each of the models trained?\nA precompiled dataset of 50,000 binarily classified (positive/negative) movie reviews from IMDB was used to both train and assess each model. The first 25,000 reviews\nwere used for the training portion, and the latter for testing the models.\nHow were the models analyzed?\nA confusion matrix for each model was created. The SciKit-Learn library has an import that allows for the creation of a confusion matrix with\nfalse-positives, false-negatives, true-positives, and true-negatives.\n'], 'url_profile': 'https://github.com/AbhirKarande', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['GRE-Acceptance-Analysis\nAnalysis of the acceptance of applicants for graduate school.\nAnalysis consists of the following components:\n\n\nPrinciple Conponents Analysis\n-Uses the priciples of linear algebra, specifically eigenvalues and eigenvectors to perform dimension reduction. In this case the number of variables is not excessive however PCA is still helpful in finding the highest contributing variables.\n\n\nMANOVA\n-Investigates the impact of categorical variables: Univeristy Rank, Research, and CGPA.\n\n\nRandom Forest\n-Uses the concept of Random Forests to investigate the impact of multicolinearity as well as perform a useful measure of predicting future admissions results.\n\n\nPlots\n-Uses GGplot2 in order to show the relationships that were investigated using the previous models mentioned.\n\n\n'], 'url_profile': 'https://github.com/16ap3400', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Data-Mining\nFinding relation between variables in a data set ( House_Predict ) then trying to predict a price using Linear Regression Algorithm using R language.\n'], 'url_profile': 'https://github.com/MaiAhmed18', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Black Friday Sales Prediction\nThis Repository contains a Python Notebook which provides a step by step approach on how to solve the following\nproblem posted by AnalyticsVidya.\nProblem Statement\nA retail company “ABC Private Limited” wants to understand the customer purchase behavior\n(specifically, purchase amount) against various products of different categories.\nThey have shared a purchase summary of various customers for selected high volume products from last month.\nThe data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city),\nproduct details (product_id and product category) and Total purchase_amount from last month. Now, they want to build a model to predict the purchase amount of customers against various products which will help them to create a personalized offers for customers against different products.\nFor more explanation refer to my\nBlog on Medium.com where I have documented the entire process.\nResults:\nI used XGBoost Regressor to achieve a Mean Square Error of 2700.\n\n'], 'url_profile': 'https://github.com/JanhaviZarapkar', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['endoSwitch\nThe R package estimates the endogenous switching regression model using full maximum likelihood estimation, and calculates treatment effects of a binary treatment.\nThe function can replicate the regression results of the movestay command in STATA, though minor difference could occur due to differences in the optimization methods.\nPackage installation\nTo install the package, run the following codes in R:\ninstall.packages(""devtools"") # Run this if the devtools package is not installed.\ndevtools::install_github(""cbw1243/endoSwitch"")\nNote\nThe package is still under development. I am making regular changes to the package for performance improvement. Please use the most recent version of the package. Install again before using it.\nIf there is anything wrong, shoot me an email.\nContact: Bowen Chen, PhD (bwchen@illinois.edu)\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""E-commerce-mobile-and-website-engagement-analysis\nAn Ecommerce company based in New York City sells clothing online but they also have in-store style and clothing advice sessions. Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want.\nThe company is trying to decide whether to focus their efforts on their mobile app experience or their website.\nWe analyse the artificially generated customer data by creating a linear model using Linear Regression.\nThe data has Customer info, suchas Email, Address, and their color Avatar. Then it also has numerical value columns:\n\nAvg. Session Length: Average session of in-store style advice sessions.\nTime on App: Average time spent on App in minutes\nTime on Website: Average time spent on Website in minutes\nLength of Membership: How many years the customer has been a member.\n\nThe data is explored using seaborn by finding general relationships between the Time on Website and Yearly Amount Spent, the Time Spent on App and Yearly Amount Spent, Time on App and Length of Membership.\nAlso, plotting all the variables against each other using pairplot to find the most correlated feature. Based on this plot Length of membership looks to be the most correlated feature with Yearly Amount Spent.\nA linear model is created with 'Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership' to predict 'Yearly Amount Spent' and then predictions are made for the test data.\nThen the model performance is evaluated by calculating the Mean Absolute Error, Mean Squared Error, and the Root Mean Squared Error.\nThe conclusion about the question is made by interpreting the coefficients of the linear model.\nSo the company should focus more on their mobile app or on their website?\nThere are two ways to think about this: Develop the Website to catch up to the performance of the mobile app, or develop the app more since that is what is working better. This sort of answer really depends on the other factors going on at the company, you would probably want to explore the relationship between Length of Membership and the App or the Website before coming to a conclusion.\n""], 'url_profile': 'https://github.com/prachij94', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'HTML', 'Updated Apr 13, 2020', 'Python', 'Updated Aug 18, 2020', '1', 'R', 'Updated Feb 20, 2020', 'Updated Feb 18, 2020', '1', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Feb 23, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '103 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lanle793', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aseghir', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\n\n\nSAMBA\nHealth research using data from electronic health records (EHR) has\ngained popularity, but misclassification of EHR-derived disease status\nand lack of representativeness of the study sample can result in\nsubstantial bias in effect estimates and can impact power and type I\nerror for association tests. Here, the assumed target of inference is\nthe relationship between binary disease status and predictors modeled\nusing a logistic regression model. SAMBA implements several methods for\nobtaining bias-corrected point estimates along with valid standard\nerrors as proposed in Beesley and Mukherjee (2020), currently under\nreview.\nInstallation\nSAMBA can be downloaded from Github via the R Package devtools\ndevtools::install_github(""umich-cphds/SAMBA"", build_opts = c())\n\nVignette\nOnce you have SAMBA installed, you can type\nvignette(""UsingSAMBA"")\n\nin R to bring up a tutorial on SAMBA and how to use it.\nQuestions\nFor questions and comments about the implementation, please contact\nAlexander Rix (alexrix@umich.edu). For questions about the method,\ncontact Lauren Beesley (lbeesley@umich.edu).\nReference\nStatistical inference for association studies using electronic health\nrecords: handling both selection bias and outcome misclassification\nLauren J Beesley, Bhramar Mukherjee medRxiv\n2019.12.26.19015859\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['UnicornPrediction-Machine-learning\nUsed a decision tree model to train a linear regression in predicting the probability of a startup becoming a unicorn entity in Govtech Data Science internship.\n\n'], 'url_profile': 'https://github.com/junchernwu', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Tractor-Sales-Forecasting\nA case study on forecasting sales of tractors, firstly univariate with respect to time and then with the application of regression in addition to it [Exogenous factors]\n'], 'url_profile': 'https://github.com/c0dewithMAK', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Piscataway, NJ', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Cross_Validation\nNote: I did not add the random N(0,1) error term to the data (X). Need to fix. (04/13/2020)\n'], 'url_profile': 'https://github.com/peter-ehmann', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '242 contributions\n        in the last year', 'description': ['Machine-Learning-Practice\nThis Repository includes the implementation of some of the machine learning algorithms.\n'], 'url_profile': 'https://github.com/Madhur215', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Los Angeles, California', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ashutosh-Kumar832', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 20, 2020', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 5, 2020', 'MATLAB', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020']}"
"{'location': 'Ireland', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['A-Linear-Regression-Analysis-On-Women-s-Representation-In-National-Parliaments\nPRE-PROCESSING DATA FOR ANALYSIS :\nThe data is almost clean and properly labeled however for\neasy prediction and calculation of metrics, we change the null\nvalues into zeroes. This makes it easier for us to perform the\ndata analysis as the platform can easily identify null values as\nzero and make appropriate calculations based on that and the\nutilized algorithm.\nCHOICE OF ALGORITHM:\nSince the data inside the dataset is labeled, we use one of\nthe supervised learning algorithms to perform the data analysis\nand it is expected that our linear regression model should\nperform well on the dataset of such size.\nThrough Databricks the dataset called ”Seats.csv” is uploaded\nand opened in a notebook. A cluster created for our\nproject was attached to this notebook. Once the dataset was\nlaunched in the notebook, it was converted into a dataframe as\ndataframes are easier to work with. The dataset was visualized\nin the notebook using the select SQL query.\nNow, to begin with, building our model, the necessary\nlibraries were imported. The library LinearRegression minimizes\nthe specified loss function (squaredError and an amalgam\nof squared error for reasonably small errors and absolute\nerror for comparatively large ones), with regularization. The\nVectorAssembler combines several columns into a vector column.\nThe features are standardized by the StandardScaler by\neliminating the mean and scaling to unit variance using statistics\nof column summary on the training set samples whereas\nPipeline functions as an estimator. A Pipeline comprises of\na series of stages, each of which is either an Estimator or a\nTransformer.\nThe data loaded into a dataframe was then explored and\nusing the display command, the data was described. The\nfeatures were put into a vector and scaled. The primary step\nto building our machine learning pipeline was to convert the\npredictor features from DataFrame columns to feature vectors\nusing the pyspark.ml.feature. VectorAssembler() method. The\nvectorizer takes the VectorAssembler(). The years 1990-2017\nhave been taken as input columns to the vectorizer. The “features”\ncolumn represents the output column for our analysis.\nThe data was split into the training and the test set where only\n20% of the data was allotted for the test data and the remaining\nfor the training data. In the next step, a pipeline was built for\nour linear regression model which took the stages of vectorizer\nand linear regression learner. With the calling of Pipeline.fit(),\nthe stages lr and vectorizer were executed in order. Figure 1\nshows a schematic diagram of our linear regression pipeline.\nOur model made predictions based on our chosen label,\n2018 and features. The new dataframe was created that transformed\nthe test data of the model to yield our predicted values\nand check how perfectly our model fits on our chosen dataset.\nAfter predictions, the RMSE, MSE, MAE and the R-squared\nvalues were calculated to calculate the accuracy of our model.\n'], 'url_profile': 'https://github.com/suchismitasinha', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['German-Credit-Risk-Analysis-using-Logistic-regression-and-Decision-tree-classifier\nData: The German credit scoring data is a dataset provided by Prof. Hogmann. The data set has information about 1000 individuals, on the basis of which they have been classified as risky or not.\nGoal: Compare the performance of logistic regression and decision tree classifier models on predicting the risk of the loans for 1000 individuals.\nMajor Findings:\n• The logistic model provided an in-sample misclassification rate as a 35.28% and out-of-sample misclassification rate as 37.33%.\n• The CART model provided an in-sample misclassification rate as a 30.42% and out-of-sample misclassification rate as 46.67%.\n• On using the different split ratios (70:30 and 80:20), we observed a decrease in out-of-sample misclassification for both models.\n'], 'url_profile': 'https://github.com/sagar8086', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ginahanspal', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Ship_Valuation_RandomForest\nUse past ship data to estimate price of target ship using Random Forest Regressor in Python\n'], 'url_profile': 'https://github.com/saniya-k', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Perf Kit\nPerf kit is a toolchain for testing performance regressions in JavaScript code. While writing atjson, we created a set of tools for testing performance regressions in our code.\nWhat resulted was this library that leverages the V8 profiler to collect metrics about the performance of our software that we could use to identify:\n\nIf a proposed change to the codebase impacted performance significantly\nWhat, in particular, are performance bottlenecks for a piece of code\n\nProfiling code\nInstall perf-kit into the repository that you want to add profiles to:\nnpm install --save-dev @condenast/perf-kit\nAfter it\'s been installed, add a file under perf-kit that will contain the performance suites to run:\nimport { run, suite } from ""@condenast/perf-kit"";\n\nrun(\n  suite({\n    name: ""my-first-profiler"",\n    cases: [],\n    runner: test => {\n      // Run your code here\n    }\n  })\n);\nYou can pass as many suites to the run function as you would like.\nA suite is made up of the suite name, cases to be run, and the suite runner.\nThe name is where profiles will be stored for the cpuprofiles that will be generated by the profile script. Cases will be run for every run of the suite. The test cases are shuffled between each run to introduce randomess into the runner to ensure that the results have a stronger significance (and is not due to run order).\nThe runner is a function that will receive a test case, and is the bit of code that will be measured when profiling.\nCommands\nprofile\nnpx ts-node ./perf-kit profile --times=10\n\nOptions\n\ntimes— The number of times to run each code block.\nout— The name of the directory to store the profiles. This can be a full path or a relative path\nverbose— Output progress information during the running of the command.\n\nThis will generate cpuprofiles for the whole performance suite.\ncompare\nnpx ts-node ./perf-kit compare baseline current\n\nGenerating performance profiles\nPerformance test suites are defined in performance/index.ts and specify a number of test cases to be run by a runner function. Within a test suite, a test run will generate a .cpuprofile of the test runner acting on all of the test cases in a random order. To run the test suites, run npm run perf which by default will run each test suite 10 times and store the .cpuprofile files in performance/profiles/{test suite name}/current. It will additionally aggregate over the test runs to generate a timing.json file in the same directory. The timing file will have distribution data for the sample and cumulative times for each function sampled in the test runs. For now, only functions defined in packages in this repo are included in the tiing file.\nPerformance profile generation can be configured by including parameters to the npm script. Including a --baseline flag will store the output files in a baseline folder instead of current, or a different folder can be further specified if this is passed as an option. Additionally, the number of runs can be configured by including the --times option:\nnpm run perf -- --baseline new-baseline-folder --times 20\nAs a shortcut, npm run perf:baseline is equivalent to npm run perf -- --baseline baseline --times 10.\nComparing performance profiles\nOnce performance profiles are generated, we can compare the results of one baseline against another by running npm run perf-tstats which will perform a Student\'s T-Test for every function timing collected over the test runs, as collected in the respective timing.json file in the baseline folder. In short, we consider the timing for every function to be its own random variable, and the timing recorded in each test run to be a sample. The T-Test asks whether the population of timings for a function of sampled in one baseline meaningfully differ from those sampled from a different one. If they are found to be different (ie, if the 95% confidence interval of their difference is entirely positive or negative), their T-Test results are included in a {baseline1}-{baseline2}-tstat.json file in the baseline2 directory and their summary is output to the console. The output file will additionally include timing information for any added function calls sampled in baseline2 but not baseline1, and any dropped function calls sampled in baseline1 but missing from baseline2.\nComparison can be conifugred by included parameters to the npm script. A --baseline option can be included to specify the baseline to compare from, and a --current option can be included to specify the baseline to compare to. npm run perf-tstats is equivalent to npm run perf-tstats -- --baseline baseline --current current.\n'], 'url_profile': 'https://github.com/CondeNast', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""Kaggle Housing Prices using Decision Trees 🌲 and Random Forest 🌴 🌿 🌻 🐰\nTopics Covered\n\nTraining and predicting using Decision Trees\nAnalyzing and preventing overfitting by changing model parameters\nRandom forest(Ensemble Model) for prediction housing prices\nHow to handle missing values using Imputation of the following kinds:\n\nMean\nMedian\nMost Frequent\n\n\nHow to handle Categorical columns using:\n\nLabel Encoder\nOne-hot Encoder\n\n\nPipelines using the following\n\nImputer for missing values\nOne-hot encoder for categorical values\nRandom forest for regression\n\n\nCross Validation with matplotlib\nXGBoost\nParameter selection using GridSearchCV\n0 mean, 1 SD standardization of data using StandardScalar\n\nHow to run\n\n\nBuild docker image\ndocker build -t predicthousing .\n\n\nRun docker container\ndocker run -it -v $(pwd):/workdir -p 8888:8888 -t predicthousing\n\n\nFrom inside the container workdir run to start jupyter notebooks\njupyter-notebook --ip='0.0.0.0' --port=8888 --no-browser --allow-root\n\n\nResources\n\nKaggle Tutorial\nDataset\n\n""], 'url_profile': 'https://github.com/pjindal91', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['IA2\nRepositorio para las prácticas de Inteligencia Artificial 2 (fdi @ UCM)\n'], 'url_profile': 'https://github.com/Al3xFreeman', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 17, 2020', 'TypeScript', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}"
"{'location': 'Lincolnshire', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Algorithms-For-Data-Mining-CMP3744m-1819\nRidge Regression - Assignment 1\nRequirements\nTask 1\nIn this task, we want to analyse the performance of ridge regression in the given feature space and estimate the optimal regularisation parameter. To achieve the task, you will need to implement ridge regression and evaluate its performance on the training as well on an independent test set. You will also have to analyse these performance metrics and discuss the choice of the regularisation parameter and its relation to under- and over-fitting.\nTask 2\nIn this task, several data for a specific plant have been collected. The data include four features, which are stem length, stem diameter, leaf length, and leaf width (in cm) of this plant. The purpose of this experiment is to figure out whether these features could be used to cluster the plants into 3 groups. Therefore, you are asked to make a clustering analysis over the data, which will help the plant biologists to understand their experimental results.\nArtificial Neural Network - Assignment 2\nRequirements\nTask 1\nThe objective of this assignment is to analyse a dataset concerning nuclear reactor data, specifically on fuel assemblies cluster vibrations, alterations of thermal-hydraulic parameters, etc. For over half a century, the nuclear industry – in the UK and worldwide - has primarily focused on the technological evolution of reliable nuclear power plants to produce electricity. By monitoring nuclear reactors while running at nominal conditions, it is possible to gather valuable insight for early detection of anomalies. Various types of fluctuations can be caused by the turbulent nature of flow in the core, mechanical vibrations within the reactor, the boiling coolant and stochastic character (random noise). The dataset can be downloaded from Blackboard. It is based on data from a research project that investigates how to detect various anomalies in nuclear reactors. The dataset includes two classes (normal/abnormal condition) and a number of features, which will need to be summarised.  The class membership of each row is stored in the field ‘Status’. Our task is to develop a set of classification models for automatically classifying reactors as normal or abnormal, based on their parameters/features\n'], 'url_profile': 'https://github.com/brandontaslater', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'Los Angeles, California', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ashutosh-Kumar832', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'South Carolina', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cheddahz', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'Islamabad, Pakistan', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Guluna', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Analyzing_Factors_That_Impact_Snapchat_Election_Ad_Impressions\nBackground\nWhat factors affect the number of impressions an advertisement receives? Looking at the 2020 election advertisements in the US regardless of political platform from Snapchat, we can predict this using regression analysis. Specifically, I looked at these factors:\n\nDuration\nTargeted Gender (if a specific gender was targeted)\nTargeted Age (if a specific age group was targeted)\nAmount Spent\n\nMultiple Regression\nTo find the relationship between these variables and impression, I conducted a multiple regression analysis.\n\nThe R squared value means that around 60% of the data can be displayed using this model. The significance F, the probability that none of the variables matter, is very small, meaning that the model is pretty accurate. However, the p-values for targeted age group, duration, and targeted gender are greater than 0.05, making these variables statistically insignificant. This means that we must run this model again with only the amount spent variable to get a more accurate model.\nLinear Regression\n\nThe R squared value is still around 60% and the significant F value decreased, meaning that there is a very high probability that the amount spent impacts the number of impressions. We can better visualize this relationship with a scatter plot.\n\nSummary and Implications\n\nRegardless of political campaign, 60% of the number of implications can be predicted using this formula: number of impressions = 348.84(amount spent) + 67694.\nOut of the factors analyzed, the only statistically significant finding is that spending impacts impressions.\n\n'], 'url_profile': 'https://github.com/AndrealZhang', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Crude-oil-price-forecasting\nIn this project, we compared different machine learning algorithms, such as the Support Vector Regression, Random Forest and Gaussian Process, analyzed the prediction results of those methods and found the optimal algorithm and the best fitted model.\nThe data could be downloaded from the link\n'], 'url_profile': 'https://github.com/TheoRepo', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': [""Mod1_Project - King County House Sales\nRepository Files\nThe followings are the file you can find in this repository and their descriptions.\n\nMod 1 -                   Project Final.ipynb - Module 1 Project's Jupyter Notebook.\npresentation.pdf -        High-level presentation of your methodology and recommendations for non-technical stakeholders\nREADME.md -               Descriptions of contents of the repository\nkc_house_data -           Original dataset of King Country House Prices\n\nProject Motivation\nThis is my module 1 project for Flatiron Data Science Selft Paced Program. This is my very first data sciene project. The purpose of this project is to construct a multiple linear regression model that can be used to predict the house prices. I will use the King County House Sales dataset to construct my model.\nProject Process\nMy approach for this project is to follow the OSEMN framework. The steps of this framework is as follow -\n\nObtain the data\nScrub the data\nExplore the data\nModel the data\nInterpret the data\n\nThe Model\nln(Y) = 12.2612 + 0.1919 G + 0.0002 *ln(L) - 0.0502 B\nY = Price\nB = Bathrooms\nL = Squared Footage of House\nG = Grade\nThe following points can be analyzed from the model -\n\nThe predictor variables explain 35% of changes in house prices\nAverage price of employ lots is $211,335\nSquare footage of the house impacts the most on the house prices\nGrade of the house is the second most influential factor\nThe number of bathrooms contributes the least\nThe number of bathrooms is negatively related to the price\n\nInterpreting the Model\nThe coefficients in my model can be interpretted as below -\n\nPrice increases by 33% for every 100% square footage increase\nPrice increases by 18% for every 50% square footage increase\n\n\n\nPrice increases by 21% for every grade increase\n\n\n\nPrice decreases by 4.9% for every bathroom added\nThe grade and square footage remain fixed\n\n\n""], 'url_profile': 'https://github.com/kyawsawhtoon', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': [""bars-classifier\nexploration of fundamental NLP text embeddings (dense and sparse), as well as important models (softmax regression, Naive Bayes, XGBoost, neural networks, LSTM) on a data set of lyrics scraped from the web\nWhat does it do?\n\nImports a dataset of rap lyrics from every member of the Wu Tang scraped off genius.com.\nConverts the raw data into Tf-Idf and Count features\nDoes LSA and covariance analysis to look into how distinct the features and classes we're using really are\nBuilds logistic regression, Naive Bayes, XGBoost, multi-layer perceptron, and LSTM models for classification\nBuilds gloVe, word2vec embeddings; imports gloVe embeddings, compares the two\n\nWe compare them all, and learn the following time-honored lessons\n\nUnneccesary model complexity hurts performance on small data sets\nBasic data augmentations like adding synonyms from a thesaurus helps\n\n""], 'url_profile': 'https://github.com/johnbowen42', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karishmasaikia', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}","{'location': 'India, Earth', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Movie-Reviews-Sentiment-Analysis\nSentiment Analysis of Movie Reviews is either positive or negative review, the dataset which is used is ""IMDB Dataset of 50K Movie Reviews"" and the following machine learning which I used is Logistic Regression , Random Forest and LinearSVC.\nDataset\nhttps://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\nRequirement\n\nScikit-learn\nPython\nSpaCy\n\n'], 'url_profile': 'https://github.com/SkyThonk', 'info_list': ['Python', 'Updated Feb 24, 2020', '1', 'MATLAB', 'Updated Feb 22, 2020', 'Updated Feb 18, 2020', 'Updated Feb 21, 2020', 'MIT license', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Jupyter Notebook', 'Updated Sep 17, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Identify-spam-in-social-listening-data\nThe project did pre-processing on tweets, used suspended account information to label tweets as spam, and finally built a logistic regression and a MLP model to predict an unseen tweet.\n'], 'url_profile': 'https://github.com/Chiayen0503', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/meenushastri', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/craigybaeb', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Onkar-Kakade', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/keyvar', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/latifaahmadi007', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ArthurGrb', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Machine-Learning-in-Financial-Engineering\nEngineering  Updated 2 minutes ago This project conducts various basic machine learning method for financial prediction: Linear Regression, Random Forest, SVM and so on. Deep Learning with Tensor Flow is also conducted.\n'], 'url_profile': 'https://github.com/Blue-Universe', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['Link: Notebook\n'], 'url_profile': 'https://github.com/amoeedm', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/geekyhitman274', 'info_list': ['Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}"
"{'location': 'Colombia', 'stats_list': [], 'contributions': '1,328 contributions\n        in the last year', 'description': ['Preprocessor\nA simple timeseries data pre-processor.\n\n\n\n\n\nDescription\nImplements modular components for dataset preprocessing: a data-trimmer, a standardizer, a feature selector and a sliding window data generator.\nAll modules are usable both from command line and from class methods.\nInstallation\nTo install the package via PIP, use the following command:\n\npip install -i https://test.pypi.org/simple/ harveybc-preprocessor\n\nAlso, the installation can be made by clonning the github repo and manually installing it as in the following instructions.\nGithub Installation Steps\n\nClone the GithHub repo:\n\n\ngit clone https://github.com/harveybc/preprocessor\n\n\nChange to the repo folder:\n\n\ncd preprocessor\n\n\nInstall requirements.\n\n\npip install -r requirements.txt\n\n\nInstall python package (also installs the console command data-trimmer)\n\n\npython setup.py install\n\n\nAdd the repo directory to the environment variable PYTHONPATH\n(Optional) Perform tests\n\n\npython setup.py test\n\n\n(Optional) Generate Sphinx Documentation\n\n\npython setup.py docs\n\nModules\nAll the CLI commands and the class modules are installed with the preprocessor package, the following sections describe each module briefly and link to each module\'s basic documentation.\nDetailed Sphinix documentation for all modules can be generated in HTML format with the optional step 6 of the installation process, it contains documentation of the classes and methods of all modules in the preprocessor package.\nData-Trimmer\nA simple data pre-processor that trims the constant valued columns.  Also removes rows from the start and the end of a dataset with features with consecutive zeroes.\nSee Data-Trimmer Readme for detailed description and usage instructions.\nStandarizer\nStandardizes a dataset and exports the standarization configuration for use on other datasets.\nSee Standardizer Readme for detailed description and usage instructions.\nSliding Window\nPerforms the sliding window technique and exports an expanded dataset with configurable window_size.\nSee Sliding Window Readme for detailed description and usage instructions.\nFeature Selector\nPerforms the feature selection based on a classification or regression training signal and a threeshold.\nSee Feature Selector Readme for detailed description and usage instructions.\nExamples of usage\nThe following examples show both the class method and command line uses for one module, for examples of other modules, please see the specific module´s documentation.\nExample: Usage via Class Methods (data_trimmer module)\nfrom preprocessor.data_trimmer.data_trimmer import DataTrimmer\n# configure parameters (same variable names as command-line parameters)\nclass Conf:\n    def __init__(self):\n        self.input_file = ""tests/data/test_input.csv""\nconf = Conf()\n# instance trimmer class and loads dataset\ndt = DataTrimmer(conf)\n# perform the module\'s core method\ndt.core()\n# save output to output file\ndt.store()\nExample: Usage via CLI (data_trimmer module)\n\ndata_trimmer --input_file ""tests/data/test_input.csv""\n\n'], 'url_profile': 'https://github.com/harveybc', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Tokyo, Japan', 'stats_list': [], 'contributions': '336 contributions\n        in the last year', 'description': [""AB_test_analysis\nThis is the third project done on Udacity's Data Analyst Nanodegree program, where I analyzed results of an A/B test run by an e-commerce website. My goal was to work through this notebook to help the company understand if they should implement this new page, keep the old page, or perhaps run the experiment longer to make their decision.\nGetting started\nYou need an installation of Python, plus the following libraries:\n\nnumpy\npandas\nmatplotlib.pyplot\nrandom\nscipy.stats\nstatsmodels.api\n\nStatistical analysis\n\nBootstrapping sampling distributions and p-value calculations\nZ-core test\nLogistic regression\nMultiple linear regression\n\nKey findings\n\nThe conversion rate for the new page is not better than for the old page\nThe country parameter of the user did not impact the conversion rate\n\n""], 'url_profile': 'https://github.com/aigera2007', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['f18-news-popularity\nOnline News Popularity\nBy: Shivam Pandit, Nandini Krupa Krishnamurthy, Rohit Chundru\nIntroduction and Motivation\nThe Project aims at finding the best classification model that fits the Online news popularity dataset. The chosen methods for fitting the model were selected incrementally for increasing model accuracy.  Implemented methods are mentioned below:\n•\tNaive-Bayes (NB)\n•\tK-Nearest Neighbors (KNN)\n•\tClassification and Regression Tree (CART)\n•\tC5.0\nData Description\nThe dataset – Online News Popularity – is available at https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#. It possess a set of features about articles published by Mashable in a period of two years.\n•\tNumber of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)\n•\tNumber of Datapoints: 39797\nPre-Processing and Data Cleaning\nIn preprocessing the data, we removed outliers in our data. Also, we removed some variables that were not important for prediction like URL and is_weekend. Some variables like data channels and weekdays were transformed to factor from integer to improve model fit. Also, shares variable needed log transformation after checking the frequency distribution graph.\nExploratory Data Analysis\nWe plotted frequency histograms of all the variables to analyze the variable distribution and finally used forward step wise regression model to select the best variables. Initially our dataset had 61 variables that got reduced to 30 variables using forward stepwise regression.\nData Science Model\nOur goal was to find the best fit model for our dataset. We incrementally fitted models to our data set which included models like Nayive Bayes, Classification and Regression Trees, K-nearest neighbors, C5.0 to find the best data science model that accurately predicts the news popularity based on the total number of shares for the variable.\nModel Evaluation\nWe used confusion matrix to evaluate model performance in terms of accuracy. We had fitted our model after splitting dataset in 70:30. We trained all the models on 70% data that acted as training data and tested the model accuracy on 30% data that acted as testdata. We defined articles with log(shares) larger than 7.244 (median) as popular article.\nResults\n•\tOut of the four methods implemented, C5.0 is the best to fit the model. This gives highest accuracy of 65.9%.\n•\tThe data set on a whole gives average accuracy of 61.895% which shows that the dataset is inconsistent indicating that irrelevant information has been used.\n• Therefore, this data is insufficient to predict the number of shares with high levels of accuracy for a news article considering its popularity.\n'], 'url_profile': 'https://github.com/krupakmurthy', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Portland, Oregon', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': [""[Machine Learning Project]\nProjekt strojového učenia\nProject Topic\nCan you make spurious correlations among seemingly unrelated categories using maching learning?  Also, can one category (i.e. music preference) predict the response to another (i.e. phobias)?\nRationale\nWe all have our preferences and our idiosyncrasies.  It would be interesting for our audience to see how some may correlate.  We also are interested in being able to predict some characteristic based on others.\nData Sets\nA survey was given out to young people (15-30) who were associated with students in the Social\nand Economic Sciences department of Comenius University in Bratislava.  The questions can be grouped in to the following categories: music preferences, movie preferences, hobbies & interests, phobias, health habits, personality traits, spending habits, and demographics.\nThe dataset:\nhttps://www.kaggle.com/miroslavsabo/young-people-survey\nProject Stack\nWhat ML algorithms are you considering?\nWhich libraries, tools, and technology will you use?\nScikit learn\nstats models: logistic regression https://www.statsmodels.org/stable/regression.html\nPython/pandas\nTableau (and maybe D3)\nProject Goals\nMake some correlations and also some predictions.  Have a website with some stuff on it.\nDo you any alternative plans if something doesn't go as expected?\nOur dataset is varied enough that if a question doesn't work, we can create a new question.\n""], 'url_profile': 'https://github.com/BDomogalla', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': [""No-show appointments\nIn this project, I'll explore a dataset of no-show medical appointments in Brazil. The dataset was originally\npublished on Kaggle by Joni Hoppen and can be found here. The main question that this dataset raises is whether\nit is possible to predict the likelihood of no-show appointments based on determining factors (demographics,\nmedical information, time, etc.).\nI performed the following operations:\n\nSaved the dataset in a pandas dataframe\nCleaned the column names, removed incorrect datapoints, and changed datatypes in the data wrangling phase\nAnalyzed the following research questions in the Exploratory Data Analysis (EDA) phase:\na. Is demographic information indicative of no-show appointments?\nb. Is the day of week a significant determinant of no-show appointments?\nc. Is the time between the scheduled and appointment day a good indicator of no-show appointments?\nd. How do medical conditions affect the likelihood of no-show appointments?\n\nI arrived at the following conclusions:\n\nThe gender variable isn't a significant factor contributing to the high number of no-show appointments by itself.\nCombined with age, we see that there is a higher no-show rate for male patients between the ages of 0 and 10 and betwen\n15 and 60 years old for female patients. We also observe that older patients of both genders seem more reliable to show\nup to their appointment.\nTiming also matters a lot in the likelihood of observing a no-show. There is a higher number of no-shows on Tuesdays\nand Wednesdays. In addition, patients who scheduled their appointment far in advance seemed to have a lower number of\nno-show appointments than those who scheduled theirs within a shorter timeframe.\nFinally, medical conditions are determining factors that can be used to estimate the probability of a no-show. As we\nnotice from our analysis above, two medical conditions in particular seem to affect the no-show rate: hypertension and\nhaving a single handicap.\n\n""], 'url_profile': 'https://github.com/chloelubin', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Tuticorin', 'stats_list': [], 'contributions': '501 contributions\n        in the last year', 'description': ['Iris_Classification_Plotting\nThe data set consists of: 150 samples. 3 labels: species of Iris (Iris setosa, Iris virginica and Iris versicolor) 4 features: Sepal length,Sepal width,Petal length,Petal Width in cm.This project is meant to test classical Classification algorithm, such as Logistic Regression, on the popular Iris dataset.\n'], 'url_profile': 'https://github.com/Nivitus', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Student Performance\nTrained an ML model using Multivariate Linear Regression to predict student grades upto 91% accuracy.\nIn addition, created an GUI using Tkinter, Python to find the weak students and dislay their details on screen.\nDataset Credit:  P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. Available at: [Web Link].\n'], 'url_profile': 'https://github.com/mrudhularaya', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'USA ', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['Hybrid-Learning-Aided-Inactive-Constraints-Filtering-Algorithm-to-Enhance-AC-OPF-Solution-Time\nAbstract—The Optimal power flow (OPF) problem contains many constraints. However, equality constraints along with a limited set of active inequality constraints encompass sufficient information to determine the problem’s feasible space. In this paper, a hybrid supervised regression-classification learning-based algorithm is proposed to identify active and inactive sets of inequality constraints of AC OPF solely based on nodal power demand information. The proposed algorithm is structured using several classifiers and regression learners. The combination of classifiers with regression learners enhances the accuracy of active/inactive constraints identification procedure. The proposed algorithm modifies the OPF feasible space rather than a direct mapping of OPF results from demand. Inactive constraints are removed from the design space to construct a truncated AC OPF. This truncated optimization problem can be solved faster than the original problem with less computational resources. Numerical results on several test systems show the effectiveness of the proposed algorithm for predicting active and inactive constraints and constructing a truncated AC OPF. We have posted our code for all simulations on arxiv and have uploaded the data used in numerical studies to IEEE DataPort as an open access dataset.\nKeywords—Optimal power flow, machine learning, active constraint identification.\nArticle Link\n'], 'url_profile': 'https://github.com/FouadHasan', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'Peking', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Commercial-House-Pricing-in-Beijing\nHi all!\nThis is a personal final project for a class, Regression Analysis in Management Research. I extracted house information from anjuke.com using Selenium and re in Python and implemented variance analysis using R.\nI learned web crawler methods from the book, Financial Data  Mining and Analysis Using Python, which is written by Yutao Wang. So the case study of Baidu News Report in the book is also  included as the exercise part in the code.\nContents\nFinal_Code.ipynb -- web scrawler code in Jupyter Notebook\n\nFinal_code.R     -- variance analysis code in R\n\nFinal_data.xlsx  -- data extracted using Final_Code.ipynb and used in Final_code.R\n\nThank you,\nYitong Li\n11/2019\n'], 'url_profile': 'https://github.com/yitong14', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""Project_Churn\nThe project aims at reducing Customers' Churning with an ensemble approach and Stratified K Fold Sampling to reduce Variance and Bias due to Target Class Imbalance.\nPositive response being False - 0 - Customers' who didn't Churn\nNegative response being True - 1 - Customers' who Churn\nModels' accuracy in Predicting Negative Response:\nGBM - 92%\nSVM - 88%\nRF - 91%\nKNN - 78%\nLogistic Regression - 56%\nImportant Features:\n\nTotal Day, Evening, International Calls\nNumber of Customer Service Calls\n\n""], 'url_profile': 'https://github.com/Nivetha4192', 'info_list': ['2', 'Python', 'MIT license', 'Updated Jun 4, 2020', 'HTML', 'Updated Apr 20, 2020', 'R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Python', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 23, 2021', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Measuring the Impact of Amenities of Market Share and Revenue\n\xa0\xa0\xa0\xa0\xa0\xa0In this project, we evaluate the ROI of offering meeting rooms for a Mountain View hotel by measuring the impact of hotel amenities on online reviews and ratings using linear and multinomial logistic regressions.\n\nOur goal is to identity one impactful (and statistically significant) amenity that Best Western Crest View does not currently offer and recommend, using breakeven analysis, whether offering the amenity makes economic sense.\nTo do so, we measure the impact of different amenities (e.g. pool, gym, and meeting rooms) on a hotel’s average ratings and number of reviews.\nThen, using the model given in the Cornell paper as representative of the Mountain View market, we estimate the impact that increasing Crest View’s number of reviews—by offering the amenity—would  have on its market share (i.e. probability that a user purchases). We assume:\no\tThat Crest View’s 5% market share reflects customers who book through online travel agencies, such as Travelocity.\no\tThat the list of amenities shown on a hotel’s TripAdvisor page is complete. In other words, hotels list all amenities offered on\nTripAdvisor.\n\nData Understanding\nWe scrapped the following data from the 18 listed hotels in Mountain View on Sunday, February 2nd:\n\nComposite (i.e. average) Rating Score\nNumber of Reviews\nTotal number of amenities listed and type (e.g. pool, gym)\n\nData Preparation\n\nTo regress amenities on rating score and number of reviews, each amenity was given its own column. Hotels with an amenity were given 1s and hotels without an amenity were given 0s in the amenity’s respective column.\nModeling\nFirst, a linear regression is used to predict the number of reviews using type of amenities offered. It is reasonable to assume that offering or not offering certain amenities might motivate guests to write reviews. Indeed, we observe that many reviews written center around service, quality, and/or presence of an amenity. Because we have more covariates (i.e. amenities) than observations, stepwise feature selection was employed to select the most important variables.\nNext, a multinomial logistic regression—since ratings are categorical—is used to predict ratings using type of amenities offered.\n\nImpact of Amenities on Reviews\nInline-style:\n\n\xa0\xa0\xa0\xa0\xa0\xa0The linear regression output above shows the impact of certain amenities (chosen using stepwise feature selection) on number of reviews. Of the amenities listed, meeting rooms is statistically significant at a 5% level and laundry service is statistically significant at 10% level.\nImpact of Amenities on Ratings\nInline-style:\n![alt text]https://raw.githubusercontent.com/Yehudib09/measuringimpactofamenities/master/multinomial%20regression%20output.PNG ""MR Output"")\n\xa0\xa0\xa0\xa0\xa0\xa0Above, we display the output of the multinomial logistic regression, including coefficients and p-values, which displays the impact that certain amenities have on the likelihood that a hotel received a certain rating. The coefficients represent the marginal impact of an amenity on the logit of outcome relative to the referent group, which we set as 3.5, the current compositive rating of Crest View. For example, adding a meeting room increases that chances that a hotel has a 4-star rating vs a 3.5-star rating. We note, however, that none of our coefficients is statistically significant.\nRecommendation\n\xa0\xa0\xa0\xa0\xa0\xa0We identify “meeting rooms” as an important amenity, given its impact on number of reviews. Crest View does not currently offer meeting rooms.\nInline-style:\n![alt text]https://raw.githubusercontent.com/Yehudib09/measuringimpactofamenities/master/regression%20equation.png ""Regression Equation"")\n\xa0\xa0\xa0\xa0\xa0\xa0To evaluate the value of a meeting room at Crest view, we compare the benefit to the costs. To estimate the benefit, we measure how much the probability of purchase would change as we increase reviews through the introduction of meeting rooms. From our linear regression, we see that adding a meeting room would increase reviews by 322, on average, holding all else constant. This corresponds to a .805 change in the logit, or 5% additional market share for Crest View.\n\n\n\nCurrent Market Share\n5%\n\n\n\n\nImplied logit\n-2.945\n\n\nchange in Logit\n.805\n\n\nNew Implied Logit\n-2.14\n\n\nNew Market Share\n10.5%\n\n\nNew Customers Gained\n(.105-.05)*500,000=27,500\n\n\n\nInline-style:\n![alt text]https://raw.githubusercontent.com/Yehudib09/measuringimpactofamenities/master/CLV%20framework.png ""MR Output"")\nInline-style:\n![alt text]https://raw.githubusercontent.com/Yehudib09/measuringimpactofamenities/master/CLV%20calculation.png ""MR Output"")\nTotal Value of New Customers Gained\n\xa0\xa0\xa0\xa0\xa0\xa0If we compare the $12.5M in incremental value to the estimated $1M in costs (finding cost estimates for adding a meeting room to a hotel is extremely difficult so we took 1/22 of the overall price of constructing a hotel), we would recommend that Crest View offer meeting rooms in its hotel to increase market share and profitability. This is consistent with Crest View’s strategy to cater more to the business segment, having introduce a brand new business center in early spring 2019. If you read through Crest Views review, you will notice that a number of reviews mention Crest View’s business focused amenities—or lack of.\n\xa0\xa0\xa0\xa0\xa0\xa0Admittedly, $12.5M seems like a very high number. While our method is sound, we point out a few limitations of our analysis that could contribute to a less than accurate final value. For one, we are only able to collect data on 18 hotels. Secondly, we use a representative model to measure the impact of reviews on market share—it is possible, and likely, that the mountain view market behaves very differently than the market studies in the Cornell paper.\n\xa0\xa0\xa0\xa0\xa0\xa0Nevertheless, our analysis shows how one can connect an amenity, which lives at the bottom of the CRM framework, to profit, which sits at the top of the framework. To improve on this analysis, we would collect better data and train more sophisticated models.\n'], 'url_profile': 'https://github.com/Yehudib09', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}","{'location': 'Alexandria', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fahdmekawy', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '392 contributions\n        in the last year', 'description': ['yelpVegas\n\nProject Owners:\nElizabeth Combs (lcombs)\nAnu-Ujin Gerelt-Od\nWendy Hou (hwendy12)\nEmmy Phung (Emmyphung)\nData source: https://www.kaggle.com/yelp-dataset/yelp-dataset\nModels: Decision Tree, Random Forest, and Logistic Regression.\nAbstract:\nThe purpose of this data mining project is to examine how restaurants can improve their Yelp profile to become\nmore “successful” on Yelp in Las Vegas, Nevada.\n\nDifferent from the traditional approaches to this dataset,\nour methodology defines “success” as a binary variable through an exploratory analysis of the restaurants’\nreview counts and ratings on Yelp. Feature variables include categories and attributes that Yelp users can use\nto select which restaurant to visit. For this project, we ran Decision Tree, Random Forest, and Logistic Regression\nto explore key features associated with “success” and obtain recommendations for restaurants to improve their Yelp profile.\nFinal results indicate that determinants of success vary by cuisine type.\n'], 'url_profile': 'https://github.com/hwendy12', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Rent_Prediction\nThis project aims at predicting the number of bike rentals on given seasonal and environmental features. Achieved a 90% accuracy using Linear Regression with Temperature Type, Day type, Season being the important predicting parameters.\n1.1 Problem Statement:\nThe objective is to forecast bike rental demand of Bike sharing program in Washington, D.C based on historical usage patterns in relation with weather, environment and other data. We would be interested in predicting the rentals on various factors including season, temperature, weather and building a model that can successfully predict the number of rentals on relevant factors.\n1.2 Data\nThis dataset contains the seasonal and weekly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding temperature and humidity information. Bike sharing systems are a new way of traditional bike rentals. The wohle process from memberhsip to rental and retrun back has become automatic. The data was generated by 500 bike-sharing programs and was collected by the Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto. Given below is the description of the data which is a (731, 16) shaped data, The variables are:\nweathersit:\n1: Clear, Few clouds, Partly cloudy,\n2: Mist and Cloudy, Mist and Broken clouds, Mist and Few clouds, Mist\n3: Light Snow, Light Rain and Thunderstorm and Scattered clouds, Light Rain an Scattered clouds\n4: Heavy Rain and Ice Pallets and Thunderstorm and Mist, Snow and Fog\ninstant: record index\ndteday: date\nseason: season (1:spring, 2:summer, 3:fall, 4:winter)\nyr: year (0: 2011, 1:2012)\nmnth: month ( 1 to 12)\nholiday: weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\nweekday: day of the week\nworkingday: if day is neither weekend nor holiday is 1, otherwise is 0.\ntemp: Normalized temperature in Celsius. The values are divided to 41 (max)\natemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\nhum: Normalized humidity. The values are divided to 100 (max)\nwindspeed: Normalized wind speed. The values are divided to 67 (max)\ncasual: count of casual users\nregistered: count of registered users\ncnt: count of total rental bikes including both casual and registered\nI have used a combination of regression methods and several hypothesis testing to validate relevant features for the model.\nThe model achieved 90% Accuracy in prediction using Linear Regression.\n'], 'url_profile': 'https://github.com/Nivetha4192', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}","{'location': 'Walnut Creek, CA', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['World-Happiness-Report\nThe World Happiness Report is a landmark survey of the state of global happiness that ranks 156 countries by how happy their citizens perceive themselves to be. The rankings of World Happiness Report 2019 use data that come from the Gallup World Poll. The rankings are based on answers to the main life evaluation questions asked in the poll, with the best possible life being a 10, and the worst possible life being a 0. The rankings are from nationally representative samples, based entirely on the survey scores, using the Gallup weights to make the estimates representative. The purpose of this notebook is to analyze the Happiness Score of the 156 Countries using 3 different clustering algorithms, namely : K-Means Clustering, Agglomerative Clustering and Affinity Propagation and also to predict the Happiness Score using Linear Regression Model.\n'], 'url_profile': 'https://github.com/kiranbenny-git', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshallbangar', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Semantic-Textual-Similarity\nAbstract\nSemantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications of this task include machine translation, summarization, text generation, question answering, short answer grading, semantic search, dialogue and conversational systems.we have use unsupervised method CCA(Canonical correlation Analysis)using cosine similarity and WMD(word movers distance).\n'], 'url_profile': 'https://github.com/Sahrawat1', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Springboard Data Science Track /n Capstone Project-1\nairbnb_NYC\nThis dataset is going to be used to create a model that explores pricing metrics in NYC and predicts rental prices for the upcoming period.\nTargeted stakeholders are:\n-NYC Housing Authority \\n -Tourism Tour Organizations while accounting their tour costs \\n -Tourists planning to visit NYC \\n -Airbnb App users \\n -Airbnb Hosts And Potential Hosts while balancing their rent prices   -Airbnb Application Improvers -Other Hosting Application Member Users(such as Booking.com) -NewBie Hosting Application Startup Investors.\nThis project is going to be useful for these beneficiaries because:\nSource/Data:\nAn open source data set includes 16 columns and 48895 rows provides Airbnb listing information and metrics for hosts, places and geographical features in NYC, NY area between the years 2008 and 2019.\nThe data is sourced from the Inside Airbnb website http://insideairbnb.com/get-the-data.html which hosts publicly available data from the Airbnb site.\nMethod/Technique:\n1)EDA/Visualization -Comparing boroughs, locations, geographical features -Comparing hosts, number of their lists -Comparing reviews and their quantity 2)Hypothesis testing (t-test, z-test) -Analysing data above and creating hypothesis e.g: is mean price in borough 1 difference from borough 2 3)Machine Learning -Regression models for predicting listing price\nDeliverable:\n-Github Repository via Jupyter Notebook includes codes I create and comments for reasoning. -Power Point Slide including Data Visualisation images. -Milestone report -Final Report -Slide Deck\n'], 'url_profile': 'https://github.com/veraguzelsoy', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}","{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""Probability-and-Statistics-Using-R---Part-2\nIn today's fast-paced world sleep is not only a biological necessity but also a physiological drive, though, a good night's sleep is often the primary thing to go. The impacts of inadequate sleep are more than mere annoyances: they affect our mood as well as the performance in daily life. The analysis undergone in this part of assignment is concerned with finding the most significant parameters to response variable and generating a hypothetical model of the relationship between the outcome variable and  several predictos.  The initial exploration in Part I of this assignment was focused on identifying factors, which influence problems in sleep and finding those related paramaters that impact the average hours of sleep during week night. The initial analysis found that factors such as age, hourwend and trubslp all had a small but significant contribution toward a average hours of sleep/ weeknight and sleep problems. This paper will continue and build upon that analysis identifying additional factors, which contribute towards finding those predictors which are most significant and then combining all investigated factors to build a multiple linear regression model capable of predicting average hours of sleep/ weeknight*(hourwnit)*.\n""], 'url_profile': 'https://github.com/tijothomas95', 'info_list': ['Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Sep 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', '3', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Updated Feb 20, 2020']}",
