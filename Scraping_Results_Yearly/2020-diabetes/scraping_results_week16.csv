"{'location': 'NONE', 'stats_list': [['4', '          followers'], ['3', '          following'], ['12']], 'contributions': '379 contributions\n        in the last year', 'description': ['Diabetes-Predictor-Application\nDiabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Victims of this disease are increasing day by day.\nIn this respository, a diabetes prediction application is created having machine learning frontend and web based backend. Here different classification models are used to predict if a patient is diabetic or not by taking essential patient details as input such as number of pregnancies, Plasma glucose concentration, Diastolic blood pressure, Triceps skin fold thickness, 2-Hour serum insulin, Body mass index, Diabetes pedigree function, and Age to predict if the patient is diabetic or normal.\nThe dataset is obtained from kaggle: https://www.kaggle.com/uciml/pima-indians-diabetes-database\nBelow are the various classification models applied to the dataset are compared using accuracy_score r2_score:\n\n\n\nRegression Models\nAccuracy\nR2 Score\n\n\n\n\nK Nearest Neighbor\n75.32\n-0.1636\n\n\nNaive Bayes\n79.22\n0.0200\n\n\nRandom Forest Classification\n75.32\n-0.1636\n\n\nLinear Support Vector Classification\n70.12\n-0.3780\n\n\nSupport Vector Classification\n81.81\n0.1425\n\n\n\naccuracy_score is the percentage of the success of a model to predcit the independent attribute and r2_score is a statistical measure that represents the goodness of fit of a regression model. The ideal value for r2_score is 1, its range is from -1 to 1. Some other methods to determine the success of a classification model are mean_squared_error, mean_absolute_error, confusion_matrix, calssification_report.\nOut of all the  Classification models above Support Vector Classification has the highest accuracy of 81.81%, this model is deployed using flask inorder to provide a web based interactive interface for users.\n'], 'url_profile': 'https://github.com/RiturajSaha', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [['0', '          followers'], ['0', '          following'], ['3']], 'contributions': '3 contributions\n        in the last year', 'description': ['\n\n\npage_type\nlanguages\nproducts\ndescription\n\n\n\n\nsample\n\n\n\npython\n\n\n\n\n\n\n\nazure\nazure-machine-learning-service\nazure-devops\n\n\n\n\nCode which demonstrates how to set up and operationalize an MLOps flow leveraging Azure Machine Learning and Azure DevOps.\n\n\n\nMLOps with Azure ML\n\nMLOps will help you to understand how to build a Continuous Integration and Continuous Delivery pipeline for an ML/AI project. We will be using the Azure DevOps Project for build and release/deployment pipelines along with Azure ML services for model retraining pipeline, model management and operationalization.\n\nThis template contains code and pipeline definitions for a machine learning project that demonstrates how to automate an end to end ML/AI workflow.\nArchitecture and Features\nArchitecture Reference: Machine learning operationalization (MLOps) for Python models using Azure Machine Learning\nThis reference architecture shows how to implement continuous integration (CI), continuous delivery (CD), and retraining pipeline for an AI application using Azure DevOps and Azure Machine Learning. The solution is built on the scikit-learn diabetes dataset but can be easily adapted for any AI scenario and other popular build systems such as Jenkins and Travis.\nThe build pipelines include DevOps tasks for data sanity tests, unit tests, model training on different compute targets, model version management, model evaluation/model selection, model deployment as realtime web service, staged deployment to QA/prod and integration testing.\nPrerequisite\n\nActive Azure subscription\nAt least contributor access to Azure subscription\n\nGetting Started\nTo deploy this solution in your subscription, follow the manual instructions in the getting started doc. Then optionally follow the guide for integrating your own code with this repository template.\nRepo Details\nYou can find the details of the code and scripts in the repository here\nReferences\n\nAzure Machine Learning (Azure ML) Service Workspace\nAzure ML CLI\nAzure ML Samples\nAzure ML Python SDK Quickstart\nAzure DevOps\n\nContributing\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com.\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\n'], 'url_profile': 'https://github.com/setuchokshi', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'Auckland, New Zealand', 'stats_list': [['3', '          followers'], ['1', '          following'], ['0']], 'contributions': '64 contributions\n        in the last year', 'description': ['Type-II Diabetes Prediction Model\nThis study will provide validated T2D prediction models, relevant in contemporary New Zealand‚Äôs primary care.\nCore Data\nSee Wiki for details regarding the core dataset including variable descriptions.\nThe core dataset is a PREDICT population from 2006 - 2018. Individuals are eligeable if they are diabetes-free at time of PREDICT risk assessment and where an index HbA1c value is avaiable.\nInclusion does not limit the number of prior PREDICT assessments or prior CVD. Where eligeability is met at multiple PREDICT assessments, the earliest PREDICT record is used as baseline.\nInformation from the National Health Collection are linked to provide demographic, hospitalised history, hospitalised outcomes, death-specific outcomes, and baseline treatment.\nTo ensure consistency, the exclusion criteria (see Wiki) have been applied in data management.\nExclude IF\n\nPrior admission for diabetes; OR\nTreated with antidiabetic drugs in last 6 months; OR\nNoted as diabetic in PREDICT; OR\nNon-existing HbA1c test in prior 2 years; OR\nAny HbA1c in prior 2 years >= 50mmol/mol\nExclusion critiera met within 30 days of study index date\nQC Conflicts\n\nRoll to next\nExamine each visit and apply exclusion criteria. Continue with each nth visit to find those who could meet the inclusion criteria in subsequent visits.\n\nStarting with 564180 participants\n371362 met criteria after first visit\n12191 meet criteria in subsequent visit\n\nFurther Removals\n\nRemove out of age Limits 25-74: -14806\nRemove non-avaliable HbA1c within +30 days of Predict: -75764\nRemove dispensing records 1 month beyond DOD :-107\nRemove people with renal dialysis & transplantation: -826\nRemove exclusions detected within 30 days of study index: -179\nTotal Remaining = 277075\n\n'], 'url_profile': 'https://github.com/VIEW2020', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'Ohio, United States', 'stats_list': [['1', '          follower'], ['1', '          following'], ['1']], 'contributions': '8 contributions\n        in the last year', 'description': ['A crossplatform program used to store, and calculate glucose numbers, carbohydrate intake, and the insulin dosages necessary to counteract both.\nCurrently offers basic functionality, but the plan is to be able to use this on Windows, Android, iOS, and web interfaces to help manage your diabetes.\n'], 'url_profile': 'https://github.com/ethan-lefeb', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pvjoshi28', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [['1', '          follower'], ['2', '          following'], ['0']], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amrutha-thalappan', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'Colombo, Sri Lanka', 'stats_list': [['30', '          followers'], ['0', '          following'], ['37']], 'contributions': '2,297 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dulajkavinda', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'Mexico City, Mexico', 'stats_list': [['2', '          followers'], ['9', '          following'], ['8']], 'contributions': '84 contributions\n        in the last year', 'description': ['Diabetes-Trees\n'], 'url_profile': 'https://github.com/pablolopez2733', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'La Piedad, Michoac√°n, M√©xico', 'stats_list': [['1', '          follower'], ['9', '          following'], ['13']], 'contributions': '163 contributions\n        in the last year', 'description': ['Diabetes-Prediction üè•\nThis project uses a Machine Learning algorithm called ID3 (Iterative Dichotomiser), which has the purpose of allowing students of the Artificial Intelligence course to learn different techniques to approach machine learning problems.\nIn this case, a Decision Tree algorithm which is also easy to understand, provides the basis on these type of algorithms.\nThis algorithm uses a data-set which looks like this.\n\n\n\nPregnancies\nGlucose\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nDecision\n\n\n\n\n2\n138\n62\n35\n0\n33.6\n0.127\n47\n\n\n0\n84\n82\n31\n125\n38.2\n0.233\n23\n\n\n\nIf you wish to view the data-set, you can find it here. Link to Kaggle\nThe project was uploaded to a web server which cannot keep its IP address, it was also intended to be moved into a more elegant Framework, such as Django or Flask, but for the sake of learning and course completness it wil remain in CGI format temporarly.\n\nFeel free to provide your feedback or leave your comments.\n\n'], 'url_profile': 'https://github.com/ElioEfra99', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [['1', '          follower'], ['2', '          following'], ['0']], 'contributions': '63 contributions\n        in the last year', 'description': ['diabetes-article\n'], 'url_profile': 'https://github.com/easymath', 'info_list': ['5', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020', 'R', 'Updated Apr 24, 2020', '1', 'C#', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Java', 'Updated May 18, 2020', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'HTML', 'Updated Apr 20, 2020']}"
"{'location': 'NONE', 'stats_list': [['1', '          follower'], ['0', '          following'], ['0']], 'contributions': '8 contributions\n        in the last year', 'description': ['Diabetes Prediction\nSource code for predicting complications of Diabetes Mellitus type 2 (DM2).\n'], 'url_profile': 'https://github.com/bljubic', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['Diabetes-Prediction\nThis Use case predicts the chances of having Diabetes to a female by observing certain features . The training data set comprises of features like num_preg ,\tglucose_conc ,\tdiastolic_bp ,\tthickness ,\tinsulin, \tbmi ,\tage \t,skin. The variable diab_pred is then encoded to binary form while applying the model.\n'], 'url_profile': 'https://github.com/PreeBh', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Pune', 'stats_list': [['9', '          followers'], ['40', '          following'], ['36']], 'contributions': '580 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Adi1222', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [['7', '          followers'], ['5', '          following'], ['1']], 'contributions': '134 contributions\n        in the last year', 'description': ['Classifica√ß√£o\nPredi√ß√£o da diabetes por meio da an√°lise de caracter√≠sticas de pacientes\n'], 'url_profile': 'https://github.com/gizattos', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Israel', 'stats_list': [['1', '          follower'], ['2', '          following'], ['0']], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ItaiZeilig', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Charlottesville', 'stats_list': [['3', '          followers'], ['2', '          following'], ['3']], 'contributions': '98 contributions\n        in the last year', 'description': ['diabetes-classifier\nDiabetes classifier using PyTorch on the Pima Indians Diabetes Database\n'], 'url_profile': 'https://github.com/mohitsudhakar', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Mexico City, Mexico', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['covid_diabetesmx\nCode for evaluation of type 2 diabetes and obesity as risk factors for COVID-19 related outcomes and lethality within Mexican population\n'], 'url_profile': 'https://github.com/oyaxbell', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [['2', '          followers'], ['3', '          following'], ['9']], 'contributions': '205 contributions\n        in the last year', 'description': ['Pima Indian Diabetes Prediction :\nThe model predicts the person who are likely to be affected by diabetes.\nRequired Python Libraries:\nPandas\nMatplotlib\nSklern\n'], 'url_profile': 'https://github.com/Nikhil-L', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['diabetesdietadvisor.me\n'], 'url_profile': 'https://github.com/jieni2', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'nanded', 'stats_list': [['39', '          followers'], ['56', '          following'], ['50']], 'contributions': '368 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sayyss', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020', 'Dart', 'Updated Jul 26, 2020', 'Python', 'Updated Apr 18, 2020', 'R', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'CSS', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}"
"{'location': 'NONE', 'stats_list': [['3', '          followers'], ['3', '          following'], ['0']], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/renitahsn', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KasiditPha', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Diabetes-Prediction\nIn this, I used xgboost algorithm for predicting.\n'], 'url_profile': 'https://github.com/DixitTrivedi', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Pima_Diabetes_Analysis\nDiagnostics of Diabetics using Pima_Diabetes dataset\nData Set Information:\nSeveral constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. ADAP is an adaptive learning routine that generates and executes digital analogy of perceptron-like devices.\nAttribute Information:\nNumber of times pregnant 2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test 3. Diastolic blood pressure (mm Hg) 4. Triceps skin fold thickness (mm) 5. 2-Hour serum insulin (mu U/ml) 6. Body mass index (weight in kg/(height in m)^2) 7. Diabetes pedigree function 8. Age (years) 9. Class variable (0 or 1) **\nThis project is about the female patient records from Pima Indian Heritage, Arizona which deals about the diagnostics of Diabetics. We will be using the packages like pandas, numpy, seaborn, matpotlib to find some insights about the dataset\n'], 'url_profile': 'https://github.com/praveenpno', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'Mumbai', 'stats_list': [['11', '          followers'], ['3', '          following'], ['11']], 'contributions': '478 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GauravSahani1417', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/birindwa-dev', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'Joypurhat', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Diabetes_Prediction_Portal\n'], 'url_profile': 'https://github.com/faysalislam', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [['1', '          follower'], ['1', '          following'], ['0']], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/furqanhermawan23', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'Itajub√°, MG', 'stats_list': [['3', '          followers'], ['1', '          following'], ['5']], 'contributions': '42 contributions\n        in the last year', 'description': ['Diabetes Onset Detection with DNN\n\n\n\n\n\nThe main objective of this project is to use a Deep Neural Network to predict the onset of diabetes for a set of patients.\nSummary\nIn this project, we build a DNN and found the optimal hyperparameters using the scikit-learn grid search. We also learned how to optimize a network by tuning the hyperparameters. The results that we get might not be the same for all of us, but as long as we get similar predictions, we can consider our model a success.\nWhen you start training on new data, or if you are trying to address a different problem with another dataset, you will have to go through this process again.\n\nCreated by costaruan\n'], 'url_profile': 'https://github.com/costaruan', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Diabetes_linear_model_again\n'], 'url_profile': 'https://github.com/shi-star', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 13, 2020', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Poppop179', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['diabetes_linear_model\n'], 'url_profile': 'https://github.com/shi-star', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'LUDHIANA PUNJAB', 'stats_list': [['6', '          followers'], ['39', '          following'], ['30']], 'contributions': '22 contributions\n        in the last year', 'description': ['5G-Smart-Diabetes\nRecent advances in wireless networking and big data technologies, such as 5G networks, med- ical big data analytics, and the Internet of Things, along with recent developments in wearable computing and artificial intelligence, are enabling the development and implementation of innovative diabetes monitoring systems and applications. Due to the life-long and systematic harm suffered by diabetes patients, it is critical to design effective methods for the diagnosis and treatment of diabetes. Based on our comprehensive investigation,this article classifies those methods into Diabetes 1.0 and Diabetes 2.0, which exhibit deficiencies in terms of networking and intelligence. Thus, our goal is to design a sustainable, cost-effective, and intelligent diabetes diagnosis solution with personalized treatment. In this article, we first propose the 5G-Smart Diabetes system, which combines the state-of-the-art technologies such as wearable 2.0, machine learning, and big data to generate comprehensive sensing and analysis for patients suffering from diabetes. Then we present the data sharing mechanism and personalized data analysis model for 5G-Smart Diabetes. Finally, we build a 5G-Smart Diabetes testbed that includes smart clothing, smartphone, and big data clouds. The experimental results show that our system can effectively provide personalized diagnosis and treatment suggestions to patients.\n'], 'url_profile': 'https://github.com/TARANPREETS1999', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [['7', '          followers'], ['8', '          following'], ['2']], 'contributions': '80 contributions\n        in the last year', 'description': ['This was a short project I worked on for AI club in Ohlone college.\nIt was to show our members an example of using Keras and how we can categorize someone being diabetic or not\nbased on a given data set.\n'], 'url_profile': 'https://github.com/Tann1', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Unidad-de-Medicina-Interna\nmadicina interna, diabetes y enfermedades cardiometabolicas\n'], 'url_profile': 'https://github.com/drcarlospt', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'Mumbai', 'stats_list': [['15', '          followers'], ['9', '          following'], ['2']], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akhiilkasare', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'Mumbai, India', 'stats_list': [['43', '          followers'], ['45', '          following'], ['11']], 'contributions': '1,037 contributions\n        in the last year', 'description': ['Diabetes-prediction-using-KNN-algorithm\n'], 'url_profile': 'https://github.com/prakashjha18', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['myDiabetesSupplies_iOS\n'], 'url_profile': 'https://github.com/Lollators', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'New Delhi, India', 'stats_list': [['1', '          follower'], ['0', '          following'], ['10']], 'contributions': '90 contributions\n        in the last year', 'description': [""Diabetes-classificatio-using-knn-CB\n['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']  and your task is to predict whether a person is suffering from diabetes or not (Binary Classification)  Tasks  1) Plot a bar graph showing number of classes and no of examples in each class.  2) Classification Task, classify a person as 0 or 1 (Diabetic or Not) using K-Nearest Neighbors classifier. The column names or headers of submission file must match with that given in sample submission file.  Datatype of the columns of submission file must match with that of the given sample_submission file.  The number of rows must be equal to given test cases and number of columns must be equal to the columns given in the sample submission file.\n""], 'url_profile': 'https://github.com/Anshuljainonline', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Task\nDiabetes is a highly prevalent and expensive chronic condition, costing about $330 billion to Americans annually. Most of the cost is attributed to the ‚Äòtype-2‚Äô version of diabetes, which is typically diagnosed in middle age. Today is December 31, 2016. A commercial health insurance company has contracted you to predict which of their members are most likely to be newly-diagnosed with type-2 diabetes in\nYour goal is to featurize their data, train and optimize a predictive model, and explain your results and approach. (Note: ‚Äúnewly-diagnosed‚Äù means members who were \u200b NOT previously coded \u200bwith diabetes \u200bprior\u200b to 2016-12-31, inclusive).\nDetails about the dataset\nThe data provided are real patient claims records from a large insurance company, appropriately de-identified. The data sets have already been split into training and test sets (‚Äòtrain.txt‚Äô & ‚Äòtest.txt‚Äô). The proportions of members are about 70% train and 30% test. Each line in both text files is a patient record, represented as a json string. The health record is parameterized by a set of encounter dates in a YYYY-MM-DD format. The structure of each patient json is as follows: - ‚Äòbday‚Äô - patient date of birth in YYYY-MM-DD format - ‚Äòis_male‚Äô - True = Male, False = Female - ‚Äòpatient_id‚Äô - de-identified patient id (each patient is given a unique value) - ‚Äòresources‚Äô - dictionary of encounter_date ‚Üí list of codes (described below) - ‚Äòobservations‚Äô - dictionary of encounter_date ‚Üí list of dictionaries (described below) - ‚Äòtag_dm2‚Äô - indicates date of first type-2 diabetes diagnosis - will either have a YYYY-MM-DD date or be an empty ‚Äò‚Äô string; this information will be censored from the holdout set. (described above) - ‚Äòsplit‚Äô - indicates a member is in the ‚Äòtrain‚Äô or ‚Äòtest‚Äô set; \u200binformation beyond 2017-01- has been \u200b **removed** \u200b from test.txt\u200b. Each patient record has a key ‚Äòtag_dm2‚Äô, whose value is \u200b either \u200b a ‚ÄòYYYY-MM-DD‚Äô date string indicating the date of first code of a diagnosis of diabetes, \u200b or \u200b an empty string ‚Äò‚Äô (indicating no diabetes in their record). Your task is to predict each test set member‚Äôs probability of being \u200bnewly-diagnosed\u200b with diabetes in 2017. Information for \u200beach test set member‚Äôs\u200b health record beyond 2017-01-01 has been removed; the true diabetes status of the member is hidden from you and will be used by Lumiata‚Äôs data science team to evaluate your solution. You should cohort your data (i.e construct the response variable) in the training set according to the following definitions (check your work with the training set counts given below for each definition):\nA ‚Äò\u200bclaim\u200b‚Äô is someone whose ‚Äòtag_dm2‚Äô date is between 2017-01-01 and 2017-12-31, inclusive (training set count of ‚Äòclaim‚Äô = 3410) - the response for these members is a ‚Äò1‚Äô A ‚Äò\u200bnever-claim\u200b‚Äô is someone whose ‚Äòtag_dm2‚Äô date is \u200b either \u200b after 2017-12-31, exclusive, or \u200b is an empty string ‚Äò‚Äô (training set count of ‚Äònever-claim‚Äô = 70110) - the response for these members is a ‚Äò0‚Äô A ‚Äò\u200bprior\u200b‚Äô is someone whose ‚Äòtag_dm2‚Äô date is \u200b before \u200b 2017-01-01, exclusive - typically ‚Äòpriors‚Äô are filtered out of the matrix before training. You may include these people in training, but keep in mind they will be filtered out of ‚Äòtest‚Äô when we evaluate your solution. Each patient record also has two keys describing their health history - ‚Äòresources‚Äô & ‚Äòobservations‚Äô. The ‚Äòresources‚Äô key specifies the diagnoses, medications, and procedures that were noted/prescribed/performed at each doctor‚Äôs visit - these are represented by different coding systems (icd9/10, rxnorm, cpt, respectively.) Each encounter date in the ‚Äòresources‚Äô key is mapped to the corresponding list of codes issued at that doctor‚Äôs visit. The codes have the format _. For instance, ‚Äòicd9_272.0‚Äô, which corresponds to high cholesterol: http://www.icd9data.com/2015/Volume1/240-279/270-279/272/272.0.htm Note \u200b - encounter dates in ‚Äòresources‚Äô can sometimes have no codes in the code list! The ‚Äòobservations‚Äô key specifies the lab tests that were completed - each encounter date is mapped to a list of dictionaries, each of which has the following keys: ‚Äòcode‚Äô - the ‚Äòloinc‚Äô code corresponding to the lab test ‚Äòinterpretation‚Äô - whether the lab was ‚ÄòH‚Äô for high, ‚ÄòL‚Äô for low, ‚ÄòN‚Äô for normal, or ‚ÄòA‚Äô for abnormal ‚Äòvalue‚Äô - the value extracted from the lab For instance, the lab could have been a blood glucose test ‚Äòloinc_2345-7‚Äô, whose value may have been 130, and hence whose interpretation would be ‚ÄòH‚Äô (a cut-off for high blood glucose is 106: https://s.details.loinc.org/LOINC/2345-7.html?sections=Comprehensive\u200b ) Note \u200b - the values in the ‚Äòinterpretation‚Äô and ‚Äòvalue‚Äô keys can sometimes be ‚ÄòNone‚Äô! The keys in the ‚Äòresources‚Äô and ‚Äòobservation‚Äô dictionary correspond to the encounter date with the doctor. All dates are formatted as string in YYYY-MM-DD format, e.g. ‚Äú2016-04-30‚Äù. The format of the file you submit to us should be a csv file, formatted as ‚Äò_dm2_solution.csv‚Äô. We should be able to read in your solution using pandas as follows: for each test set patient_id.\nExplore the structure of the data first\nImporting Libraries\nimport csv\nimport warnings\nwarnings.filterwarnings(\'ignore\')\nimport matplotlib\nimport re\nfrom tqdm import tqdm\nimport datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import sparse\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import resample\nfrom sklearn.linear_model import LogisticRegression\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\nRead in train.txt data\nimport json\nfrom pprint import pprint\n\nls = []\nwith open(\'train.txt\') as f:\n    for line in f:\n        ls.append(json.loads(line))\nShow structure of each patient json\n(key_name, type_of_value_associated_to_key)\npprint(zip(ls[0].keys(), [type(ls[0][x]) for x in ls[0].keys()]))\n<zip object at 0x7f793a323af0>\n\nDisplay patient json\n#pprint(ls[0])\nShow \'resources\' key\n#pprint(ls[0][\'resources\'])\nShow \'observations\' key\n#pprint(ls[2][\'observations\'])\nFill your code below this line\n1. Experimental setup/design\nChecking the number of points in the train dataset:\nlen(ls)\n73597\n\nThere are 73597 points in the train set\nPreprocessing the Raw Data:\nRemoving the points whose tag_dm2 date is before 01-01-2017. We are basically filtering out the priors in this step:\nls_without_priors=[]\nfor i in range(0,len(ls)):\n    if ls[i][""tag_dm2""]!="""":\n        if int(ls[i][""tag_dm2""].split(\'-\')[0])>=2017:\n            ls_without_priors.append(ls[i])\n    elif ls[i][""tag_dm2""]=="""":\n        ls_without_priors.append(ls[i])\nprint(len(ls_without_priors))\n73520\n\nAfter removing the priors we see that there are around 73520 points left.\nCreating a label vector:\nlabels=[]\nfor i in range(0,len(ls_without_priors)):\n    if ls_without_priors[i][""tag_dm2""]!="""":\n        if int(ls_without_priors[i][""tag_dm2""].split(\'-\')[0])>2017:\n            labels.append(0)\n        elif int(ls_without_priors[i][""tag_dm2""].split(\'-\')[0])==2017:\n            labels.append(1)\n    elif ls_without_priors[i][""tag_dm2""]=="""":\n        labels.append(0)\nlabels=np.array(labels)\nprint(len(labels))\n73520\n\nNow, we have a label vector of size 73520.\nPreprocessing the birthday feature and converting it into age in years:\nage=[]\nfor i in range(len(ls_without_priors)):\n    age.append(2017-int(ls_without_priors[i][\'bday\'].split(\'-\')[0]))\ndf = pd.DataFrame({\'Age\':age})\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\n\n\n\n\n0\n58\n\n\n1\n14\n\n\n2\n28\n\n\n3\n60\n\n\n4\n24\n\n\n\n\nPreprocessing the \'is_male\' feature:\nis_male=[]\nfor i in range(len(ls_without_priors)):\n    if ls_without_priors[i][\'is_male\']==True:\n        is_male.append(1)\n    elif ls_without_priors[i][\'is_male\']==False: \n        is_male.append(0)\ndf[\'is_male\']=is_male\nPreprocessing the Observation feature:\ntest_list=[]\ntest_code=set()\nfor i in range(len(ls_without_priors)):\n    m = ls_without_priors[i][\'observations\']\n    for k in m:\n        for x in m[k]:\n            test_code.add(x[\'code\'])\n            test_list.append(x[\'code\'])\nprint(""Number of unique tests:"",len(test_code),""Number of total tests performed for all the people:"",len(test_list))\nNumber of unique tests: 3910 Number of total tests performed for all the people: 4158790\n\nAfter running the above patch of code, it was foind out that the same test was taken by a person multiple times over a period of time. In order to encode this feature only the most recent observation of every unique test is going to be taken into consideration.\nlist_of_tests=[]\nfor i in range(len(ls_without_priors)):\n    test_list=[]\n    test_code=set()\n    dici={}\n    m = ls_without_priors[i][\'observations\']\n    for k in m:\n        \n        for x in m[k]:\n            if x[\'code\'] not in dici:\n                k_split=k.split(\'-\')\n                k_time_obj=datetime.date(int(k_split[0]),int(k_split[1]),int(k_split[2]))\n                dici[x[\'code\']]=[k_time_obj]\n            elif x[\'code\'] in dici:\n                k_split=k.split(\'-\')\n                k_time_obj=datetime.date(int(k_split[0]),int(k_split[1]),int(k_split[2]))\n                dici[x[\'code\']]+=[k_time_obj]\n    list_of_tests.append(dici)\nfor i in list_of_tests:\n    for j in i:\n        a=sorted(i[j])\n        i[j]=a[-1]  \nnew_list_of_tests = [] \nfor i in range(len(list_of_tests)):\n    test={}\n    for key, value in list_of_tests[i].items(): \n       if value in test: \n           test[value].append(key) \n       else: \n           test[value]=[key] \n    new_list_of_tests.append(test)\nprint(new_list_of_tests[0])\n{datetime.date(2014, 11, 13): [\'loinc_13457-7\', \'loinc_1742-6\', \'loinc_1751-7\', \'loinc_1759-0\', \'loinc_17861-6\', \'loinc_1920-8\', \'loinc_1975-2\', \'loinc_1989-3\', \'loinc_2028-9\', \'loinc_2075-0\', \'loinc_2085-9\', \'loinc_2093-3\', \'loinc_2160-0\', \'loinc_2276-4\', \'loinc_2345-7\', \'loinc_2498-4\', \'loinc_2500-7\', \'loinc_2571-8\', \'loinc_2823-3\', \'loinc_2885-2\', \'loinc_2951-2\', \'loinc_3016-3\', \'loinc_3094-0\', \'loinc_33037-3\', \'loinc_33914-3\', \'loinc_44734-2\', \'loinc_6768-6\', \'loinc_9830-1\'], datetime.date(2014, 5, 21): [\'loinc_24111-7\', \'loinc_6356-0\'], datetime.date(2014, 10, 30): [\'loinc_20405-7\', \'loinc_21000-5\', \'loinc_2514-8\', \'loinc_25428-4\', \'loinc_26444-0\', \'loinc_26449-9\', \'loinc_26474-7\', \'loinc_26484-6\', \'loinc_28539-5\', \'loinc_30451-9\', \'loinc_32167-9\', \'loinc_32623-1\', \'loinc_32776-7\', \'loinc_33825-1\', \'loinc_4544-3\', \'loinc_49754-5\', \'loinc_5770-3\', \'loinc_5778-6\', \'loinc_5794-3\', \'loinc_5799-2\', \'loinc_5802-4\', \'loinc_5803-2\', \'loinc_5804-0\', \'loinc_5811-5\', \'loinc_6690-2\', \'loinc_718-7\', \'loinc_777-3\', \'loinc_786-4\', \'loinc_787-2\', \'loinc_789-8\']}\n\n The above obtained new_list_of_tests contains only the unique tests taken for each person. The test taken on the most recent date has been retained.\nPreprocessing the interpretation feature under observations:\nThe interpreation feature of a laboratory test for each observation can take the values \'Abnormal\', \'Normal\', \'Low\', \'High\',\'None\' or it may have never been done for a particular person. It makes sense to convert this feature to a Bag of Words Representation:\n#Buliding the structure of that vector\ntest_list=[]\ntest_code=set()\nfor i in range(len(ls_without_priors)):\n    m = ls_without_priors[i][\'observations\']\n    for k in m:\n        \n        for x in m[k]:\n            test_code.add(x[\'code\'])\n            test_list.append(x[\'code\'])\n#print(len(test_code), len(test_list))\ntest_code_vector_structure={}\nct=0\ntest_value_vector=[]\nfor i in test_code:\n    test_code_vector_structure[i]=ct\n    ct+=1\nfor i in range(len(ls_without_priors)):\n    temp=[0 for i in range(3910)]\n    for date in new_list_of_tests[i]:\n        m=ls_without_priors[i][\'observations\'][date.strftime(\'%Y-%m-%d\')]\n        for dici in m:\n            if dici[\'code\'] in new_list_of_tests[i][date]:\n                temp[test_code_vector_structure[dici[\'code\']]]=dici[\'interpretation\']\n    test_value_vector.append(temp)\n#print(len(test_value_vector))\nfor i in range(len(test_value_vector)):\n    for j in range(len(test_value_vector[0])):\n        if test_value_vector[i][j]==None:\n            test_value_vector[i][j]=\'Noresult\'\n        elif test_value_vector[i][j]==0:\n            test_value_vector[i][j]=\'Never\'\n        elif test_value_vector[i][j]==\'L\':\n            test_value_vector[i][j]=\'Low\'\n        elif test_value_vector[i][j]==\'H\':\n            test_value_vector[i][j]=\'High\'\n        elif test_value_vector[i][j]==\'N\':\n            test_value_vector[i][j]=\'Normal\'\n        elif test_value_vector[i][j]==\'A\':\n            test_value_vector[i][j]=\'Abnormal\'\n        else:\n            continue\nyalla=list(test_code)\ndf_interpretation=pd.DataFrame(test_value_vector)\ndf_interpretation.columns=yalla\n\n#Converting to BOW\nvectorizer = CountVectorizer()\ndf_interpretation_bow=pd.DataFrame()\nfor i in range(len(yalla)):\n    t=df_interpretation[yalla[i]].values\n    X=vectorizer.fit_transform(t)\n    \n    l=vectorizer.get_feature_names()\n    temp_l=[yalla[i]+\'_\'+j for j in l]\n    df_interpretation_bow[temp_l]=pd.DataFrame.sparse.from_spmatrix(X)\nprint(len(df_interpretation_bow.columns))\n9097\n\nPreprocessing the Resources Feature\nstr_rep_of_resources=[]\nfor i in range(len(ls_without_priors)):\n    m=ls_without_priors[i][\'resources\']\n    temp_s=\'\'\n    for j in m:\n        temp_s+=\' \'.join(m[j])\n        temp_s+=\' \'\n        temp_s=re.sub(\' +\', \' \', temp_s)\n    str_rep_of_resources.append(temp_s)\nstr_rep_of_resources[0] \n\' cpt_99213 icd9_V85.23 cpt_87210 icd9_789.03 cpt_87591 cpt_87491 icd9_616.10 cpt_73620 cpt_99213 icd10_L84 cpt_11055 cpt_73610 icd9_719.47 cpt_99213 icd10_L57.0 cpt_99213 icd10_L84 cpt_17000 icd9_564.00 cpt_99213 cpt_74000 icd9_V85.1 icd9_789.00 icd10_Z12.31 icd9_V76.12 cpt_77057 cpt_73510 icd9_789.03 cpt_99213 cpt_97140 icd9_719.45 cpt_97110 cpt_97001 cpt_72195 icd9_789.09 cpt_99213 icd9_V85.22 icd9_278.02 icd9_700 cpt_83540 cpt_83550 cpt_80053 cpt_82728 icd9_V72.31 cpt_82306 cpt_77080 cpt_84443 icd9_733.90 cpt_80061 cpt_99213 icd9_789.03 icd9_278.02 cpt_97140 icd9_719.45 cpt_97110 icd9_V76.12 cpt_77057 icd9_733.99 icd9_729.5 cpt_73630 cpt_99213 icd9_703.0 icd10_R30.0 cpt_99213 icd10_J40 cpt_87804 cpt_99213 icd9_727.9 cpt_99213 icd10_F41.1 cpt_99213 icd9_715.97 icd9_796.2 cpt_73590 icd9_719.47 cpt_99213 cpt_L1902 cpt_76856 icd9_789.03 icd9_625.9 cpt_76830 cpt_73721 icd9_719.05 cpt_92133 icd10_H40.003 cpt_92083 icd9_719.45 cpt_97110 cpt_97140 icd9_719.45 cpt_97110 cpt_97140 icd9_719.45 cpt_97110 icd9_780.79 icd9_V72.31 cpt_97110 cpt_81003 icd9_V49.81 cpt_81001 cpt_85025 icd9_719.45 cpt_77080 icd9_599.72 \'\n\ncv = CountVectorizer()\ncv_fit=cv.fit_transform(str_rep_of_resources)\nresources_freq=cv_fit.toarray()\n\nprint(resources_freq.shape)\nfeature_freq_names=cv.get_feature_names()\n(73520, 14805)\n\nWe have converted the resorces feature into frequency counts. That is, for every patient we will have the number of times a doctor has performed a specific test/diagnosis.So the length of encoding for each patient would be the total number of unique diagonosis/medications or procedures specified by the doctors.From the above we see that the length of encoding for each patient would be 14805.\nPreprocessing the Value feature under Observations:\nThis is a numerical feature and for each of the unique tests we are going to have a value for each patient if he has taken the test\n#Buliding the structure of that vector\ntest_list=[]\ntest_code=set()\nfor i in range(len(ls_without_priors)):\n    m = ls_without_priors[i][\'observations\']\n    for k in m:\n        \n        for x in m[k]:\n            test_code.add(x[\'code\'])\n            test_list.append(x[\'code\'])\nprint(len(test_code), len(test_list))\ntest_code_vector_structure={}\nct=0\ntest_value_vector=[]\nfor i in test_code:\n    test_code_vector_structure[i]=ct\n    ct+=1\nfor i in range(len(ls_without_priors)):\n    temp=[0 for i in range(3910)]\n    for date in new_list_of_tests[i]:\n        m=ls_without_priors[i][\'observations\'][date.strftime(\'%Y-%m-%d\')]\n        for dici in m:\n            if dici[\'code\'] in new_list_of_tests[i][date]:\n                temp[test_code_vector_structure[dici[\'code\']]]=dici[\'value\']\n    test_value_vector.append(temp)\nprint(len(test_value_vector))\nfor i in range(len(test_value_vector)):\n    for j in range(len(test_value_vector[0])):\n        if test_value_vector[i][j]==None:\n            test_value_vector[i][j]=0 \ndef column(matrix, i):\n    return [row[i] for row in matrix if row[i]!=0]\ndici_non_sparse_feature={}\nfor j in range(0,3910):\n    a=column(test_value_vector,j)\n    dici_non_sparse_feature[j]=len(a)\nimport operator\ntest_code_list=list(test_code)\nsorted_d = sorted(dici_non_sparse_feature.items(), key=operator.itemgetter(1))\nnon_sparse_features=sorted_d[-50:]\n#the above matrix holds the index of the 50 most non-sparse features present\ndef column_return(matrix, i):\n    return [row[i] for row in matrix]\nfeat_vector=[]\nfeat_nos=[]\nfor i in non_sparse_features:\n    a=column_return(test_value_vector,i[0])\n    feat_nos.append(test_code_list[i[0]])\n    feat_vector.append(a)\nnp_feat_vector=np.array(feat_vector)\nnp_feat_vector_t=np_feat_vector.T\nnp_feat_vector_t[0]\nprint(feat_nos)\n3910 4158790\n73520\n[\'loinc_26444-0\', \'loinc_32623-1\', \'loinc_30451-9\', \'loinc_26449-9\', \'loinc_10834-0\', \'loinc_26474-7\', \'loinc_26484-6\', \'loinc_33037-3\', \'loinc_770-8\', \'loinc_706-2\', \'loinc_713-8\', \'loinc_736-9\', \'loinc_5905-5\', \'loinc_21000-5\', \'loinc_9830-1\', \'loinc_1759-0\', \'loinc_788-0\', \'loinc_33914-3\', \'loinc_3016-3\', \'loinc_704-7\', \'loinc_785-6\', \'loinc_711-2\', \'loinc_731-0\', \'loinc_742-7\', \'loinc_751-8\', \'loinc_13457-7\', \'loinc_2093-3\', \'loinc_2571-8\', \'loinc_2085-9\', \'loinc_1751-7\', \'loinc_1742-6\', \'loinc_1920-8\', \'loinc_2885-2\', \'loinc_1975-2\', \'loinc_6768-6\', \'loinc_786-4\', \'loinc_787-2\', \'loinc_2028-9\', \'loinc_2075-0\', \'loinc_17861-6\', \'loinc_6690-2\', \'loinc_2345-7\', \'loinc_2823-3\', \'loinc_2951-2\', \'loinc_777-3\', \'loinc_718-7\', \'loinc_2160-0\', \'loinc_3094-0\', \'loinc_789-8\', \'loinc_4544-3\']\n\nSince this is a numerical feature we are going to have to scale and and here it has been chosen to scale between 0 and 1\nmin_max_scaler=preprocessing.MinMaxScaler()\nnp_feat_vector_t_scaled=min_max_scaler.fit_transform(np_feat_vector_t)\ndf_observation_feature=pd.DataFrame(np_feat_vector_t_scaled)\ndf_observation_feature.columns=feat_nos\nExploratory Data Analysis:\nChecking the distribution of Classes, that is, if we have a balanced dataset or an imbalanced dataset:\n##https://medium.com/@krishnakummar/donut-chart-with-python-matplotlib-d411033c960b\n##Code to plot the distribution of 2 classes:\nno_of_ones=np.count_nonzero(labels)\nno_of_zeros=len(labels)-no_of_ones\nlabel = \'0\', \'1\'\ncolors = [\'lightskyblue\', \'yellow\']\nclasses=[no_of_zeros,no_of_ones]\nplt.pie(classes, labels=label, colors=colors,\n        autopct=\'%1.1f%%\', shadow=True)\ncentre_circle = plt.Circle((0,0),0.75,color=\'black\', fc=\'white\',linewidth=1.25)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.axis(\'equal\')\nplt.show() \nprint(""Number of Zeros(Not diagnosed type-2 diabetes in 2017)"",no_of_zeros)\nprint(""Number of Ones(Diagnosed with type-2 diabetes in 2017)"",no_of_ones)\n\nNumber of Zeros(Not diagnosed type-2 diabetes in 2017) 70110\nNumber of Ones(Diagnosed with type-2 diabetes in 2017) 3410\n\nWe observe that the dataset is highly imbalanced from the above plot. A very large number of people belong to the class with no type-2 diabetes diagnosed in 2017 compared to the number of people who were diagnosed with type-2 diabetes in 2017.\nChecking how the AGE feature which we had engineered previously affects the chance of being affected by type-II diabetes:\ndf[\'Labels\']=labels\nboxplot = df.boxplot(\'Age\',by=\'Labels\')\nboxplot.set_ylabel(""Age"")\nboxplot.set_xlabel(""Type-II diabetes"")\nboxplot\n<matplotlib.axes._subplots.AxesSubplot at 0x7f7971692ed0>\n\n\nFrom the above boxplot we observe that the chances of being affected by type-II diabetes varies with age. We say this because the means are completely different so we can make some sort of generalisation from it. For example, we can say that there may be a higher risk of being affected by type-II diabetes if the person if of age>60.\nfig, axes = plt.subplots(nrows=2, ncols=1)\nax0, ax1= axes.flatten()\nax0.hist(df[df[\'Labels\']==1][\'Age\'].values,bins=25)\nax0.set_title(\'HISTOGRAM OF NUMBER OF PEOPLE WITH AGE FOR PEOPLE AFFECTED BY TYPE-II DIABETES\')\nax0.set_xlabel(\'Age\')\nax0.set_ylabel(\'No. of People\')\nax1.hist(df[df[\'Labels\']==0][\'Age\'].values,bins=25)\nax1.set_title(\'HISTOGRAM OF NUMBER OF PEOPLE WITH AGE FOR PEOPLE NOT AFFECTED BY TYPE-II DIABETES\')\nax1.set_xlabel(\'Age\')\nax1.set_ylabel(\'No. of People\')\nfig.tight_layout(pad=1.0)\nfig.set_figheight(12)\nfig.set_figwidth(15)\n\nAgain, from the histogram we can observe a difference in the distribution. Here, it is more evident that people who are aged less than 20 have very less chance of being affected by diabetes.\nConclusion:\nThis feature age may be sort of important since it provides a good distinction for people above 60 and people below the age 20.\nExploring the feature \'is_male\':\nWe are going to explore here if a person\'s gender affects the possibility of being affected by Type-II daibetes or not\nChecking number of females diagnosed with diabetes in 2017 and the number of females who weren\'t diagnosed with diabetes in 2017:\nfemale_df=df[df[\'is_male\']==0]\nfemale_df[\'Labels\'].value_counts()\n##https://medium.com/@krishnakummar/donut-chart-with-python-matplotlib-d411033c960b\n##Code to plot the distribution of 2 classes:\nlabel = \'0\', \'1\'\ncolors = [\'lightskyblue\', \'yellow\']\nclasses=[female_df[\'Labels\'].value_counts()[0],female_df[\'Labels\'].value_counts()[1]]\nplt.pie(classes, labels=label, colors=colors,\n        autopct=\'%1.1f%%\', shadow=True)\ncentre_circle = plt.Circle((0,0),0.75,color=\'black\', fc=\'white\',linewidth=1.25)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.axis(\'equal\')\nplt.show() \nprint(""Number of Zeros(Females who were not diagnosed type-2 diabetes in 2017)"",female_df[\'Labels\'].value_counts()[0])\nprint(""Number of Ones(Feales who were diagnosed with type-2 diabetes in 2017)"",female_df[\'Labels\'].value_counts()[1])\n\nNumber of Zeros(Females who were not diagnosed type-2 diabetes in 2017) 38465\nNumber of Ones(Feales who were diagnosed with type-2 diabetes in 2017) 1624\n\nChecking number of Males diagnosed with diabetes in 2017 and the number of Males who weren\'t diagnosed with diabetes in 2017:\nmale_df=df[df[\'is_male\']==1]\nmale_df[\'Labels\'].value_counts()\n##https://medium.com/@krishnakummar/donut-chart-with-python-matplotlib-d411033c960b\n##Code to plot the distribution of 2 classes:\n\nlabel = \'0\', \'1\'\ncolors = [\'lightskyblue\', \'yellow\']\nclasses=[male_df[\'Labels\'].value_counts()[0],male_df[\'Labels\'].value_counts()[1]]\nplt.pie(classes, labels=label, colors=colors,\n        autopct=\'%1.1f%%\', shadow=True)\ncentre_circle = plt.Circle((0,0),0.75,color=\'black\', fc=\'white\',linewidth=1.25)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.axis(\'equal\')\nplt.show() \nprint(""Number of Zeros(Males who were not diagnosed type-2 diabetes in 2017)"",male_df[\'Labels\'].value_counts()[0])\nprint(""Number of Ones(Males who were diagnosed with type-2 diabetes in 2017)"",male_df[\'Labels\'].value_counts()[1])\n\nNumber of Zeros(Males who were not diagnosed type-2 diabetes in 2017) 31645\nNumber of Ones(Males who were diagnosed with type-2 diabetes in 2017) 1786\n\nFrom the above donut plots,we can say that the gender of the person does not clearly affect the fact that whether he has been diagnosed with diabetes in 2017 or not. This is because the percentages of affected and not affected by diabetes is not changing so much(little more than 1%) with respect to gender.\nExploring the feature, the number of times a person goes for a medical test:\nThis may be a useful feature because generally diabetes patients tend to get the blood test more number of times than people who are not affected by diabetes.\nno_of_days_medical_test=[]\nfor i in range(len(ls_without_priors)):\n    no_of_days_medical_test+=[len(ls[i][\'observations\'])]\ndf[\'no_of_days_medical_test\']=no_of_days_medical_test\nfig, axes = plt.subplots(nrows=2, ncols=1)\nax0, ax1= axes.flatten()\nax0.hist(df[df[\'Labels\']==1][\'no_of_days_medical_test\'].values,bins=60,range=[0,20])\nax1.hist(df[df[\'Labels\']==0][\'no_of_days_medical_test\'].values,bins=60,range=[0,20])\nax0.set_title(\'HISTOGRAM OF NUMBER OF PEOPLE WITH N0. OF MEDICAL TESTS FOR PEOPLE AFFECTED BY TYPE-II DIABETES\')\nax0.set_xlabel(\'No. of medical tests\')\nax0.set_xlabel(\'No. of People\')\nax1.set_title(\'HISTOGRAM OF NUMBER OF PEOPLE WITH N0. OF MEDICAL TESTS FOR PEOPLE NOT AFFECTED BY TYPE-II DIABETES\')\nax1.set_xlabel(\'No. of medical tests\')\nax1.set_xlabel(\'No. of People\')\nfig.tight_layout(pad=1.0)\nfig.set_figheight(8)\nfig.set_figwidth(20)\n\n We learn from the above histograms that this feature does not help greatly in distinguishing between the people affected and not affected by diabetes since the distribution somewhat remains the same for both and also the dataset is highly imbalanced.\nExploring the interpretation of the Lab tests feature:\nThis feature has been encoded into Bag of Words format.Hence,we will have a column corresponding to each of the possible observed value for each test. Since,this will be of very high dimensionality, we will only be visualising the most important of these features. That is the feature which has the most correlation with the label. This will be identified using the SelecKBest feature of sklearn.\nbestfeatures = SelectKBest(score_func=chi2, k=20)\nfit = bestfeatures.fit(df_interpretation_bow,df[\'Labels\'])\nscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(df_interpretation_bow.columns)\nfeatureScores = pd.concat([dfcolumns,scores],axis=1)\nfeatureScores.columns = [\'Specs\',\'Score\']\nprint(featureScores.nlargest(100,\'Score\'))\nbest_20_features=featureScores.nlargest(100,\'Score\')[\'Specs\']\n                      Specs        Score\n3403      loinc_4548-4_high  4747.484130\n2008     loinc_27353-2_high  3497.438775\n5699      loinc_2345-7_high  1683.673463\n5518   loinc_14957-5_normal  1269.742131\n8695    loinc_9318-7_normal   893.697679\n...                     ...          ...\n4356     loinc_1975-2_never    96.240647\n7954   loinc_19123-9_normal    96.037637\n1838       loinc_1968-7_low    95.149258\n8002  loinc_5778-6_noresult    94.224749\n7314       loinc_1989-3_low    93.282036\n\n[100 rows x 2 columns]\n\n We can see a pretty high score for the feature \'loinc_4548-4_high\'. Lets take a look at how this feature is distributed for both the classes \na_0=df_interpretation_bow[df[\'Labels\']==0][\'loinc_4548-4_high\'].value_counts()\na_1=df_interpretation_bow[df[\'Labels\']==1][\'loinc_4548-4_high\'].value_counts()\ndf_interpretation_bow[\'Labels\']=labels\nfig, axes = plt.subplots(nrows=2, ncols=1)\nax0, ax1= axes.flatten()\nli=[a_0[0],a_0[1]]\nl1_1=[a_1[0],a_1[1]]\nax0.bar([\'Low\',\'High\'],li)\n\nax0.set_title(\'Barplot showing the number of people actually not affected by diabetes by loinc_4548-4_high feature having a high value or not\')\nax0.set_ylabel(\'No. of People\')\nax1.bar([\'Low\',\'High\'],l1_1)\n\nax1.set_title(\'Barplot showing the number of people actually affected by diabetes by loinc_4548-4_high feature having a high value or not\')\nax1.set_ylabel(\'No. of People\')\nfig.tight_layout(pad=5.0)\nfig.set_figheight(10)\nfig.set_figwidth(7)\n\nFrom the above plots we see that there are about 200 people who are having a low value of \'loinc_4548-4_high\' actually have diabetes. This looks less but it is actually an important feature, the reason being we have a very imbalanced dataset here hence we have a small value but this feature will help us tremendously in distinguishing since it acts as a very good identifier for True negatives\nExploring the Resources feature which contained the medications,diagnosis and procedures performed or described by the Doctor for each patient:This feature has also been encoded into Bag of Words representation\nSince this is also a very large sized vector, we are going to find the feature which is most correlated with the output labels and then visualize it.\ndf_resources_freq=pd.DataFrame(resources_freq)\nbestfeatures_freq = SelectKBest(score_func=chi2, k=10)\nfit_freq = bestfeatures_freq.fit(df_resources_freq,df[\'Labels\'])\nscores = pd.DataFrame(fit_freq.scores_)\ndf_feature_freq_names=pd.DataFrame(feature_freq_names)\nfeature_freq_Scores = pd.concat([df_feature_freq_names,scores],axis=1)\nfeature_freq_Scores.columns = [\'Specs\',\'Score\']\nprint(feature_freq_Scores.nlargest(10,\'Score\'))\n           Specs          Score\n12459  icd10_e11  151036.966015\n12720  icd10_i10   27299.054325\n9701   cpt_99232   17280.828909\n12731  icd10_i25   15061.720085\n12848  icd10_j96   14041.550226\n12505  icd10_e78   13576.758745\n12823  icd10_j44   13103.078898\n12754  icd10_i50   12957.811643\n13090  icd10_n18   10744.907784\n8261   cpt_83036   10095.513621\n\nBy looking at the scores obtained by using chi-square as the metric we see that the first couple of features may have a good amount of influence on the class.\nNow lets visualize how the feature icd10_e11 is distributed for both the classes\ndf_resources_freq.columns=feature_freq_names\na_0=df_resources_freq[\'icd10_e11\']\na_0=a_0[df[\'Labels\']==0].value_counts()\na_1=df_resources_freq[\'icd10_e11\']\na_1=a_1[df[\'Labels\']==1].value_counts()\nfig, axes = plt.subplots(nrows=2, ncols=1)\nax0, ax1= axes.flatten()\nli=[a_0[0],a_0[1]]\nl1_1=[a_1[0],a_1[1]]\nax0.bar([\'not_prescribed\',\'prescribed\'],li)\nax0.set_title(\'Barplot showing the number of people actually not affected by diabetes against icd10_e11 being prescribed or not\')\nax0.set_ylabel(\'No. of People\')\nax1.bar([\'not_prescribed\',\'prescribed\'],l1_1)\nax1.set_title(\'Barplot showing the number of people actually affected by diabetes against icd10_e11 being prescribed or not\')\nax1.set_ylabel(\'No. of People\')\nfig.tight_layout(pad=1.0)\nfig.set_figheight(20)\nfig.set_figwidth(30)\n\nFrom the above plots we observe that this seems to be one of the important features and is going to be very important in distinguishing because we can clearly see that there are a very large number of people who been affected by diabetes and been prescribed this icd10_e11 and also there is almost no one who has been not affected by diabetes and been prescribed with this.\nExploring the icd10_i10 feature\nThis feature is also bag of words encoded hence we have 0 if this has been prescribed else 1\na_0=df_resources_freq[\'icd10_i10\']\na_0=a_0[df[\'Labels\']==0].value_counts()\na_1=df_resources_freq[\'icd10_i10\']\na_1=a_1[df[\'Labels\']==1].value_counts()\nfig, axes = plt.subplots(nrows=2, ncols=1)\nax0, ax1= axes.flatten()\nli=[a_0[0],a_0[1]]\nl1_1=[a_1[0],a_1[1]]\nax0.bar([\'not_prescribed\',\'prescribed\'],li)\nax0.set_title(\'Barplot showing the number of people actually not affected by diabetes against icd10_i10 being prescribed or not\')\nax0.set_ylabel(\'No. of People\')\nax1.bar([\'not_prescribed\',\'prescribed\'],l1_1)\nax1.set_title(\'Barplot showing the number of people actually affected by diabetes against icd10_i10 being prescribed or not\')\nax1.set_ylabel(\'No. of People\')\nfig.tight_layout(pad=1.0)\nfig.set_figheight(20)\nfig.set_figwidth(30)\n\nThis I believe would also an important feature because we can clearly see that there are a very large number of people who are affected by diabetes who have been prescribed this medication.\n Some of the basic features and some engineered features have been explored above. Now lets proceed into the feature design section where we will will select the features we are going to use in our model and also engineer and explore a couple of more features.\n2. Feature design/feature selection\n-->Some of the important features that we identified above that will be used to model are Age, icd10_e11 , icd10_e10 and loinc_4548-4_high.\nApart from these we will design a few more features\nNow we are going to engineer another feature which is going to comprise sum of the most important features given by SelectKBest and which were not used in the feature above\ndiabetes_icd_imp=df_resources_freq[[\'cpt_99232\',\'icd10_i25\',\'icd10_j96\',\'icd10_e78\',\'icd10_j44\',\'icd10_i50\',\'icd10_n18\',\'cpt_83036\']]\ndiab_feat_imp=diabetes_icd_imp.sum(axis=1)\ndiab_feat_imp.columns=[\'sum_of_imp_resources\']\n#,\'cpt_99232\',\'icd10_i25\',\'icd10_j96\',\'icd10_e78\',\'icd10_j44\',\'icd10_i50\',\'icd10_n18\',\'cpt_83036\'\n\'icd10_e08\',\'icd10_e09\',\'icd10_e10\',\'icd10_e11\',\'icd10_e13\'\n(\'icd10_e08\', \'icd10_e09\', \'icd10_e10\', \'icd10_e11\', \'icd10_e13\')\n\nfig, axes = plt.subplots(nrows=2, ncols=1)\nax0, ax1= axes.flatten()\nax0.hist(diab_feat_imp[df[\'Labels\']==0],bins=100,range=[0,20])\nax1.hist(diab_feat_imp[df[\'Labels\']==1],bins=100,range=[0,20])\nax0.set_title(\'HISTOGRAM OF NUMBER OF PEOPLE WITH SUM OF SELECT10BEST IMPORTANT DIABETIC PRESCRIPTION FOR PEOPLE NOT AFFECTED BY TYPE-II DIABETES\')\nax0.set_xlabel(\'SUM OF DIABETIC PRESCRIPTIONS \')\nax0.set_ylabel(\'No. of People\')\nax1.set_title(\'HISTOGRAM OF NUMBER OF PEOPLE WITH SUM OF SELECT10BEST IMPORTANT DIABETIC PRESCRIPTIONS FOR PEOPLE AFFECTED BY TYPE-II DIABETES\')\nax1.set_xlabel(\'SUM OF DIABETIC PRESCRIPTIONS \')\nax1.set_ylabel(\'No. of People\')\nfig.tight_layout(pad=2.0)\nfig.set_figheight(10)\nfig.set_figwidth(20)\n\nBy looking at the above plot we can say that this definitely becomes another feature that we have to incorportate into our model since it again helps us distinguish between both the classes since the distribututions are different.\nSince we have decided which features to proceed with, we are going to start to put together these features into one dataframe.\nfinal_features=pd.concat([df[\'Age\'],df_interpretation_bow[\'loinc_4548-4_high\'],df_resources_freq[\'icd10_i10\'],df_resources_freq[\'icd10_e11\'],diab_feat_imp],axis=1)\nfinal_features.rename(columns={ final_features.columns[4]: ""sum_of_imp_resources"" }, inplace = True)\nprint(final_features.head())\n   Age  loinc_4548-4_high  icd10_i10  icd10_e11  sum_of_imp_resources\n0   58                  0          0          0                     0\n1   14                  0          0          0                     0\n2   28                  0          0          0                     0\n3   60                  0          0          0                    10\n4   24                  0          0          0                     1\n\nSince we have an imbalanced dataset we are going to Upsample the dataset.\nUpsampling the dataset using random resampling method\nminority=final_features[df[\'Labels\']==1]\ndf_minority_upsampled = resample(minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=70110,    # to match majority class\n                                 random_state=123)\nmajority_class=final_features[df[\'Labels\']==0]\nlabels=[1 for i in range(70110)]\nlabels.extend([0 for i in range(70110)])\ndesign_matrix=pd.concat([df_minority_upsampled,majority_class])\ndesign_labels=pd.DataFrame(labels)\ndesign_matrix.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nloinc_4548-4_high\nicd10_i10\nicd10_e11\nsum_of_imp_resources\n\n\n\n\n28387\n57\n0\n0\n3\n7\n\n\n23645\n84\n0\n26\n1\n52\n\n\n38006\n74\n0\n11\n1\n5\n\n\n67017\n55\n0\n0\n0\n3\n\n\n47262\n64\n0\n2\n1\n6\n\n\n\n\n3. Model selection/tuning\nLogistic Regression Model\nSplitting into train-set(80%) and cross-validation set(20%):\nX_train, X_cv, y_train, y_cv = train_test_split(design_matrix,labels, test_size=0.20, random_state=42)\nCross-Validation:\nWe are going to use cross validation to find the right regularisation parameter. We will use the PRAUC(same as the average precision) as the metric for cross-validation. So we have to seperate our train data now into train and cross-validation set:\ntrain_prauc=[]\ncv_prauc=[]\nprecision_cv=[]\nrecall_cv=[]\nregu_para=[10**-9,10**-8,10**-7,10**-6,10**-5,10**-4,10**-3,10**-2,10**-1,10,10**1,10**2]\nfor i in tqdm(range(len(regu_para))):\n    clf = LogisticRegression(random_state=0,penalty=\'l2\',C=regu_para[i]).fit(X_train,y_train)\n    train_probs=clf.predict_proba(X_train)[:,1]\n    cv_probs=clf.predict_proba(X_cv)[:,1]\n    train_prauc+=[average_precision_score(y_train,train_probs)]\n    cv_prauc+=[average_precision_score(y_cv,cv_probs)]\n    precision, recall, _ = precision_recall_curve(y_cv,cv_probs)\n    precision_cv.append(precision)\n    recall_cv.append(recall)\n  0%|          | 0/12 [00:00<?, ?it/s]ÔøΩ[AÔøΩ[A\n\n  8%|‚ñä         | 1/12 [00:00<00:06,  1.64it/s]ÔøΩ[AÔøΩ[A\n\n 17%|‚ñà‚ñã        | 2/12 [00:01<00:06,  1.62it/s]ÔøΩ[AÔøΩ[A\n\n 25%|‚ñà‚ñà‚ñå       | 3/12 [00:01<00:05,  1.61it/s]ÔøΩ[AÔøΩ[A\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 4/12 [00:02<00:05,  1.51it/s]ÔøΩ[AÔøΩ[A\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 [00:03<00:04,  1.41it/s]ÔøΩ[AÔøΩ[A\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 6/12 [00:04<00:04,  1.32it/s]ÔøΩ[AÔøΩ[A\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7/12 [00:05<00:04,  1.18it/s]ÔøΩ[AÔøΩ[A\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 [00:06<00:03,  1.09it/s]ÔøΩ[AÔøΩ[A\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 9/12 [00:07<00:02,  1.05it/s]ÔøΩ[AÔøΩ[A\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 [00:08<00:02,  1.12s/it]ÔøΩ[AÔøΩ[A\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 11/12 [00:10<00:01,  1.23s/it]ÔøΩ[AÔøΩ[A\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:12<00:00,  1.01s/it]ÔøΩ[AÔøΩ[A\n\nWe are going to plot the precision-recall curve for all the values of regularisation parameter\nfig, axes = plt.subplots(nrows=4, ncols=3)\nax0, ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10,ax11= axes.flatten()\nax0.step(recall_cv[0], precision_cv[0], color=\'b\', alpha=0.8,where=\'post\')\nax1.step(recall_cv[1], precision_cv[1], color=\'b\', alpha=0.8,where=\'post\')\nax2.step(recall_cv[2], precision_cv[2], color=\'b\', alpha=0.8,where=\'post\')\nax3.step(recall_cv[3], precision_cv[3], color=\'b\', alpha=0.8,where=\'post\')\nax4.step(recall_cv[4], precision_cv[4], color=\'b\', alpha=0.8,where=\'post\')\nax5.step(recall_cv[5], precision_cv[5], color=\'b\', alpha=0.8,where=\'post\')\nax6.step(recall_cv[6], precision_cv[6], color=\'b\', alpha=0.8,where=\'post\')\nax7.step(recall_cv[7], precision_cv[7], color=\'b\', alpha=0.8,where=\'post\')\nax8.step(recall_cv[8], precision_cv[8], color=\'b\', alpha=0.8,where=\'post\')\nax9.step(recall_cv[9], precision_cv[9], color=\'b\', alpha=0.8,where=\'post\')\nax10.step(recall_cv[10], precision_cv[10], color=\'b\', alpha=0.8,where=\'post\')\nax11.step(recall_cv[11], precision_cv[11], color=\'b\', alpha=0.8,where=\'post\')\n\nax0.set_title(\'Regu para=10^-9\')\nax0.set_xlabel(\'recall\')\nax0.set_ylabel(\'precision\')\n\nax1.set_title(\'Regu para=10^-8\')\nax1.set_xlabel(\'recall\')\nax1.set_ylabel(\'precision\')\n\nax2.set_title(\'Regu para=10^-7\')\nax2.set_xlabel(\'recall\')\nax2.set_ylabel(\'precision\')\n\n\nax3.set_title(\'Regu para=10^-6\')\nax3.set_xlabel(\'recall\')\nax3.set_ylabel(\'precision\')\n\nax4.set_title(\'Regu para=10^-5\')\nax4.set_xlabel(\'recall\')\nax4.set_ylabel(\'precision\')\n\nax5.set_title(\'Regu para=10^-4\')\nax5.set_xlabel(\'recall\')\nax5.set_ylabel(\'precision\')\n\nax6.set_title(\'Regu para=10^-3\')\nax6.set_xlabel(\'recall\')\nax6.set_ylabel(\'precision\')\n\nax7.set_title(\'Regu para=10^-2\')\nax7.set_xlabel(\'recall\')\nax7.set_ylabel(\'precision\')\n\nax8.set_title(\'Regu para=0.1\')\nax8.set_xlabel(\'recall\')\nax8.set_ylabel(\'precision\')\n\nax9.set_title(\'Regu para=1\')\nax9.set_ylabel(\'precision\')\nax9.set_xlabel(\'recall\')\n\nax10.set_title(\'Regu para=10\')\nax10.set_ylabel(\'precision\')\nax10.set_xlabel(\'recall\')\n\nax11.set_title(\'Regu para=100\')\nax11.set_ylabel(\'precision\')\nax11.set_xlabel(\'recall\')\n\n\nfig.tight_layout(pad=0.5)\nfig.set_figheight(20)\nfig.set_figwidth(20)\n\nThe above are the precision-recall curves for different values of the hyperparameter\nprauc=pd.concat([pd.Series(regu_para),pd.Series(cv_prauc)],axis=1)\nprauc.columns=[""Hyperparameter"",\'Average Precision Score on cross-validation\']\nConclusion:\nBy looking at the above cross-validation curves, we observe that a value of 10^-4 wold be the right value to choose. This is because we get a good average-presion score at that point and we also don\'t overfit at this point.\n4. Model performance evaluation (i.e how well it does along the metrics above)\nTrained on the best hyperparameter value found:\nclf = LogisticRegression(random_state=0,penalty=\'l2\',C=10**-4).fit(X_train,y_train)\ntrain_probs=clf.predict_proba(X_train)[:,1]\ncv_probs=clf.predict_proba(X_cv)[:,1]\nprint(average_precision_score(y_cv,cv_probs))\n0.9788710767106079\n\ncv_new_pred=[]\nfor i in cv_probs:\n    if i>0.5:\n        cv_new_pred.append(1)\n    else:\n        cv_new_pred.append(0)\n        \nThe average precision score obtained is 0.9788 which means the model is performing extremely well. This fact can also be directly observed from the precsion recall curve plotted in the previous section.\nfrom sklearn.metrics import confusion_matrix\nprint(""CV confusion matrix"")\nconf_matrix= pd.DataFrame(confusion_matrix(y_cv,cv_new_pred),range(2),range(2))\nfig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(1,2,2)\nsns.heatmap(conf_matrix,annot=True,ax=ax,fmt=\'g\')\nplt.ylabel(\'Real Classes\')\nplt.xlabel(\'Predicted Classes\')\nplt.show()\nCV confusion matrix\n\n\nLooking at the above confusion matrix we see that there are a large number of false negatives and since its a  medical application we never want to falsely predict that the person is not affected by a disease rather it is okay to tell a person that he has a disease and let him take a second opinion, hence I would mind more false positives than false negatives.\nChanging the threshold value of classification to 0.4 so that we get a better tradeoff between fp and fn:\ncv_new_pred=[]\nfor i in cv_probs:\n    if i>0.4:\n        cv_new_pred.append(1)\n    else:\n        cv_new_pred.append(0)\nprint(""CV confusion matrix"")\nconf_matrix= pd.DataFrame(confusion_matrix(y_cv,cv_new_pred),range(2),range(2))\nfig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(1,2,2)\nsns.heatmap(conf_matrix,annot=True,ax=ax,fmt=\'g\')\nplt.ylabel(\'Real Classes\')\nplt.xlabel(\'Predicted Classes\')\nplt.show()  \nCV confusion matrix\n\n\nThe above confusion matrix shows us that by having a threshold value of 0.4 we are doing much better for this application since we are okay with having more number of false positives than false negatives. This is a business decision that we have made to change the threshold which would be better for this application.\nTest data Preprocessing and testing on test set:\ndef preprocess_test_set(test_data,cv,feature_freq_names):\n    ls_without_priors=[]\n    with open(\'test.txt\') as f:\n     for line in f:\n        ls_without_priors.append(json.loads(line))\n    age=[]\n    for i in range(len(ls_without_priors)):\n        age.append(2017-int(ls_without_priors[i][\'bday\'].split(\'-\')[0]))\n    df = pd.DataFrame({\'Age\':age})\n    str_rep_of_resources=[]\n    for i in range(len(ls_without_priors)):\n        m=ls_without_priors[i][\'resources\']\n        temp_s=\'\'\n        for j in m:\n            temp_s+=\' \'.join(m[j])\n            temp_s+=\' \'\n            temp_s=re.sub(\' +\', \' \', temp_s)\n        str_rep_of_resources.append(temp_s)\n    cv_fit=cv.transform(str_rep_of_resources)\n    resources_freq=cv_fit.toarray()\n    df_resources_freq=pd.DataFrame(resources_freq)\n    df_resources_freq.columns=feature_freq_names\n    list_of_tests=[]\n    for i in range(len(ls_without_priors)):\n        test_list=[]\n        test_code=set()\n        dici={}\n        m = ls_without_priors[i][\'observations\']\n        for k in m:\n\n            for x in m[k]:\n                if x[\'code\'] not in dici:\n                    k_split=k.split(\'-\')\n                    k_time_obj=datetime.date(int(k_split[0]),int(k_split[1]),int(k_split[2]))\n                    dici[x[\'code\']]=[k_time_obj]\n                elif x[\'code\'] in dici:\n                    k_split=k.split(\'-\')\n                    k_time_obj=datetime.date(int(k_split[0]),int(k_split[1]),int(k_split[2]))\n                    dici[x[\'code\']]+=[k_time_obj]\n        list_of_tests.append(dici)\n    for i in list_of_tests:\n        for j in i:\n            a=sorted(i[j])\n            i[j]=a[-1]  \n    new_list_of_tests = [] \n    for i in range(len(list_of_tests)):\n        test={}\n        for key, value in list_of_tests[i].items(): \n           if value in test: \n               test[value].append(key) \n           else: \n               test[value]=[key] \n        new_list_of_tests.append(test)\n\n    feati=[]\n    for i in range(len(new_list_of_tests)):\n        cd=0\n        for j in new_list_of_tests[i]:\n            \n            for m in new_list_of_tests[i][j]:\n                if m==\'loinc_4548-4\':\n                    r=ls_without_priors[i][\'observations\'][j.strftime(\'%Y-%m-%d\')]\n                    for q in r:\n                        if q[\'code\']==\'loinc_4548-4\':\n                            if q[\'interpretation\']==\'H\':\n                                feati+=[1]\n                                cd=1\n                            else:\n                                feati+=[0]\n                                cd=1\n                        if cd==1:\n                            break\n                if cd==1:\n                    break\n            if cd==1:\n                    break\n        if cd!=1:\n                feati+=[0]\n\n    df_interpret=pd.Series(feati)\n    diabetes_icd_imp=df_resources_freq[[\'cpt_99232\',\'icd10_i25\',\'icd10_j96\',\'icd10_e78\',\'icd10_j44\',\'icd10_i50\',\'icd10_n18\',\'cpt_83036\']]\n    diab_feat_imp=diabetes_icd_imp.sum(axis=1)\n    diab_feat_imp.columns=[\'sum_of_imp_resources\']\n    final_features=pd.concat([df[\'Age\'],df_interpret,df_resources_freq[\'icd10_i10\'],df_resources_freq[\'icd10_e11\'],diab_feat_imp],axis=1)\n    predictions=clf.predict(final_features)\n    predictions_probs=clf.predict_proba(final_features)[:,1]\n    \n    with open(\'prashanth_dm2_solution.csv\', \'a\') as file:\n            ct=0\n            writer = csv.writer(file)\n            writer.writerow([\'patient_id\',\'dm2_prob\'])\n            for i in range(len(ls_without_priors)):\n                writer.writerow([ls_without_priors[i][\'patient_id\'],predictions_probs[ct]])\n                ct+=1\n        \n    \n    \n    \n    \n    \n    \n    return predictions,predictions_probs\n                            \n                        \nop,predis=preprocess_test_set(\'test.txt\',cv,feature_freq_names)\ndf=pd.read_csv(\'prashanth_dm2_solution.csv\')\nprint(df.head(10))\n  patient_id             dm2_prob\n0      pat_1   0.2951823911315482\n1      pat_2  0.38658285274904036\n2      pat_4  0.27314622248737047\n3      pat_8  0.27747051126826133\n4     pat_11  0.18584597996556518\n5     pat_17  0.10645579767898171\n6     pat_18  0.15525364907500588\n7     pat_19   0.1024021043546173\n8     pat_20   0.2246428555125333\n9     pat_23  0.30460711841754273\n\n5. A short write-up (2-3 paragraph) of your approach. Be sure to include the following:\n- Top 5 most important features for predicting diabetes - have a rationale for why you think these are most important\n- Briefly describe the model you used for training\n- Explain how you optimized your model, and what performance metrics you optimized for\n-How did you prevent overfitting?\nTop 5 most important features for predicting diabetes:\n\nThe most important feature I could observe after deep exploration of the data was the feature called icd10_e11 . This feature was extracted from the resources key under the record of each patient.The reason why I think this feature is the most important is because it acted as a direct signal to let me know if a person is affected by type-II diabetes or not. This is because icd10_e11 is actually the code which indicates type-II diabetes in medical literature hence acting sort of like a label by itself.\n\nThe above fact can also be proven by looking at the coefficients of the line which seperates the classes in logistic regression. We can interpret this fact by looking at the coefficients. We can very well observe below that the fourth number below corresponds to the feature icd10_e11 and this is the largest number of all thereby proving that its the most important feature.\nclf.coef_\narray([[0.02167449, 0.13493343, 0.03281268, 1.34845179, 0.01319281]])\n\n\n\nThe 2nd most indicative feature to me was the feature which was obtained from the observations key of each patients record. This was the interpreatation of the lab test \'loinc_4548-4\' .This was encoded into bag of words and this being high turned out to be a very important feature. Only the fact that it was high or not was retained in a single column of vector. The scientific reason why this is so important is because this indicates the level of haemogloibin. Haemoglobin is a compound which directly affects the blood glucose level because it carries glucose molecules in it and therefore blood glucose level increase leads to the condition of diabetes. This again can be observed from the output of coefficients above where the second number is the second largest.\n\n\nThe next most important feature that I could see from exploring the data was the age of a person.This feature was designed using the bithday dates of each patient. The age of a person had a good correlation with the fact that a person was affected by diabetes or not. This is because a person above the age 60 is more likely to get affected by type-II diabetes and a person below the age 20 was less likely to be affected by this. This was observed by observing the distribution pertaining to both the classes.\n\n\n4)Another very important feature that I found from the dataset was the code icd10_i10 which relates to hypertension in medical literature. This again influences the fact that a person will have diabetes or not by a large factor. A paper states that there is a very large relation between hypertension and diabetes.\nhttps://care.diabetesjournals.org/content/40/9/1273\n5)Another important feature that I personally engineered was a combination of multiple codes indicating various health conditions in medical literature\n(\'cpt_99232\',\'icd10_i25\',\'icd10_j96\',\'icd10_e78\',\'icd10_j44\',\'icd10_i50\',\'icd10_n18\',\'cpt_83036\'). This was again obtained from the features key in the dataset. These were the most imporatant features indicated by SelectKBest but they did not have any significant effect when individually added to the model but they ended up having a very significant effect when added up together.  The sum was taken after bow encoding. The distribution of the sum of these features was observed and it indicated a significant difference in the way it was distributed for patients with diabetes and patients without diabetes.\nModel used for training:\nWhy was logistic regression chosen?\nThe model that the data was finally trained on was logistic regression model. A decision tree was also tried out and a decision tree with a depth of 4 gave the same exact performance of logistic regression. Since Logistic Regression gave results on par with decision tree it was chosen to model the data rather than decision trees. The reason for this being logistic regression is a simpler model and if it gives such a good performance it is sort of indicative that the data is Linearly seperable.\nSpecifications of the model:\nLogistic regression is a simple classification algorithm to seperate linearly seperable data. The data that it was trained on had about 5 features hence we had to train 5 parameters for the model. Also L2 REGULARISATION was used.\nOptimisation of the Model:\nI personally believe that one of the most important thing for a data scientist is exploring at the data  and gathering the best features from it. Hence I spent most of my initial time engineering the features and making sure to get the best out of them. Most of the important features was explored using various plots and the best were retained.\nThe model was later\noptimized for best performance using a single fold cross-validation. We just had one parameter to tune that is the coefficient of the regularisation term. A linear search across different values of the regularisation parameter was done and we obtained the best regularisation parameter value to be 10^-4. The metric used to evaluate this was PRAUC(same as average precision score). The precision-recall curve for different values of the hyperparameter was also plotted and then the best model was choosen by looking at the metrics and the curve and taking into consideration the bias-variance tradeoff. The threshold value of the model was chosen to be 0.4. This was done because with a standard threshold of 0.5 we were getting more false negatives than false positives. In a medical application we are always okay with a person being falsely clssified to have a disease because in that case he can always take a second opinion but he/she cannot be falsely diagnosed with having no disease since that could be really dangerous to the person. By changing threshold we had slightly more false positives but we reduced the false negatives by a lot.\nHow overfitting was prevented?\nOverfitting here was prevented using L2-regularisation. I searched linearly across various values of the regularisation parameter and the best value was obtained using cross-validation. The best value was found to be 10^-4.\n'], 'url_profile': 'https://github.com/prashusat', 'info_list': ['Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', '1', 'C#', 'Apache-2.0 license', 'Updated Apr 17, 2020', 'Python', 'Updated Aug 25, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Swift', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated May 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['R-Lab-Mini-Project\nR lab Mini Project EDA, Visualization & Prediction on Diabetes Dataset\nProject Description :\nThe system will allow us to predict if the patient has diabetes on the basis of certain diagnostic measures available in the dataset. The different steps involved in EDA include: 1.Data Collection, 2.Data Cleaning and 3.Data Visualization. This project first conducts Exploratory Data Analysis (EDA) and data visualization on the diabetes dataset and then predict the diabetes.\nExploratory Data Analysis (EDA)\n\nDescriptive statistics\n\nAttribute type, Class distribution, Mean, Standard Deviation, Median, Quartile, Skewness, Correlation\n\nData visualization\n\nHistogram plot\nDensity plot\nBox and Whisker plot\nBar plot\nMissing data map\nPair-wise correlation plot\nPrediction on Diabetes\nWe compare the performance for the following classifiers:\n\n\nLogistic Regression\n\n\nSupport Vector Machine (SVM)\n\n\nrandom Forest\n\n\nFor EDA and Visualization Steps performed:\n\nLoad libraries\nLoad the data\nClean the data\nDelete columns 15 and 16 due to many missing values and column 1 (id), column 7 (location)          because they contain no useful information\nRemove the row with missing values\nEncode the class label (column 5): Glycosolated hemoglobin  > 7.0 is taken as a positive            diagnosis of diabetes\nEncode the categorical data (column-7 gender)\nEncode the categorical data (column-10 frame)\nDescriptive statistics\nDisplay the first 20 rows\nDisplay the dimensions of the dataset\nList types for each attribute\nDistribution of the class labels\nSummarize the dataset\nStandard Deviations for the non-categorical columns\nPerrom other operations like: Skewness, Correlation, and Visualization Plots\n\nPredcition and Classification Steps performed:\n\nLoad libraries\nLoad the data\nClean the data\nDelete columns 15 and 16 due to many missing values and column 1 (id), column 7 (location)          because they contain no useful information\nRemove the row with missing values\nEncode the class label (column 5): Glycosolated hemoglobin  > 7.0 is taken as a positive            diagnosis of diabetes\nEncode the categorical data (column-7 gender)\nEncode the categorical data (column-10 frame)\nSplit the data into training and validation sets\nComparison among different classifiers\n\nLogistic Regression\nSupport Vector Machine\nrandom forest\nParameter tunning via grid search for random forest\n\n\nMake predictions on the validation set\nConfusion matrix\nSave the final classifier model into disk\nLoad the model from the disk\nMake predictions using the loaded model\nMake predictions using the loaded model\n\n'], 'url_profile': 'https://github.com/Tejalc-B-09', 'info_list': ['R', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Jun 6, 2020', 'R', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 17, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Predciting-future-spread-of-Diabetes-in-Nova-Scotia\nWe have designed a deep learning model for predicting the spread of diabetes across Nova Scotia in 2020 so that the required arrangements for tackling the consequences can be made beforehand. The predictions have been recorded on a heat map. This heat map would help the decision-makers to arrange the required number of resources and physicians. This project was developed in 48- hours as a part of Nova Scotia Open Data Challenge.\n'], 'url_profile': 'https://github.com/Haard30', 'info_list': ['R', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Jun 6, 2020', 'R', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Lab_1\nLab 1\n'], 'url_profile': 'https://github.com/Harsh-Panchal01', 'info_list': ['R', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Jun 6, 2020', 'R', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '276 contributions\n        in the last year', 'description': ['medremind-webapp\nFinal year project. Medication reminder progressive web app for type 2 diabetes patients.\nBuilt using nodejs, express, pug, mongodb, Twilio for SMS reminder and materialize for UI.\n'], 'url_profile': 'https://github.com/prabin1997', 'info_list': ['R', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Jun 6, 2020', 'R', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Project Description :\nThe system will allow us to predict if the patient has diabetes on the basis of certain diagnostic measures available in the dataset. The different steps involved in EDA include: 1.Data Collection, 2.Data Cleaning and 3.Data Visualization. This project first conducts Exploratory Data Analysis (EDA) and data visualization on the diabetes dataset and then predict the diabetes.\nExploratory Data Analysis (EDA)\n\nDescriptive statistics\n\nAttribute type, Class distribution, Mean, Standard Deviation, Median, Quartile, Skewness, Correlation\n\nData visualization\n\nHistogram plot\nDensity plot\nBox and Whisker plot\nBar plot\nMissing data map\nPair-wise correlation plot\nPrediction on Diabetes\nWe compare the performance for the following classifiers:\n\n\nLogistic Regression\n\n\nSupport Vector Machine (SVM)\n\n\nrandom Forest\n\n\nFor EDA and Visualization Steps performed:\n\nLoad libraries\nLoad the data\nClean the data\nDelete columns 15 and 16 due to many missing values and column 1 (id), column 7 (location)          because they contain no useful information\nRemove the row with missing values\nEncode the class label (column 5): Glycosolated hemoglobin  > 7.0 is taken as a positive            diagnosis of diabetes\nEncode the categorical data (column-7 gender)\nEncode the categorical data (column-10 frame)\nDescriptive statistics\nDisplay the first 20 rows\nDisplay the dimensions of the dataset\nList types for each attribute\nDistribution of the class labels\nSummarize the dataset\nStandard Deviations for the non-categorical columns\nPerrom other operations like: Skewness, Correlation, and Visualization Plots\n\nPredcition and Classification Steps performed:\n\nLoad libraries\nLoad the data\nClean the data\nDelete columns 15 and 16 due to many missing values and column 1 (id), column 7 (location)          because they contain no useful information\nRemove the row with missing values\nEncode the class label (column 5): Glycosolated hemoglobin  > 7.0 is taken as a positive            diagnosis of diabetes\nEncode the categorical data (column-7 gender)\nEncode the categorical data (column-10 frame)\nSplit the data into training and validation sets\nComparison among different classifiers\n\nLogistic Regression\nSupport Vector Machine\nrandom forest\nParameter tunning via grid search for random forest\n\n\nMake predictions on the validation set\nConfusion matrix\nSave the final classifier model into disk\nLoad the model from the disk\nMake predictions using the loaded model\nMake predictions using the loaded model\n\n'], 'url_profile': 'https://github.com/ashutoshb7', 'info_list': ['R', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Jun 6, 2020', 'R', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 17, 2020']}","{'location': 'Winnipeg, Manitoba', 'stats_list': [], 'contributions': '693 contributions\n        in the last year', 'description': [""Storke Prediction\nThe objective is to predict brain stroke from patient's records such as age, bmi score, heart problem, hypertension and smoking practice. The dataset includes 100k patient records. Among the records, 1.5% of them are related to stroke patients and the remaining 98.5% of them are related to non-stroke patients. Therefore, the data is extremely imbalanced.\n‚Ä¢ Dataproc and Google Cloud Platform is used to set up spark clusters.\n‚Ä¢ PySpark and MLlib is used to develop the model.\n‚Ä¢ Different data imputation techniques are applied to process the missing data.\n‚Ä¢ Edited Nearest Neighbours under-sampling technique is used on the majority class (non-stroke patient) and SMOTE over sampling technique is used on the minority class (stroke-patient).\n‚Ä¢ Bagging (i.e., Random Forest) and Boosting approach (i.e., Gradient Boosting Tree) are applied on the processed data. \n‚Ä¢ Without sampling AUC=0.5. After applying sampling techniques, the best performance is achieved using the bagging approach with AUC = 0.8.\nData\nThe dataset is collected from the following link:\nhttps://bigml.com/dashboard/dataset/5e92c6d14f6bfd2dd00044a9\nBefore Sampling\n\nAfter Sampling\n\nHow to run:\nPlease check the Stroke Prediction.ipynb for details.\n""], 'url_profile': 'https://github.com/ShahedSabab', 'info_list': ['R', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Jun 6, 2020', 'R', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 17, 2020']}","{'location': 'Hayward, California ', 'stats_list': [['0', '          followers'], ['1', '          following'], ['10']], 'contributions': '88 contributions\n        in the last year', 'description': ['Assignment 1\nhttps://www.kaggle.com/uciml/pima-indians-diabetes-database\n1.Import relevant commands for numpy, pandas, sklearn.\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, read_csv, to_numeric\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nimport math\nimport scipy.stats as stats\n2.Using the appropriate pandas function, read the diabetes.csv into a dataframe. Pay good attention to the necessary arguments.\ndata = read_csv(\'diabetes.csv\', sep="","")\na = pd.DataFrame(data)\nprint(a)<br/><br/>\n     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0              6      148             72             35        0  33.6\n1              1       85             66             29        0  26.6\n2              8      183             64              0        0  23.3\n3              1       89             66             23       94  28.1\n4              0      137             40             35      168  43.1\n..           ...      ...            ...            ...      ...   ...\n763           10      101             76             48      180  32.9\n764            2      122             70             27        0  36.8\n765            5      121             72             23      112  26.2\n766            1      126             60              0        0  30.1\n767            1       93             70             31        0  30.4\n\n     DiabetesPedigreeFunction  Age  Outcome\n0                       0.627   50        1\n1                       0.351   31        0\n2                       0.672   32        1\n3                       0.167   21        0\n4                       2.288   33        1\n..                        ...  ...      ...\n763                     0.171   63        0\n764                     0.340   27        0\n765                     0.245   30        0\n766                     0.349   47        1\n767                     0.315   23        0\n\n[768 rows x 9 columns]\n\n\n3.use naivebayes, logistic regression and 3-nn classifiers (library) to train on the training sets and compute training and validation errors for each fold. The target label is Outcome.\nX=a.iloc[:,0:8].values\ny=a.iloc[:,-1].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\nkf = KFold(n_splits=10)\nkf.get_n_splits()\n#print(kf)\nfor train_index, test_index in kf.split(X):\n    print(""TRAIN:"", train_index, ""TEST:"", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\nnb_model = GaussianNB()\nlr_model = LogisticRegression(solver= \'liblinear\')\nknn_model = KNeighborsClassifier(n_neighbors=3)\n\nnb_error = []\nlr_error = []\nknn_error =[]\n\nfor train, validation in kf.split(X_train):\n    nb_fit = nb_model.fit(X_train[train], y_train[train])\n    nb_pred = nb_model.predict(X_train[validation])\n    #print(nb_pred)\n    error = np.sum(nb_pred!= y_train[validation])/len(nb_pred)\n    nb_error.append(error)\n\n    lr_fit = lr_model.fit(X_train[train], y_train[train])\n    lr_pred = lr_model.predict(X_train[validation])\n    #print(lr_pred)\n    error = np.sum(lr_pred!=y_train[validation])/len(lr_pred)\n    lr_error.append(error)\n\n    knn_fit = knn_model.fit(X_train[train], y_train[train])\n    knn_pred = knn_model.predict(X_train[validation])\n    #print(knn_pred)\n    error = np.sum(lr_pred!=y_train[validation])/len(knn_pred)\n    knn_error.append(error)\n\nError_list=np.transpose([nb_error,lr_error,knn_error])\nz = pd.DataFrame(data=Error_list, columns =(""Naivebayes"",""logistic regression"",""KNN""))\nprint(z)\nTRAIN: [ 77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94\n  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328\n 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346\n 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364\n 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382\n 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400\n 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418\n 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436\n 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454\n 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472\n 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490\n 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508\n 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526\n 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544\n 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562\n 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580\n 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598\n 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616\n 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634\n 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652\n 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670\n 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688\n 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706\n 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724\n 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742\n 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760\n 761 762 763 764 765 766 767] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n 72 73 74 75 76]\n\n   Naivebayes  logistic regression       KNN\n0    0.328571             0.314286  0.314286\n1    0.157143             0.171429  0.171429\n2    0.231884             0.188406  0.188406\n3    0.333333             0.304348  0.304348\n4    0.304348             0.333333  0.333333\n5    0.217391             0.202899  0.202899\n6    0.173913             0.159420  0.159420\n7    0.202899             0.188406  0.188406\n8    0.202899             0.159420  0.159420\n9    0.246377             0.260870  0.260870\n\n4.is the error of naive bayes <0.2 with confidence 0.9\nx=np.std(nb_error)\ny=np.mean(nb_error)\nz= math.sqrt(10)*(y-0.2)/x\nprint(z)\na=stats.t.ppf(0.9,9)\nif z > a :\n    print(""No"")\nelse :\n    print(""Yes"")\n2.122373130045732\nNo\n\n5.have naive bayes and knn the same error?\nx=np.absolute(np.subtract(np.array(knn_error),np.array(nb_error)))\nprint(x)\ny=np.std(x)\nz=np.mean(x)\na= math.sqrt(10)*z/y\nprint(a)\nb=stats.t.ppf(0.9,9)\nif a > b :\n    print(""No"")\nelse :\n    print(""Yes"")\n[0.01428571 0.01428571 0.04347826 0.02898551 0.02898551 0.01449275\n0.01449275 0.01449275 0.04347826 0.01449275]\n6.296258830957976\nNo\n\n6.do the three classifiers have different errors?\ne_table = np.array(np.transpose([nb_error,lr_error,knn_error]))\nNo_of_models = 3\nFolds= 10\naverage_of_models =np.mean(e_table, axis=0)\nbet_same_model = Folds*np.var(average_of_models)\nbet_all_model = np.sum(np.var(e_table,axis=0)/No_of_models)\nx=stats.f.ppf(0.9,dfn=No_of_models-1 ,dfd= No_of_models*(Folds-1))\nratio= bet_same_model/bet_all_model\nprint(""F-statistic"",x)\nprint(""Estimater ratio"",ratio)\nif x < ratio:\n    print(""All models have same error"")\nelse:\n    print(""All models have differnt error"")\nF-statistic 2.5106086665585408\nEstimater ratio 0.0753450085421447\nAll models have differnt error\n\n7.Use Bayes rule to decide on the label using the Glucose feature. Compute the mean and std of Glucose feature on the whole dataset (marginal distribution) and for each class separately, (class condition distribution Prob(x|C=0), Prob(x|C=1))). Assume that the feature is distributed according to the mean and std you computed. Compute the predictions for the 10 validation sets. Compare with the naivebayes classifier (library) using only the Glucose feature\nfrom sklearn.naive_bayes import GaussianNB\ndata_g = data.Glucose.values\ndata_o = data.Outcome.values\n\nerr_model = []\nerr_lib = []\n\ndef Gaussian(x,mean,var):\n    numerator = np.exp(-((x-mean )**2)/(2*var))\n    denominator = np.sqrt(2*np.pi*var)\n    return numerator/denominator\n\nfor train, validation in kf.split(data_o):\n    x_train = data_g[train]\n    y_train = data_o[train]\n\n    var_0 = np.var(x_train[y_train==0])\n    var_1 = np.var(x_train[y_train==1])\n\n    p_prob_0 = len(x_train[y_train==0]/len(x_train))\n    p_prob_1 = len(x_train[y_train==1]/len(x_train))\n\n    mean_0 = np.mean(x_train[y_train==0])\n    mean_1 = np.mean(x_train[y_train==1])\n\n    prediction= []\n\n    for x in data_g[validation]:\n        gaussian_0= Gaussian(x,mean_0,var_0)\n        gaussian_1= Gaussian(x,mean_1,var_1)\n\n        class_0 = p_prob_0*gaussian_0\n        class_1 = p_prob_1*gaussian_1\n\n        if class_0 > class_1:\n            prediction.append(0)\n        else:\n            prediction.append(1)\n\n    err_model.append(np.sum(np.array(prediction)!=data_o[validation])/len(prediction))\n\n    nb_classifier = GaussianNB()\n    nb_classifier.fit(data_g.reshape(-1,1), data_o.reshape(-1,1))\n    nb_pred = nb_classifier.predict(data_g[validation].reshape(-1,1))\n    error = np.sum(nb_pred!=data_o[validation])/len(data_g[validation])\n    err_lib.append(error)\n\nError_list = np.transpose([err_model, err_lib])\nz= pd.DataFrame(data=Error_list , columns =(""My model Error"",""Library Model error""))\nprint(""\\n"",z)\nprint(""\\n Both model have same error\\n"")\n    My model Error  Library Model error\n0        0.337662             0.337662\n1        0.246753             0.246753\n2        0.298701             0.298701\n3        0.324675             0.324675\n4        0.246753             0.246753\n5        0.220779             0.194805\n6        0.181818             0.181818\n7        0.207792             0.207792\n8        0.210526             0.210526\n9        0.250000             0.250000\n\n Both model have same error\n\n'], 'url_profile': 'https://github.com/kunjp19', 'info_list': ['R', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Jun 6, 2020', 'R', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [['2', '          followers'], ['0', '          following'], ['0']], 'contributions': '127 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pooja2893', 'info_list': ['R', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Jun 6, 2020', 'R', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 17, 2020']}",,
