"{'location': 'Taiwan', 'stats_list': [['2', '          followers'], ['5', '          following'], ['1']], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChamaniS', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Shanghai', 'stats_list': [['10', '          followers'], ['30', '          following'], ['32']], 'contributions': '219 contributions\n        in the last year', 'description': ['DiabetesDetection\n‰∫∫Â∑•Êô∫ËÉΩËØæÁ®ãÈ°πÁõÆAndroidÁ´Ø‰ª£Á†Å\nÁõÆÁöÑÊòØÂÆûÁé∞Á≥ñÂ∞øÁóÖÈ£éÈô©ËØÑ‰º∞ÁöÑ‰∫§‰∫íÁïåÈù¢\n'], 'url_profile': 'https://github.com/syz913', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Kingston', 'stats_list': [['6', '          followers'], ['6', '          following'], ['5']], 'contributions': '30 contributions\n        in the last year', 'description': ['diabettis\nThe aim of this project is to explore the Pima Indian dataset of women with and without diabetes.\n\nDescriptive Analytics: examining the composition of my dataset.\nPredictive Analytics: unsupervised and supervised techniques.\n\nI first reduced the features using an unsupervised machine learning algorithm called Principal Component Analysis, PCA. Then, becasue I had far more cases in the negative class (no diabetes) than the positive class (diabetes), I used an oversampling technique on my training dataset in order to balance out the classes. I used 4 supervised algorithms using the PCA-reduced and balanced datset: k-Nearest Neighbors, Support Vector Machine (SVM), Decision Tree Classification, and Gaussian Naive Bayes.\nThen, I used another unsupervised technique called k-means clustering to plot 2 features.\n'], 'url_profile': 'https://github.com/emmusic', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [['4', '          followers'], ['4', '          following'], ['2']], 'contributions': '5 contributions\n        in the last year', 'description': ['kNN-Diabetes\nModel for classifying test objects in two classes - has diabetes and no diabetes\nThere are two versions of the model:\n\nOne using euclidian (Knn-euc.py)\nOne using cosine similarity (Knn-cos.py) (\nThis report used the cosine proximity measure)\n\nUnderstanding the model and the dataset\nSumary\n\nIntroduction\nDevelopment\nConclusion\n\n1- Introduction\nIn this work we used the database ‚ÄúPima Indians Diabetes Database‚Äù to\napply the concepts learned in the classroom, such as data processing techniques,\nproximity and classification algorithms. Its purpose is to train an algorithm with\ntraining objects so that it can classify new test objects into two classes\ndistinct: people with diabetes and people without diabetes.\nThe database used has 768 objects, representing women with at least\n21 years of age, 500 of them are objects of class 0 (do not have diabetes) and 268 are of the\nclass 1 (have diabetes), in addition the attributes of this database are:\n1. Number of times you became pregnant (Standard deviation: 3.4)\n2. 2-hour plasma glucose concentration in an oral glucose tolerance test (Standard deviation: 32.0)\n3. Diastolic blood pressure (Standard deviation: 19.4)\n4. Triceps skinfold thickness (Standard deviation: 16.0)\n5. 2 hour serum insulin (Standard deviation: 115.2)\n6. Body mass index (Standard deviation: 7.9)\n7. Diabetes pedigree function (Standard deviation: 0.3)\n8. Age in years (Standard deviation: 11.8)\n\nTo perform this task of classifying objects, the following techniques were used:\n\nData processing method: Min-max\nProximity Measure: Cosine Similarity\nClassification algorithm: k Nearest Neighbors (kNN)\n\nBy analyzing the standard deviation of each attribute we can see that there is a huge\ndiscrepancy between their values. This fact is one of the reasons for using the method\nmin-max for data processing because it transforms all database values\nin a new value between 0 and 1. Another reason for using this type of method is\nuse a scale where the proximity value indicates the similarity / dissimilarity fraction\nbetween two objects.\nAfter all values \u200b\u200bare on a scale between zero and one, the next step is to apply\nthe k-NN algorithm, that is, for each new test object, we must calculate the measure of\nproximity between it and all the training objects and then sort by increasing or\ndecreasing (according to the measure used) and analyze the class of the first k objects. The\ntest object class will be defined from the most predominant class among these first k\nobjects.\nThe way we are going to order the proximity measurements depends on the same as it was\nchosen. Basically we sort of ascending as we choose\nexplains how two objects are different, that is, the smaller the value obtained, the more\nobjects are similar (they are close). Analogously we order the measures so\ndecreasing when the measure we choose spells out how much two objects are equal, ie\nthe higher the value obtained, the more similar the objects are. These two methods portray the\nsimilarity and dissimilarity, respectively.\n2 - Development\nTwo tests were performed with the program, one with the value of k equal to 3 and the other with k\nequal to 7. In the first test (k == 3) the algorithm correctly classified 124 objects, obtaining\n64.58% accuracy and its confusion matrix was organized as follows:\n\nTrue Positives: 30\nTrue Negatives: 94\nFalse Positives: 27\nFalse Negatives: 41\nSensitivity: 42.25%\nSpecificity: 77.69%\n\nIn the second test (k == 7) the algorithm correctly classified 119 objects, obtaining\n61.98% accuracy and its confusion matrix was organized as follows:\n\nTrue Positives: 23\nTrue Negatives: 96\nFalse Positives: 25\nFalse Negatives: 48\nSensitivity: 32.39%\nSpecificity: 79.34%\n\nBased on the results of the two tests we can say that the first one got better\nresults, because its accuracy was higher and mainly its Sensitivity was also higher.\nAmong all performance and efficiency measures, we are primarily interested in\nSensitivity as possible, because for this particular database, our main\nThe objective is to identify who are the people who have diabetes, ie the ""positive"" class.\nKnowing this, High Sensitivity is the most desired performance measure because your result\ndemonstrates that 42.25% of all people who actually had\ndiabetes.\nThe best choice of parameter ""k"" depends on the data we are using, on average\nthe higher the k value, the greater the reduction in the negative effect that noise causes on\nclassification but on the other hand it can relate the test object to other training objects that\nthey are relatively distant in terms of the proximity measure chosen.\nLet\'s look at a third test, this time with k equal to 81:\nThe algorithm correctly classified 127 objects with an accuracy of 66.15%, obtaining the\nfollowing matrix of confusion:\n\nTrue Positives: 20\nTrue Negatives: 107\nFalse Positives: 14\nFalse Negatives: 51\nSensitivity: 28.17%\nSpecificity: 88.43%\n\nAnalyzing the results of the third test with the other two we can see that the\naccuracy for k equal to 81 is higher than for values \u200b\u200b3 and 7. But despite obtaining the highest\naccuracy, this test obtained the lowest Sensitivity, that is, this algorithm classified\nerroneously 71.83% of all people who had diabetes. In terms of utility\n(correctly identify people with diabetes), the k test of 81 obtained the\nworse performance compared to others.\nAnother factor that directly impacts the test result is the proximity measurement.\nused, in this case we are using a measure that is not recommended to solve the type\nproblem that our chosen database addresses. The similarity of the cosine is very\nused to analyze documents and make them look alike even though\nVery different size.\nIn general, the more training objects we have and we can eliminate the more\nThe more noise, the better the k-NN algorithm results. However, in the case of this\ndata we are analyzing, choose another measure of proximity (such as the\nEuclidean distance or that of Manhattan) would be an option that will most likely influence\nin a more efficient classification algorithm, that is, that classifies the test objects of\nmost accurate way. cosine similarity is not the most appropriate measure for\nsolve the kind of problem we are dealing with.\n3 - Conclusion\nAll the results we got from the tests done in topic 2, in terms of\naccuracy, sensitivity and specificity are totally dependent on the proximity measure\nthat we use. Cosine similarity is a measure of proximity that makes explicit the\nsimilarity between two objects. More specifically, cosine similarity measures\ncosine of the angle between two vectors projected in a multidimensional space. The main\nfunctionality of this measure is to determine how two documents look alike regardless\nof their sizes.\nThis measure is widely used, and is also quite efficient for spam detection. THE\nfrom parsing words that typically characterize an email class that is not\nrelevant to a user (the concept of spam is different for each type of user, taking into\ninterests), the algorithm that uses cosine similarity can\nclassify new emails very accurately by classifying them as spam and not spam.\nHowever, by applying such proximity measure in the classification of diabetic and non-diabetic people\ncosine similarity did not perform well because its accuracy and\nmainly its sensitivity in this application are not at a desirable value for a\npossible application that would be used in real life.\nThe explanation for this measure not performing well in our database is\nbecause it deals with data that have different characteristics from the other bases that the\ncosine similarity is indicated. The relationship between the attributes is different as well as the\nclassification (quantitative and qualitative), variance, and various other factors. This question of\ndata type incompatibility with the proximity measure used could be\nresolved if specific preprocessing was performed to change properties of the\ndata in our database in order to improve the performance of cosine similarity.\nSo the main improvement our algorithm could have to better classify the\ntest objects would be to alter the proximity measure used, since the similarity of the cosine\nIt is not intended to solve the type of problem we are using. In addition our base\nof data has a specific characteristic that makes the classification itself difficult:\nattributes have very discrepant values, while one has a standard deviation of 0.3 (att\nnumber 7), another has a standard deviation of 115.2 (att number 5).\n'], 'url_profile': 'https://github.com/Cardoso-CHM', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['diabetes-tech\n'], 'url_profile': 'https://github.com/phillipsr17736', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JoshuaScharfe', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['diabetes_readmission\ndiabetes readmission prediction (kaggle contest from PML course 5th edition by Vladimir)\n'], 'url_profile': 'https://github.com/andrzej-malina', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [['0', '          followers'], ['10', '          following'], ['1']], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bsoni08', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['diabetes-prediction\nTo run diabetes-prediction please follow the below steps (commands) -\n$ git clone https://github.com/jitendra-github-lab/diabetes-prediction.git\n$ cd diabetes-prediction/kubedemo/app\n$ docker build -f Dockerfile -t diabetes-score:latest .\n$ kubectl apply -f deployment.yaml\n$ kubectl get all\nNote: From service take the newly generated ip:port and fire on browser\n'], 'url_profile': 'https://github.com/jitendra-github-lab', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['diabetes_prediction\nBinary Classification using Python for diabetes prediction\n'], 'url_profile': 'https://github.com/mahadik313', 'info_list': ['1', 'Python', 'Updated Jan 5, 2020', 'Java', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Python', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}"
"{'location': 'Pittsburgh, PA', 'stats_list': [['16', '          followers'], ['3', '          following'], ['8']], 'contributions': '166 contributions\n        in the last year', 'description': ['diabetes-traveller\nA handy dandy international translation guide for people with diabetes.\n'], 'url_profile': 'https://github.com/BergFulton', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Diabetes_Prediction\n'], 'url_profile': 'https://github.com/guptaabhay92', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Plot No. 7 Sector 24, Near Purkhoti Muktangan, Atal Nagar, Chhattisgarh 493661', 'stats_list': [['0', '          followers'], ['1', '          following'], ['4']], 'contributions': '9 contributions\n        in the last year', 'description': ['Diabetes-detection-using-Keras\nDiabetes is a serious health issue which causes an increase in blood sugar. Many complications occur if diabetes remains untreated and unidentified.\nThe aim of this guide is to build a classification model to detect diabetes. We will be using the diabetes dataset which contains 768 observations and 9 variables, as described below:\n\npregnancies - Number of times pregnant\nglucose - Plasma glucose concentration\ndiastolic - diastolic blood pressure (mm Hg)\ntriceps - Skinfold thickness (mm)\ninsulin - Hour serum insulin (mu U/ml)\nbmi ‚Äì Basal metabolic rate (weight in kg/height in m)\ndpf - Diabetes pedigree function\nage - Age in years\ndiabetes - 1 represents the presence of diabetes while 0 represents the absence of it. This is the target variable.\n\nAlso, the classification algorithm selected is the Logistic Regression Model, which is one of the oldest and most widely used algorithms.\n'], 'url_profile': 'https://github.com/Amardeep001', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [['26', '          followers'], ['19', '          following'], ['4']], 'contributions': '52 contributions\n        in the last year', 'description': ['Diabetes-On-Set-Detection\nA machine learning model which uses artificial neural networks to predict the on set of diabetes in a patient. Based on certain parameters. The model uses grid search in hyperparameter tuning to get an efficient model. The model has acquired 85% accuracy on the test set.\n'], 'url_profile': 'https://github.com/vedantrokde', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [['0', '          followers'], ['1', '          following'], ['7']], 'contributions': '0 contributions\n        in the last year', 'description': ['machinelearning-course-diabetes\nRepo to contain all the code for the diabetes exercise.\nCourse can be found at:\nhttps://app.pluralsight.com/player?course=python-understanding-machine-learning&author=jerry-kurata&name=python-understanding-machine-learning-m3&clip=0&mode=live\n'], 'url_profile': 'https://github.com/movags95', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Guwahati', 'stats_list': [['3', '          followers'], ['4', '          following'], ['5']], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sajit9285', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['diabetes_patient_readmission\nObjective\nIdentify factors that make a patient more likely to be readmitted to a hospital for diabetes related illness and study\nthe characteristics of patients via their medical records to predict which patients are likely to be readmitted within a\ncertain window of time.\n'], 'url_profile': 'https://github.com/anniebui', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Nigeria', 'stats_list': [['0', '          followers'], ['1', '          following'], ['7']], 'contributions': '3 contributions\n        in the last year', 'description': ['Machine-Learning-on-Diabetes-Dataset\n'], 'url_profile': 'https://github.com/Kemade', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Diabetes Neural Network\nBy:Mokshda Sharma\nData used to train this model: https://datahub.io/machine-learning/diabetes\nArchitecture:\nInput Neurons:8\n(Input parameters:NumberOfPregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age)\nOutput Neurons:1(Diabetic/Non-diabetic)\nHidden Layers:1\nActivation Function:Sigmoid\nThis Neural Network is feedforward and fully connected. Every node in a given layer is fully connected to every node in next layer.\nFlow of Program\nFiles:\nThere are two files,one containing the neural network and other containing the dataset to train the network. The file Diabetes_diagnosis_nn.py contains the neural network.\nIntroduction:\nThe Neural Network is programmed in Python mainly using the NumPy library. NumPy(Numerical Python) consists of  Mutidimensional array objects and a collection of routines for processing those arrays.\ndiabetes_diagnosis_nn.py code breakdown:\nThe following lines import all libraries and dependencies used:\nfrom numpy import exp, array, random, dot\nimport numpy as np\nThen data from csv file is converted into arrays:\nimport csv\n\npreg = [] #number of pregnancies\nglu = []  #Glucose\nBlood = [] #blood pressure\nSkin_Thickness = [] #skin thickness\nInsulin = []\nBMI = []\nDPF = [] #diabetes pedigree function\nAge = []\n\n\n\nwith open(r\'C:\\Users\\hp\\Downloads\\diabetes.csv\') as csvDataFile:\n    csvReader = csv.reader(csvDataFile)\n    for row in csvReader:\n        preg.append(int(row[0]))\n        glu.append(int(row[1]))\n        Blood.append(int(row[2]))\n        Skin_Thickness.append(int(row[3]))\n        Insulin.append(int(row[4]))\n        BMI.append(float(row[5]))\n        DPF.append(float(row[6]))\n        Age.append(int(row[7]))\nCreate normalization and denormalization functions for input parameters:\ndef nor_denor_preg(v):\n   v=int(v)\n   if(v>1):\n       normal = (v-min(preg))/(max(preg)-min(preg))\n       return normal\n   else:\n       denormal = v*(max(preg)-min(preg)) + min(preg)\n       return denormal\nUsig for loop,traverse over the length of the array.\n for i in range (0, len(preg)):\n  a = nor_denor_preg(preg[i])\n  preg[i] = a\nThe normalization and denormalization functions for rest of the parameters are created in a similar manner\nThen initialize input and output arrays with random data and append the data from csv file as elements of the arrays inside NumPy array.\nwith open(r\'C:\\Users\\hp\\Downloads\\diabetes.csv\') as csvfile:\n    reader = csv.reader(csvfile, quoting=csv.QUOTE_NONNUMERIC) # change contents to floats\n    for row in reader: # each row is a list\n        results.append(row)\nnpa = np.asarray(results, dtype=np.float32)    \nprint(npa)         \ncount = 0  \ntemp_arr=[0.5]\nwith open(r\'C:\\Users\\hp\\Downloads\\outputNew.csv.xlsx\') as csvDataFile:\n    csvReader = csv.reader(csvDataFile)\n    for row in csvReader:\n        temp = int(row[0])\n        temp_arr[0] = temp\n        out_arr[count] = temp_arr\n        count = count + 1\nDefine function to assign Weights to the layers inside neural network. This fuction randomly assigns weights.\nclass NeuronLayer():\n    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):\n        self.synaptic_weights = 2 * random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\nDefine the layers of neural network i.e. hidden layer(Layer 1) and output layer(Layer 2)\nclass NeuralNetwork():\n    def __init__(self, layer1, layer2):\n        self.layer1 = layer1\n        self.layer2 = layer2\nSigmoid Function is used as activation function to train the network. It is a S shaped curve whose value lies between 0 and 1. Pass the weighted sum of the inputs through this function to normalize them between 0 ad 1.\ndef __sigmoid(self, x):\n       return 1 / (1 + exp(-x))\nDefine the derivative of sigmoid function. This function is differentiable everywhere on the curve.\ndef __sigmoid_derivative(self, x):\n       return x*(1 - x)\nTrain the neural network by backpropagation in which the weights are repeatedly adjusted to minimize the difference between actual output and desired output.\ndef train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n       for iteration in range(number_of_training_iterations):\nPass the training set through neural network:\noutput_from_layer_1, output_from_layer_2 = self.think(training_set_inputs)\nCalculate the error for output layer by taking the difference between actual and desired output and define the derivative of the sigmoid function\nlayer2_error = training_set_outputs - output_from_layer_2\nlayer2_delta = layer2_error * self.__sigmoid_derivative(output_from_layer_2)\nCalculate the error for layer 1. By analyzing the value of weights in layer 1,the contribution of layer 1 to the error i layer 2 is found\n layer1_error = layer2_delta.dot(self.layer2.synaptic_weights.T)\n layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)\nTo calculate the adjustment required in the values of weights:\nlayer1_adjustment = training_set_inputs.T.dot(layer1_delta)\nlayer2_adjustment = output_from_layer_1.T.dot(layer2_delta)\nAdujst the weights:\nself.layer1.synaptic_weights += layer1_adjustment\nself.layer2.synaptic_weights += layer2_adjustment\nDefine a function to take the dot product of synaptic weights with the inputs of respective layers:\ndef think(self, inputs):\n     output_from_layer1 = self.__sigmoid(dot(inputs, self.layer1.synaptic_weights))\n     output_from_layer2 = self.__sigmoid(dot(output_from_layer1, self.layer2.synaptic_weights))\nPrint the weights:\ndef print_weights(self):\n       print( ""    Layer 1(Hidden) : "")\n       print(self.layer1.synaptic_weights)\n       print( ""    Layer 2 :"")\n       print(self.layer2.synaptic_weights)\nCreate Layer 1 having 8 inputs and 6 neurons and Layer 2 having a single neuron with 6 inputs:\nlayer1 = NeuronLayer(6, 8)\nlayer2 = NeuronLayer(1,6)\nCombine the layers to create a neural network:\n neural_network = NeuralNetwork(layer1, layer2)\nTrain the neural network using the training set, doing it 6,00,000 times and making small adjustments each time:\n neural_network.train(training_set_inputs, training_set_outputs, 60000)\nPrint new synaptic weights after training:\nneural_network.print_weights()\nTest the neural network with a new situation:\n print( ""Stage 3) Considering a new situation --- "")\n   \n   test = []\n   with open(r\'C:\\Users\\hp\\Downloads\\diabetes.csv\') as csvfile:\n       reader = csv.reader(csvfile, quoting=csv.QUOTE_NONNUMERIC) # change contents to floats\n       for row in reader: # each row is a list\n           test.append(row)\n   input_test = np.asarray(test, dtype=np.float32)\n   out_plot_arr=np.array([[.5],[.5],[.2],[.5],[.5],[.2],[.5],[.5],[.2],[.5],[.9]])\nPrint the output\n hidden_state, output = neural_network.think(array(input_test))\n print(output)\nThis is the neural network to predict Diabetes. To increase the accuracy of the network we can either increase the number of hidden layers or the input neurons in each layer.  Increasing the size of training data also improves the accuracy of the model.\n~Mokshda Sharma\n'], 'url_profile': 'https://github.com/Mokshda13', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [['0', '          followers'], ['1', '          following'], ['1']], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nitin125', 'info_list': ['HTML', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Dec 30, 2019', 'Updated Jan 4, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KalhapureP', 'info_list': ['Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Feb 7, 2021', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Swift', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020']}","{'location': 'Chennai, India', 'stats_list': [['3', '          followers'], ['5', '          following'], ['45']], 'contributions': '177 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/farookjintha', 'info_list': ['Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Feb 7, 2021', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Swift', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020']}","{'location': 'Oxford, UK', 'stats_list': [['4', '          followers'], ['3', '          following'], ['22']], 'contributions': '281 contributions\n        in the last year', 'description': ['QDiabetes \n\n\n\n\nGeneral info\nThis project is an R package for calculating the risk of developing type 2 diabetes. The package uses R implementations of the QDiabetes algorithms, which were initially derived by ClinRisk using the C++ programming language. The QDiabetes package comprises QDiabetes-2013 and QDiabetes-2018, although older (and eventually more recent) versions of QDiabetes may be included in future releases.\nDisclaimer\nClinRisk do not support of endorse this code. End users should see the original C++ source as the ‚Äògold standard‚Äô open source implementation. Please note that the QDiabetes R package has been created as a research tool for scientific purposes only. The QDiabetes R package has not been granted Medicines and Healthcare products Regulatory Agency (MHRA) approval as a medical device, and hence, should not be used as part of any individualised risk assessment.\nHistory\nThe first QDiabetes algorithm (termed ‚ÄúQDScore‚Äù at the time) was published in 20091. In 2011, two substantial changes were made, whereby the age range permissible by the algorithm was expanded from 25‚Äì79 to 25‚Äì84, and the smoking status variable was expanded from two levels [current smoker and non-smoker] to five levels [non-smoker, ex-smoker, light smoker (1‚Äì9/day), moderate smoker (10‚Äì19/day) and heavy smoker (‚â•20/day)]. In 2012, QDiabetes-2013 was released, and from 2013 to 2016 the algorithm coefficients were periodically updated, until 2017, when the more expansive QDiabetes-2018 algorithm was published2. At some point along the way, QDiabetes switched from sourcing Townsend deprivation data from the 2001 to the 2011 UK Census, however, the point at which this occurred is not clear. For now, all the can be said for certain is that the original 2009 QDScore algorithm used Townsend data from the 2001 UK Census, while QDiabetes-2018 uses Townsend data from the 2011 UK Census. More details about the variables used in QDiabetes-2013 and QDiabetes-2018 can be found in the following sections.\nQDiabetes-2013\nThe QDiabetes-2013 algorithm consists of two separate risk prediction models (one per gender), in which the following 11 variables are used to calculate risk:\n\nGender [sex]\n\nFemale ‚Äî ""Female""\nMale ‚Äî ""Male""\n\n\nAge [age], in years\nBody mass index [bmi], in kg/m2\nEthnicity [ethn], nine categories:\n\nWhite or not stated ‚Äî ""WhiteNA""\nIndian ‚Äî ""Indian""\nPakistani ‚Äî ""Pakistani""\nBangladeshi ‚Äî ""Bangladeshi""\nBlack Caribbean ‚Äî ""BlackCaribbean""\nBlack African ‚Äî ""BlackAfrican""\nChinese ‚Äî ""Chinese""\nOther Asian ‚Äî ""OtherAsian""\nOther ethnic group ‚Äî ""Other""\n\n\nSmoking status [smoke], five levels:\n\nNon-smoker ‚Äî ""Non""\nEx-smoker ‚Äî ""Ex""\nLight smoker (1‚Äì9/day) ‚Äî ""Light""\nModerate smoker (10‚Äì19/day) ‚Äî ""Moderate""\nHeavy smoker (‚â•20/day) ‚Äî ""Heavy""\n\n\nDeprivation [tds], as measured by Townsend scores, where higher values indicate higher levels of deprivation\nFamily history of diabetes in first degree relative [fhdm]\nHistory of treated hypertension [htn], being diagnosis of hypertension and treatment with at least one hypertensive drug\nHistory of cardiovascular disease [cvd], defined as: ischaemic heart disease, stroke, or transient ischaemic attack\nHistory of use of corticosteroids [ster] listed in British National Formulary chapter 6.3.2, including oral or injections of systemic: prednisolone, betamethasone, cortisone, depo-medrone, dexamethasone, deflazacort, efcortesol, hydrocortisone, methylprednisolone, or triamcinolone\nSurvival time [surv], being the time period over which risk of developing type-2 diabetes is to be calculated.\n\nThe QDiabetes-2013 algorithm is implemented within the QDR2013() function of the QDiabetes package.\nQDiabetes-2018\nThe QDiabetes-2018 algorithm is actually six separate risk predictions models (three sub-models, subdivided by gender).\nModel A\nThe basic (core) model, ‚Äòmodel A‚Äô, uses the same risk predictors as QDiabetes-2013, with the omission of the Survival time variable (in favour of a fixed 10-year survival window), and the addition of the following 6 variables:\n\nHistory of gestational diabetes [gdm] (women only)\nHistory of polycystic ovary syndrome [pcos] (women only)\nHistory of learning disabilities [learn]\nHistory of schizophrenia or bipolar affective disorder [psy]\nHistory of use of statins [stat]\nHistory of use of second generation ""atypical"" antipsychotics [apsy], including: amisulpride, aripiprazole, clozapine, lurasidone, olanzapine, paliperidone, quetiapine, risperidone, sertindole, and zotepine\n\nModel A of the QDiabetes-2018 algorithm is implemented within the QDR2018A() function of the QDiabetes package.\nModel B\n‚ÄòModel B‚Äô, uses the same variables as model A, with the addition of:\n\nFasting plasma glucose level [fpg], in mmol/L\n\nModel B of the QDiabetes-2018 algorithm is implemented within the QDR2018B() function of the QDiabetes package.\nModel C\n‚ÄòModel C‚Äô, uses the same variables as model A, with the addition of:\n\nGlycated haemoglobin A1c value [hba1c], in mmol/mol\n\nModel C of the QDiabetes-2018 algorithm is implemented within the QDR2018C() function of the QDiabetes package.\nInstallation\nYou can install the released version of QDiabetes from CRAN with:\ninstall.packages(""QDiabetes"")\nAlternatively, the development version may be installed from GitHub with:\nif (!{""remotes"" %in% installed.packages()}) install.packages(""remotes"")\nremotes::install_github(""Feakster/qdiabetes"")\nPackage ethos\nIn building this package, we wanted to make something that was simple to write and easy to maintain (KISS principles), performant, but compatible with the latest and older versions of R. With this in mind, we have written this package to be as faithful to R‚Äôs core language as possible, using minimal dependencies. Hence, you will not find any Rcpp here. Instead, all functions have been written entirely in base R; the only exception being the getTDS() function, which uses the median() function from the stats package (although we may re-write this at some point). All other packages listed under ‚ÄúSuggests‚Äù in the DESCRIPTION file only serve to illustrate the use of QDiabetes in examples or vignettes, or in testing the package. The primary factor limiting the package‚Äôs compatibility with older versions of R is the data storage method CRAN require us to use for the data frame backend of the getTDS() function. Owing to the memory footprint of this object (‚âà200MB), we need to make use of XZ compression to reduce the overall size of the package as much as possible. XZ compression was first implemented in R version 2.10.\nNote\nMany of the default values used in the risk prediction functions of this package were selected to be representative of a UK population. These values are only intended to minimise the amount of typing required when using the risk prediction functions in an exploratory manner. They are unlikely to be useful in a research setting, and you would need to know the exact values to assign to all function parameters in order to make an accurate risk prediction. Hence, while you can get risk predictions from the QDR2013() and QDR2018A() functions through the specification of only sex, age, and bmi, you would be assuming White or missing ethnicity, non-smoking status, a Townsend deprivation score of 0, and the complete absence of any relevant medical history/conditions and concomitant drug therapies. In the case of QDR2013(), you would also be assuming that a 10-year risk window is desired.\nExamples\nBelow are some very simple examples using the QDiabetes package. Note that for convenience, either BMI [bmi] or height [ht] and weight [wt] may be specified in any of the risk prediction functions in this package.\nIn the interest of making life a little easier, a getTDS() helper function has been added to the package, which uses a lookup table to obtain Townsend deprivation scores from full or partial UK postcodes.\n### Load Package Namespace ###\nlibrary(QDiabetes)\n\n### Simple Usage ###\nQDR2013(sex = ""Female"", age = 35, bmi = 25)\n# [1] 0.6324508\nQDR2018A(sex = ""Male"", age = 45, bmi = 35)\n# [1] 9.88593\nQDR2018B(sex = ""Female"", age = 65, bmi = 30, fpg = 6)\n# [1] 18.43691\nQDR2018C(sex = ""Male"", age = 25, bmi = 40, hba1c = 42)\n# [1] 8.226301\n\n### Making Use of the getTDS() Helper Function ###\ngetTDS(""OX2 6GG"")\n# [1] 2.022583\nQDR2013(sex = ""Female"", age = 41, ht = 1.65, wt = 60, tds = getTDS(""OX3 9DU""))\n# [1] 0.5004499\nQDR2018A(sex = ""Male"", age = 33, bmi = 26, tds = getTDS(""OX3 7LF""))\n# [1] 0.6472644\n\n### Making Use of Vectorisation ###\ngetTDS(c(""OX3 7LF"", ""OX2 6NW"", ""OX2 6GG"", ""OX1 4AR""))\n#    OX37LF    OX26NW    OX26GG    OX14AR\n# -1.032394  1.640422  2.022583  2.309777\nQDR2013(sex = ""Female"", age = 35, bmi = seq(20, 40, 5))\n#        20        25        30        35        40\n# 0.1801226 0.6324508 1.7885233 3.8983187 6.2964702\nQDR2018A(sex = ""Female"", age = seq(25, 75, 10), bmi = 35)\n#       25        35        45        55        65        75\n# 1.085179  2.921454  5.893499  9.082108 10.713717  9.567516\nQDR2018B(sex = ""Male"", age = 65, bmi = 35, fpg = 2:6)\n#         2          3          4          5          6\n# 0.9123063  0.5911511  1.8416081  7.8554831 30.8096968\nQDR2018C(sex = ""Female"", age = 80, bmi = 28, hba1c = seq(15, 45, 5))\n#          15           20           25           30           35           40           45\n# 0.008084487  0.033019655  0.121238952  0.412396004  1.320727239  4.005759509 11.409509026\n\n### Data Frame Usage ###\ndata(dat_qdr) # Synthetic sample data\n\n## Using base R ##\ndat_qdr[[""risk""]] <- with(dat_qdr, QDR2013(sex = sex,\n                                           age = age,\n                                           ht = ht,\n                                           wt = wt,\n                                           ethn = ethn,\n                                           smoke = smoke,\n                                           tds = tds,\n                                           htn = htn,\n                                           cvd = cvd,\n                                           ster = ster))\n\n## Using dplyr ##\nlibrary(dplyr)\ndf_qdr <- as_tibble(dat_qdr)\ndf_qdr <- df_qdr %>%\n  mutate(risk = QDR2013(sex = sex,\n                        age = age,\n                        ht = ht,\n                        wt = wt,\n                        ethn = ethn,\n                        smoke = smoke,\n                        tds = tds,\n                        htn = htn,\n                        cvd = cvd,\n                        ster = ster))\n\n## Using data.table ##\nlibrary(data.table)\ndt_qdr <- as.data.table(dat_qdr)\ndt_qdr[, risk := QDR2013(sex = sex,\n                         age = age,\n                         ht = ht,\n                         wt = wt,\n                         ethn = ethn,\n                         smoke = smoke,\n                         tds = tds,\n                         htn = htn,\n                         cvd = cvd,\n                         ster = ster)]\nKnown issues\nSee Issues on the QDiabetes GitHub repository.\nSimilar packages\n\nQRISK3: An R implementation of ClinRisk‚Äôs QRISK3 risk prediction algorithms.\n\nFunding\nThis project was funded by the National Institute for Health Research (NIHR) School for Primary Care Research (SPCR) [project number: 412]. The views expressed are those of the author(s) and not necessarily those of the NIHR or the Department of Health and Social Care.\n\nReferences\n1: Hippisley-Cox J, Coupland C, Robson J, Sheikh A & Brindle P. (2009). Predicting risk of type 2 diabetes in England and Wales: prospective derivation and validation of QDScore. BMJ 338, b880\n2: Hippisley-Cox J & Coupland C. (2017). Development and validation of QDiabetes-2018 risk prediction algorithm to estimate future risk of type 2 diabetes: cohort study. BMJ 359, j5019\n'], 'url_profile': 'https://github.com/Feakster', 'info_list': ['Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Feb 7, 2021', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Swift', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020']}","{'location': 'Dubai, UAE', 'stats_list': [['1', '          follower'], ['0', '          following'], ['4']], 'contributions': '0 contributions\n        in the last year', 'description': ['Deep-Learning-Method-comparison-with-Logistic-Regression-\nComparison of indian_diabetes dataset using Keras Deep Learning Method with Logistic Regression\nI would like to thank Jason Brownlee as he was the inspiration for me where he is using indian_diabetes dataset and explaining to determine the accuracy using Keras with the help of Deep Learning.\nIf you want to just check the article of how he did it.The link is being given below:\nhttps://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\nI just wanted to know the accuracy of indian_diabetes dataset using Logistic Regression and how much change is happening in the accuracy so I am posting my github code along with his code.\n'], 'url_profile': 'https://github.com/nairshobhit94', 'info_list': ['Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Feb 7, 2021', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Swift', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020']}","{'location': 'Oslo', 'stats_list': [['0', '          followers'], ['6', '          following'], ['4']], 'contributions': '11 contributions\n        in the last year', 'description': ['Experimenting with the Droplet, Bubble and MiaoMiao transmitters I bought for the Abbott FreeStyle Libre sensor and trying something new compared to the current apps:\n\na universal SwiftUI application for iPhone, iPad and Mac Catalyst;\nscanning the Libre directly via NFC;\nusing online servers for calibrating just like Abbott‚Äôs algorithm;\na detailed log to check the traffic from/to the BLE devices and remote servers.\n\nStill too early to decide the final design (and the evil logo üòà), here there are the first rough screenshots:\n\xa0\xa0\nThe project started as a single script for the iPad Swift Playgrounds and was quickly converted to an app by using a standard Xcode template.\nIt should compile finely without dependencies just after changing the Bundle Identifier in the General panel and the Team in the Signing and Capabilities tab of Xcode (Spike users know already very well what that means... ;) ).\nPlease refer to the TODOs list for the up-to-date status of all the current limitations and known bugs of this prototype.\n\nCredits: xDrip for iOS, LibreMonitor, WoofWoof, bubbledevteam.\n'], 'url_profile': 'https://github.com/abumiqb', 'info_list': ['Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Feb 7, 2021', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Swift', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020']}","{'location': 'NY', 'stats_list': [['0', '          followers'], ['1', '          following'], ['0']], 'contributions': '0 contributions\n        in the last year', 'description': ['Econometrics-Project\nThis analysis aims at finding a relationship that links personal metrics (i.e. Race, Sex, Region) with health disparities across the United States. The use of multilinear regression is used to find a linear relationship between the predictors and targets (Chronic Disease, Smoking Habits, Drinking Habits, BMI levels).\nThe data is extracted from IPUMS/NHIS database. Courtsy of Lynn A. Blewett, Julia A. Rivera Drew, Miriam L. King and Kari C.W. Williams. IPUMS Health Surveys: National Health Interview Survey, Version 6.4 [dataset]. Minneapolis, MN: IPUMS, 2019. https://doi.org/10.18128/D070.V6.4 http://www.nhis.ipums.org\n'], 'url_profile': 'https://github.com/Mah-Duece', 'info_list': ['Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Feb 7, 2021', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Swift', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020']}",,,,
