"{'location': 'Vitória, ES, Brasil', 'stats_list': [], 'contributions': '444 contributions\n        in the last year', 'description': ['\nPolyLaneNet\n\n\nDescription\nCode for the PolyLaneNet paper, accepted to ICPR 2020, by Lucas Tabelini, Thiago M. Paixão, Rodrigo F. Berriel, Claudine Badue,\nAlberto F. De Souza, and Thiago Oliveira-Santos.\nNews: The source code for our new state-of-the-art lane detection method, LaneATT, has been released. Check it out here.\nTable of Contents\n\nInstallation\nUsage\nReproducing the paper results\n\n\nInstallation\nThe code requires Python 3, and has been tested on Python 3.5.2, but should work on newer versions of Python too.\nInstall dependencies:\npip install -r requirements.txt\n\n\nUsage\nTraining\nEvery setting for a training is set through a YAML configuration file.\nThus, in order to train a model you will have to setup the configuration file.\nAn example is shown:\n# Training settings\nexps_dir: \'experiments\' # Path to the root for the experiments directory (not only the one you will run)\niter_log_interval: 1 # Log training iteration every N iterations\niter_time_window: 100 # Moving average iterations window for the printed loss metric\nmodel_save_interval: 1 # Save model every N epochs\nseed: 0 # Seed for randomness\nbackup: drive:polylanenet-experiments # The experiment directory will be automatically uploaded using rclone after the training ends. Leave empty if you do not want this.\nmodel:\n  name: PolyRegression\n  parameters:\n    num_outputs: 35 # (5 lanes) * (1 conf + 2 (upper & lower) + 4 poly coeffs)\n    pretrained: true\n    backbone: \'efficientnet-b0\'\n    pred_category: false\nloss_parameters:\n  conf_weight: 1\n  lower_weight: 1\n  upper_weight: 1\n  cls_weight: 0\n  poly_weight: 300\nbatch_size: 16\nepochs: 2695\noptimizer:\n  name: Adam\n  parameters:\n    lr: 3.0e-4\nlr_scheduler:\n  name: CosineAnnealingLR\n  parameters:\n    T_max: 385\n\n# Testing settings\ntest_parameters:\n  conf_threshold: 0.5 # Set predictions with confidence lower than this to 0 (i.e., set as invalid for the metrics)\n\n# Dataset settings\ndatasets:\n  train:\n    type: PointsDataset\n    parameters:\n      dataset: tusimple\n      split: train\n      img_size: [360, 640]\n      normalize: true\n      aug_chance: 0.9090909090909091 # 10/11\n      augmentations: # ImgAug augmentations\n       - name: Affine\n         parameters:\n           rotate: !!python/tuple [-10, 10]\n       - name: HorizontalFlip\n         parameters:\n           p: 0.5\n       - name: CropToFixedSize\n         parameters:\n           width: 1152\n           height: 648\n      root: ""datasets/tusimple"" # Dataset root\n\n  test: &test\n    type: PointsDataset\n    parameters:\n      dataset: tusimple\n      split: val\n      img_size: [360, 640]\n      root: ""datasets/tusimple""\n      normalize: true\n      augmentations: []\n\n  # val = test\n  val:\n    <<: *test\nWith the config file created, run the training script:\npython train.py --exp_name tusimple --cfg config.yaml\nThis script\'s options are:\n  --exp_name            Experiment name.\n  --cfg                 Config file for the training (.yaml)\n  --resume              Resume training. If a training session was interrupted, run it again with the same arguments and this option to resume the training from the last checkpoint.\n  --validate            Wheter to validate during the training session. Was not in our experiments, which means it has not been thoroughly tested.\n  --deterministic       set cudnn.deterministic = True and cudnn.benchmark = False\n\nTesting\nAfter training, run the test.py script to get the metrics:\npython test.py --exp_name tusimple --cfg config.yaml --epoch 2695\nThis script\'s options are:\n  --exp_name            Experiment name.\n  --cfg                 Config file for the test (.yaml). (probably the same one used in the training)\n  --epoch EPOCH         Epoch to test the model on\n  --batch_size          Number of images per batch\n  --view                Show predictions. Will draw the predictions in an image and then show it (cv.imshow)\n\nIf you have any issues with either training or testing feel free to open an issue.\n\nReproducing the paper results\nModels\nAll models trained for the paper can be found here.\nDatasets\n\nTuSimple\nELAS\nLLAMAS\n\nHow to\nTo reproduce the results, you can either retrain a model with the same settings (which should yield results pretty close to the reported ones) or just test the model.\nIf you want to retrain, you only need the appropriate YAML settings file, which you can find in the cfgs directory.\nIf you just want to reproduce the exact reported metrics by testing the model, you\'ll have to:\n\nDownload the experiment directory. You don\'t need to download all model checkpoints if you want, you\'ll only need the last one (model_2695.pt, with the exception of the experiments on ELAS and LLAMAS).\nModify all path related fields (i.e., dataset paths and exps_dir) in the config.yaml file inside the experiment directory.\nMove the downloaded experiment to your exps_dir folder.\n\nThen, run:\npython test.py --exp_name $exp_name --cfg $exps_dir/$exp_name/config.yaml --epoch 2695\nReplacing $exp_name with the name of the directory you downloaded (the name of the experiment) and $exps_dir with the exps_dir value you defined inside the config.yaml file. The script will look for a directory named $exps_dir/$exp_name/models to load the model.\n'], 'url_profile': 'https://github.com/lucastabelini', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/jasp-stats', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery\nThis repository is the official implementation of\nIntegration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery\nPlease cite the above paper if you use this code for your work.\nSymbolic regression, in which a model discovers an analytical equation describing a dataset as opposed to finding\nthe weights of pre-defined features, is normally implemented using genetic programming.\nHere, we demonstrate symbolic regression using neural networks,\nwhich allows symbolic regression to be performed using gradient-based optimization techniques,\ni.e., backpropagation.\nIt can be integrated with other deep learning architectures, allowing for end-to-end training of a system that produces\ninterpretable and generalizable results. This repository implements symbolic regression with neural networks and\ndemonstrates its integration with deep learning for multiple tasks,\nincluding arithmetic on MNIST digits and extracting the equations of kinematic and differential equation datasets.\nRequirements\n\nPython 3.5\nTensorFlow 1.15\nNumPy 1.16 (does not work on 1.17)\nScipy 1.3\nSympy 1.6\nMatplotlib (optional)\n\nAll dependencies are in requirements.txt.\nTo install required packages, you can simply run the following code in your shell.\npip install -r requirements.txt\n\nNote that the pretty_print functions in SymPy 1.4 only works with TensorFlow <=1.13.\nPackage Description\nThe core code is all contained inside the utils/ directory\n\n\nfunctions.py contains different primitives, or activation functions. The primitives are built as classes so that different parts of the code (TensorFlow versus NumPy versus SymPy) have a unified way of addressing the functions.\n\n\npretty_print.py contains functions to print out the equations in the end in a human-readable format from a trained EQL network.\n\n\nsymbolic_network.py contains the core code of the EQL network, including methods for L0 regularization.\n\n\nQuick Intro\nThis demonstrates a minimal example for how to use this library for training the EQL network.\nimport numpy as np\nimport tensorflow as tf\nfrom utils import functions, pretty_print\nfrom utils.symbolic_network import SymbolicNetL0\nfrom utils.regularization import l12_smooth\n\nfuncs = functions.default_func\n\n# Set up TensorFlow graph for the EQL network\nx_placeholder = tf.placeholder(shape=(None, x_dim), dtype=tf.float32)\nsym = SymbolicNetL0(symbolic_depth=2, funcs=funcs)\ny_hat = sym(x_placeholder)\n\n# Set up loss function with L0.5 loss\nmse = tf.losses.mean_squared_error(labels=y, predictions=y_hat)\nloss = mse + 1e-2 * l12_smooth(sym.get_weights())\n\n# Set up TensorFlow graph for training\nopt = tf.train.RMSPropOptimizer()\ntrain = opt.minimize(loss)\n\n# Random data for a simple function\nx = np.random.rand(100, 1)\ny = x ** 2\n\n# Training\nwith tf.Session as sess:\n  sess.run(tf.global_variables_initializer())\n  for i in range(1000):\n    sess.run(train, feed_dict={x_placeholder: x})\n\n  # Print out the expression\n  weights = sess.run(sym.get_weights())\n  expr = pretty_print.network(weights, funcs, ['x'])\n  print(expr)\nFor a more complete example with training stages or L0 regularization, see below.\nTraining\nEach task are trained independently.\nRefer to the paper https://arxiv.org/abs/1912.04825 for a description of each of the tasks.\nBenchmark\nbenchmark_accuracy.py/benchmark_accuracy_l0.py:\nRun EQL benchmarks on various functions using smoothed L0.5 and relaxed L0 regularization, respectively.\nMNIST\nmnist_math.py: Learn arithmetic operations on MNIST digits.\nmnist_math_relu.py: Same as mnist_math.py,\nbut using a conventional neural network with ReLU activation functions instead of the EQL network.\nKinematics\nkinematics_data.py: Generate data for the kinematics task. This must be run before training the model.\nkinematics_sr.py/kinematics_sr_l0.py: Dynamics encoder combined with a recurrent EQL network for the kinematics task\nusing smoothed L0.5 and relaxed L0 regularization, respectively.\nkinematics_sr.py implements an unrolled RNN to demonstrate the internal architecture, while\nkinematics_sr_l0.py implements the RNN using the built-in TensorFlow libraries.\nkinematics_relu.py: Same as kinematics_sr.py\nbut using a conventional neural network with ReLU activation functions instead of the EQL network.\nSimple Harmonic Oscillator (SHO)\nsho_data.py: Generatedata for the SHO task. This must be run before training the model.\nsho_sr.py/sho_sr_l0.py: Dynamics encoder combined with a recurrent EQL network for the kinematics task\nusing smoothed L0.5 and relaxed L0 regularization, respectively.\nBoth implement the RNN using the built-in TensorFlow libraries.\nsho_relu.py: Same as sho_sr.py\nbut using a conventional neural network with ReLU activation functions instead of the EQL network.\nAuthors\nSamuel Kim, Peter Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir Ceperic, Marin Soljacic\nContributing\nIf you'd like to contribute, or have any suggestions for these guidelines,\nyou can contact Samuel Kim at samkim (at) mit (dot) edu or open an issue on this GitHub repository.\nAll content in this repository is licensed under the MIT license.\n""], 'url_profile': 'https://github.com/samuelkim314', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['README\nContact and citation\nAuthors and affiliation:\nWeiguan Wang w.wang34@lse.ac.uk, https://weiguanwang.github.io/\nProf. Johannes Ruf j.ruf@lse.ac.uk, http://www.maths.lse.ac.uk/Personal/jruf/\nDepartment of Mathematics, London School of Economics and Political Science, London, United Kingdom\n15 December 2020\nSuggested citation:\nJ. Ruf and W. Wang, Hedging with Linear Regressions and Neural Networks, SSRN 3580132, 2020. Accepted by the Journal of Business and Economic Statistics subject to minor corrections. Download at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3580132\nSupplementary reading:\nJ. Ruf and W. Wang, Neural Networks for Option Pricing and Hedging: A Literature Review, Journal of Computational Finance, volume 24, number 1, pages 1-45. Download at  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3486363\nIntroduction\nThis documentation explains the code structure and data folders to reproduce the results in Ruf and Wang (2020). To run the code provided here, the user needs to:\n\nOverwrite the DATA_DIR variable in the setup.py to your own choice.\nObtain raw data (should you want to work with real datasets) and rename files as detailed in [link](#Data folder structure).\n\nCode structure\nThe code consists of four subfolders. They are libaray, Simulation, OptionMetrics, and Euroxx. The library folder contains functions used by other parts of the code. The library consists of:\n\nbs.py : This file contains a function used to simulate the Black-Scholes dataset.\ncleaner_aux.py: This file contains functions used to clean raw data.\ncommon.py : This file contains functions that calculate and inspect the hedging error.\nheston.py: This file contains functions used to simulate the Heston dataset, as well as calculating option prices in the Heston model.\nloader_aux.py : This file contains functions used to load clean data (before training the ANN or linear regressions).\nnetwork.py : This file implements HedgeNet and auxiliary functions.\nplot.py: This file contains functions used to plot diagnostic figures.\nregression_aux.py: This file contains functions that implement the linear regression methods.\nsimulation.py: This file contains functions that implement the CBOE rules, and organize data.\nstoxx.py: This file contains function used to clean the Euro Stoxx 50 dataset only.\nvix.py: This file contains the function that simulates an Ornstein-Uhlenbeck  process, used as the fake VIX feature.\n\nIn each of the other three folder, there are two python files that are used by other notebooks:\n\nSetup.py: This file contains all the flags to configure experiments. It varies by datasets, and contains two major configurations:\n\nIt specifies the hedging period, time window size, data cleaning choice, and other experimental setup.\nIt specifies the location of raw data, clean data, and the stored results.\n\n\nLoad_Clean_aux.py loads the clean data and implements some extra cleaning, before running linear regressions or ANNs.\n\nThe notebooks have a very similar structure as follows:\n\nIn the simulation folder, the first notebook implements the data simulation. In the OptionMetrics and Euroxx folder, the first notebook implements the cleaning of the real raw datasets downloaded from data providers.\n2_Regression_Generate.ipynb implements all linear regressions on sensitivities and stores the PNL (MSHE) files.\n3_Tuning _Hyper.ipynb implements the tuning of $L^2 $ regularisation parameters.\n4_Network.ipynb implements the training of the ANN and stores the PNL files (MSHE of ANN).\n5_Diagnostic.ipynb creates tables to summarize PNL (MSHE) files in terms of given performance measure, across several experimental setups, i.e. globally for each dataset.\n6_Local_Diag_And_Plots.ipynb implements the diagnostics of PNL files for a single experimental setup. Plots made from PNL files are generated in this file. They include linear regression coefficients, mean squared hedging error plots, MSHE vs sensitivities, confidence interval and etc.\n7_Analysis_of_(Semi-)CleanData.ipynb implements the analysis of raw and clean data. They include histograms of certain features, number of samples in each time window, volatility, leverage effect, etc.\n8_Permute_VIX_Analysis.ipynb implements the analysis of permutation and fake VIX experiments. The implementation of the experiment is done in notebook 4 and 5, by giving the corresponding setup flags. This notebook only exists for the Simulation and OptionMetrics folders.\n9_Bucket_Moneyness.ipynb splits the data set by moneyness into several buckets, and runs statistical models on each bucket independently.\n\nData folder structure\nBefore running the code, one needs to specify the directory that stores the simulation data, (or real data) and the results. This is done by overwriting the DATA_DIR variable in each of the setup.py file.\nThe data folders  have two common subfolders,\n\nCleanData: It stores simulation data in case of Black-Scholes or Heston data, or clean data generated by 1_Clean.ipynb in case of real data.\nResult: It store the PNL files and other auxiliary files, either from the linear regressions or ANN. They also include  tables made by 5_Diagnostic.ipynb. For the ANN, it additionally contains loss plots, checkpoints, etc. For the linear regression, it additionally contains regression coefficients, standard errors, etc.\n\nFor the two real datasets, there is an extra folder RawData to store data given by data providers. Data needs to be arranged and renamed in the following way for the code to run.\n\n\nFor the S&P 500 data. There are 4 files:\n\n\noption_price.csv contains option quotes downloaded from OptionMetrics.\n\n\nspx500.csv contains the close-of-day price of S&P 500.\n\n\nonr.csv contains the overnight LIBOR rate downloaded from Bloomberg.\n\n\ninterest_rate.csv contains the interest rate derived from zero-coupon bond for maturity larger than 7 days, downloaded from OptionMetrics.\n\n\n\n\nFor the Euro Stoxx data. Data needs to be put in four folders:\n\nfutures contains two files, futures.csv and refData.csv; the former contains the tick trading data of futures, and the latter contains the contract specifications of futures in the former.\noptions contains two files, options.csv and refData.csv; they are tick trading data of options and their reference.\ninterest_rate contains seven files. They are LIBOR_EURO_ON, LIBOR_EURO_1M.csv,  LIBOR_EURO_3M.csv, LIBOR_EURO_6M.csv LIBOR_EURO_12M.csv; namely, LIBOR rate of overnight, maturity 1 month, 3 months, 6 months, 12 months. The other two files are ZERO_EURO_5Y and ZERO_EURO_10Y; namely, interest rate derived from zero-coupon bond of maturity 5  and 10 years.\nstoxx50.csv is the end-of-day spot of Euro Stoxx 50 index.\n\n\n\nPackage information\n\n\n\nPackage\nVersion\n\n\n\n\nAnaconda\n2019.03\n\n\nKeras\n2.2.4\n\n\nPython\n3.6\n\n\nNumpy\n1.16.3\n\n\nPandas\n0.24.2\n\n\nScikit-learn\n0.20.3\n\n\nScipy\n1.2.1\n\n\nSeaborn\n0.9\n\n\nTensorflow\n1.13.1\n\n\n\n'], 'url_profile': 'https://github.com/weiguanwang', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '330 contributions\n        in the last year', 'description': ['house-price-predicton-RFE-OLS\nHouse Price Prediction using Recursive Feature Elimination (RFE) and Ordinary Least Squares (OLS) Linear Regression\n'], 'url_profile': 'https://github.com/junmoan', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""harmony-tests\nharmony-tests executes a set of test cases for testing that regular and staking transactions work properly on Harmony's blockchain.\nIt uses the harmony-tf testing framework under the hood.\nInstallation\nrm -rf harmony-tests && mkdir -p harmony-tests && cd harmony-tests\nbash <(curl -s -S -L https://raw.githubusercontent.com/harmony-one/harmony-tests/master/scripts/install.sh)\n\nBuild/deploy\nIf you need to build/compile the tool from scratch:\nRegular build:\nmake all\nStatic build:\nmake static\nStatic build, package all testcases and upload everything to Amazon S3:\nmake upload-linux\n(Static builds have only been tested on Linux)\nUsage\nFunding\nExisting address\nIf you already have a funded account, simply pass the address of that account using --address:\n\n./tests --network NETWORK --address YOUR_FUNDED_ADDRESS\n\nPrivate keys\nIf you want to import and use private keys:\n\nmkdir -p keys/NETWORK/\nnano keys/NETWORK/private_keys.txt\npaste your private keys and save the file\n\nKeystore files\nIf you want to import and use keystore files:\n\nmkdir -p keys/NETWORK/\ncp -r PATH/TO/YOUR/KEYSTORE/FOLDER keys/NETWORK\n\nHarmony TF will automatically identify keyfiles no matter what you call the folders or what the files are called - as long as they reside under keys/NETWORK they'll be identified.\nRunning tests\nTo run all test cases:\n./tests --network NETWORK\nTo run all test cases using an already funded address:\n./tests --network NETWORK --address YOUR_FUNDED_ADDRESS\nTo connect to custom defined RPC nodes configured using config.yml:\n./tests --network NETWORK --mode custom --address YOUR_FUNDED_ADDRESS\nTo connect to custom nodes using command line args:\n./tests --network NETWORK --mode custom --nodes http://SHARD0NODEIP:9500,http://SHARD1NODEIP:9500 --address YOUR_FUNDED_ADDRESS\nSpecific test cases\nStaking\n\nOnly run staking tests: ./tests --network NETWORK --test staking\nOnly run staking -> create validators: ./tests --network NETWORK --test staking/validators/create\nOnly run staking -> edit validators: ./tests --network NETWORK --test staking/validators/edit\nOnly run staking -> delegation: ./tests --network NETWORK --test staking/delegation/delegate\nOnly run staking -> undelegation: ./tests --network NETWORK --test staking/delegation/undelegate\n\nTransactions\n\nOnly run tx tests: ./tests --network NETWORK --test transactions\nOnly run tx -> cross app shard tests: ./tests --network NETWORK --test transactions/cross_app_shard\nOnly run tx -> cross beacon shard tests: ./tests --network NETWORK --test transactions/cross_beacon_shard\nOnly run tx -> same app shard tests: ./tests --network NETWORK --test transactions/same_app_shard\nOnly run tx -> same beacon shard tests: ./tests --network NETWORK --test transactions/same_beacon_shard\n\nExporting test data\nHarmony-tests currently supports exporting the test suite results as a CSV report:\n./tests --network NETWORK --address YOUR_FUNDED_ADDRESS --export csv\nJSON export is also planned, but not yet implemented.\nWriting test cases\nTest cases are defined as YAML files and are placed in testcases/ - see this folder for existing test cases and how to impelement test cases.\nThere are some requirements for these files - this README will be updated with a list of these eventually :)\n""], 'url_profile': 'https://github.com/harmony-one', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Sofia, Bulgaria', 'stats_list': [], 'contributions': '653 contributions\n        in the last year', 'description': ['Visual Regression Testing for PDFs in JavaScript\n\n\n\nGetting started\nFirst download and install GraphicsMagick for your platform. For macOS you can simply use Homebrew and do:\nbrew install graphicsmagick\nthen use npm:\nnpm install -D pdf-visual-diff\nDescription\nThis package exports single function comparePdfToSnapshot. With the following signature:\n/**\n * Compare pdf to persisted snapshot. If one does not exist it is created\n * @param pdf - path to pdf file or pdf loaded as Buffer\n * @param snapshotDir - path to a directory where __snapshots__ folder is going to be created\n * @param snapshotName - uniq name of a snapshot in the above path\n * @param compareImageOpts - settings for image comparation\n * @param compareImageOpts.highlightColor - color for differences in the diff image, defaults to Black\n * @param compareImageOpts.highlightStyle - highlight style as documented by the {@link http://www.graphicsmagick.org/GraphicsMagick.html#details-highlight-style gm package}, defaults to Tint\n * @param compareImageOpts.tolerance - number value for error tolerance, defaults to 0\n * @param compareImageOpts.writeDiff - flag to enable/disable diff file creation, defaults to true\n * @param compareImageOpts.maskRegions - exclude regions from the diff by masking them with solid rectangles\n */\ntype ComparePdfToSnapshot = (\n  pdf: string | Buffer,\n  snapshotDir: string,\n  snapshotName: string,\n  compareImageOpts?: Partial<CompareImagesOpts>,\n) => Promise<boolean>\nWhen function is executed it has following side effects:\n\nIn absence of a previous snapshot file it converts pdf to an image, saves it as a snapshot and returns true\nIf there is a snapshot, then pdf is converted to an image and gets compared to the snapshot:\n\nif they differ function returns false and creates next to the snapshot image two other versions with suffixes new and diff. new one is the current view of the pdf as an image, where diff shows the difference between the snapshot and new images\nif they are equal function returns true and in case there are new and diff versions persisted it deletes them\n\n\n\nSample usage\n\nNB! You can find sample projects inside  examples folder.\n\nWrite a test file:\nimport { comparePdfToSnapshot } from \'pdf-visual-diff\'\nimport { expect } from \'chai\'\n\ndescribe(\'test pdf report visual regression\', () => {\n  const pathToPdf = \'path to your pdf\' // or you might pass in Buffer instead\n  it(\'should pass\', () =>\n    comparePdfToSnapshot(pathToPdf, __dirname, \'my-awesome-report\').then(\n      (x) => expect(x).to.be.true,\n    ))\n})\nUsage with Jest\nThis packages provides custom jest matcher toMatchPdfSnapshot\nSetup\n""jest"": {\n  ""setupFilesAfterEnv"": [""pdf-visual-diff/lib/toMatchPdfSnapshot""]\n}\nIf you are using Typescript add import(\'pdf-visual-diff/lib/toMatchPdfSnapshot\') to your typings.\nUsage\nAll you have to do in your tests is pass a path to the pdf or pdf content as Buffer.\nconst pathToPdf = \'path to your pdf\' // or you might pass in Buffer instead\ndescribe(\'test pdf report visual regression\', () => {\n  it(\'should match\', () => expect(pathToPdf).toMatchPdfSnapshot())\n})\nAs you can see no need to fiddle with any dirs nor names. Needed information is extracted from jest context.\n'], 'url_profile': 'https://github.com/moshensky', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Kas, Antalya, Turkey', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': ['Wine Rating Predictor\nIn this project, I built a wine rating predictor for an online wine seller. This wine predictor aims to show good prediction is possible using the wine_dataset .\nWine rating is a score between 80 and 100 and represents the quality of wine. With the current set of features, random forest classifier and its tuned parameter wine rating predictor can predict the quality of wine with the mean square error of 4.9. This metric shows that fully-automated machine learning solution on production is feasible and effective for the client.\nThis predictor runs the machine learning pipeline with Docker and Luigi tasks. So, it can be run on any machine that has docker and docker-compose installed.\nMachine learning pipeline consists of the steps below:\n\nDownload Data\nMake Dataset\nClean Data\nExtract Features\nTransform Data\nImpute Data\nTrain Model\nEvaluate Model\n\nand creates a file contains the random forest model, and evaluation plots of the model performance.\nThe output file of the step 1 can be found in the data_root > raw. The output files of the steps 2, 3, 4, 5 and 6 can be found in the data_root > interim. The final output files of step 7 and 8 can be found in the data_root > output.\nThe exploration of the wine_dataset and the chain of thougts of for the feature and model selection can be found in the notebooks in the notebooks folder. The complete machine learning workflow followed in the notebooks are as follows:\n\nUnderstand & Clean & Format Data\nExploratory Data Analysis\nFeature Engineering & Pre-processing\nSet Evaluation Metric & Establish Baseline\nModel Selection & Tune Hyperparameters of the Model\nTrain Model\nEvaluate Model on Test Data\nInterpret Model Predictions\nConclusions\n\nThis project is built as part of the interviews for the Machine Learning Engineer position at Data Revenue.\n'], 'url_profile': 'https://github.com/cereniyim', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Guangzhou,Guangdong,China', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Linear-Regression-Model\nA simple Linear Regression Model in Machine Learning by Python.\n'], 'url_profile': 'https://github.com/stargirl-awa', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '330 contributions\n        in the last year', 'description': ['car-price-prediction-Ridge-Lasso\nCar Price Prediction using Ridge and Lasso Regression Models\n'], 'url_profile': 'https://github.com/junmoan', 'info_list': ['162', 'Python', 'MIT license', 'Updated Jan 14, 2021', 'R', 'Updated Mar 2, 2021', '12', 'Python', 'Updated Jan 21, 2021', '10', 'Jupyter Notebook', 'Updated Dec 28, 2020', '5', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Shell', 'Updated Feb 4, 2021', '6', 'TypeScript', 'MIT license', 'Updated Nov 18, 2020', '10', 'Jupyter Notebook', 'Updated May 30, 2020', '7', 'Python', 'Updated Apr 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': [""Regression-Analysis\nML supervised algorithm : Linear Regression\nDatasets used: computers.csv and cars93.csv\n*In computers.csv file , we want to predict the repair time of reqired in minutes for different units of computer.\ndataset description: columns:=['units','minutes']\nAccuracy attained: 92.2%\n*In second dataset i.e. ,cars93.csv we want predict the length of wheelbase of cars.\ndataset description: columns:=[col.no', 'Manufacturer', 'Model', 'Type', 'Min.Price', 'Price',\n'Max.Price', 'MPG.city', 'MPG.highway', 'AirBags', 'DriveTrain',\n'Cylinders', 'EngineSize', 'Horsepower', 'RPM', 'Rev.per.mile',\n'Man.trans.avail', 'Fuel.tank.capacity', 'Passengers', 'Length',\n'Wheelbase', 'Width', 'Turn.circle', 'Rear.seat.room', 'Luggage.room',\n'Weight', 'Origin', 'Make'],\ndtype='object'\nAccuracy attained: 60.8%\n""], 'url_profile': 'https://github.com/devendra45', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '330 contributions\n        in the last year', 'description': ['simple-linear-regresion\nSimple Linear Regression Sample\n'], 'url_profile': 'https://github.com/junmoan', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['CNN_GRU-Regression\nThis project use CNN+GRU in tensorflow1.x/python to implement regression about time_series.The main content is to predict the wind power at the current time based on the wind speed and wind power data at the historical time。if you want this code for academic, please contact me qq 2919218574\n此代码是用来做风功率时间序列预测的，利用过去时刻的风速与风功率数据为输入来预测当前时刻的风功率，文件采用tensorflow1.x编写，如果你需要的话，可以联系我qq2919218574，有偿，想白嫖的不要来了哈\n1，mlp结果\n\n\n2，gru结果\n\n\n3，cnn_gru结果\n\n\n\n4,对比\n\n'], 'url_profile': 'https://github.com/fish-kong', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Canada; USA; Singapore; Hong Kong', 'stats_list': [], 'contributions': '1,264 contributions\n        in the last year', 'description': ['robobisect\nBisect the WebKit Git repository to find regression windows. Currently focuses on JavaScriptCore (jsc) binaries.\nWhy robobisect?\nNOTE: robobisect is still in alpha stage - do not use on a production machine.\nDepending on the computer robobisect is run on, with an Intel Core i7-9700K (8C/8T), a NVMe SSD and 32GB RAM on Ubuntu 18.04, each changeset compiles in less than 4 minutes, and due to binary search, a bisect result is obtained within about 15 compiles (assuming the starting and ending working states are known), thus only takes approximately an hour to run fully. If you have a slower computer (e.g. with a spinning HDD), your compile times are expected to be slower. If you have ccache, and if your changeset has already been compiled before and is in the cache, your compile times should speed up considerably for that changeset.\nAs of April 2020, robobisect is able to test as far back as May 2017, or about 3 years worth of history.\nHaving an exact regressing changeset in a bug report is extremely useful information for developers, as it can show whether it is the cause of the bug or one that caused the latent bug to show up.\nrobobisect can test when builds started to fail compilation, and verify when testcases were fixed by which revision. It can also be used to find when a bug is fixed, or stopped reproducing, or when the stdout message changed.\nHow do I use robobisect?\nrm -rf ~/webkit/WebKitBuild/ && time { date && python3.8 -m robobisect 2>&1 | tee ~/rb_log.txt ; date ; }\nNote: argparse is on the to-do list. Some functions not working well yet - one has to change robobisect itself to test various stuff.\nDoes this work on macOS / Windows 10?\nrobobisect has been tested to run on Ubuntu 18.04. I do not yet have a recent macOS machine powerful enough for sane compilation times, ideas welcome. Windows 10 support will fall behind Ubuntu Linux and macOS for now. Other flavours of Linux probably are not yet a priority anytime soon.\nWhy not bisect-builds (from the WebKit repository)?\nWebKit already has the bisect-builds script, but this uses downloaded pre-compiled builds. This method is faster than robobisect, which uses source-compiled builds, but whether bisect-builds gives a single regressing changeset will depend on how often the pre-compiled builds were created. If they are per-push, bisect-builds will give a single regressing changeset, but if they are per-day, bisect-builds will give a range of changesets.\nrobobisect usually provides a tighter bisect unless it falls within a range of non-compilable changesets. Moreover, robobisect will allow supporting different configurations which are not part of the pre-compiled builds. For bugs which occur only on specific systems (and not with pre-compiled builds), provided the binary can be compiled and the dev toolchain installed, robobisect may be able to come up with a regressing changeset.\nHow does this work?\nrobobisect compiles into a cache folder, which is then used to test against a given testcase. It acts as a higher-level interface on top of git bisect.\nWill this support the WebKit browser itself?\nPossibly, at some point. Previous incantations of autobisectjs used to support the Firefox browser instead of just SpiderMonkey.\nWill this support other browsers?\nGecko has autobisect for Firefox and autobisectjs for SpiderMonkey, while Chrome has bisect-builds.py and Chrome Bisect. Edge Chromium only publishes source code dumps.\nAt some point in the distant future, robobisect may be adapted to work with Gecko-dev.\nCan this be in a Docker/<favourite container> format?\nNot sure about the benefits of the container format for now.\nTODO:\n\nargparse\nBe able to specify desired repo start and end-points. Currently defaults to earliest known working revision, and master.\nImprove the cache - it seems like only the jsc binary and the files in Debug/lib are needed\nAdd test for +2/-2 from (WebKit bug 187947) - when testcase is fixed\nAdd test for +2/-2 from (WebKit bug 203406) - when testcase started failing\nConsider adding a lock dir (via fasteners?) when robobisect is running.\nCI support (via Travis?)\nCode coverage (via codecov.io?)\n\nAssumptions:\n\nThe WebKit/ repository is cloned into ~/webkit/.\nWhen bisection is in-progress, the ~/webkit/ directory is left alone by the user/other programs that may interfere with the directory state of the repository.\n\n'], 'url_profile': 'https://github.com/nth10sd', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '628 contributions\n        in the last year', 'description': ['\n\n\n\nTeam\n\n\n\nFeli Gentle\nTyler Woods\nWesley Nguyen\n\n\n\nTable of Contents\n\nOverview\nExploring Data\n\nInitial Intake\nFeature Engineering\nVisualizations\n\n\nPredictive Modeling\n\nBaseline\nEvaluation\nTuning\n\n\nPerformance\nFuture Considerations\nLicense\nCredits\nThanks\n\nOverview\nMain Goal: \nPredicting the sales price of a particular piece of equipment at auction, based on it\'s usage, equipment type, and configuration, and other available features. \nBusiness Context:\nWhether you\'re buying, selling, or analyzing market dynamics, predicting sales prices is a valuable insight for a business.\nWhen analyzing a business\'s own inventory, a reliable price prediction model can inform annual budgets and projected revenue. Additionally, the information gained sheds light on what it is about an item that tends to have the biggest impact on sale price at auction time. Knowing these features can inform a business on how to build and maintain an inventory that holds value over time.\nKnowing the projected prices of other business\'s items can inform what a fair market value is when growing one\'s inventory. This insight can additionally help with budgeting and negotiation when purchasing.\nLet\'s look further at how we built our predictive model.\nNote: This data consists of data from auction postings and sales prices. It includes information on the usage and specifications of the machinery.\nEvaluating Success:\nThe evaluation of our model will be based on Root Mean Squared Log Error.\nWhich is computed as follows:\n\nwhere pi are the predicted values (predicted auction sale prices)\nand ai are the actual values (the actual auction sale prices).\nExploring Data\n\nInitally going into this case study, we decided to tackle the tasks of cleaning the data, and getting a baseline model together as a group. We did this to ensure that everyone was on the same level of understanding before we delved into partitioned tasks.\nInitial Intake\nInitial Data\n\n\nCleaning Data:\nWith the intent of using Linear Regression in mind, we processed and cleaned some of the data in order for this to be possible. Using a custom function, we dropped rows that had a high percentage of null values.\n\n\n    Columns with more than 50% Missing Values\n  \n\n\ndf.drop(columns=[\'UsageBand\',\'Blade_Extension\', \'Blade_Width\', \'Enclosure_Type\',\n                     \'Engine_Horsepower\', \'Pushblock\', \'Scarifier\', \'Tip_Control\',\n                     \'Coupler_System\', \'Grouser_Tracks\', \'Hydraulics_Flow\',\'Backhoe_Mounting\', \n                     \'Blade_Type\', \'Travel_Controls\',\'Differential_Type\',\'Steering_Controls\',\n                     \'SalesID\',\'fiBaseModel\',\'fiSecondaryDesc\',\n                     \'fiModelSeries\',\'fiModelDescriptor\', \'auctioneerID\',\n                     \'Drive_System\', \'datasource\'\n                    ], inplace=True)\n\n\n\nProduct Size\nRipper Values\n\n\n\n\n\n\n\n\n\nFeature Engineering\nCleaning Functions:\ndef getNullCount(df:pd.DataFrame) -> None:\n    """"""Prints metrics of null values from a dataframe""""""\n    columns = df.columns\n    for col in columns:\n        total_nan = sum(pd.isna(df[col]))\n        total_all = df[col].size\n        print(f""Column: {col}  Total:{total_all}  Missing:{total_nan}  {round(total_nan/total_all, 2) * 100}%"")\n\n# One Hot Encode Categoricals\ndef set_ohe(df:pd.DataFrame, col_name:str) -> None:\n    """"""One Hot Encodes Dataframe column""""""\n    for val in auction_train[col_name].value_counts().index:\n        df[f""{col_name}: {val}""] = df[col_name].map(lambda x: 1.0 if x==val else 0.0)\nOne Hot Encoding:\nWe noticed that there were groupings of items within certain columns and decided to use OHE to convert these values to binary values.\nCleaned Data\n\nVisualzations\n\nPredictive Modeling\nBaseline\nBaseline Model: Linear Regression\nWe started out using a Linear Regression as a baseline model to start with, and then figure out where to go from there.\n# Split up Data Between Features (X) and SalePrice, i.e. the Target Values (y))\nX = clean_df.drop(columns=[\'SalePrice\'])\ny = clean_df[\'SalePrice\']\n\nsummary_model(X, y)\n\n\n    OLS Summary \n  \n\n\n# Split up Data Between Features (X) and SalePrice, i.e. the Target Values (y))\nX = clean_df.drop(columns=[\'SalePrice\'])\ny = clean_df[\'SalePrice\']\n\ny.hist(bins=100)\nplt.show()\n\nTo get a sense of what the distribution of our target values were, we plotted it in a histogram\n\n\n\n    Sales Price Histogram\n  \n\n\n# Split up Data Between Features (X) and SalePrice, i.e. the Target Values (y))\nX = clean_df.drop(columns=[\'SalePrice\'])\n# Log the Target\ny = np.log(clean_df[\'SalePrice\'])\n\ny.hist(bins=100)\nplt.show()\n\nAs seen below, the distribution of the target values are bunched to the left, so we needed to find a way to center the mean in order to create a more accurate model\n\n\n\n    Sales Price Histogram (Log)\n  \n\n\n\n\n\n    RMSLE: Cross Validation Errors (Pre-Log)\n  \n\nn_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True)\ntest_cv_errors, train_cv_errors = np.empty(n_folds), np.empty(n_folds)\nX_array = np.array(X)\ny_array = np.array(y)\n\nfor idx, (train, test) in enumerate(kf.split(X)):\n    model = LinearRegression()\n    model.fit(X_array[train], y_array[train])\n    y_hat = model.predict(X_array[test])\n    y_train = model.predict(X_array[train])\n    \n    train_cv_errors[idx] = rmsle(y_array[train], y_train)\n    test_cv_errors[idx] = rmsle(y_array[test], y_hat)\n\ntrain_cv_errors, test_cv_errors\n\n\n(array([15049.50547112, 15077.59371394, 15064.28882563, 15064.36987342,\n        15070.18482936, 15076.58622347, 15073.01997752, 15054.20483307,\n        15063.71687785, 15065.55533862]),\n array([15214.63656495, 14961.68865696, 15081.91996435, 15081.54609959,\n        15028.80838643, 14971.77556378, 15003.28669371, 15172.473795  ,\n        15086.84958932, 15070.36424814]))\n\n\n\n    RMSLE: Cross Validation Errors (Post-Log)\n  \n\nn_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True)\ntest_cv_errors, train_cv_errors = np.empty(n_folds), np.empty(n_folds)\nX_array = np.array(X)\ny_array = np.log(np.array(y))\n\nfor idx, (train, test) in enumerate(kf.split(X)):\n    model = LinearRegression()\n    model.fit(X_array[train], y_array[train])\n    y_hat = model.predict(X_array[test])\n    y_train = model.predict(X_array[train])\n    \n    train_cv_errors[idx] = rmsle(y_array[train], y_train)\n    test_cv_errors[idx] = rmsle(y_array[test], y_hat)\n\ntrain_cv_errors, test_cv_errors\n\n\n(array([0.03681113, 0.03676828, 0.03682234, 0.03681786, 0.03676724,\n        0.03680314, 0.03682065, 0.0368372 , 0.03679243, 0.0367921 ]),\n array([0.03673228, 0.03713358, 0.03661953, 0.03666285, 0.03713166,\n        0.03680484, 0.03665091, 0.03649925, 0.03690903, 0.03691538]))\nEvaluation\nOLS Summary on Features\n \n\nRidge Regression \nWe decided that using a Ridge Regression would be the best model for this situation in order to find out which features would be most important.\ny = np.array(clean_auction[\'SalePrice\'])\nX = np.array(clean_auction.drop(columns=\'SalePrice\'))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nmodel = Pipeline([(\'standardize\', StandardScaler()),\n                   (\'regressor\', Ridge())])\n\nmodel.fit(X_train, y_train)\n\nTuning\n\nPerformance\nModel Prediction \ny_hat_train = model.predict(X_train)\ny_hat_test = model.predict(X_test)\n\nprint(\'Training error: {}\'.format(rmsle(y_train, y_hat_train)))\nprint(\'Testing error: {}\'.format(rmsle(y_test, y_hat_test)))\n\n\n\nTraining Error:\nTesting Error\n\n\n\n\n0.0368246720958709\n0.03672861629228472\n\n\n\n\n \nUsing data from the file data/test.csv, we used our model to obtain an RMSLE of 0.573.\nNote: The best RMSLE was only 0.23 (obviously lower is better).  Note that if you were to simply guess the median auction price for all the pieces of equipment in the test set you would get an RMSLE of about 0.7.\nInterpreting our Results\nThis root mean squared log error signifies that our model predicts a sale price that is within 1 order of magnitude of the actual price. Although there is plenty of room for improvement, we\'re able to get \'in-the-ballpark\' of the sale price, so that our customer will have an idea of how much their heavy machinery will sell for.\nFuture Considerations\nCleaning the data and finding the important columns was the biggest hurdle. We decided to do one-hot-encoding for multiple columns of the dataset and drop most of the other columns.\nOnce we felt that we had a data set that was cleaned and ready, using different models was quick.\nLicense\nMIT ©\nCredits\nThanks\n'], 'url_profile': 'https://github.com/boogiedev', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/subham22022000', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Burlington, Vermont', 'stats_list': [], 'contributions': '694 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jayceslesar', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Faridabad', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cosmos-ankur', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['regression\nCodes to perform Linear, multiple linear, polynomial regression using sklearn.\n'], 'url_profile': 'https://github.com/huzaifatausif0410', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '779 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/Aravindh020', 'info_list': ['3', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Aug 6, 2020', '4', 'Updated Apr 17, 2020', '2', 'Python', 'LGPL-2.1 license', 'Updated May 21, 2020', 'HTML', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '239 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ZackFra', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Linear Regression Theory:\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x).\nSo, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.\nIf we plot the independent variable (x) on the x-axis and dependent variable (y) on the y-axis, linear regression gives us a straight line\nthat best fits the data points\nThe Regression equation => Y= mx + b\nWhere b is the intercept and m is the slope of the line. So basically, the linear regression algorithm gives us the most optimal value for\nthe intercept and the slope (in two dimensions). The y and x variables remain the same, since they are the data features and cannot be\nchanged. The values that we can control are the intercept(b) and slope(m). There can be multiple straight lines depending upon the values\nof intercept and slope. Basically what the linear regression algorithm does is it fits multiple lines on the data points and returns the\nline that results in the least error.\nThe above equation can also be extended to cases where there are more than 2 variables. This is called Multiple Linear Rregression\nRegression equation => y = b0 + m1b1 + m2b2 + m3b3 + … … mnbn\nAbove equation can be used in scenarios, where target variable is dependent upon multiple independent variables.\n'], 'url_profile': 'https://github.com/saivishwanathgoud', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aishreepatra', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Ridge_Lasso_Regression\nComparison of Linear Regression vs Ridge Regression vs Lasso Regression\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['dli_gpr\nGP regression model for T-cell cluster changes in response to donor lymphocyte infusion (DLI) therapy.\nThis repository implements two models: a vanilla Gaussian process regression model for modeling tumor dynamics, and a hierarchical model for heteroscedastic noise.\nDependencies\nThe following packages are required to run the demo notebooks. They are also specified in requirements.txt.\nmatplotlib\nnumpy\npandas\ntorch\npyro-ppl\ntqdm\n\n'], 'url_profile': 'https://github.com/dpeerlab', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['regression_model_for_car_sales_price_prediction\nan ML regression model for predicting car sales prices\n'], 'url_profile': 'https://github.com/luigivendetta', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Regression\nRegression using Machine Learning models like Linear Regression,\nPolynomial Regression, Support Vector Regression,\nDecision Tree and Random Forest\nLibraries installation\nscikit Learn\npip install -U scikit-learn\nmatplotlib\npip install matplotlib\nPandas\npip install pandas\nNumpy\npip install numpy\nModels\nLinear Regression\nfrom sklearn.linear_model import LinearRegression\nPolynomial Regression\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 4)\nSupport Vector Regression\nfrom sklearn.svm import SVR\nDecision Tree\nfrom sklearn.tree import DecisionTreeRegressor\nRandom Forest\nfrom sklearn.ensemble import RandomForestRegressor\n'], 'url_profile': 'https://github.com/Anish-AV', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Surat', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Linear-regression\nLinear Regression\n'], 'url_profile': 'https://github.com/virenvvarasadiya', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Numpy-Tricks\nregression project\n'], 'url_profile': 'https://github.com/lakshmoji1', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['liveer-diease\nlogistic regression\n\n'], 'url_profile': 'https://github.com/uzairifti56', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Jun 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iamranjan', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mbelc', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Istanbul, TURKEY', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['KNN-Regression\nThe k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.\n'], 'url_profile': 'https://github.com/tekinadem', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Indore, Madhya Pradesh, India', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['Regression-Model-Predicting-Price-Of-Pre-Owned-Cars\nA Machine Learning Model that predicts price of pre-owned cars using Linear Regression and Random Forrest Regressor.\n'], 'url_profile': 'https://github.com/Sk70249', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['ml_classifiers_scratch\nLinear Regression,Logistic Regression,K means classifier from scratch,\nRegression and classification algorithms using oops\n'], 'url_profile': 'https://github.com/Nikitaa1908', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benjiPro-Bayou', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/brijesh1100', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Sweden', 'stats_list': [], 'contributions': '470 contributions\n        in the last year', 'description': ['Predict house prices\nLinear Regression Using SGD.\nLinks\nTutorial\n'], 'url_profile': 'https://github.com/helenabarmer', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Ithaca, NY', 'stats_list': [], 'contributions': '563 contributions\n        in the last year', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Boston Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Boston Housing Data again!\nThis time, let's only include the variables that were previously selected using recursive feature elimination. We included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nb = boston_features['B']\nlogdis = np.log(boston_features['DIS'])\nloglstat = np.log(boston_features['LSTAT'])\n\n# Min-Max scaling\nboston_features['B'] = (b-min(b))/(max(b)-min(b))\nboston_features['DIS'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features['LSTAT'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nX = boston_features[['CHAS', 'RM', 'DIS', 'B', 'LSTAT']]\ny = pd.DataFrame(boston.target, columns = ['target'])\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Importing and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n\n<matplotlib.legend.Legend at 0x1a24d6cef0>\n\n\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 100 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n\n<matplotlib.legend.Legend at 0x1a26e93438>\n\n\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/merb92', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KrmGL', 'info_list': ['Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '2', 'Jupyter Notebook', 'Updated Nov 30, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tohidul-Haque-Sagar', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['LogisticRegression\nLogistic Regression 분석 페이지\n'], 'url_profile': 'https://github.com/ujini', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Cat-Classification\nLogistic Regression from scratch\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Handong Global University,Pohang, Korea', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['regression_peakvalue\nregression for peak value\n'], 'url_profile': 'https://github.com/dlwlgus53', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tohidul-Haque-Sagar', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': [""Linear Regression\nefficiently linear regression O( n^2 log( √n/ε ) ) Time algorithm ( ε-accuracy ).\nan Hebrew analyze of the time running could be found here : https://github.com/dudupo/linear-regression/blob/master/svd.pdf\nI have 'copy-pasted' the document as it was, directly from assignment, so it might seem strange (for who is't an exercises examiner). I hope to update the manner in future.\ncurrently, it's not working (yet), and has a buggy nature. As a first step I focusing on demonstrate a polynomial regression. after that I planning to improve the code by constant Time factor, and 'righter cpp literary'.\nIssuses\n(I) reordering the vectors of the orthonormal matrix\nthe svd decompaction, calculating the vectors by ascending order ( matched to the eigenvalue ), currently there is no regard to the subject while assembling back.\n(II) results of polynomial regression aren't even similar to the expectations.\nTesting\nnote that you might want to decrease the size of the matrix.\nmingw32-make\npython ./scripts/svdtest_gen.py\npython ./scripts/svdtest_run.py\n\n""], 'url_profile': 'https://github.com/dudupo', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BrandtKruger', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Linear-Regression-Assignment\nAssignment of Linera Regression\n'], 'url_profile': 'https://github.com/divyasriv', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '270 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pitaconsumer', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['The-Effect-Of-Sleep-On-Exam-Scores\nLogistics Regression From Scratch\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'C++', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}"
"{'location': 'Dehradun/Gurgaon', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Regression-using-TensorFlow\nRegression using TensorFlow\n'], 'url_profile': 'https://github.com/saileshraturi', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Genetic-Programming\nGenetic Programming -> Symbolic Regression\nSymbolic Regression is a technique to find mathematical relation among given parameters or variables.\nSo here is a basic symbolic regression python code to accompolish the task.\nIt is intended to find the Newton\'s gravitational law which explain how much gravitational force (F) lies\nbetween two objects of mass m1 & m2 separated by distance r. i.e. F=(G m1 m2 )/ r^2\nThe text file ""m1m2r.txt\' contains 4 parameters i.e. 3 input variable (m1, m2 & r) and 1 target variable (F)\nwith 6351 instances. So if one feed these parameters into symbolic regressors, it should find the gravitational\nlaw equation proposed by Newtons.\nThe saved symbolic regressor model ""gravitational_law.sav"" has already found the graviational equation through\ntraining & uploading it here for reference.\nIf you wish to feed different parameters to find underlying mathematical relation among them, please modify the\ncode accordingly.\nYou may need a computer with Juputer notebook & python 3\n'], 'url_profile': 'https://github.com/Prasanth-Code', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'China_LiaoNing', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': [""Python-Nine-Chess-AI\nLogistic Regression Machine Learning.\nIt's a practice.\nUse Numpy and Pandas to build an Logistic Regression Network. (I don't know how to use TensorFlow or PyTorch)\nIt's not finished yet but could run now.\n""], 'url_profile': 'https://github.com/OrangeSun', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Techatach', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Logistic-Regression-Assignment\nAssignment for Logistic Regression\n'], 'url_profile': 'https://github.com/divyasriv', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['predictive_models\nRegression analysis for revenue\n'], 'url_profile': 'https://github.com/1370-hub', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Linear_Regression\nLinear Regression HW Assignment\n'], 'url_profile': 'https://github.com/rkantamneni', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Girish97', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tugbatasbasi', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MounikaKukudala', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Dec 12, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'JavaScript', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}"
"{'location': 'Indonesia', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ryan-26', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['How Does a Recession Affect the Elasticity of Residential Property Prices with Changes in Different Housing Characteristics?\nUsing a sample of 1460 residential properties that were sold in Iowa between 2006 and 2010, the impact of the Great Recession on the elasticity of housing prices is analysed with respect to changes in the lot area. Given the logarithm of inflation-adjusted sale prices as the dependent variable, the linear model specified in data_analysis.ipynb is comprised of 11 covariates—all of which are statistically significant at the 0.10 significance level. The model explains 80% of the variability in sale prices since hedonic features were included. There exists an interaction term between the logarithm of lot area (initially measured in square feet) and whether the property was sold during the recession. In that regard, every 10% increase in the lot area increases the sale price by 1.94% on average if there was no recession. However, given the same property during a recession, it becomes slightly less elastic: the return of lot area on the price decreases by 0.32 percentage points from 1.94% to 1.62%. From the perspective of a homeowner who is looking to sell a house, the Great Recession easily amounted to thousands of dollars in unrealized losses after controlling for hedonic features and adjusting for inflation.\n'], 'url_profile': 'https://github.com/teddythepooh', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'Munich, Germany', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['AttractiveNet\nAn End-to-End Deep Learning Tutorial in Python.\nThe Complete code is available on GitHub featuring an all-in-one jupyter notebook.\nLearnables covered in this article:\n\nDownload, unzip and store public datasets\nLoad and format image and label data\nSplit dataset into training, validation and test sets\nLoad a keras model with pretrained weights\nModify a keras model to fit a specific task (e.g. regression, classification)\nTrain a keras model in multiple stages and on multiple gpus\nuse callbacks to monitor training process\nPlot and interprete training results\nuse trained models for live inference\nRun jupyter notebooks on remote machines and access them locally\n\nIntroduction\nThis article walks through all steps necessary to be able to implement a deep learning project with TensorFlow and Keras.\nAs example use-case we are implementing and training a neural network image regressor that predicts an attractiveness score of images of human faces.\n\nExcursus: Regression predicts a continuous variable (e.g. a price like $2.10) whereas classification predicts a categorical variable (e.g. 0 or 1). Basically, a classification can also be achieved via a regression, if the continuously predicted variable is subsequently divided into distinct ranges (Example: 0.00 - 1.00 -> class 0, 1.01 - 2.00 -> class 1).\n\nThe dataset that we are using to train our regressor is the SCUT-FBP5500: A diverse benchmark database (5500 images of shape 350x350, size = 172MB) for multi-paradigm facial beauty prediction, released by Human Computer Intelligent Interaction Lab of South China University of Technology, which is publicly avialble on GitHub.\n\nOpinion: The first thing one notices when looking at the data is that there are no pictures of people of color and moreover a bias of the evaluation regarding the beauty ideal of the asian area cannot be excluded. Therefore, the numeric result of the individual prediction should not be our primary objective, but we should rather focus on the technology enabling it. For this reason I decided not to post pictures of any faces with related attractiveness score. But of course you can do this on your own using what you learn in this article.\n\nThe model we are using is MobileNetV2 which is a convolutional neural network developed by google. The MobileNet model series are known to come very handy in size and are therefore quick in training and inference, while achieving state-of-the art performances.\nFurthermore we are going to modify the model slighty to fit our task.\nFinally besides the tensorflow.keras library that lifts the main part of our tutorial, this article covers the following usefull libraries that you may not know before but can be helpful in future projects.\nzipfile36\ngdown\nglob2\nsklearn\nmatplotlib\nalt_model_checkpoint\njupyter\nopencv-python\n\nTo complete the use-case we are having a look on how to run a jupyter notebook on a remote server and access it on your local machine via ssh tunneling.\nStep 1: Getting the data\nAs this is a End-to-End tutorial our first step is to get the data, which means we need to download, unzip and store the images plus labels from the given database url which in this case lays on google drive https://drive.google.com/uc?id=1w0TorBfTIqbquQVd6k3h_77ypnrvfGwf.\n\nAs you can see in the screenshot above, the database has its individual format and structure, containing lots of additional stuff we don\'t need.\nTherefore we are writing a function download_data() that not just simply downloads the archive, but uses a function extract_zipfile() to filter only the necessary files (image files with .jpg suffix  and the label file All_labels.txt) and stores them locally in this repository under data/ while unzipping.\nDATA_DIR = \'data/\'\nLABELS_FILE = \'All_labels.txt\'\nDATA_URL = \'https://drive.google.com/uc?id=1w0TorBfTIqbquQVd6k3h_77ypnrvfGwf\'\nZFILE = \'SCUT-FBP5500_v2.1.zip\'\n\ndef download_data():\n    # Download Dataset\n    if os.path.isfile(ZFILE) or os.path.isfile(DATA_DIR+LABELS_FILE):\n        print(\'data already downloaded\')\n    else:\n        print (""data does not exist. downloading it."")\n        gdown.download(DATA_URL, ZFILE, quiet=False)\n    # Extract ZipFile\n    if os.path.isfile(DATA_DIR+LABELS_FILE):\n        print(""data already extracted."")\n    else:\n        print(""extracting data."")\n        if not os.path.exists(DATA_DIR):\n            os.mkdir(DATA_DIR)\n        extract_zipfile()\n        os.remove(ZFILE)\n\ndef extract_zipfile():\n    with zipfile.ZipFile(ZFILE) as zip_file:\n        for member in zip_file.namelist():\n            filename = os.path.basename(member)\n            # skip directories, all non-jpgs, except labels\n            if filename.endswith("".jpg"") or filename == LABELS_FILE:\n                # copy file (taken from zipfile\'s extract)\n                source = zip_file.open(member)\n                target = open(os.path.join(DATA_DIR, filename), ""wb"")\n                with source, target:\n                    shutil.copyfileobj(source, target)\nFurthermore the implementation of download_data() ensures that you won\'t repeat this step over and over again if the files are already available from a previous run. File existance is checked via os.path.isfile().\nStep 2: Load image and label data\nAfter successfully getting the data the next step is to load all images (Input X) and labels (Target y) as variables into memory as numpy arrays as this is the format that machine learning libraries like tensorflow and keras expect.\n\nExcursus: An array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.\n\nTo achieve this we are writing a function create_dataset() that reads all image and label files in our data directory and stores them in the variables X and y.\ndef create_dataset(target_size):\n    X = []\n    y = []\n    labels_dict = get_labels_dict()\n    img_files = glob.glob(DATA_DIR+\'*.jpg\') # glob library to get all files in path as list\n    print(f\'reading {len(img_files)} images into dataset\')\n    for f in img_files:\n        img = preprocess_image(cv2.imread(f), target_size) # open-cv library to read and transform images\n        X.append(img)\n        y.append(labels_dict[os.path.split(f)[-1]])\n    return np.array(X), np.array(y)\nThis function makes use of two more custom functions. Once get_labels_dict() which reads all image names and according labels from the text file All_labels.txt and returns them in a dictionary object as key:value pairs.\nThe file All_labels.txt contains a line for each image in the datasat containing the image file name and the ""attractiveness score value"" as label in the form of CF437.jpg 2.883333.\ndef get_labels_dict():\n    labels_dict = {}\n    with open(DATA_DIR + LABELS_FILE) as fp:\n        for line in fp:\n            # each line looks like: CF437.jpg 2.883333\n            img,label = line.split(\' \', 1)\n            labels_dict[img] = float(label)\n    return labels_dict\nSecond it uses a function preprocess_image() which does multiple transformations on the raw image to convert it to the desired format using the open-cv library.\ndef preprocess_image(image,target_size):\n    return cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB),target_size) / .255\nThe transformations include:\n\nconversion from BGR to RGB color format\nresizing of the image to the desired target size\npoint wise devision of each pixel by 255 (neural networks perform better on normalized data with values between 0 and 1)\n\nStep 3: Prepare data for training\nAfter loading the input images and labels to the variables X and y\ntarget_size = (350,350)\nX,y = create_dataset(target_size)\nwe are splitting the data into training, validation and test data.\nTherefore we use the function train_test_split() of the sklearn library.\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=1/3, random_state=42)\nresulting in following split:  70% Training, 20% Validation and 10% Test data.\nNow our data has the following shape:\nX_train shape: (3960, 350, 350, 3), y_train shape: (3960,)\nX_val shape: (1100, 350, 350, 3), y_val shape: (1100,)\nX_test shape: (440, 350, 350, 3), y_test shape: (440,)\nFurthermore fixing the random_state ensures we are always getting the same split results making our project reproducible.\nThe following step is not necessary but highly usefull if you want to further increase and diversify your training data:\nFirst we create ImageDataGenerator objects for training as well for test and validation splits whereas only the training split recieves data augmentation arguments, as validation and test data should never be modified.\n\nExcursus: Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n\nThe ImageDataGenerator class is part of the keras.preprocessing.image modul.\ntest_val_datagen = ImageDataGenerator()\ntrain_datagen = ImageDataGenerator(horizontal_flip=True,\n                                   rotation_range=40,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2)\nSecond we flow the generators by passing the data and desired batch_size.\nbatch_size = 32\n\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\nval_generator = test_val_datagen.flow(X_val, y_val, batch_size=batch_size)\ntest_generator = test_val_datagen.flow(X_test, y_test, batch_size=batch_size, shuffle=False)\nThis marks our last data preparation step as these generators can be passed directly to the keras built-in model.fit() and model.predict() methods. You may have noticed that the test_generator data has not been shuffled, the reason is that we need the test data unchanged as it is to be able to manually calculate the prediction metric in step 7.\nStep 4: Build the model\nBuilding a model with keras is the easiest part of the whole project.\nWe simply import the model of our choice from the modul keras.applications, in our case that\'s MobileNetV2.\nWhen creating the model object we pass the arguments include_top=False, pooling=\'avg\', weights=\'imagenet\' which tells keras we are not interested in the model head (we will add our own), we want an average pooling function to be used at the end and as pretrained weights we choose the ones trained on ImageNet.\n\nExcursus: Imagenet is the benchmark computer vision dataset. It contains over 14M images with over 21k annotated classes. Most modern deep learning models build and trained for any computer vision task are initially pre-trained on ImageNet so that the network learns to ""understand"" image data.\n\nThe weights are downloaded once when we create the basemodel the first time, in every following call they are loaded from your local storage.\nbasemodel = MobileNetV2(include_top=False, pooling=\'avg\', weights=\'imagenet\')\nNext we create a Sequential model which is the easiest way of creating a model in keras, append our basemodel and add a fully connected layer Dense() with a single output at the top (our new head of the network).\nYou can imagine a Sequential model as a model where you can just stack model components on top of each other that are added sequentially / layer wise.\nThe single dense / fully connected layer at the top of our model results in one single continous output variable, which is exactly what we want in a regression task.\nmodel = Sequential()\nmodel.add(basemodel)\nmodel.add(Dense(1))\nStep 5: Multi Stage Training\nWe are training the model in two stages. First we only train the custom model head, which is the single fully connected layer.\nAfter this layer is trained until conversion we re-train the whole model (including the weights of MobileNetV2 which are pre-trained on ImageNet) with a reduced learning rate.\nStage 1:\nFirst we define our training parameters learningin rate lr to be 0.001 and training for 30 epochs\nepochs = 30\nlr=0.001\nThen we set the first layer of our model (which is the basemodel MobileNetV2) to be not trainable.\nAfter that we compile our model to use Mean Squared Error (MSE) as loss function and Adam as optimizer, passing our learning rate.\nmodel.layers[0].trainable = False\nmodel.compile(loss=\'mean_squared_error\', optimizer=Adam(learning_rate=lr))\nCalling model.summary() shows us detailed information about our model. As you can see it consists of two layers and in this stage has only 1281 trainable paramters which are the ones of the final dense (or fully connected) layer. So this confirms that everything is how we want it and we can proceed.\nModel: ""attractiveNet_mnv2""\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmobilenetv2_1.00_224 (Model) (None, 1280)              2257984   \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 2,259,265\nTrainable params: 1,281\nNon-trainable params: 2,257,984\nIf we are training our model on multiple GPUs we need to create and compile a second mulit-gpu model.\nIn this case we are creating a gpu model that runs on 4 GPUs (modify this to your needs) by passing the normal model as argument.\nparallel_model = multi_gpu_model(model, gpus=4)\nparallel_model.compile(loss=\'mean_squared_error\', optimizer=Adam(learning_rate=lr))\nNext we define our callbacks that we will pass to the model.fit() method in the actual training call.\nHere it is important to note that we are not passing the default keras ModelCheckpoint class, but instead the AltModelCheckpoint which we import from the alt_model_checkpoint.tensorflow modul. This allows us to safe checkpoints of our normal model, while training the gpu model.\nFurthermore we add EarlyStopping, as we want only the best model at the earliest point in training.\ncallbacks = [\n    EarlyStopping(\n        monitor=\'val_loss\',\n        min_delta=1e-3,\n        patience=7,\n        verbose=1,\n        ),\n    AltModelCheckpoint(\n        model_path,\n        model,\n        monitor=\'val_loss\', \n        verbose=1, \n        save_best_only=True, \n        save_weights_only=False, \n        )]\nFinally we call train our model with the model.fit() method, passing all our predefined arguments including train and validation data generators\nhistory1 = parallel_model.fit(\n    train_generator,\n    epochs=epochs,\n    steps_per_epoch=len(y_train) // batch_size,\n    validation_data=val_generator,\n    verbose=1,\n    callbacks = callbacks)\nthe history object we are getting back from the training call includes all training metrics which we are using to visualize our training results in step 6.\nStage 2:\nWe are basically doing the same as in Stage 1, with the diffefence that we decrease the learning rate by a factor of ten and make the whole model trainable.\nSo we re-use the complete code from stage 1, but modify those two lines:\nlr=0.0001\nmodel.trainable = True\nStep 6: Visualize Training Results\nTo visualize the training results (after each stage) we write a function plot_metrics() that plots both the training loss and the validation loss over time. Both losses are included in the history object which is returned from a finished model.fit() training run. For this purpose we use use the modul pyplot of the matplotlib library.\nDOCS_DIR = \'docs/\'\ndef plot_metrics(history, model_name, stage):\n    f,(ax1) = plt.subplots(1, 1, figsize=(15,7))\n    f.suptitle(f\'Stage {stage} Model ""{model_name}"" training Metrics\')\n    ax1.plot(history.history[""loss""], color=\'darkblue\', label=""Train"")\n    ax1.plot(history.history[""val_loss""], color=\'darkred\', label=""Test"")\n    ax1.set_title(\'Loss (Mean Squared Error) over epoch\')\n    ax1.set_xlabel(\'Epoch\')\n    ax1.set_ylabel(\'Loss (MSE)\')\n    ax1.legend()\n    plt.show()\n    plt.savefig(f\'{DOCS_DIR}metrics_stage_{stage}_{model_name}\')\nwhich produces plots that look like the following and are furthermore saved locally under docs/ folder.\n\nStep 7: Testing\nTo test a desired model we first load it from file and then call model.predict() on it, passing the previously created test_generator object containing the test data which our model did not see yet during the training process.\nmodel = load_model(model_path)\nprediction = model.predict_generator(test_generator)\nWhen the prediction of the complete test dataset is finished we calculate the metrics Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) which are among the most meaningfull and therefor the most often used metrics for regression tasks.\n\nExcursus: Choosing the right metrics to a specific machine learning task to be able to make meaningful statements about the quality of your model is an often underrated problem. Here is a small overview about when to choose which metrics .\n\nBoth RMSE and MAE are implemented in the metrics modul of sklearn.\nRMSE = mean_squared_error(y_test, prediction, squared=False)\nMAE = mean_absolute_error(y_test, prediction)\nThe model that was trained using the notebook to this article and which is also included to the github repository achieves\nRMSE: 0.28585716016204066\nMAE: 0.21198338088908802\nwhich is even slightly better than the results achieved by the original database authors with other models like AlexNet, ResNet-18 and ResNeXt-50.\n\nStep 8: Live Inference\nTo write a quick live inference script using our webcam as input we first need to load our model\nmodel_path = \'models/attractiveNet_mnv2.h5\'\nmodel = load_model(model_path)\nThen we use OpenCV to capture our video frame (0 means our built-in webcam as input, but you can pass any video file or source).\ncap = cv2.VideoCapture(0)\nAfter that we start an infinite loop capturing video frames, feeding them to the model and visualizing the result on the displayed stream.\nwhile(True):\n    ret, frame = cap.read()\n    score = model.predict(np.expand_dims(helper.preprocess_image(frame,(350,350)), axis=0))\n    text1 = f\'AttractiveNet Score: {str(round(score[0][0],1))}\'\n    text2 = ""press \'Q\' to exit""\n    cv2.putText(frame,text1, (10,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2, cv2.LINE_AA)\n    cv2.putText(frame,text2, (10,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2, cv2.LINE_AA)\n    cv2.imshow(\'AttractiveNet\',frame)\nStill in the loop we add a break condition for hitting the Q key.\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\nFinally after the loop we release the video capture and destroy all opencv windows.\ncap.release()\ncv2.destroyAllWindows()\nAdd-On: Running jupyter notebooks on remote servers\nAs a lot of us do not have powerfull gpu machines at home it often makes sense to run a jupyter notebook executing the training job on a remote server hosted from cloud service providers like AWS.\nLaunch a jupyter notebook on the server with the following command:\njupyter notebook --no-browser --port=XXXX --ip 0.0.0.0\n\nand then connect to that server via ssh tunneling on your local machine.\nssh -N -f -L localhost:YYYY:remotehost:XXXX remoteuser@remotehost\n\nXXXX represents the port where the notebook is running on server side and YYYY represents the port on your local machine.\nYou could set both to 8888, which is the default port for jupyter applications.\nremotehost and remoteuser represent the credential you are using to log into the server.\nFinally open a new browser tab with localhost:YYYY where jupyter will prompt you to input a session token.\nThis token can be found in the console/terminal under the jupyter command you executed on the server.\nIf you closed that console already you can check for running notebooks with jupyter notebook list\nFinal Words\nNo final words. Just get on GitHub, clone this repository and get started yourself.\nI hope you enjoyed it.\n'], 'url_profile': 'https://github.com/gustavz', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'China ', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['tfLinear-regression\n#TensorFlow version 1.14\n#Python version 3.7\n#使用TensorFlow1.14进行线性回归拟合\n#本人微信公众号：AI初学者的学习笔记\n#有完整的教程以及代码说明\n#导入pycharm可直接运行出线性回归的动态拟合效果\n'], 'url_profile': 'https://github.com/qianyuqianxun-DeepLearning', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': [""IPSO_GRU-Regression\nUsing tensorflow1.x/python to implement a IPSO_GRU for regression，IPSO's here, it was mainly uesd to optimize hyper-parameters includess learning rate ,hidden_layer's number.\n使用python下tensorflow编写改进的粒子群优化两个隐含层的GRU用于回归拟合，需要的可以加我qq 2919218574，收费非白嫖\n1，数据格式：采用前几列为输入，最后一列为输出\n\n2,PSO对比改进后的PSO用于Sphere函数极值寻优\n\n3,LSTM用于回归拟合\n\n4,GRU用于回归拟合\n\n5,IPSO-GRU用于回归拟合\n5.1 IPSO优化结果\n\n5.2 采用IPSO优化的结果训练GRU\n\n6，对比\n\n""], 'url_profile': 'https://github.com/fish-kong', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['\n\nMachine Learning Regression Project\n\n You can click SalesPrediction.ipynb to directly see the results\n\n\n'], 'url_profile': 'https://github.com/AuliaAPratama', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'Greenville, NC, USA', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['ECU Physics Labs Jupyter Notebooks for Regression Analysis in Python\nThis repository contains Jupyter notebooks that use Python to do linear regression analysis to use for data analysis for introductory physics labs at ECU.\n\n\nRun on Binder:  No account required, but it may take time for the configuration to load and build. Once the configuration finishes building, open the notebook RegressionPy.ipynb.\n\n\nRun on Azure:  Sign in with Microsoft (or ECU) account. Follow the prompts to import the repository.  Click the run button, and then open the notebook RegressionPy.ipynb.\n\n\n'], 'url_profile': 'https://github.com/sprague252', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Machine-Learning-with-Python\nRegression, Classification, Clustering, Recommendation Systems\n'], 'url_profile': 'https://github.com/Sadabrata', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['titanic_survivor_prediction\nA Simple Linear Regression Model\n'], 'url_profile': 'https://github.com/SreedeepRVS', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Linear Regression\nLinear regression using gradient descent.\nReferences / Inspiration\nMenon, A., 2018. Linear Regression Using Gradient Descent. [online] towardsdatascience. Available at: https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931 [Accessed 17 April 2020].\nKayan, R., 2018. Years Of Experience And Salary Dataset. [online] kaggle. Available at: https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset [Accessed 17 April 2020].\n'], 'url_profile': 'https://github.com/hkattt', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Nov 28, 2020', '4', 'Jupyter Notebook', 'MIT license', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 17, 2020', '2', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PPendri', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Vietnam', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mrtrunghieu1', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Edmonton, Alberta, Canada', 'stats_list': [], 'contributions': '181 contributions\n        in the last year', 'description': ['Tensorflow-Class and multiclass\nLogistic regression model using tensorflow\n'], 'url_profile': 'https://github.com/GustavoAlcaraz', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['PS9 lite: Linear regression via linear algebra\nYour work\n\nYou will compute least-squares fitted beta-hat values using linear\nregression.\nThis problem is very short and straightforward, so there will be no\ngroups.\n\nWhat’s due when?\n\nBy Friday 4/24 9pm Eastern (note evening due time) you must\nsubmit all your work on GitHub.\nYou will submit your work not via GitHub pull request, but rather as\nyou have for all other problem sets.\n\nEvaluation criteria\n\n“If others can’t reproduce your work, they will throw it in the\ntrash.” Submissions that don’t knit will be penalized harshly.\n“Presentation and communication-style matters.” Related to point\nabove, for example\n\nPlots: Keep the “ink-to-information” ratio in mind. Ensure your\nplots have labeled axes and informative titles.\nUse markdown formatting to make your presentation effective.\nIs code cleanly written, well-documented, and well-formatted. As\nthe semester progresses, I’ll be giving feedback from The\ntidyverse style guide.\n\n\nDid you get the right answer?\n\n'], 'url_profile': 'https://github.com/293-master', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BrandtKruger', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Madison, WI', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dpmaloney', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Ithaca, NY', 'stats_list': [], 'contributions': '563 contributions\n        in the last year', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/merb92', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['polynomial_regression\npolynimial regression model using sckilearn\n'], 'url_profile': 'https://github.com/shobanapriyan', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Predictive Analytics Sales\nPredictive analytics using liner regression modal for sales and revenue.\nAWS Lambda integration\n'], 'url_profile': 'https://github.com/uxify', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Regression-Models\nDifferent types of Regression Models:\n\nSimple Linear regression is used to predict the Salaries of a set of people based on their attribute.(Salary_data.csv)\nMultiple Linear Regression is used to predict the Profits of 50 Startups based on their features.(50_Statrtups.csv)\nPolynomial, Support vector, Decision tree, Random Forest Regression models are used to predict the salary of a person. (Salaries.csv)\n\n'], 'url_profile': 'https://github.com/Incursion-beta', 'info_list': ['JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'HTML', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 14, 2020', 'Go', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 16, 2020']}"
"{'location': 'Doha,Qatar', 'stats_list': [], 'contributions': '328 contributions\n        in the last year', 'description': ['Bulldozer-price\n'], 'url_profile': 'https://github.com/s4rv4d', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'tokyo, japan', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['Polynomial-Regression\nproject model using polynomial regression\n'], 'url_profile': 'https://github.com/Mayaz9156', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '297 contributions\n        in the last year', 'description': [""Project\nPython project on logistic regression\nProject Structure\nThe hands on project on Logistic Regression with NumPy and Python is divided into the following tasks:\nTask 1: Introduction and Project Overview\nIntroduction to the data set and the problem overview.\nSee a demo of the final product you will build by the end of this project.\nIntroduction to the Rhyme interface.\nTask 2: Load the Data and Import Libraries\nLoad the dataset using pandas.\nImport essential modules and helper functions from NumPy and Matplotlib.\nExplore the pandas dataframe using the head() and info() functions.\nTask 3: Visualize the Data\nBefore starting on any task, it is often useful to understand the data by visualizing it.\nFor this dataset, we can use a scatter plot using Seaborn to visualize the data, since it has only two variables: scores for test 1, scores for test 2.\nTask 4: Define the Logistic Sigmoid Function 𝜎(𝑧)\nWe can interpret the output of the logistic sigmoid function as a probability, since this function outputs in the range 0 to 1 for any input.\nWe can threshold the function at 50% to make our classification.\nIf the output is greater than or equal to 0.5, we classify it as passed, and less than 0.5 as failed.\nThe maximal uncertainty, we can easily see if we plug in 0 as the input. So when the model is most uncertain it tells you the data point has a 50% probability of being in either of the classes.\nWe’re going to be using this function to make our predictions based on the input.\nTask 5: Compute the Cost Function 𝐽(𝜃) and Gradient\nNow that we have defined the logistic sigmoid, we can go ahead and define the objective function for logistic regression.\nThe mathematics of how we arrived at the result is beyond the scope of this project. But I highly recommend you do some reading on your own time.\nWe can use the standard tool from convex optimization, the simplest of which is gradient descent to minimize the cost function.\nTask 6: Cost and Gradient at Initialization\nBefore doing gradient descent, never forget to do feature scaling for a multivariate problem.\nInitialize the cost and gradient before any optimization steps.\nTask 7: Implement Gradient Descent from scratch in Python\nRecall that the parameters of our model are the 𝜃_j values.\nThese are the values we will adjust to minimize the cost J(𝜃).\nOne way to do this is to use the batch gradient descent algorithm.\nIn batch gradient descent, each iteration performs the following update.\nWith each step of gradient descent, the parameters 𝜃_j come closer to the optimal values that will achieve the lowest cost J(𝜃).\nSince we already have a function for computing the gradient previously, let’s not repeat the calculation and add on an alpha term here to update Θ.\nLet’s now actually run gradient descent using our data and run it for 200 iterations.\nThe alpha parameter controls how big or small of a step you take in the direction of steepest slope. Set it too small, and your model may take a very long time to converge or never converge. Set it too large and your model may overshoot and never find the minimum.\nTask 8: Plotting the Convergence of 𝐽(𝜃)\nLet’s plot how the cost function varies with the number of iterations.\nWhen we ran gradient descent previously, it returns the history of J(𝜃) values in a vector “costs”.\nWe will now plot the J values against the number of iterations.\nTask 9: Plotting the Decision Boundary\nLet's over the scatterplot from Task 3 with the learned logistic regression decision boundary.\nTask 10: Predictions Using the Optimized 𝜃 Values\nIn this final task, let’s use our final values for 𝜃 to make predictions.\n""], 'url_profile': 'https://github.com/heisenberg01010', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': [""Coursera_ML_Andrew-Ng_ex1\nThis is my codes for Andrew Ng's Machine Learning course, Excercise 1.\nIn this part of this exercise, Linear Regression with one variable will be implemented to predict profits for a food truck\nInstructions are provided in ex1.pdf.\n""], 'url_profile': 'https://github.com/farnazage', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'Udupi', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['Important MACHINE LEARNING Algorithms\nthis repository consists of 3 ML algorithms i.e KNN, K-means and locally weighted regression\nby executing this program you can understand how these algorithm works and where we can use these algorithms.\n'], 'url_profile': 'https://github.com/sunilshetty07', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['Flood-Inundation-prediction-using-GIS\n'], 'url_profile': 'https://github.com/imrahul361', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Automatic kidney segmentation in ultrasound images using subsequent boundary distance regression and pixelwise classification networks-TensorFlow\nThis is an implementation of the proposed model in TensorFlow for kidney ultrasound image segmentation.\nModel Description：\nWe first use deep neural networks pre-trained for classification of natural images to extract high-level image features from US images.\nThese features are used as input to learn kidney boundary distance maps using a boundary distance regression network and the predicted boundary distance maps are classified as kidney pixels or non-kidney  pixels using a pixelwise classification network in an end-to-end learning fashion. We also adopted a data augmentation method based on kidney shape registration to generate enriched training data from a small number of US images with manually segmented kidney labels.\nWe refer to the boundary distance regression network followed by post-processing for segmenting kidneys as a boundary detection network (Bnet).\nFor more details on the underlying model please refer to the following paper:\n@article{yin2020automatic,\ntitle={Automatic kidney segmentation in ultrasound images using subsequent boundary distance regression and pixelwise classification networks},\nauthor={Yin, Shi and Peng, Qinmu and Li, Hongming and Zhang, Zhengqiang and You, Xinge and Fischer, Katherine and Furth, Susan L and Tasian, Gregory E and Fan, Yong},\njournal={Medical Image Analysis},\nvolume={60},\npages={101602},\nyear={2020}}\nRequirements：\nThe proposed networks were implemented based on Python 3.7.0 and TensorFlow r1.11\nTraining：\nWe initialized the network from the VGG_16.npy\nTraining the end-to-end learning of subsequent segmentation networks: train_end_to_end.py\nTraining the Bnet: train_Bnet.py\nEvaluation:\nEvaluating the end-to-end learning of subsequent segmentation networks: evaluate_end_to_end.py\nEvaluating the Bnet: evaluate_Bnet.py\nData augmentation:\ndata augmentaion/main_preprocessing.m\nThe dataaugmentation code was implemented based on Matlab R2015b\n'], 'url_profile': 'https://github.com/YS181818', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'Potomac, MD', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['mlb-game-prediction\nthesis code\nUniversity of Pennsylvania\nEAS 499, Senior Capstone Thesis\nAndrew Cui\nAdvisor: Dr. Shane T. Jensen\n\nWe use these models in a predictive analysis of Major League Baseball games, extracting data from Retrosheet logs and performing extensive data wrangling, preprocessing and feature engineering to identify smart covariates to use. We targeted binary classification of whether a game would be won by the home team or not.\nOverall, the logit elastic net model scored an accuracy of 61.77%, exceeding our naive classifiers and many examples from the literature. This repository details the code bank that was used in analysis, including relevant charts and graphics used.\nFurther detail about the analytical approach can be found in the paper itself. Please direct questions to Andrew Cui (andrewc@seas.upenn.edu)\n'], 'url_profile': 'https://github.com/andrew-cui', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""N67\nImprove members data science skills with a Kaggle's challenge.\nProject Organization\n├── LICENSE\n├── Makefile           <- Makefile with commands like `make data` or `make train`\n├── README.md          <- The top-level README for developers using this project.\n├── data\n│\xa0\xa0 ├── interim        <- Intermediate data that has been transformed.\n│\xa0\xa0 ├── processed      <- The final, canonical data sets for modeling.\n│\xa0\xa0 └── raw            <- The original, immutable data dump.\n│\n├── models             <- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n│\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n│\n├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n│\xa0\xa0 └── figures        <- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze > requirements.txt`\n│\n├── src                <- Source code for use in this project.\n│\xa0\xa0 ├── __init__.py    <- Makes src a Python module\n│   │\n│\xa0\xa0 ├── clean.py\n│\xa0\xa0 ├── feature_selection.py\n│\xa0\xa0 │── predict_model.py\n│\xa0\xa0 │── train_model.py\n│   │\n│\xa0\xa0 └── visualization  <- Scripts to create exploratory and results oriented visualizations\n│\xa0\xa0     └── visualize.py\n│\n\n\nProject based on the cookiecutter data science project template. #cookiecutterdatascience\n""], 'url_profile': 'https://github.com/neo-empresarial', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['RegressionModel-\nContext\nYou work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:\nIs an automatic or manual transmission better for MPG\nQuantify the MPG difference between automatic and manual transmissions\nQuestion\nTake the mtcars data set and write up an analysis to answer their question using regression models and exploratory data analyses.\nYour report must be:\nWritten as a PDF printout of a compiled (using knitr) R markdown document.\nBrief. Roughly the equivalent of 2 pages or less for the main text. Supporting figures in an appendix can be included up to 5 total pages including the 2 for the main report. The appendix can only include figures.\nInclude a first paragraph executive summary.\nUpload your PDF by clicking the Upload button below the text box.\nPeer Grading\nThe criteria that your classmates will use to evaluate and grade your work are shown below.\nEach criteria is binary: (1 point = criteria met acceptably; 0 points = criteria not met acceptably)\nYour Course Project score will be the sum of the points and will count as 40% of your final grade in the course.\nGrading Questions\nDid the student interpret the coefficients correctly?\nDid the student do some exploratory data analyses?\nDid the student fit multiple models and detail their strategy for model selection?\nDid the student answer the questions of interest or detail why the question(s) is (are) not answerable?\nDid the student do a residual plot and some diagnostics?\nDid the student quantify the uncertainty in their conclusions and/or perform an inference correctly?\nWas the report brief (about 2 pages long) for the main body of the report and no longer than 5 with supporting appendix of figures?\nDid the report include an executive summary?\nWas the report done in Rmd (knitr)?\n'], 'url_profile': 'https://github.com/monicajawahar', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '2', 'MATLAB', 'Updated Sep 21, 2020', '2', 'Python', 'Updated Apr 13, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Python', 'Updated Apr 16, 2020', '2', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Apr 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""RegressionLinear\nPrédiction de prix d'une maison\n""], 'url_profile': 'https://github.com/LBSF12', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'San Antonio, Texas', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danbar0', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gsheng0', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'Ghaziabad,India', 'stats_list': [], 'contributions': '259 contributions\n        in the last year', 'description': ['MNIST-with-Softmax-Regression\nSoftmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes.\n'], 'url_profile': 'https://github.com/Ayushi13598', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': ['E-commerce Linear Regression Project\nTable of Contents\n\nAbout the Project\nLibraries used\nDataset used\nBuilt on\nQuestions answered\nModel Training and Testing Steps\nLive Demo\nAckowledgements\nAuthor\n\nAbout the Project\nExploratory Data Analysis of a small e-commerce dataset to answer the problem statement given.\nThe python notebook ""02-Linear Regression Project"" contains an initial EDA of data from the hypothetical e-commerce platform and analyzes how different features impact the company\'s sales.\nIt also suggests actionable insights as well as the features to concentrate on in order to increase their sales. We also offer different ways to answer the problem statement and train a linear regression model to be able to predict ""Yearly Amount Spent"" as well as to analyze the coefficients of our model.\nLibraries used\n\nNumpy\nPandas\nMatplotlib\nSeaborn\nsklearn\n\nimport numpy as np                                                \nimport pandas as pd                                               \nimport pandas_profiling\nimport matplotlib.pyplot as plt\nimport seaborn as sns            \n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn import metrics \nDataset used\n\nPerian Data - Ecommerce Customers\n\nBuilt with\n\nJupyter Notebook\n\nQuestions answered\n\nHow does the amount a customer spends per year correlate to the time they spend on the website?\nHow does the amount a customer spends per year correlate to the time they spend on the mobile app?\nHow does the average amount of time a customer spends on the mobile application correlate to the length of their membership?\nHow does the average amount of time a customer spends on the website correlate to the length of their membership?\nHow does the average session length of a customer correlate to the amount that they spend in a year?\nHow does the length of membership influence the amount spent per year?\n\nModel Training and Testing Steps\n\nTraining the Model\nPredicting the Test Data\nEvaluating the Model\nExaminnig the Residuals\nAnalyzing our Coefficients\n\nLive Demo\nBelow is a link that will direct you to the live project which contains input fields for each independant variable and a predict button which, when clicked, will use the model we have trained to output a value of estimated yearly expenditure of a customer.\nLive Demo - Demo\nAckowledgements\n\nPerian Data - Dataset\n\nAuthor - Sharan Sukesh\n'], 'url_profile': 'https://github.com/SharanSukesh', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Supervised-Machine-Learning-Techniques\nProjects related to Linear Regression, Logistic Regression, Regularization, PCA and Time Series implemented in R.\n'], 'url_profile': 'https://github.com/Indrajit1996', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'Johor | Kuala Lumpur ', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aqillakhamis', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': [""KC Housing Regression\nProject Purpose and Description\nIn this project, we looked at the the various features that influence the price of a home in King's County.  We analyzed these features, developed and tested models that would predict the price of the home.\nTools (all using Python and its various libraries)\n\nPandas\nNumpy\nSeaborn\nMatplotlib\nScikit Learn\n\nData:\nThe data used was in the form of a csv from Kaggle.\n Features \nOver 20,000 data points\nid, date, price, bedrooms, bathrooms, sqft_living, sqft_loft, floors, waterfront, view, condition, grade, sqft_above, sqft_basement, yr_built, yr_renovated, zipcode, latitude, and longitude.\n\nTaking a Closer Look at the Data\n\nModeling\nAfter cleaning the data, examining various correlations, feature engineering, and determining it the dataset was a good candidate for a Linear Regression, we were able to run a model and achieve an accuracy rate of 77%.\n\n\nProbability Plot\n\nConclusion and Future work\nIn our final model, we achieved an adjusted r-squared value of 85% using a Ridge Regression.  Using the evaluation metric mean absolute error, we were able to pedict the price of a home within roughly $100,000.  The feature that was most important to the analysis was the zip code.  Homes from a specific neighborhood or area tended to cost more. Although not excellent, the result was adequate.  In the future, we would like to explore using other models such as the Lasso Regression or a Neural Network, as well as exploring the effect of grouping zip codes into specific and measurable zones:  upon the grouping, we would create an interactive geomap of the most expensive areas.\n\n""], 'url_profile': 'https://github.com/eddiecp426', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'Campina Grande ', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': [""Welcome to RWLmodel pages\nThe RWLmodel package fits regression model using the RWL model as gamlss in the gamlss package.\nInstall\nTo install the RWLmodel package run the following command\ndevtools::install_github('santosneto/RWLmodel')\n\nSupport or Contact\nsantosnetoce@protonmail.com\n""], 'url_profile': 'https://github.com/santosneto', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['covid19regression\nRegression Analysis of Epidemiological COVID-19 Infections\nR^2 Best Fit Normal and Cumulative Distribution Functions\nChina\n\n\n\nUSA\n\n\n\nItaly\n\n\n\nNew York\n\n\n\nNew Jersey\n\n\n\nMichigan\n\n\n\nMaryland\n\n\n\n'], 'url_profile': 'https://github.com/QuantumAnalysisGroup', 'info_list': ['Python', 'Updated Jan 10, 2021', 'Python', 'Updated Apr 19, 2020', 'Java', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '2', 'Jupyter Notebook', 'Updated Sep 19, 2020', 'HTML', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '253 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/funkey', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '330 contributions\n        in the last year', 'description': ['review-sentiment-classifier-LR\nReview Sentiment Classifier using Logistic Regression\n'], 'url_profile': 'https://github.com/junmoan', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'Copenhagen ', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['Probabilistic-Graphical-Models-Classification-Model\nThis project aims at implementing binary logistic/probit regression, multinomial logistic regression and hierarchical logistic regression models on three different datasets.\n'], 'url_profile': 'https://github.com/Meghdad-DTU', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'lahore', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Msalmannasir', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ahmedmoawad124', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'Mar del Plata', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/martinsilvaing', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Data set\nhttps://www.kaggle.com/mirichoi0218/insurance\n'], 'url_profile': 'https://github.com/kunaljain94', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'Pune, Maharashtra', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Linear-regression-Andrew-NG-ML-course\nUni and Multi variable linear regression\n'], 'url_profile': 'https://github.com/nishitaagrawal', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TorokLev', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['XOLAIR_logit\nLogistic Regression of XOLAIR ATU Data\n'], 'url_profile': 'https://github.com/chiragshahCBP', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TorokLev', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['XOLAIR_logit\nLogistic Regression of XOLAIR ATU Data\n'], 'url_profile': 'https://github.com/chiragshahCBP', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hotafrosauce1', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['Logistic-Regression\nImplementation of logistic regression from scratch.\n'], 'url_profile': 'https://github.com/mohit138', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hnchang', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Herndon, 20171, VA', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['bikesharing_regression\nRegression analysis on UCI bike sharing dataset\n'], 'url_profile': 'https://github.com/apy444', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Machine-Learning-Linear-Regression\nEstimation of House Price Using Linear Regression.\nLinear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on – the kind of relationship between dependent and independent variables, they are considering and the number of independent variables being used.\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.\nOverfitting\nOverfitting means that model we trained has trained “too well” and is now, well, fit too closely to the training dataset. This usually happens when the model is too complex (i.e. too many features/variables compared to the number of observations). This model will be very accurate on the training data but will probably be very not accurate on untrained or new data. It is because this model is not generalized (or not AS generalized), meaning you can generalize the results and can’t make any inferences on other data, which is, ultimately, what you are trying to do. Basically, when this happens, the model learns or describes the “noise” in the training data instead of the actual relationships between variables in the data. This noise, obviously, isn’t part in of any new dataset, and cannot be applied to it.\nUnderfitting\nIn contrast to overfitting, when a model is underfitted, it means that the model does not fit the training data and therefore misses the trends in the data. It also means the model cannot be generalized to new data. As you probably guessed (or figured out!), this is usually the result of a very simple model (not enough predictors/independent variables). It could also happen when, for example, we fit a linear model (like linear regression) to data that is not linear. It almost goes without saying that this model will have poor predictive ability (on training data and can’t be generalized to other data).\n'], 'url_profile': 'https://github.com/Vaishnav2804', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['simple_linear_regression\nimplement simple linear regression model using scikitlearn\n'], 'url_profile': 'https://github.com/shobanapriyan', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nadaelmo07', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""Predict student's GPA based on given SAT marks using simple linear regression\nWhat is linear regression\nLinear regression is a linear approach to modelling the relationship between a dependent variable (response) and one or more independent variables (explanatory).\nTo understand difference between dependent & independent, look at the equation:\nY = aX + b\nX can take any value from the sample space, and Y value depends on X value, and thus it is a dependent.\nLinear regression can be divided into two divisions based on the number of independent variables, if we have one independent variable we call it simple linear regression model, other than that it is called multiple linear regression model.\nThe equation that describes how y is related to X is called the regression model!\nThere are different types of regression models:\nSimple regression, Multiple regression, Polynomial regression, Support Vector Regression.\nOur example looking at simple linear regression.\nKeep in mind, regression only tries to find relationship out of the given information. Correlation doesn't prove causation!\nInstallation\nPython 3.8 + your preferred IDE.\nI am using Anaconda with python 3.8 (Jupyter notebook).\nNeeded packages shown inside the code file.\nDataset is downloaded from Kaggle.\nSteps\n\nLoading data from the csv file into arrays (X, Y).\nCleaning data (missing data, pessimist values, etc)\nSplitting data into train data & test data.\nImport LinearRegression for actual training process, to learn a pattern from your data.\nPredict GPA from testing values.\nVisualize your predictions!\n\n""], 'url_profile': 'https://github.com/YHassan002', 'info_list': ['Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 10, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}"
"{'location': 'Ithaca, NY', 'stats_list': [], 'contributions': '563 contributions\n        in the last year', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Boston Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Boston Housing Data\nWe pre-processed the Boston Housing data again. This time, however, we did things slightly different:\n\nWe dropped 'ZN' and 'NOX' completely\nWe categorized 'RAD' in 3 bins and 'TAX' in 4 bins\nWe transformed 'RAD' and 'TAX' to dummy variables and dropped the first variable to eliminate multicollinearity\nWe used min-max-scaling on 'B', 'CRIM', and 'DIS' (and log transformed all of them first, except 'B')\nWe used standardization on 'AGE', 'INDUS', 'LSTAT', and 'PTRATIO' (and log transformed all of them first, except for 'AGE')\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nboston_features = boston_features.drop(['NOX', 'ZN'],axis=1)\n\n# First, create bins for based on the values observed. 3 values will result in 2 bins\nbins = [0, 6, 24]\nbins_rad = pd.cut(boston_features['RAD'], bins)\nbins_rad = bins_rad.cat.as_unordered()\n\n# First, create bins for based on the values observed. 4 values will result in 3 bins\nbins = [0, 270, 360, 712]\nbins_tax = pd.cut(boston_features['TAX'], bins)\nbins_tax = bins_tax.cat.as_unordered()\n\ntax_dummy = pd.get_dummies(bins_tax, prefix='TAX', drop_first=True)\nrad_dummy = pd.get_dummies(bins_rad, prefix='RAD', drop_first=True)\nboston_features = boston_features.drop(['RAD','TAX'], axis=1)\nboston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\nage = boston_features['AGE']\nb = boston_features['B']\nlogcrim = np.log(boston_features['CRIM'])\nlogdis = np.log(boston_features['DIS'])\nlogindus = np.log(boston_features['INDUS'])\nloglstat = np.log(boston_features['LSTAT'])\nlogptratio = np.log(boston_features['PTRATIO'])\n\n# Min-Max scaling\nboston_features['B'] = (b-min(b))/(max(b)-min(b))\nboston_features['CRIM'] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\nboston_features['DIS'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features['AGE'] = (age-np.mean(age))/np.sqrt(np.var(age))\nboston_features['INDUS'] = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\nboston_features['LSTAT'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nboston_features['PTRATIO'] = (logptratio-np.mean(logptratio))/(np.sqrt(np.var(logptratio)))\nboston_features.head()\nRun a linear model in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nInterpret the coefficients for PTRATIO, PTRATIO, LSTAT\n\nCRIM: per capita crime rate by town\nINDUS: proportion of non-retail business acres per town\nCHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\nRM: average number of rooms per dwelling\nAGE: proportion of owner-occupied units built prior to 1940\nDIS: weighted distances to five Boston employment centres\nRAD: index of accessibility to radial highways\nTAX: full-value property-tax rate per $10,000\nPTRATIO: pupil-teacher ratio by town\nB: 1000(Bk - 0.63)^2 where Bk is the proportion of African American individuals by town\nLSTAT: % lower status of the population\n\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nCRIM: 0.15\nINDUS: 6.07\nCHAS: 1\nRM:  6.1\nAGE: 33.2\nDIS: 7.6\nPTRATIO: 17\nB: 383\nLSTAT: 10.87\nRAD: 8\nTAX: 284\n\nSummary\nCongratulations! You pre-processed the Boston Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Boston Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/merb92', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Ohio', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['logistic_marginals\n'], 'url_profile': 'https://github.com/svenhalvorson', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'bhubaneswar', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Logistic-Regression2\nI have done a logistic regression example with data wrangling and analysing\n'], 'url_profile': 'https://github.com/iamchiranjeeb', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MamathaCherukuri', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Ithaca, NY', 'stats_list': [], 'contributions': '563 contributions\n        in the last year', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Boston Housing Data once more\nWe pre-processed the Boston Housing data the same way we did before:\n\nWe dropped \'ZN\' and \'NOX\' completely\nWe categorized \'RAD\' in 3 bins and \'TAX\' in 4 bins\nWe transformed \'RAD\' and \'TAX\' to dummy variables and dropped the first variable\nWe used min-max-scaling on \'B\', \'CRIM\', and \'DIS\' (and logtransformed all of them first, except \'B\')\nWe used standardization on \'AGE\', \'INDUS\', \'LSTAT\', and \'PTRATIO\' (and logtransformed all of them first, except for \'AGE\')\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nboston_features = boston_features.drop([\'NOX\', \'ZN\'],axis=1)\n\n# First, create bins for based on the values observed. 3 values will result in 2 bins\nbins = [0,6,  24]\nbins_rad = pd.cut(boston_features[\'RAD\'], bins)\nbins_rad = bins_rad.cat.as_unordered()\n\n# First, create bins for based on the values observed. 4 values will result in 3 bins\nbins = [0, 270, 360, 712]\nbins_tax = pd.cut(boston_features[\'TAX\'], bins)\nbins_tax = bins_tax.cat.as_unordered()\n\ntax_dummy = pd.get_dummies(bins_tax, prefix=\'TAX\', drop_first=True)\nrad_dummy = pd.get_dummies(bins_rad, prefix=\'RAD\', drop_first=True)\nboston_features = boston_features.drop([\'RAD\', \'TAX\'], axis=1)\nboston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\n\nage = boston_features[\'AGE\']\nb = boston_features[\'B\']\nlogcrim = np.log(boston_features[\'CRIM\'])\nlogdis = np.log(boston_features[\'DIS\'])\nlogindus = np.log(boston_features[\'INDUS\'])\nloglstat = np.log(boston_features[\'LSTAT\'])\nlogptratio = np.log(boston_features[\'PTRATIO\'])\n\n# Min-Max scaling\nboston_features[\'B\'] = (b-min(b))/(max(b)-min(b))\nboston_features[\'CRIM\'] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\nboston_features[\'DIS\'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features[\'AGE\'] = (age-np.mean(age))/np.sqrt(np.var(age))\nboston_features[\'INDUS\'] = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\nboston_features[\'LSTAT\'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nboston_features[\'PTRATIO\'] = (logptratio-np.mean(logptratio))/(np.sqrt(np.var(logptratio)))\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this function provided on your preprocessed Boston Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nThe stepwise procedure mentions that \'INDUS\' was added with a p-value of 0.0017767, but our statsmodels output returns a p-value of 0.000. Use some of the stepwise procedure logic to find the intuition behind this!\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.742981  \n# adjusted_r_squared is 0.740411\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Boston Housing dataset!\n'], 'url_profile': 'https://github.com/merb92', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Morocco', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Olivier-Patrick', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['prediction-uncertainty\n'], 'url_profile': 'https://github.com/GoldeneyeRohan', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Christina97716', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/divyanamani', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Corona-Virus-analization\nUsing Polynomial Linear Regression and Matplotlib\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/divyanamani', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/justincessna', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Real Estate Price Prediction\nCode Highlights\n1) Use of Startified Shuffel Split model in prediction.\n2) Pipeline for Imputer & Standard Scaler\n3) Out of Linear Regression, DecisionTree Regressor & Random forest Regressor \n      Random Forest Regressor performs best.\n4) Model has evaluated using RMSE & Cross Validation Technique.\n\nHistograms of Differnt Features on dataset\n\n\n'], 'url_profile': 'https://github.com/rohit-dudhal', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'bhubaneswar', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Linear-Regression2\ni have done some linear regression test over here\n'], 'url_profile': 'https://github.com/iamchiranjeeb', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'BANGALORE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['CREDIT-CARD-SPEND-PREDICTION\nCREDIT CARD SPEND PREDICTION - LINEAR REGRESSION\n'], 'url_profile': 'https://github.com/Shaid20', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': ['Logistic-Regression-from-Scratch\nBuilding a logistic regression from scratch\nBelow is a description of all the cells in the code\n\nImports all the necessary code needed to build the model\nDefine a sigmoid activation functioon\nTest the sigmoid function\nDefine a function to initialize the weights and bias to zeros\nTest the weight initialization function\nDefine a forward function to calculate the cost and weight,bias change factor using gradient descent equations\ntest the forward function defined above\nDefine a function to train the model given the number of epochs and learning rate of the algorithm#\n9.Test the training function\ndefnine a function to classify item based on the trained weights from the algorithm above\nput all the functions together to create a robust logistic regression model\nafter training you can plot the change in cost against the number of epochs to see how your model is training.\n\n'], 'url_profile': 'https://github.com/ToluClassics', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['RegressionModel-ML\n'], 'url_profile': 'https://github.com/CLozy', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Linear-Regression2\n'], 'url_profile': 'https://github.com/chennainani', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'San Mateo', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Linear Regression Using Apache Spark\nWe will use Apache Spark to build a linear regression model to predict the ""Yearly Amount Spent"" by a person on an app based on featues such as\n\nAvg Session Length\nTime on app\nTime on website\nLength of membership\nAvatar color\n\nWe will use Avatar Color to understand how to map a categorical variable using One Hot Encoding in Spark\n'], 'url_profile': 'https://github.com/VardhiniM', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '345 contributions\n        in the last year', 'description': ['LINEAR-REGRESSION2\n'], 'url_profile': 'https://github.com/NarayanaReddy29', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Titanic-\nLogistic Regression to predict percentage of survival rate\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Linear-Regression2\n'], 'url_profile': 'https://github.com/chennainani', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'San Mateo', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Linear Regression Using Apache Spark\nWe will use Apache Spark to build a linear regression model to predict the ""Yearly Amount Spent"" by a person on an app based on featues such as\n\nAvg Session Length\nTime on app\nTime on website\nLength of membership\nAvatar color\n\nWe will use Avatar Color to understand how to map a categorical variable using One Hot Encoding in Spark\n'], 'url_profile': 'https://github.com/VardhiniM', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '345 contributions\n        in the last year', 'description': ['LINEAR-REGRESSION2\n'], 'url_profile': 'https://github.com/NarayanaReddy29', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['MLAssignment\nThis file contains implementation of LINEAR REGRESSION,LOGISTIC REGRESSION,K MEANS CLUSTERING from scratch using numpy and oops\n'], 'url_profile': 'https://github.com/vishnuvardhanreddyr28', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'Rajkot', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/d2Kapuriya', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': [""\nI don't really understand the reason that my Linear.ipynb file was not processed.\n\n""], 'url_profile': 'https://github.com/Y2Data', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': [""Logistic-regression\nThis session covers basic techniques for classification. With a practical focus in mind, we'll cover:\nLogistic regression\nMaking predictions\nLinear discriminant analysis\nQuadratic discriminant analysis\nComparison of logistic regression and discriminant analysis to KNNadding a line\nadding a line\n""], 'url_profile': 'https://github.com/Dat-Vuong07', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SergiPlanesBassas', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['numeric_regression\nExamples of nemeric regression applications using deep learning.\n'], 'url_profile': 'https://github.com/ZiyangTian', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}"
"{'location': 'FRANCE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JacquesGarre', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['16.04LogisticRegression\nIntroduction to Logistic Regression and Exploring Titanic Dataset\n'], 'url_profile': 'https://github.com/bangalorebyte-cohort32-2004', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'Mississauga, ON, Canada', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['ML-Regression\nMachine learning regression analysis on canadian industry employment\n'], 'url_profile': 'https://github.com/marc254', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""HousePrices\nSubmissions for Kaggle's House Prices Advances Regression Techniques Competition\n""], 'url_profile': 'https://github.com/petremihaivalentin', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'Zaria, Nigeria', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Gaussian-Process-Regression-with-GPyTorch\nThis code contains code for Gaussian Process Regression with GPyTorch\n'], 'url_profile': 'https://github.com/Alikerin', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'San francisco', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': [""FIFA\nDecoding FIFA players performance score using different regression models\nGoal\nTo compare the performance of different regression models predicting FIFA players' overall performance score.\nSome Lesson learnt from this project :\n1.\tAbout significantly heterogenousity\nIdeally, we should randomly choose sample from the significantly heterogenous dataset to build a model, so that the sample we choose could represent the original dataset as much as possible, then the estimators we get will be more accurate to predict the test dataset. Alternatively, stratified sampling can be utilized if the heterogeneity is from a specific variable.We can stratify the dataset according to different levels of this variable, and choose sample proportionally from each level.\nIn addition, we could use cross validation to enhance our model. Cross validation guarantees each observation is left-out for validation in at least one fold, and lowers the sampling variance of CV model selection.\nWe made some changes to the original model.\nFirstly, we treated 'International Reputation', 'Weak Foot', 'Skill Moves', 'Contract Valid Until' as continuous variable instead of categorical variable to put into the model. Because their numbers are corresponding to their levels.\nSecondly, we cleaned ‘Value’ and ‘Wage’ columns, and put them into our model because it is highly possible these variables are correlated to overall score. It’s reasonable to assume that value and wage are correlated with the overall score of the player. We don’t have to worry about multicollinearity too much because Lasso will take care of this.\nThen we split the data into training and test set, and used training set to fit a simple linear regression model. There are 77 features in the model and the OOS R^2 is 0.894, which improved 0.004 compared to the original model.\n2.\tCross-validation on simple linear regression\nAfter implementing 5-fold cross validation to the test dataset, the R^2 improved to 0.899. While the basic linear regression model used training data set to fit the model and then predicted test dataset, the cross validation repeated this process 5 times using test dataset and gave overall accuracy of the model. For FIFA dataset, the R^2 of cross validation improved slightly because it lowers the sampling variance, and it shows that the model's ability to predict new data is quite good, especially because we have a heterogeneous data set.\n3.\tLasso Regression\nAfter fitting the Lasso regression model, the R^2 dropped a little to 0.862 compared to 0.894 in basic linear regression. However, Lasso only used 26 features in the model instead of 77 features in the basic model. We can see that Lasso works well in this dataset that it reduced 51 features but the R^2 only dropped 0.032. It helped us to simplify the model and keep a high predicting accuracy at the samein the meantime.\n4.\tWhat if using ridge- or log- instead of lasso- penalties?\nRidge regression use 𝛽^2 in the penalty term  instead of |𝛽| as in lasso regression. As the alpha increases, Ridge regression will shrink the coefficients but not that aggressively to zero like lasso. So the number of features used by the model will greatly increase if we use ridge regression.\nAs the Ridge regression will keep more variables, it will yield a higher R^2 almost certainly, because R^2 always increases as more variables are put into the model with the risk of overfitting.\nThe log- regularization will result in even fewer features as compared to lasso as it is more aggressive in making betas to zero.\n5.\tLasso regression with an ideal value for alpha\nIdeal alpha is 0.01.  53 features are being used.\nR^2  dropped 0.0003, very slightly from  0.8946 to 0.8943. This means most variables dropped weren’t playing a big role in the linear regression model. And the lasso regression model is better because the complexity is reduced without compromising the model fit. This is inline with getting a parsimonious model.\n6.\tTo use AIC and BIC to evaluate\nLinear regression model: AIC -26576, BIC-27177, Corrected AIC- 26576\nLasso regression model: AIC- 26567, BIC- 26983, Corrected AIC- 26567\nThe model with lower AIC is Lasso and is better.\nBIC is usually higher than AIC, but when sample size is less than 7 BIC is more tolerant and is small. But sample size less than 7  will be an uninformative dataset.\nCorrected AIC is almost same as AIC because for higher sample size ( >40 times the number of parameters) both AIC and corrected AIC converge to the same value.\n7.\tICs comparing to CVs.\nICs and CV both have their pros and cons.\nAIC is best for prediction as it is asymptotically equivalent to cross-validation.We prefer correct AIC when the number of observations is small (<40 times the number of parameters). BIC is best for explanation as it is allows consistent estimation of the underlying data generating process.\nAIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.\nAIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.\nCross validation is time consuming as compared to ICs but is the true test for out of sample performance.It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\nFormula-\nAIC = -2ln(likelihood) + 2k,\nBIC = -2*ln(likelihood) + ln(N)*k,\nAICc= AIC+ 2k(k+1)/(n-k-1)\nwhere: k =number of parameters estimated by model, N = number of observations\n""], 'url_profile': 'https://github.com/JacquelineHSH', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'Boston, MA, US', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['MNIST-Digit-Classifier\n\nExtracted digit image classification data and created view for digit image\nImplemented logistic classifcation and fetched probability based value\nUsed softmax function to get the maximum accuracy\nCompred performance with Bagging algorithm\nDeveloped Bagging algorithm using randomly generated trees and their output\nClassified digit and identified accuracy using confusion matrix\n\n'], 'url_profile': 'https://github.com/tmihir', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vickeydreamss', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'Aachen - NRW', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['Stocks Prediction\n\n\n\nAbout this Project\nThe idea of the Web App is:\n""Share knowledge in the form of stocks predictions, providing a simple way to learn"".\nWhy?\nThis project is part of my personal portfolio, so, I\'ll be happy if you could provide me any feedback about the project, code, structure or anything that you can report that could make me a better developer!\nEmail-me: CaetanooG@hotmail.com\nConnect with me at LinkedIn.\nObservation about this App\n1 - The web app is in development\nInstallers\nIf you want to test the Web App, the installers are listed below:\nPython 3.8.x\nFunctionalities\n\n\nGet the stock price of the next work day and also the error of the prediction\n\n\nGet the stock information\n\nLast open and close values in the period searched\nLast high and low values in the period searched\n\n\n\nSee the hystorical graphics of the selected stock\n\nTime series chart\nCandlestick chart\nOHLC chart\n\n\n\nGetting Started\nPrerequisites\nTo run this project in the development mode, you\'ll need to have a basic environment to run a Python, that can be found here.\nInstalling\nCloning the Repository\n$ git clone https://github.com/CaetanoGS/stocksForecast.git\n\n$ cd stocksForecast/app\n\nInstalling dependencies\n$ pip3 install -U yfinance flask stocker numpy\n\nRunning\nWith all dependencies installed and the environment properly configured, you can now run the app:\n$ python3 app.py\n\nContributing\nYou can send how many PR\'s do you want, I\'ll be glad to analyse and accept them! And if you have any question about the project...\nEmail-me: CaetanooG@hotmail.com\nConnect with me at LinkedIn\nThank you!\n'], 'url_profile': 'https://github.com/CaetanoGS', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Project Overview\nThe ""spam"" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography…\nOur collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word \'george\' and the area code \'650\' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\nNumber of Instances: 4601 (1813 Spam = 39.4%)\nNumber of Attributes: 58 (57 continuous, 1 nominal class label)\nAttribute Information:\nThe last column of \'spambase.data\' denotes whether the e-mail was considered spam (1) or not (0)\n48 attributes are continuous real [0,100] numbers of type word freq WORD i.e. percentage of words in the e-mail that match WORD\n6 attributes are continuous real [0,100] numbers of type char freq CHAR i.e. percentage of characters in the e-mail that match CHAR\n1 attribute is continuous real [1,…] numbers of type capital run length average i.e. average length of uninterrupted sequences of capital letters\n1 attribute is continuous integer [1,…] numbers of type capital run length longest i.e. length of longest uninterrupted sequence of capital letters\n1 attribute is continuous integer [1,…] numbers of type capital run length total i.e. sum of length of uninterrupted sequences of capital letters in the email\n1 attribute is nominal {0,1} class of type spam i.e denotes whether the e-mail was considered spam (1) or not (0),\nMissing Attribute Values: None\nClass Distribution: Spam 1813 (39.4%) Non-Spam 2788 (60.6%)\nLearnings from the project\napplying\nCorrelation\nChi Square\nConfusion Matrix\nAnova\nPCA\nApproach taken to solve the problem\nI\'ve implemented everything as guided and applied all filtering methods one by one and printed the accuracy to find the best one.\nIn PCA ,chi square methods created a list starting from 5 ,10,15,......55 and created a function to do so.\nChallenges faced\nNA\nAdditional pointers\nNA\n'], 'url_profile': 'https://github.com/Prem547', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'MATLAB', 'Updated Apr 13, 2020', '3', 'HTML', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/santiagocantu98', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lottelin', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Face-Recognition\nPerformed PCA and Logistic Regression over image dataset\n'], 'url_profile': 'https://github.com/Puttu710', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['LinRegNumpy\nLinear Regression implemented from scratch in Python with numpy.\n'], 'url_profile': 'https://github.com/Servitor666', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Project Overview\nThe ""spam"" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography…\nOur collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word \'george\' and the area code \'650\' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\nNumber of Instances: 4601 (1813 Spam = 39.4%)\nNumber of Attributes: 58 (57 continuous, 1 nominal class label)\nAttribute Information:\nThe last column of \'spambase.data\' denotes whether the e-mail was considered spam (1) or not (0)\n48 attributes are continuous real [0,100] numbers of type word freq WORD i.e. percentage of words in the e-mail that match WORD\n6 attributes are continuous real [0,100] numbers of type char freq CHAR i.e. percentage of characters in the e-mail that match CHAR\n1 attribute is continuous real [1,…] numbers of type capital run length average i.e. average length of uninterrupted sequences of capital letters\n1 attribute is continuous integer [1,…] numbers of type capital run length longest i.e. length of longest uninterrupted sequence of capital letters\n1 attribute is continuous integer [1,…] numbers of type capital run length total i.e. sum of length of uninterrupted sequences of capital letters in the email\n1 attribute is nominal {0,1} class of type spam i.e denotes whether the e-mail was considered spam (1) or not (0),\nMissing Attribute Values: None\nClass Distribution: Spam 1813 (39.4%) Non-Spam 2788 (60.6%)\nLearnings from the project\napplying\nCorrelation\nChi Square\nConfusion Matrix\nAnova\nPCA\nApproach taken to solve the problem\nI\'ve implemented everything as guided and applied all filtering methods one by one and printed the accuracy to find the best one.\nIn PCA ,chi square methods created a list starting from 5 ,10,15,......55 and created a function to do so.\nChallenges faced\nNA\nAdditional pointers\nNA\n'], 'url_profile': 'https://github.com/Prem547', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Codecademy Offplatform project: https://www.codecademy.com/courses/learn-python-3/informationals/python3-reggies-linear-regression\n'], 'url_profile': 'https://github.com/wanjapm', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'bhubaneswar', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['LogisticRegression\ni have performed logistic regression along with data wrangling\n'], 'url_profile': 'https://github.com/iamchiranjeeb', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'Switzerland', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': ['polynomial_regression\nA series of functions for polynomial regression. This project is still in progress.\nLeast Squares\nLeast squares regression produces the line of best fit which minimizes the square of the vertical offsets of each datapoint (their variance). The least squares equation is as follows:\n\nThe equations for variance and covariance can be substituted into this equation to produce:\nm = covariance/variance and c = y_mean - m * x_mean\nThis is the equation that was implemented in main.py.\nMore information about the equation can be found in this MIT lecture. The derivation of the formula through minimizing the variance equation can be found here.\nExample\nHere is an example of linear least squares using the data set found in main.py:\n\nTo Do:\n\n Variance\n Covariance\n Least Squares Regression\n Polynomial matrix regression\n\n'], 'url_profile': 'https://github.com/imbush', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/blakepiper', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}","{'location': 'Iran', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/artinZareie', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated May 3, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Python', 'Updated Apr 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['CO2_Emission_Prediciton\nUse Linear Regression to predict the (CO2 Emission) of the cars based on the engine size\n'], 'url_profile': 'https://github.com/da394', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'West Lafayette, IN', 'stats_list': [], 'contributions': '518 contributions\n        in the last year', 'description': [""Here, I have tried to implement linear regression using Gradient Descent Algorithm, based on learnings from Andrew Ng's course on Machine Learning.\nGradient Descent Algorithm:\ndef gradient_descent(x, y, theta, iterations, alpha):\n    past_costs = []\n    past_thetas = [theta]\n    for i in range(iterations):\n        prediction = np.dot(x, theta) # x and coefficient estimates\n        error = prediction - y # diff in predicted and actual values of house price\n        cost = 1/(2*m) * np.dot(error.T, error) # for minimization\n        past_costs.append(cost)\n        theta = theta - (alpha * (1/m) * np.dot(x.T, error)) # calculating...\n        past_thetas.append(theta)\n\n    return past_thetas, past_costs\n\n# getting new values back for theta and costs\npast_thetas, past_costs = gradient_descent(x, y, theta, iterations, alpha)\ntheta = past_thetas[-1]\n\n""], 'url_profile': 'https://github.com/akshay-madar', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '167 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/am610', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'addis abeba', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Endagegnehu', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'Rennes', 'stats_list': [], 'contributions': '3,050 contributions\n        in the last year', 'description': ['\n\n\n\nNCVREG.jl\nRegularization Paths for SCAD and MCP Penalized Regression Models\nThis is a quick, naive and partial Julia translation of the R package ncvreg.  Only gaussian family is translated.\nAlgorithm is described in Breheny P and Huang J (2011) ""Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection"". Annals of Applied Statistics, 5: 232–253\nI needed to do regression with SCAD penalty and I can\'t find it in any Julia package.\nPerharps it is now implemented in MLJLinearModels.jl.\njulia> using LinearAlgebra \njulia> using Random\njulia> using NCVREG \njulia> using RCall\njulia> rng = MersenneTwister(1234);\njulia> n, p = 50, 5\n(50, 5)\n\njulia> X = randn(rng, n, p)              # feature matrix\n50×5 Array{Float64,2}:\n  0.867347   -1.22672     0.183976    0.0377058   1.48494\n -0.901744   -0.541716   -1.27635    -0.490009    1.23969\n  ⋮\n -1.00978    -1.66323    -0.744522    0.427383   -1.37986\n -0.543805   -0.521229   -0.191176   -0.492253   -0.984217\n\njulia> a0 = collect(1:p)                # ground truths\n5-element Array{Int64,1}:\n 1\n 2\n 3\n 4\n 5\n\njulia> y = X * a0 + 0.1 * randn(n) # generate response\n50-element Array{Float64,1}:\n   6.411769869798991\n  -1.59817739694925\n   ⋮\n -11.769008550241434\n  -9.107294931708777\n\njulia> XX = hcat(X, randn(rng, n, p))\n50×10 Array{Float64,2}:\n  0.867347   -1.22672     0.183976    0.0377058  …   1.69129     0.969694    0.222167    -0.953909\n  ⋮                                              ⋱\n -0.543805   -0.521229   -0.191176   -0.492253      -2.5788     -0.329958    0.00775707  -0.370354\n\njulia> @rput XX\n50×10 Array{Float64,2}:\n  0.867347   -1.22672     0.183976    0.0377058  …   1.69129     0.969694    0.222167    -0.953909\n  ⋮                                              ⋱\n -0.543805   -0.521229   -0.191176   -0.492253      -2.5788     -0.329958    0.00775707  -0.370354\n\njulia> @rput y\n50-element Array{Float64,1}:\n   6.411769869798991\n  -1.59817739694925\n   ⋮\n -11.769008550241434\n  -9.107294931708777\n\njulia> R""library(ncvreg)""\nRObject{StrSxp}\n[1] ""ncvreg""    ""stats""     ""graphics""  ""grDevices"" ""utils""     ""datasets""\n[7] ""methods""   ""base""\n\njulia> R""scad <- coef(ncvreg(XX, y, lambda=0.2, penalty=\'SCAD\', eps=.0001))""\nRObject{RealSxp}\n (Intercept)           V1           V2           V3           V4           V5\n-0.003322903  1.025666431  2.000108987  2.983498545  3.997543804  4.982264144\n          V6           V7           V8           V9          V10\n 0.000000000  0.000000000  0.000000000  0.000000000  0.000000000\n\njulia> @rget scad\n11-element Array{Float64,1}:\n -0.0033229032078964105\n  1.0256664305062173\n  2.000108987156345\n  2.9834985454557157\n  3.997543803872521\n  4.982264143916537\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n\njulia> println( "" R scad = $scad"")\n R scad = [-0.0033229032078964105, 1.0256664305062173, 2.000108987156345, 2.9834985454557157, 3.997543803872521, 4.982264143916537, 0.0, 0.0, 0.0, 0.0, 0.0]\n\njulia> λ = [0.2]\n1-element Array{Float64,1}:\n 0.2\n\njulia> scad = NCVREG.coef(SCAD(XX, y, λ))\nSCAD([-0.003322960709765954; 1.0256660512338405; … ; 0.0; 0.0])\n\njulia> println( "" Julia scad = $scad"")\n Julia scad = SCAD([-0.003322960709765954; 1.0256660512338405; 2.00010933635426; 2.983498839847109; 3.99754375703709; 4.982264100245242; 0.0; 0.0; 0.0; 0.0; 0.0])\n\n\n'], 'url_profile': 'https://github.com/pnavaro', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leo-you', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'US', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AlenaSorokina', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['VinGroup-s-stock-prices-prediction\nUsing matplotlib and Linear Regression to analyze  stock prices\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': [""Fake bank notes classifier using Logistic Regression from scratch\n Data Set Description \n\n\n\nData Set Characteristics:\nMultivariate\n\n\nAttribute Characteristics:\nReal\n\n\n\xa0Task:\nClassification\n\n\nNumber of Instances:\n1372\n\n\nNumber of Attributes:\n5\n\n\nMissing Values?:\nNone\n\n\n\nSource:\nOwner of database: Volker Lohweg (University of Applied Sciences, Ostwestfalen-Lippe, volker.lohweg '@' hs-owl.de)\nDonor of database: Helene DÃ¶rksen (University of Applied Sciences, Ostwestfalen-Lippe, helene.doerksen '@' hs-owl.de)\nDate received: August, 2012\nData Set Information:\nData were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\nAttribute Information:\n\nvariance of Wavelet Transformed image (continuous)\nskewness of Wavelet Transformed image (continuous)\ncurtosis of Wavelet Transformed image (continuous)\nentropy of image (continuous)\nclass (integer)\n\nDua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science\n""], 'url_profile': 'https://github.com/kkviks', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '218 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/perfan', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Julia', 'GPL-3.0 license', 'Updated Apr 28, 2020', '1', 'HTML', 'Updated Apr 17, 2020', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Sep 18, 2020', 'Python', 'MIT license', 'Updated Apr 19, 2020']}"
"{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '427 contributions\n        in the last year', 'description': ['Startup Profit Predictor\nStartup prodit predictor using multiple linear regression model\nUsed technologies\n\nPyhton 3\nFlask python library\nLinear regression machine learning algorithm\nDocker\nAzure cloud\n\n'], 'url_profile': 'https://github.com/InfiniteCoder96', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'Thane,India', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vatsalsmehta', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'Ghana', 'stats_list': [], 'contributions': '1,464 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KwameKert', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Predict-Stock-Price\nMultiple linear regression model to predict price of stock. GUI based application built using Tkinter.\n'], 'url_profile': 'https://github.com/Gurmeet22', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'NYC', 'stats_list': [], 'contributions': '482 contributions\n        in the last year', 'description': ['IMdB Regression\nA regression project in Python.\nPredicts IMdB movie ratings (from 0 to 10) using pre-release movie metadata.\n\xa0\xa0\xa0\n\xa0\xa0\xa0\n\n\nBest Model\'s Coefficients\nHere are some of the strongest coefficients that emerged from my lasso estimator, which performed better than a linear SVR, random forest regressor, or neural net\n(Interaction terms are formatted as ""variable_x*variable_y"", e.g. ""durations * actor_1_facebook_likes_box"" is the interaction term composed of film runtimes and the facebook popularity of the leading actor.)\n\nHow to interpret the above:\nIn my lasso regresion,\n\nLonger movies are predicted to get higher scores.\nLonger movies with popular leading actors get higher score predictions.\nDocumentaries in color get higher scores.\nEnglish language films in color tend to do worse.\nEtc.\n\nRoger Ebert is quoted as saying ""No good film is too long and no bad movie is short enough.""\nBest Model\'s Loss\n\nThese are the errors from my best model, the LassoCV estimator.\nRMSE and Median Absolute Error are in terms of the ten-point IMdB rating.\nBest model\'s RMSE, compared to dummy regressor:\n\nIn plain English: If you were to naïvely guess the mean IMdB score for every sample in the test set, your average error in terms of IMdB critic score points would be around 1.1. My best model did somewhat better, with a root-mean-squared error of 0.92.\nProcess\n1. Data Collection\n\nGrab this kaggle imdb dataset:\n\n\n\n\nExtract IMdB ID\'s from each row, spam themoviedb API for more metadata, join it in:\n\n\n2. Data Cleaning\nStructured Data:\n\nKeep only seemingly predictive columns (budget, actor popularities, genre, etc.)\nDrop duplicates\nManage obvious null values (selective imputation, dropping rows)\n\nNull Value Counts by Column:\n\n\nManage non-obvious null values (e.g. zero when there is actually no info)\nBin categorical columns into somewhat balanced metacategories\n\nE.g.: ""Aspect Ratio"" Value Counts Before Binning:\n\n\nAfter Binning:\n\n(Here I used my familiarity with aspect ratios in film to condense this column.)\n\nBox-Cox transform non-normally distributed continuous value columns\n\nE.g. Film Budgets: Vanilla\n\n\nFilm Budgets: Box-Cox Transformed\n\n\nUnstructured Data (Plot Synopses):\n\nTokenize corpus. Remove stopwords, punctuation\nPart-of-speech tag each token\nLemmatize words by POS\nVectorize plot texts with n-gram CountVectorizer\nTF-IDF normalize vectors\n\n3. Feature Engineering\n\nOne-hot encode categorical variables\n\nE.g. ""genre"" column gets split into many binary variables:\n\n\n\nTrain/Test split happens here\nDrop multicollinear features\n\nMulticollinearity of features as a heatmap:\n\n\nGenerate interaction terms\nMinMax scale features\n\n4. Model Building / Benchmarking\nModels evaluated:\n\nBaseline: Dummy regressor\nLinear: Lasso\nNon-Parametric: Linear SVR\nTried incorporating vectorized text features (with PCA) — Not great\nTree-based: Random forest regressor\nNeural Net\n\nUsed Pipeline + GridSearchCV to search for better hyperparameters on text-based regressor and random forest.\nModel Loss Metrics:\n\n'], 'url_profile': 'https://github.com/boscacci', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'NG', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Logistic-Regression-Model\nUsing Logistic Regression Model to determine loan eligibility\n'], 'url_profile': 'https://github.com/Usmanjibril09', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['UK_COVID19_Cumulative_Hospital_Deaths_Model_and_Prediction\nregression method to model and predict UK cumulative hospital death\n'], 'url_profile': 'https://github.com/XingfengLee', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Truncated linear regression\nThis method is based on arXiv:1809.03986 [math] and Daskalakis, C., Gouleakis, T., Tzamos, C., & Zampetakis, M. (2019). Computationally and Statistically Efficient Truncated Regression. COLT.\nPlease check the Jupyter notebook for details.\n'], 'url_profile': 'https://github.com/phylyc', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '475 contributions\n        in the last year', 'description': ['Motivation\nIn this project, we examine the Seattle Airbnb dataset in order to gain insight into factors that affect the popularity of a listing. In particular, we want to answer the following questions:\n\nCan we build a linear regression model to accurately predict the popularity of a listing?\nWhat are the top characteristics of a listing that impact the popularity of a listing?\nAre there specific neighborhoods or property types that are preferred in a listing?\n\nThe accompanying Medium article is here. The full CRISP-DM process performed in this study is discussed in the Jupyter notebook.\nFile Descriptions\n\nmain.ipynb contains the Jupyter notebook for this analysis\nraw_data/ contains all of the raw datasets\n\nBuild Instructions\n\nFirst download and install Jupyter per these instructions.\nNext clone this repo and navigate to the main directory:\n\ngit clone https://github.com/cheuklau/airbnb-analysis\ncd airbnb-analysis\n\n\nStart Jupyter and open up main.ipynb\n\njupyter notebook\n\nLicensing\nSee MIT License.\nAuthors\nCheuk Lau\nAcknowledgements\nThe following sources were used:\n\nAirbnb for providing the data.\nKaggle for hosting the data.\n\n'], 'url_profile': 'https://github.com/cheuklau', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Predict-Stock-Market-Price\nPredict Stock Market Price - Linear Regression vs Random Forest\nFor this project, i worked with a csv file containing index prices. Each row in the file contains a daily record of the price of the S&P500 Index from 1950 to 2015. The dataset is stored in sphist.csv.\nThe columns of the dataset are:\nDate -- The date of the record.\nOpen -- The opening price of the day (when trading starts).\nHigh -- The highest trade price during the day.\nLow -- The lowest trade price during the day.\nClose -- The closing price for the day (when trading is finished).\nVolume -- The number of shares traded.\nAdj Close -- The daily closing price, adjusted retroactively to include any corporate actions. Read more here.\nI will use this dataset to develop a predictive model leveraging Linear Regression and Random Forest algorithm.\n'], 'url_profile': 'https://github.com/ShawnLiu119', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 19, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 18, 2020']}"
"{'location': 'NYC', 'stats_list': [], 'contributions': '482 contributions\n        in the last year', 'description': ['IMdB Regression\nA regression project in Python.\nPredicts IMdB movie ratings (from 0 to 10) using pre-release movie metadata.\n\xa0\xa0\xa0\n\xa0\xa0\xa0\n\n\nBest Model\'s Coefficients\nHere are some of the strongest coefficients that emerged from my lasso estimator, which performed better than a linear SVR, random forest regressor, or neural net\n(Interaction terms are formatted as ""variable_x*variable_y"", e.g. ""durations * actor_1_facebook_likes_box"" is the interaction term composed of film runtimes and the facebook popularity of the leading actor.)\n\nHow to interpret the above:\nIn my lasso regresion,\n\nLonger movies are predicted to get higher scores.\nLonger movies with popular leading actors get higher score predictions.\nDocumentaries in color get higher scores.\nEnglish language films in color tend to do worse.\nEtc.\n\nRoger Ebert is quoted as saying ""No good film is too long and no bad movie is short enough.""\nBest Model\'s Loss\n\nThese are the errors from my best model, the LassoCV estimator.\nRMSE and Median Absolute Error are in terms of the ten-point IMdB rating.\nBest model\'s RMSE, compared to dummy regressor:\n\nIn plain English: If you were to naïvely guess the mean IMdB score for every sample in the test set, your average error in terms of IMdB critic score points would be around 1.1. My best model did somewhat better, with a root-mean-squared error of 0.92.\nProcess\n1. Data Collection\n\nGrab this kaggle imdb dataset:\n\n\n\n\nExtract IMdB ID\'s from each row, spam themoviedb API for more metadata, join it in:\n\n\n2. Data Cleaning\nStructured Data:\n\nKeep only seemingly predictive columns (budget, actor popularities, genre, etc.)\nDrop duplicates\nManage obvious null values (selective imputation, dropping rows)\n\nNull Value Counts by Column:\n\n\nManage non-obvious null values (e.g. zero when there is actually no info)\nBin categorical columns into somewhat balanced metacategories\n\nE.g.: ""Aspect Ratio"" Value Counts Before Binning:\n\n\nAfter Binning:\n\n(Here I used my familiarity with aspect ratios in film to condense this column.)\n\nBox-Cox transform non-normally distributed continuous value columns\n\nE.g. Film Budgets: Vanilla\n\n\nFilm Budgets: Box-Cox Transformed\n\n\nUnstructured Data (Plot Synopses):\n\nTokenize corpus. Remove stopwords, punctuation\nPart-of-speech tag each token\nLemmatize words by POS\nVectorize plot texts with n-gram CountVectorizer\nTF-IDF normalize vectors\n\n3. Feature Engineering\n\nOne-hot encode categorical variables\n\nE.g. ""genre"" column gets split into many binary variables:\n\n\n\nTrain/Test split happens here\nDrop multicollinear features\n\nMulticollinearity of features as a heatmap:\n\n\nGenerate interaction terms\nMinMax scale features\n\n4. Model Building / Benchmarking\nModels evaluated:\n\nBaseline: Dummy regressor\nLinear: Lasso\nNon-Parametric: Linear SVR\nTried incorporating vectorized text features (with PCA) — Not great\nTree-based: Random forest regressor\nNeural Net\n\nUsed Pipeline + GridSearchCV to search for better hyperparameters on text-based regressor and random forest.\nModel Loss Metrics:\n\n'], 'url_profile': 'https://github.com/boscacci', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'NG', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Logistic-Regression-Model\nUsing Logistic Regression Model to determine loan eligibility\n'], 'url_profile': 'https://github.com/Usmanjibril09', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['UK_COVID19_Cumulative_Hospital_Deaths_Model_and_Prediction\nregression method to model and predict UK cumulative hospital death\n'], 'url_profile': 'https://github.com/XingfengLee', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Truncated linear regression\nThis method is based on arXiv:1809.03986 [math] and Daskalakis, C., Gouleakis, T., Tzamos, C., & Zampetakis, M. (2019). Computationally and Statistically Efficient Truncated Regression. COLT.\nPlease check the Jupyter notebook for details.\n'], 'url_profile': 'https://github.com/phylyc', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '330 contributions\n        in the last year', 'description': ['media-company-MLR\nMedia Company Case Study using Multiple Linear Regression Model\n'], 'url_profile': 'https://github.com/junmoan', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nThis notebook is a prediction of salary from the years of experience using simple linear regression.\n'], 'url_profile': 'https://github.com/vamsi-bel', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '451 contributions\n        in the last year', 'description': ['Student-Performance-Predictions\nA Student final grade prediction model using Linear Regression\n'], 'url_profile': 'https://github.com/Aakash-Raman', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mbelc', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'Denver', 'stats_list': [], 'contributions': '848 contributions\n        in the last year', 'description': ['Predicting Home Prices\nusing various multiple linear regression models for predicting home prices\nSlide Show:\nLink to Slide Show\nProject Organization\n├── LICENSE\n├── README.md          <- The top-level README for developers using this project.\n├── data\n│\xa0\xa0 ├── interim        <- Intermediate data that has been transformed.\n│\xa0\xa0 ├── processed      <- The final, canonical data sets for modeling.\n│\xa0\xa0 └── raw            <- The original, immutable data dump.\n│\n├── notebooks          <- Jupyter notebooks. \n│\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n│\n└── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n                          generated with `pip freeze > requirements.txt`\n\n\n'], 'url_profile': 'https://github.com/Jesse989', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}","{'location': 'Mississauga, ON, Canada', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['DM-Regression\nData mining regression analysis on King County house sales dataset\n'], 'url_profile': 'https://github.com/marc254', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Python', 'Updated Apr 19, 2020']}"
"{'location': 'India', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mihirkumar02', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mihirkumar02', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '275 contributions\n        in the last year', 'description': ['poisson_regression\nNotes and code with respect to implementing Poisson regression\n'], 'url_profile': 'https://github.com/ccirelli2', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/adityarpatidar', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Dubai', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': [""Python-code-for-Least-Square-Regression-using-Excel-Data\nPython code for Least Square Regression using Excel Data\nThis is a python code for Least Square Regression. The input data is read from the excel sheet which is present in 'data' folder\nThe procedure is\n\nOutput is\n\n""], 'url_profile': 'https://github.com/siddhaling', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Mississauga, ON, Canada', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['DM-Regression\nData mining regression analysis on King County house sales dataset\n'], 'url_profile': 'https://github.com/marc254', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Churn-Prediction-PySpark\nChurn Prediction of Customers using Logistic Regression - Pyspark\n'], 'url_profile': 'https://github.com/roheetnarayanan', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Karachi,Pakistan.', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['regression-model\nSimple & multiple linear regression model for semantic similarity data\n'], 'url_profile': 'https://github.com/zainraj50', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Denver , USA', 'stats_list': [], 'contributions': '611 contributions\n        in the last year', 'description': ['Case Study Predicting House Price\nCase study by using best model using linear regression\nObjective\nAGENDA is to build a model to predict the selling price of the new houses coming onto the market\nAnalytical Overview\n\nAnalysis of Data :\n\nTypes of Data- All the variables are numeric. No categorical data\nMissing Data- No missing data in the data set.\nAnalysis- EDA and Multiple Linear Regression followed by Model Validation\n\nPackage used :\n\nggplot,dplyr,olsrr,car,boot,DataExplorer\n\n\nVisualization:\nHistogram, Bar graphs,Density plot, Scatterplot, pairplot, heatmap\n\n\nConclusion :\n1.List Price is the proxy to Selling Price , so including list price as a predictor variable decreases the significance of the other variables included.\n2.The best model to predict the selling price of the homes in Colorado Ski Resort is SELLING PRICE= 266.26 + 20.65(BEDROOMS) + 0.042(SQ_FT) – 4.55(MOUNTAIN) + 14.14(GARAGE) + 4.16(LOT SIZE)\n3.The age or the time the house is on market do not effect the selling price.\n4.With the increasing miles of mountain base , the selling price decreases. So lower the miles of mountain better the selling price.\nBlog : Case Study Blog\n'], 'url_profile': 'https://github.com/anuraglahon16', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Dehradun', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shivanshjoshi28', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'MIT license', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['CounterfeitSalesPrediction\nLinear Regression techniques used to predict Conterfeit Medicine Sales.\n'], 'url_profile': 'https://github.com/GlendonRodrigues', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'bhubaneswar', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/iamchiranjeeb', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['India-credit-risk-default-model\nCreate an India credit risk(default) model, using the data provided in the spreadsheet raw-data.xlsx, and validate it on validation_data.xlsx. Please use the logistic regression framework to develop the credit default model. In Default variable, ""0"" indicates not default and ""1"" indicates Default.\n'], 'url_profile': 'https://github.com/Aviator10', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'Sialkot Pakistan', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Artificial-Intelligence-and-Machine-Learning topics are covered\nThis repository includes machine learning models like\nlinear regression (Regression with single variable)\nmultiple regression (Regression with multiple variables)\nlogistic regression (Simple and regularized)\nneural networks (Supervised and Unsupervised)\nclustering (Not Implemented Yet) and other artificial intelligence topics like\nBreadth first search\nDepth first search\nGreedy Approach\nA* Algorithm\nMinimax\nExpectimax and other strategies\nBayes Naive\nSentiment Analysis\n'], 'url_profile': 'https://github.com/ahmadmughal96', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karthicksundar95', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'Islamabad', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Haasha', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'mumbai,maharastra', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Ridge-Lasso-Elasticnet-regressions-\nRidge ,Lasso, Elasticnet regressions\nToyoto corolla car(used cars specifications data)\nthe aim is help the retailer to decide the best buying  price of the customers used car\nhere i have used linear regression models\n'], 'url_profile': 'https://github.com/PATELVIMALV1', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['multilinear-regression\n'], 'url_profile': 'https://github.com/Garimajoshi-star', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/abhishekgotgithub', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['ビジュアルリグレッションテストツール\nversion\n1.0.0\n概要\n前回と今回の実行時の状態を保存したスクリーンショット同時での比較を行うツールです。\n仕様\n\nブラウザ Chromium 75.0.3738.0 を使用しています。（puppeteer の仕様）\n各プロジェクトディレクトリの構成が /projectName/docs/ のように docs の配下にあること。\n\n※ docs がない場合でも l15 の変数 replaceText の値を修正すれば使用できます。\n\n\n初回起動時は比較対象がないため、指定されたファイルのスクリーンショットの撮影だけ行われます。\n\n依存 node package\n以下の package を npm install or yarn add してから使用してください。\n\npuppeteer\n\n自動化テストツール\nスクリーンショットを撮るのに使用\n\n\nresemblejs\n\n画像比較\n\n\nfs-extra\n\nファイル、ディレクトリ操作\n\n\npath\n\nパス操作\n\n\n\n使い方\n\nimageDiff.js を package.json と同階層に置く\n以下の変更箇所を参照して、ローカルに合った設定に変更する\nBash （or コマンドプロンプト or power shell）を起動\n$ node imageDiff of $ node imageDiff.js と打つ\njs ファイルが起動して、root に指定したディレクトリ以下の .html ファイルを全て比較する\n\nオプション\n実行時に --all または -a を付与すると指定ディレクトリ配下の html ファイル全て（node_modules は除く）の比較を行えます。\n$ node imageDiff --all or $ node imadeDiff -a\nl17\n変数 selectPaths\n差分を取りたいパスが一部の場合はこちらで設定してください。\n各自変更箇所\n使用する際に各々の環境に合ったものに変更しなければならない箇所があるので、ご注意ください。\nl11\n変数 device\n撮影したいデバイスに変更できます。\n種類、詳細などは以下のドキュメントを参照してください。\nDeviceDescriptors.js\nl4, l15\nlocalhostName を各々の Xampp で設定している localhost の名前にしてください。\n例：yayoi:8080\nl16\n変数 dirPath\nresolve の第二引数に指定したパス以下の html ファイルを再帰的に検索します。\n注意点\n\n初期起動時は比較用ディレクトリの作成と、元となるスクリーンショットの作成をします。\njs ファイルと同階層に「screenshot」というディレクトリが出来ます\n\ngit 管理不要な場合は .gitignore で設定してください\n\n\nデモでの確認はできません\n\nBasic 認証を突破する記述をしていないため\n\n\n除外したい、したくないディレクトリがある場合は 変数 exclusionDir に | （パイプ）区切りで増減させてください\n\n'], 'url_profile': 'https://github.com/moyashiharusamen', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'JavaScript', 'Updated Apr 14, 2020']}"
"{'location': 'Seoul, Korea', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GaengKo', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'South Korea', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Regression-2\n'], 'url_profile': 'https://github.com/Tyron26', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '297 contributions\n        in the last year', 'description': ['Does attractiveness influence upvotes on Reddit?\nProject for CS 3654\n'], 'url_profile': 'https://github.com/davidhaas6', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/mianadnan30', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'Buffalo, New York, United States', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['Linear-Regression\nThis is the simplest ML algorithm performing Linear regression and then logistic for classification\n'], 'url_profile': 'https://github.com/harshalitalele', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'Poland, Poznan', 'stats_list': [], 'contributions': '515 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GorskiMarekGM', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Stephen491', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'São Paulo', 'stats_list': [], 'contributions': '173 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/evefalcao', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Logistic-regression\n'], 'url_profile': 'https://github.com/FahadTComsats', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MamathaCherukuri', 'info_list': ['Python', 'Updated Apr 14, 2020', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 17, 2020']}"
"{'location': 'Los Angeles, California (CA)', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['Multivariate-regression\nused scikit-learn model\n'], 'url_profile': 'https://github.com/aditya-sarkar441', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['polynomial_regression\n'], 'url_profile': 'https://github.com/Garimajoshi-star', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shantaramkamath', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '414 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aitezazakhtar', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['bananamath.github.io\n'], 'url_profile': 'https://github.com/bananamath', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'Malang, ID', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhimantoro', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rudyorre', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Sampathgubbala', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tusharcastic', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}","{'location': 'Kuala Lumpur, Malaysia', 'stats_list': [], 'contributions': '629 contributions\n        in the last year', 'description': ['CoronaRegression\n'], 'url_profile': 'https://github.com/biranchi2018', 'info_list': ['Python', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'JavaScript', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'C++', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020']}"
"{'location': 'Nadiad,india', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': [""LinearRegression\nOverview\nI'm using a small dataset of student test scores and the amount of hours they studied. Intuitively, there must be a relationship right? The more you study, the better your test scores should be. We're going to use linear regression to prove this relationship.\nDependencies\n\nnumpy\n\nPython 2 and 3 both work for this. Use pip to install any dependencies.\n""], 'url_profile': 'https://github.com/ParthTrada', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/samiBEN91', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['linear-regression\npredict using the LinearRegression model\n'], 'url_profile': 'https://github.com/hasitha-ranasinghe', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Stephen491', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'Patrick F. Taylor Hall', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['linear_regression\nCSC 7333 Project\nInstructor: Dr. Jianhua Chen\nStudent: Saber Nemati\nEmail: mnemat2@lsu.edu\n##########################\nINPUT PARAMETERS\n##########################\nLearning rate (alpha)\nNumber of iterations (n)\nInput data as .csv file\n##########################\nRUNNING INSTRUCTION\n##########################\nThe folder includes two .py files.\nlin_reg_a is for part (a)\nlin_reg_b is for part (b)\nThe code is written in VSCode environment.\nPython version is 3.8.2\nFor running the program simply write this command in the command prompt:\npython lin_reg_a.py\npython lin_reg_b.py\nThe input variable can be adjusted within the first lines of the code in the specified section.\nRequired packeges are:\ncsv\nnumpy\nmatplotlib.pyplot\n##########################\nONLINE ACCESS\n##########################\nThis repository can also be found on this GitHub link:\nhttps://github.com/sabernn/linear_regression\n'], 'url_profile': 'https://github.com/sabernn', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sri07-spec', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/corbinhaugen', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': [""Simple_Regression\nThis Package has been created to give to users an easy experience, in fact the functions that are contained in it are very easy to use and also their outputs are very clear. In the package there are two different functions that can be used in many fields: the first is a function that calculates a simple regression model, while the other is a function that calculates the F-Test about R-Squared (goodness of fit). In addition you can consult the package's documentation in which are contained more information regarding the way in which you can apply the functions and the way in which you can read (correctly) the outputs.\n""], 'url_profile': 'https://github.com/AntonioP-R', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jklimmek', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '230 contributions\n        in the last year', 'description': ['Linear-Regression\nA basic implementation of Linear Regression in an attempt to predict future movie revenue based on initial budget.\n'], 'url_profile': 'https://github.com/kaseypcantu', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jklimmek', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/adi11m', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Regression-Models\nRegression models practice while doing a course on Machine Learning from Udemy in both Python and R languages.\n'], 'url_profile': 'https://github.com/GMADHURIDSP', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['LogisticRegression\n逻辑回归是一种被广泛使用的分类算法，本次实验自己根据数学公式实现逻辑回归，并与sklearn库中的LogisticRegression库函数对比发现，自己实现的逻辑回归函数在本数据集中分类准确率更高。\nspambase.data为数据集\ntest.py为进行二分类的程序文件\nlikely_function.png为程序导出的训练过程中似然函数值的变化曲线\n'], 'url_profile': 'https://github.com/BuleSky233', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Nadiad,india', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': [""LinearRegression\nOverview\nI'm using a small dataset of student test scores and the amount of hours they studied. Intuitively, there must be a relationship right? The more you study, the better your test scores should be. We're going to use linear regression to prove this relationship.\nDependencies\n\nnumpy\n\nPython 2 and 3 both work for this. Use pip to install any dependencies.\n""], 'url_profile': 'https://github.com/ParthTrada', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/samiBEN91', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['linear-regression\npredict using the LinearRegression model\n'], 'url_profile': 'https://github.com/hasitha-ranasinghe', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'South Korea', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/skyiwalker', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['linear_Regression\nthis is building a demo page for Gradient Descent with JS by following D Shiffman\n'], 'url_profile': 'https://github.com/rsurpur20', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'kolkata', 'stats_list': [], 'contributions': '173 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/97ishika', 'info_list': ['Python', 'Updated Apr 13, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mmthant03', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neilwrawlins', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '198 contributions\n        in the last year', 'description': ['Logistic-regression\n108-1 ML HW4\n'], 'url_profile': 'https://github.com/91884227', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['logistic_regression\n'], 'url_profile': 'https://github.com/suranyasunkavilli27', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'Chennai, India', 'stats_list': [], 'contributions': '284 contributions\n        in the last year', 'description': ['Regression_analysis\nPrediction of CPU job waiting time using regression techniques and time series analysis.\nUsage of various Machine Learning algorithms to predict the waiting time for a CPU job based on past data from Auver Grid dataset.\nThe models used are:\n\nLinear Regression\nDecision Tree\nRandomforest Regressor\nSupport Vector Regressor(SVR)\nK-Nearest Neighbour(KNN)\nXGBoost Regressor\nRecurrent Neural Network(Long short-term memory)\nArtificial Neural Network\nMulti Layer Perceptron\n\n'], 'url_profile': 'https://github.com/antoprince001', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'South Korea', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/skyiwalker', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['linear_Regression\nthis is building a demo page for Gradient Descent with JS by following D Shiffman\n'], 'url_profile': 'https://github.com/rsurpur20', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'kolkata', 'stats_list': [], 'contributions': '173 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/97ishika', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hp-01', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}","{'location': 'Seattle, WA, USA', 'stats_list': [], 'contributions': '421 contributions\n        in the last year', 'description': ['regression_discontinuity\n'], 'url_profile': 'https://github.com/glmack', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'JavaScript', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020']}"
"{'location': 'IIT(BHU), Varanasi, India', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['recognizance_regression\n'], 'url_profile': 'https://github.com/pkyadav73199', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '533 contributions\n        in the last year', 'description': ['stepwise_regression\nRough implementations of stepwise regressions in Python, from An Introduction to Statistical Learning and Montgomery Peck Vining books\n'], 'url_profile': 'https://github.com/riledigital', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'South Korea', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Regression-1-\n'], 'url_profile': 'https://github.com/Tyron26', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/parth4496', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '398 contributions\n        in the last year', 'description': [""GPULogisticRegression\nRunning CUDA code:\nPutting instructions in here since I'm tired of looking at a prior blackboard assignment to figure this out.\n\nLogin to HPC w/ USC credentials at @ hpc-login3.usc.edu\nSetup the CUDA toolchain with source /usr/usc/cuda/default/setup.sh from your home directory. Note: This has to be done every time you login to HPC, I don't know why.\nCompile your program with the CUDA compiler: nvcc -o <executable name> -O3 <filename>.\nRun your executable on the GPU using srun -n1 --gres=gpu:1 ./<executable name>. Note: Please do not do just ./<executable name>. It will work but it will not run on the GPU making all our CUDA code effectively useless and will keep you thinking that your code is wrong when it really might not be.\n\n""], 'url_profile': 'https://github.com/gbains8172', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': [""Regression_Algorithm\nA short description of the project.\nProject Organization\n├── LICENSE\n├── Makefile           <- Makefile with commands like `make data` or `make train`\n├── README.md          <- The top-level README for developers using this project.\n├── data\n│\xa0\xa0 ├── external       <- Data from third party sources.\n│\xa0\xa0 ├── interim        <- Intermediate data that has been transformed.\n│\xa0\xa0 ├── processed      <- The final, canonical data sets for modeling.\n│\xa0\xa0 └── raw            <- The original, immutable data dump.\n│\n├── docs               <- A default Sphinx project; see sphinx-doc.org for details\n│\n├── models             <- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n│\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n│\n├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n│\xa0\xa0 └── figures        <- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze > requirements.txt`\n│\n├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n├── src                <- Source code for use in this project.\n│\xa0\xa0 ├── __init__.py    <- Makes src a Python module\n│   │\n│\xa0\xa0 ├── data           <- Scripts to download or generate data\n│\xa0\xa0 │\xa0\xa0 └── make_dataset.py\n│   │\n│\xa0\xa0 ├── features       <- Scripts to turn raw data into features for modeling\n│\xa0\xa0 │\xa0\xa0 └── build_features.py\n│   │\n│\xa0\xa0 ├── models         <- Scripts to train models and then use trained models to make\n│   │   │                 predictions\n│\xa0\xa0 │\xa0\xa0 ├── predict_model.py\n│\xa0\xa0 │\xa0\xa0 └── train_model.py\n│   │\n│\xa0\xa0 └── visualization  <- Scripts to create exploratory and results oriented visualizations\n│\xa0\xa0     └── visualize.py\n│\n└── tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n\n\nProject based on the cookiecutter data science project template. #cookiecutterdatascience\n""], 'url_profile': 'https://github.com/Yujie-Xu', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '267 contributions\n        in the last year', 'description': ['Linear Regression-ML \nImplementation of Linear Regression Algorithm for Machine Learning from my studies\nHow to use it\nIf there is a problem with running, try to open it by console with command:\n\nCreate your virtual environment.\nInstall all packages from requirements.txt:\n\n  pip install -r requirements.txt\n\nRun the application:\n\n  python main.py\n\n📫 Contact\nfeel free to contact me! ✊\n'], 'url_profile': 'https://github.com/M0ng00se7169', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '185 contributions\n        in the last year', 'description': ['Linear Regression\n\nWhat is Linear Regression?\nHypothesis of Linear Regression\nHow does Linear Regression work?\nWhen to use Linear Regression?\nHow do we determine the best fit line?\nAssumptions in linear regression\nFew properties of Regression Line\nModel Performance\nFeature Selection\nCost Function\nGradient Descent\nTypes of linear regression\nLinear Regression with multiple variables\nData preparation for Linear Regression\nLinear Regression with statsmodels\nHow to select the right Regression Model?\nRegularization\nHow do I know these assumptions are violated in my data?\nHow can you improve the accuracy of a regression model ?\n\nWhat are the assumpations made in Linear Regression?\n\n\nThere are mainly 4 types of assumpation made in Linear regression:\n\nRelationship between dependend and independent variables should be a linear.\nThe independent variable should not be co related. ie. multicollinearity.\nThe error terms (Residuals) must have constant variance. This is know as Homoscadasticity.\nThe error term (Residual) must be normally distributed.\n\n################################################################################################################\n\nhttps://www.knowledgehut.com/blog/data-science/linear-regression-for-machine-learning\nhttps://scikit-learn.org/stable/modules/linear_model.html\nhttps://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/\n\n'], 'url_profile': 'https://github.com/imsanjit', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '311 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression from scratch using Python\n'], 'url_profile': 'https://github.com/theakhiljha', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Bangalore. ', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rakesh8667', 'info_list': ['Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', '1', 'R', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Cuda', 'Updated May 3, 2020', 'C++', 'MIT license', 'Updated Apr 28, 2020', 'Python', 'Updated Jul 7, 2020', 'Updated Sep 11, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prakash507979', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AvikKole', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['regression-by-factor\nThere is created model of linear regression.\nResponse variable is numeric.\nExplanatory variables are numeric and factors.\n'], 'url_profile': 'https://github.com/IEkaterina', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sumit4494', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['multivariableRegression\nDisplays house prices in any area using any defined data set.\n'], 'url_profile': 'https://github.com/viveksingh221297', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['Linear-Regression\nThis is a programming assignment done for the CSCI 567 - Machine Learning class.\nThe code above implements Linear, Ridge and Polynomial Regression with the metric of evaluation being Mean Absolute Error(MAE).\nlinear_regression.py implements the algorithm for Linear, Ridge and Polynomial Regression. This is a complete implementation that is also provided with a test dataset on wine qualities.\nTo run the above code: $ python linear_regression_test.py\n'], 'url_profile': 'https://github.com/SunnySingh00', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neilwrawlins', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['before running this program you should install some python library\nnumpy,\nseaborn,\npandas,\nmatplotlib,\nThis program has developed in juypyter notebook.\n'], 'url_profile': 'https://github.com/parth967', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/jainil24', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/jainil24', 'info_list': ['Updated Apr 13, 2020', '1', 'Updated Apr 13, 2020', 'Updated Jun 16, 2020', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kutieme', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'CHENNAI', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': [""Linear-Regression\nStep 1: Import necessary libraries\nimport pandas as pd  - for working with dataset\nimport numpy as np   - numerical Python\nimport matplotlib.pyplot as plt - To visualise the results\nfrom sklearn.model_selection import train_test_split - To split train and test data\nfrom sklearn.linear_model import LinearRegression - Import the LinearRegression model\nStep 2: Importing the dataset\nX=dataset.iloc[:,:-1].values - Independent Variable X\ny=dataset.iloc[:,-1].values  - Dependent Variable Y\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=2)\nStep 3: Fit and Predict\nregressor=LinearRegression()\nregressor.fit(X_train,y_train)\nypred=regressor.predict(X_train)\nStep 4: Visualise the Results\nplt.scatter(X_train,y_train,color='red')\nplt.plot(X_train,ypred)\nplt.xlabel('Experience')\nplt.ylabel('Salary')\nplt.show()\n""], 'url_profile': 'https://github.com/sakthianand7', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/frosteen', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'Delft', 'stats_list': [], 'contributions': '1,826 contributions\n        in the last year', 'description': [""hgboost - Hyperoptimized Gradient Boosting\n\n\n\n\n\n\n\n\n\n\nStar it if you like it!\n\nhgboost is short for Hyperoptimized Gradient Boosting and is a python package for hyperparameter optimization for xgboost, catboost and lightboost using cross-validation, and evaluating the results on an independent validation set.\nhgboost can be applied for classification and regression tasks.\nhgboost is fun because:\n* 1. Hyperoptimization of the Parameter-space using bayesian approach.\n* 2. Determines the best scoring model(s) using k-fold cross validation.\n* 3. Evaluates best model on independent evaluation set.\n* 4. Fit model on entire input-data using the best model.\n* 5. Works for classification and regression\n* 6. Creating a super-hyperoptimized model by an ensemble of all individual optimized models.\n* 7. Return model, space and test/evaluation results.\n* 8. Makes insightful plots.\n\nDocumentation\n\nAPI Documentation: https://erdogant.github.io/hgboost/\nGithub: https://github.com/erdogant/hgboost/\n\nRegression example\n\n\n\nClassification example\n\n\n\nSchematic overview of hgboost\n\n\n\nInstallation Environment\n\nInstall hgboost from PyPI (recommended). hgboost is compatible with Python 3.6+ and runs on Linux, MacOS X and Windows.\nA new environment is recommended and created as following:\n\nconda create -n env_hgboost python=3.6\nconda activate env_hgboost\nInstall newest version hgboost from pypi\npip install hgboost\n\nForce to install latest version\npip install -U hgboost\nInstall from github-source\npip install git+https://github.com/erdogant/hgboost#egg=master\nImport hgboost package\nimport hgboost as hgboost\nClassification example for xgboost, catboost and lightboost:\n# Load library\nfrom hgboost import hgboost\n\n# Initialization\nhgb = hgboost(max_eval=10, threshold=0.5, cv=5, test_size=0.2, val_size=0.2, top_cv_evals=10, random_state=42)\n# Import data\ndf = hgb.import_example()\ny = df['Survived'].values\ny = y.astype(str)\ny[y=='1']='survived'\ny[y=='0']='dead'\n\n# Preprocessing by encoding variables\ndel df['Survived']\nX = hgb.preprocessing(df)\n# Fit catboost by hyperoptimization and cross-validation\nresults = hgb.catboost(X, y, pos_label='survived')\n\n# Fit lightboost by hyperoptimization and cross-validation\nresults = hgb.lightboost(X, y, pos_label='survived')\n\n# Fit xgboost by hyperoptimization and cross-validation\nresults = hgb.xgboost(X, y, pos_label='survived')\n\n# [hgboost] >Start hgboost classification..\n# [hgboost] >Collecting xgb_clf parameters.\n# [hgboost] >Number of variables in search space is [11], loss function: [auc].\n# [hgboost] >method: xgb_clf\n# [hgboost] >eval_metric: auc\n# [hgboost] >greater_is_better: True\n# [hgboost] >pos_label: True\n# [hgboost] >Total dataset: (891, 204) \n# [hgboost] >Hyperparameter optimization..\n#  100% |----| 500/500 [04:39<05:21,  1.33s/trial, best loss: -0.8800619834710744]\n# [hgboost] >Best performing [xgb_clf] model: auc=0.881198\n# [hgboost] >5-fold cross validation for the top 10 scoring models, Total nr. tests: 50\n# 100%|██████████| 10/10 [00:42<00:00,  4.27s/it]\n# [hgboost] >Evalute best [xgb_clf] model on independent validation dataset (179 samples, 20.00%).\n# [hgboost] >[auc] on independent validation dataset: -0.832\n# [hgboost] >Retrain [xgb_clf] on the entire dataset with the optimal parameters settings.\n# Plot searched parameter space \nhgb.plot_params()\n\n\n\n\n# Plot summary results\nhgb.plot()\n\n\n\n# Plot the best tree\nhgb.treeplot()\n\n\n\n\n# Plot the validation results\nhgb.plot_validation()\n\n\n\n\n\n# Plot the cross-validation results\nhgb.plot_cv()\n\n\n\n# use the learned model to make new predictions.\ny_pred, y_proba = hgb.predict(X)\nCreate ensemble model for Classification\nfrom hgboost import hgboost\n\nhgb = hgboost(max_eval=100, threshold=0.5, cv=5, test_size=0.2, val_size=0.2, top_cv_evals=10, random_state=None, verbose=3)\n\n# Import data\ndf = hgb.import_example()\ny = df['Survived'].values\ndel df['Survived']\nX = hgb.preprocessing(df, verbose=0)\n\nresults = hgb.ensemble(X, y, pos_label=1)\n\n# use the predictor\ny_pred, y_proba = hgb.predict(X)\nCreate ensemble model for Regression\nfrom hgboost import hgboost\n\nhgb = hgboost(max_eval=100, threshold=0.5, cv=5, test_size=0.2, val_size=0.2, top_cv_evals=10, random_state=None, verbose=3)\n\n# Import data\ndf = hgb.import_example()\ny = df['Age'].values\ndel df['Age']\nI = ~np.isnan(y)\nX = hgb.preprocessing(df, verbose=0)\nX = X.loc[I,:]\ny = y[I]\n\nresults = hgb.ensemble(X, y, methods=['xgb_reg','ctb_reg','lgb_reg'])\n\n# use the predictor\ny_pred, y_proba = hgb.predict(X)\n# Plot the ensemble classification validation results\nhgb.plot_validation()\n\n\n\n\n\nCitation\nPlease cite hgboost in your publications if this is useful for your research. Here is an example BibTeX entry:\n@misc{erdogant2020hgboost,\n  title={hgboost},\n  author={Erdogan Taskesen},\n  year={2020},\n  howpublished={\\url{https://github.com/erdogant/hgboost}},\n}\nReferences\n* http://hyperopt.github.io/hyperopt/\n* https://github.com/dmlc/xgboost\n* https://github.com/microsoft/LightGBM\n* https://github.com/catboost/catboost\n\nMaintainers\n\nErdogan Taskesen, github: erdogant\n\nContribute\n\nContributions are welcome.\n\nLicence\nSee LICENSE for details.\nCoffee\n\nThis work is created and maintained in my free time. If you wish to buy me a Coffee for this work, it is very appreciated.\n\n""], 'url_profile': 'https://github.com/erdogant', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'Italy', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cipryyyy', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['logistic_regression_model_python\n'], 'url_profile': 'https://github.com/MariyaLe', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\n'], 'url_profile': 'https://github.com/jainil24', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Logistic-Regression-In-R\n'], 'url_profile': 'https://github.com/saideepakreddi', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': ['Least-Squares-Linear-Regression\nlaboratory #7 for Uni\n'], 'url_profile': 'https://github.com/george200150', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mianadnan30', 'info_list': ['Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 13, 2020', 'Python', 'Updated Jan 17, 2021', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'R', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Updated Apr 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""Simple-Linear-Regression\nThis Repository is created on 16 April 2020 and Contains Simple Linear Regression Using Python's Numpy and Matplotlib Module...\n""], 'url_profile': 'https://github.com/The-Bad-Coder', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['Machine-Learning-Polynomial-Regression\n'], 'url_profile': 'https://github.com/rameshbabulakshmanan84', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ahilmurugesan', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '414 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aitezazakhtar', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'Bronx, Ny, US', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TheWizard91', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Gradient-Boosting-Regression---Sklearn\nBoosting is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set.Gradient Boosting trains many models in a gradual, additive and sequential manner. Over here i am using for regression\n'], 'url_profile': 'https://github.com/kdmac', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hudson11', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'Jaipur', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Linear-Regression-Multiple-Variables\nMachine Learning With Python: Linear Regression Multiple Variables Sample problem of predicting home price in monroe, new jersey (USA) Below is the table containing home prices in monroe twp, NJ. Here price depends on area (square feet), bed rooms and age of the home (in years). Given these prices we have to predict prices of new homes based on area, bed rooms and age.    Given these home prices find out price of a home that has,  3000 sqr ft area, 3 bedrooms, 40 year old  2500 sqr ft area, 4 bedrooms, 5 year old\n'], 'url_profile': 'https://github.com/NEERAJSHARMABSDU', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nPython Implementation of Simple Linear Regression along with thorough explanation of the concept\n'], 'url_profile': 'https://github.com/Avhijit-codeboy', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}","{'location': 'Meerut, Uttar Pradesh, India', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prinzz1208', 'info_list': ['Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'HTML', 'Updated Apr 29, 2020']}"
"{'location': 'Mandi, Himachal Pradesh, India', 'stats_list': [], 'contributions': '231 contributions\n        in the last year', 'description': ['Linear-Regression-Gradient-Descent-\nThis is a from scratch implementaion of the Linear Regression Algorithm\n'], 'url_profile': 'https://github.com/NippunSharma', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'Cape Town, South Africa', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Machine-Learning-polynomial-regression\n??\n'], 'url_profile': 'https://github.com/TyroneJensen', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '342 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/magedhelmy1', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kancharlavamshi', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['Mini-Project-Logistic-Regression\n'], 'url_profile': 'https://github.com/tamercetin', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Linear-Regression-from-scratch\n'], 'url_profile': 'https://github.com/NatsuLaxus', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['This repository contains the linear regression theory and my own python implementation of the linear regression class.\nThe codes make use of the numpy library.\nThere is a comparison between the results of my own implementation and the results given by scikit learn.\n'], 'url_profile': 'https://github.com/prieslei', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['SBIR regression\nThis repo contains code for the C&G paper ""Sketching out the details: Sketch-based image retrieval using convolutional neural networks with multi-stage regression""\nDependencies\nYou will need to compile Caffe v1.0 with customized L2 normalize layer. Check caffe_utils/README.md for instructions.\nAlternatively, you can use standard Caffe, just remove the normalize layer in model/*.prototxt, then normalise the output manually using e.g. numpy.\nPretrained model\nPretrained model (and dataset) can be downloaded here.\nFeature extraction\nCheck getfeat_img.py and getfeat_skt.py for examples of extracting features from a raw image/sketch.\nDemo\n\nReference\n@article{bui2018sketching,\n  title={Sketching out the Details: Sketch-based Image Retrieval using Convolutional Neural Networks with Multi-stage Regression},\n  author={Bui, Tu and Ribeiro, Leonardo and Ponti, Moacir and Collomosse, John},\n  journal={Computers \\& Graphics},\n  year={2018},\n  publisher={Elsevier}\n}\n\n'], 'url_profile': 'https://github.com/omerhameed98', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ankitmhjn13', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/beetoogoswami', 'info_list': ['Jupyter Notebook', 'Updated Sep 8, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Apr 19, 2020']}"
"{'location': 'IIT BHU, India', 'stats_list': [], 'contributions': '609 contributions\n        in the last year', 'description': ['Simple-Multivariate-Linear-Regression\nWe will see how to start with linear regression. We will be doing multivariate linear regression using Numpy and will try to make predictions. We will be using Normal Equation method for our purpose here.\nThis project includes the code, the datasets for training and the output dataset when run on the training set.\nWe have 4 different parameters here. The parameters here are for a shopping company and we have to predict the amount spent by the customer based on four parameters.\n'], 'url_profile': 'https://github.com/shubham1710', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['pytorch-advantage-weighted-regression\nA PyTorch Implementation of AWR (github.com/xbpeng/awr)\n'], 'url_profile': 'https://github.com/tesslerc', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '194 contributions\n        in the last year', 'description': [""선형회귀분석 프로젝트\nGoals\nKBO 프로야구 타자 연봉 예측\nTechnical Skills\n\n사용언어: python\n데이터 크롤링: beautifulsoup, selenium\n데이터 전처리: pandas\n시각화: plotly\n선형회귀: statsmodels\n\nData\nstatiz의 데이터 사용\ntrain\n시즌 2010 ~ 2017 데이터\ntest\n시즌 2018 데이터\nProcedure\n\n모델1   야구선수기록에 시즌, 나이를 카테고리화 하여 나누고, 변수들에 scaling 하여 분석\n모델2\n종속변수인 '연봉'에 로그를 취하여 분석\n모델3\n수상 지표, 시즌별 팀순위, 용병 여부 추가하여 분석\n모델4\nFA계약 데이터 추가, 연봉의 현재 가치 반영하여 분석\nResult\n\n\n\n\n\n\nR-squared\n0.868\n\n\n\n\nMSE\n0.13\n\n\n\n\n""], 'url_profile': 'https://github.com/pearl-lee', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wdelawed', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'Windsor, Ontario, Canada', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Using Logistic regression on the famous Breast Cancer Dataset. Implementation done in Google Colab.\n'], 'url_profile': 'https://github.com/Aman2612', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Linear-Regression-Project\n'], 'url_profile': 'https://github.com/ian-chris', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'Rouen', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jean-Eudes-Rouffiac', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ishaq101', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abdurrahmanshidiq', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '427 contributions\n        in the last year', 'description': ['regression-mobile-games\nOverview\n\nProject aims to predict the app store rating of a mobile game using multiple linear regression, considering factors such as game size, genre, and price.\nUsed mobile strategy games dataset from Kaggle to train and test my model.\n\nData Cleaning\nThe dataset contained data on 16,847 different games. I performed data cleaning to extract data that I felt would be beneficial to train the model with and eliminate data that I deemed irrelevant to the model. I made the following changes:\n\nRemoved all entries without user rating data\nRemoved all entries with less than 100 user ratings to ensure that the ratings are credible (the higher volume of ratings the more likely they are a true representative of consumer opinion)\n\nI then added the following columns:\n\nSubtitle (y/n): an indicator variable describing whether or not the game had a subtitle.\nIn-app purchases: in-app purchases range in price from $0 to $99.99. I grouped games into 4 quadrants by price:\n\n$0 - $24.99\n$25 - $49.99\n$50 - 74.99\n$75 - $99.99\n\n\nNumber of words in description\nAge Rating: in the original dataset, there are 4 possible values for this column- 4+, 9+, 12+, and 17+. It is important to note that 9+, 12+, and 17+ games are also 4+ games, 12+ and 17+ games are also 9+ games, and 17+ games are also 12+ games. Thus, when creating indicator variables, I decided to make the categories \'< 9\', \'< 12\', and \'< 17\'. 4+ games satisfy < 9, 4+ and 9+ games satisfy < 12, 4+, 9+, and 12+ games satisfy < 17, while 17+ games do not satisfy any of the categories.\nNumber of languages the game is offered in\nSize of game (in bytes)\nGenre: I created indicator variables for each possible game classification genre (there are 35 in total, examples include ""Education"", ""Finance"", and ""Lifestyle""; I disregarded the ""Game"" genre because all data entries were marked with the ""Game"" genre.\nTime since last update (in days, as of April 20, 2020)\n\nAfter completing the data cleaning step of the project, I ended up with data from 2982 unique games to train and test my model with.\nModel Building and Performance\nI split the data into training and testing sets (50/50 split). I built the model using multiple linear regression and evaluated it using R-squared and Mean Absolute Error. I used the R-squared value to evaluate how accurately my model is able to predict mobile game ratings. I chose to use MAE to evaluate my model because it is relatively easy to interpret and outliers aren’t particularly bad in for this type of model.\nThe model has an R-squared value of 0.1678 and a Mean Absolute Error of 0.37.\nFindings\nI used this backward elimination tutorial to perform backward elimination on the variables to find the most significant variables. Using an significance level of 0.05, the following variables were deemed to be significant (P-Value < Significance Level):\nPositive Coefficient of Regression\n\nSubtitle (y/n)\nIn-app purchases ranging from $0 to $24.99\nAge Rating < 12\nSize of game ranging from 46.5 MB to 112 MB\nSize of game ranging from 112 MB to 221 MB\nSize of game larger than 221 MB\nGenre: Magazines and Newspaper\n\nNegative Coefficient of Regression\n\nAge Rating < 17\nGenre: Adventure\nGenre: Board\nGenre: Reference\nDays Since Last Update\n\nWe can make a few observations from the information above:\n\nInclude a subtitle (something like ""Original Brain Training"" for Sudoku)\nInclude in-app purchases, but the price range for these items should be from $0 to $24.99\nCater to wider varieties of audiences, ensure that your game is playable by younger audiences (4+, 9+ range)\nMake games more detailed/ more content/ better graphics- larger install sizes are positively correlated with better ratings\nGenres such as ""Magazines and Newspapers"" are generally higher rated\nGenres such as ""Adventure"" and ""Board"" are generally lower rated. This could be because personal tastes vary more drastically for apps/games of this category, and people could leave distasteful ratings if the game is not built perfectly to their liking or if the game is frustrating to play for a person of average/below average skill level.\nApps that were updated the most recently had lower ratings in general. I am unable to generate any concrete conclusions from this information, however a possible hypothesis is that there has been a recent trend in the game development industry of adding features that users dislike, such as increased advertisements or expensive in-app purchases that increase the disparity between paying and non-paying users. It would have been interesting to do an analysis on how the frequency of updates impacts average ratings. However, there was insufficient data to do so.\n\nHowever, it is important to note that using only the aforementioned significant variables to test and train a multiple linear regression model resulted in an R-squared value of 0.1684 and a Mean Absolute Error of 0.367, which are not significant improvements, so the observations generated do not carry much weight.\nThoughts and Conclusions\nIn conclusion, it appears that multiple linear regression may not be a good model when modeling this scenario, though it is possible that it is hard to predict mobile game ratings to a high degree of accuracy in general as there is an element of randomness to ratings (hence the low R-squared value). Often times, people only rate the game when they have to and do not put much time or thought into it. Speaking from personal experience, I believe that people often make impulsive extreme ratings (ratings on the extreme of both the positive and negative ends of the grading scale) depending on their first impressions of the game, which skews the ratings and makes them harder to predict.\nAn alternate possibility to the low R-squared value is that mobile game ratings are influenced by a myriad of other factors not included in the dataset, such as whether or not the game prompts the user to give it a review.\nAlthough the model had a low R-squared value, I believe that the mean absolute error of 0.37 is not all that bad because in my opinion, it still reasonably conveys public opinion on the game. I personally subconsciously round mobile game ratings to the nearest whole number to associate it with a word (very bad, bad, average, good, very good) when gauging the quality of the game, and an error of 0.37 means that I would still be rounding to the same number in most cases. However, I understand that this could be different for others and this model may not be useful to others when they gauge the quality of a game.\nFrom the game designer\'s point of view, when designing a game, it is important to keep the goals of designing the game in mind (to receive as high of a rating as possible, to reach as many people as possible, to generate as much revenue as possible, etc.). Hence, although it is never hurts to consider which factors positively or negatively impact game ratings (as stated in the ""Findings"" section), it is not the only thing to consider during game design.\nLimitations\nA few limitations of my model are:\n\nWhen counting the length of the game description, I noticed that some descriptions contained unicode for special characters such as emojis, and I was unable to find a way to exclude those from the count.\nI believe that there are more meaningful ways to categorize the in-app purchase prices, as from personal experience, certain price points are more common than others. For example, a majority of the games I have played in the past have only had in-app purchases worth $9.99 or less, so it might be beneficial to make a category from $0 to $9.99 instead of $0 to $24.99.\n\nFuture Work\n\nTry to use different models to train and test the data (not just multiple linear regression)\nPotentially add different kinds of data (different columns) to the dataset\n\n'], 'url_profile': 'https://github.com/ryanwonghc', 'info_list': ['Python', 'Updated May 2, 2020', '1', 'Python', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020']}"
"{'location': 'Nugegoda', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Linear-Regression-Analysis-Model\n'], 'url_profile': 'https://github.com/RukshanAkalanka', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'india', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pawneshg', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShrutiUdagire', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mauricio-Physics', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vkovuru', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '323 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SanjivG10', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '198 contributions\n        in the last year', 'description': ['regularized-linear-model-regression\n108-1 ML HW1\n'], 'url_profile': 'https://github.com/91884227', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Jayuya, Puerto Rico', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/andrewgonzalez4', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Kochi', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bevin7', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Advanced_Regression_project\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\nThe company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\nWhich variables are significant in predicting the price of a house, and\nHow well those variables describe the price of a house.\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal\nYou are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/sharadsharmam', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}"
"{'location': 'Nugegoda', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Linear-Regression-Analysis-Model\n'], 'url_profile': 'https://github.com/RukshanAkalanka', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'india', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pawneshg', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShrutiUdagire', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mauricio-Physics', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vkovuru', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '323 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SanjivG10', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '198 contributions\n        in the last year', 'description': ['regularized-linear-model-regression\n108-1 ML HW1\n'], 'url_profile': 'https://github.com/91884227', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Jayuya, Puerto Rico', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/andrewgonzalez4', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Kochi', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bevin7', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Advanced_Regression_project\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\nThe company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\nWhich variables are significant in predicting the price of a house, and\nHow well those variables describe the price of a house.\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal\nYou are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/sharadsharmam', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}"
"{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '414 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aitezazakhtar', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '283 contributions\n        in the last year', 'description': ['NYC-Airbnb-regression\nMultiple regression analysis of price determinants for Airbnb in New York City.\n'], 'url_profile': 'https://github.com/adxpillar', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Tree-Based-Regression\nWe seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable\nSteps\n(a) Splitted the data set into a training set and a test set.\n(b) Fitted a regression tree to the training set. Plot the tree, and interpret the results. Then computed the test MSE.\n(c) Pruned the tree obtained in (b). Used cross validation to determine the optimal level of tree complexity. Plotted the pruned tree to interpret the results\n(d) Used the Bagging approach to analyze the data. Explored three different values for hyper-parameters to document Test MSE, to determine the variables that are most important.\n(e) Used Random Forest to analyze the data. Explored three different values for hyper-parameters to document Test MSE for determining the variables that are most important.\n(d) Used the Boosting approach to analyze the data. Explored three different values for hyper-parameters to document Test MSE for determining the variables that are most important.\n'], 'url_profile': 'https://github.com/Hirenkr', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': [""LinearRegression_project\nProblem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Preparation\nThere is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. You need to consider only company name as the independent variable for model building.\nModel Evaluation:\nWhen you're done with model building and residual analysis, and have made predictions on the test set, just make sure you use the following two lines of code to calculate the R-squared score on the test set.\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred)\nwhere y_test is the test data set for the target variable, and y_pred is the variable containing the predicted values of the target variable on the test set.\nPlease don't forget to perform this step as the R-squared score on the test set holds some marks. The variable names inside the 'r2_score' function can be different based on the variable names you have chosen.\n""], 'url_profile': 'https://github.com/sharadsharmam', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Linear_regression_testing\nHelper function for performing simple linear regression and several types of associated tests.\n'], 'url_profile': 'https://github.com/Leilusen', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'Udaipur', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Logistic-Regression-and-KNN\n'], 'url_profile': 'https://github.com/DhanuSingh', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Regression-Analysis---R\nBasic regression analysis for weekly sales volume of juices in the grocery stores chain using R programming\n'], 'url_profile': 'https://github.com/asyla02', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '345 contributions\n        in the last year', 'description': ['LASSO-AND-RIDGE-REGRESSION-\n'], 'url_profile': 'https://github.com/NarayanaReddy29', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nithin-cy', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['StreamflowStatRegression\n'], 'url_profile': 'https://github.com/bangen', 'info_list': ['Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 15, 2020', 'R', 'GPL-3.0 license', 'Updated Apr 16, 2020']}"
"{'location': 'nit hamirpur', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['Regression-ML-Model\nMachine Learning Regression  Model which decides the price of a bulldozer by its features.\nThe dataset is taken from a competition from kaggle\nThe dataset has null values, and categorical as well as numerical entries\n'], 'url_profile': 'https://github.com/rishik-00', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ishaq101', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'Noida', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Simple_Linear_Regression\n'], 'url_profile': 'https://github.com/Aarti1907', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'Bay Area, CA', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/olgOk', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['linear_regression_model_python\n'], 'url_profile': 'https://github.com/MariyaLe', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'Suwon, South Korea', 'stats_list': [], 'contributions': '835 contributions\n        in the last year', 'description': ['서울시 시위 규모와 정치 뉴스 \'화나요\' 상관관계 분석\n\n\n2019년도 2학기 경희대 웹파이썬 수업 개인 텀 프로젝트 문서입니다\n\n\n주피터 파일로 정리했습니다.\n\ny = 서울시 주요 집회 신고 인원, x = 네이버 정치 뉴스 ""화나요""\nselenium 으로 서울지방경찰청 사이트에서 이미지를 크롤링 했습니다.\nBeautifulSoup 로 네이버 정치 랭킹 뉴스 url을 크롤링하고, selenium으로 \'화나요\' 수를 크롤링 했습니다.\ntesseract로 이미지로부터 숫자를 추출했습니다. (OCR)\npandas로 간단한 데이터 전처리 과정을 거쳤습니다.\nmatplotlib으로 시각화했습니다.\nstatsmodels 으로 회귀분석을 실시했습니다.\ntensorflow로 학습을 시켰으나 결과가 좋지 못했습니다.\n\n\n\n'], 'url_profile': 'https://github.com/seung-00', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanchoalric', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/praveen6993', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nandan-pattanayak', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['Regressão\nConsumo de cerveja na capital paulista\n'], 'url_profile': 'https://github.com/gizattos', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Aug 3, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pratikasarkar', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '208 contributions\n        in the last year', 'description': ['Logistic-Regression-Project\nThis repository contains my solution of Logistic-Regression-Project(Provided by Udemy in its course Python for Data Science)\n'], 'url_profile': 'https://github.com/dharma610', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['predict_sales_regression_model\n'], 'url_profile': 'https://github.com/pj35134', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Linear_regression_model\nLinear regression model Implementation in python with out using any library. For better intuition how each pieces moves.\n'], 'url_profile': 'https://github.com/pramod4040', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,014 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Annarien', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '198 contributions\n        in the last year', 'description': ['-Baysian-Linear-regression\n108-1 ML HW3\n'], 'url_profile': 'https://github.com/91884227', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Tangerang Selatan, Indonesia', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ridhoaryo', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['#Problem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large data set of different types of cars across the America market.\nBusiness goal\nWe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/ekohn13', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bbclue', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/estelle-eteki', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}"
"{'location': 'China', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['EMD-GA-DBN-Regression\n（MATLAB CODE） Establishing a time series forecasting model for wind speed prediction based on DBN，Due to the autocorrelation of the wind speed sequence, the predicted value and the actual value lag, so EMD is used to decompose the wind speed sequence, and then the decomposed components are modeled in turn. To further mention accuracy, the genertic  algorithm （GA）is used to optimize the DBN。 (MATLAB代码)采用深度置信网络DBN建立风速预测的时间序列预测模型，由于数据本身的自相关性，导致得到的预测值与实际值存在滞后。针对这个问题，首先对风速序列进行EMD得到imf分量，然后对各分量进行DBN建模。最后为进一步提高精度，采用遗传算法的DBN各隐含层节点进行优化，需要的可以加我qq2919218574 ，效果可以看csdn博客https://blog.csdn.net/qq_41043389/article/details/104517495 程序是matlab代码，采用的是模拟退火算法，我新做的只是把模拟退火换成了遗传算法，对应的博客就没写了\n'], 'url_profile': 'https://github.com/fish-kong', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'İzmir,Turkey', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MustafaSerdarKonca', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['multiple-linear-regression\nI have build a model by using multiple linear regression , backward elimination technique. by using backward elimination technique we can only optimal independent variable which is highly statistically significant and has great impact on the dependent variable profit.\nA very simple python program to implement Multiple Linear Regression using the LinearRegression class from sklearn.linear_model library.\nThe program uses the statsmodels.formula.api library to get the P values of the independent variables. The variables with P values greater than the significant value ( which was set to 0.05 ) are removed. The process is continued till variables with the lowest P values are selected are fitted into the regressor ( the new dataset of independent variables are called X_Optimal ).\nX_Optimal is again split into training set and test set using the test_train_split function from sklearn.model_selection.\nThe regressor is fitted with the X_Optimal_Train and Y_Train variables and the prediction for Y_Test ( the dependent varibale) is done using the regressor.predict(X_Optimal_Test)\n'], 'url_profile': 'https://github.com/Pooja-vispute6', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VellankiSaiKiran', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Crime-model-using-Regression\nThe model uses classical statistics (in R) to build a regression model, analyzing CLM assmptions and build different model specifications\n'], 'url_profile': 'https://github.com/meghab01', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['simple-linear-regression\n'], 'url_profile': 'https://github.com/praveen6993', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sreekar1996', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amina8924', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Support-Vector-Regression\nSVR or Support Vector Regression is a regression technique developed from the classic classification method of SVM. Here, I have implemented 2 models of SVR. The first being epsilon-SVR which minimizes the regularized MSE for regression. The second being the RHSVM, which converts the regression problem into that of classification in the Wolfe Dual space by introducting a new dimension using the labels. Reference: https://www.sciencedirect.com/science/article/abs/pii/S0925231203003801\n'], 'url_profile': 'https://github.com/abhisikdar', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '345 contributions\n        in the last year', 'description': ['FEATURE-ENGINEERING-FINAL-REGRESSION\n'], 'url_profile': 'https://github.com/NarayanaReddy29', 'info_list': ['1', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'HTML', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '345 contributions\n        in the last year', 'description': ['MULTICOLLINEARITY-IN-LINEAR-REGRESSION\n'], 'url_profile': 'https://github.com/NarayanaReddy29', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': [""Info\nThis script takes two screenshots with the help of Selenium automation framework, one with the chrome driver, one with the firefox driver and compares them. The results will be printed as an image with red squares to show different areas.\nUsage\n\nAfter clone, cd into the cloned directory and create a virtualenv, then activate it.\n\nvirtualenv /venv \n./venv/Scripts/activate\n\nInstall requirements.\n\npip install -r requirements.txt\n\n\nThe script also requires the used browser driver to be installed and added to PATH. \nIf you use chocolatey -> https://chocolatey.org/packages/selenium-all-drivers \nOtherwise install them manually.\n\n\nChange url constants to compare screenshots of the pages you would like to check out.\n\n\nTo compare custom pictures just put screenshots with the correct name into the 'screenshots' folder and comment the 'capture_screens' function.\n\n\nHow it works\nComparation is done by calculating average brightness per square and it draws the squares which are different.\nBased on\n""], 'url_profile': 'https://github.com/szattila98', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'TAIWAN', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['maching-learning-advanced-regression\n'], 'url_profile': 'https://github.com/ztex020713925', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vivek4chandra', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanchoalric', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'CHENNAI', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sakthianand7', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['multi-linear-regression\nThis project explains clearly about the implementation of  Multi Linear Regression in a simple dateset where the goal is to find the profit of a startup after all the expenses are considered\n'], 'url_profile': 'https://github.com/tkrath', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'Belgrade,Serbia', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['House Prices: Advanced Regression Techinques\nSolution for Kaggle competetion with accent on feature engineering and comparing different regression algorithms done as course project at Faculty of Mathematics, University of Belgrade.\n'], 'url_profile': 'https://github.com/DenisAlicic', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SubhasishSarkarIndia', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Apply linear regression for multiple features to predict the price of an unknown house. Use learning rate of 0.01 and complete 2 iterations. Use Gradient Descent Learning Algorithm.\nSize (feet2)\nNumber of bedrooms\nNumber of floors\nAge of home (years)\nPrice ($1000)\n2104\n5\n1\n45\n460\n1416\n3\n2\n40\n232\n1534\n3\n2\n30\n315\n852\n2\n1\n36\n178\n'], 'url_profile': 'https://github.com/ZeeshanMirza74', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 16, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 14, 2020', 'HTML', 'Updated Jul 17, 2020', 'Updated Apr 20, 2020']}"
"{'location': 'New York City', 'stats_list': [], 'contributions': '4,222 contributions\n        in the last year', 'description': [""Modeling NYC Apartment Rental Pricing on Craigslist\n\n\n\nby Austin Poor\nI did this analysis as my second project for the Metis Data Science Bootcamp. For this project we chose our own topics but were required to use gather our data by web-scraping and use a linear regression model.\nAs someone who spent most of my life living in New York, I picked a topic near and dear to my heart – I chose to model New York City apartment rental prices, using data scraped from Craigslist.\nData was collected from NYC area Craigslist (newyork.craigslist.com) with listings that were posted in the range 2019-12-24 to 2020-01-23.\n\nProcess\nI used two python scripts to scrape and clean my data – scrape.py and clean.py – which download apartment listing data to an sqlite database data/craigslist_apts.db.\nFrom there, the notebook craigslist_regression.ipynb loads the data, further cleans it, and then models the data. There's an additional notebook, geometry_conversion.ipynb, which is used to calculate apartment neighborhoods based on the latitude and longitude data from the Craigslist apartment listing.\nResults\nAfter testing multiple types of linear models (linear regression, degree-2 polynomial regression, degree-3 polynomial regression, LASSO, and Ridge), my final model (degree 3 polynomial regression) was able to get an R^2 score of 0.768 on test data.\nPresentation\nI've included a pdf of the slide deck used for my presentation, here.\n""], 'url_profile': 'https://github.com/a-poor', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': [""Developing Logistic Regression Algorithm From Scratch\nHere i show Logistic Regression can be developed from scratch. Firstly Logistic regression is a generalized linear model that we can use to model or predict cases where the target variable is categorical by type. So simply put Logistic Regression Models are Classification models. They are used to predict the category or class an element or entity falls into. For the case where we have to classify a set of variables into two classes we call that Binary Classification Problem. For binary classification case the model(in this case Logistic Regression) predict the target variable by producing outputs of 0 when it belongs to a class and 1 when it belongs to the other class.\nApproximations\nLogistic Regression function is derived from the transformation of a Linear Regression function using what is known as a sigmoid function.\n1. Generating the Logistic Regression from Linear Regression\nFirst the linear regression is given below as:\n\nwhere w represents the weights, x represents the matrix of features or the input vector and b represents the bias\nRecall that inorder to transform the above function we make use of the sigmoid function. The sigmoid function is shown below:\n\nIts graphically represented as:\n\nThe purpose of the sigmoid function is to transform the above linear regression into generating output values as in form probabilities of that fall between 0 and 1.\nEven after this tranformation the outputs do not yet reflect what the typical classification output should look like. So, we set a threshold is set at x = 0.5, such that all values greater than and equal to 0.5 are approximated as 1 and others less than this threshold are rounded down as 0.\nSubstituting the linear regression equation into the sigmoid function, satisfying the above conditions for x= 0.5 as the threshold. We have new sigmoid function to be:\n\non substituting, x from the sigmoid function equals (wx + b) from the linear regression fumction\nwhere     h(x) is the transformed linear regression function, which is the logistic regression function\ny_hat is the predicted value of the target varible\n2. Cost Function\nThe cost function we use here is the Cross Entropy Cost Function. It is shown mathematically as:\n\nWhat is a COST FUNCTION?\nThe cost function measures how poorly a model does in terms of its ability to estimate the relationship between the independent variable(X) and the dependent variable (y). The goal therefore in this case is minimise or reduce the cost function. In reducing the Cost function what we are doin in essence is reducing the weights or finding what value of weights and biases would minimise the cost function and in turn optimise our Machine Learning model.\nTo minimise the cost function by applying something called a Gradient descent on the cost function. The gradient descent is simply a partial differentation applied of the cost function with respect to the weights and biases.\n\n\nOn applying gradient descent the updated weights, biases and cost function becomes:\n\n'''Note: J'= updated cost functions\ndJ/dw = dw\ndJ/db = db\nN = number of samples'''\nCode Description\n\n\nA class LogisticRegression was created.\n'''class LogisticRegression:'''\n\n\nAn iniialisation function is created to intitialise all parameters namely: learning rate(lr), number of iterations (n_iters), biases and weights.\n'''def init(self, learning_rate=0.001, n_iters=1000):\nself.lr = learning_rate\nself.n_iters = n_iters\nself.weights = None\nself.bias = None'''\n\n\nA function called _sigmoid is created that returns the mathematical convention of the sigmoid function as cited above.\n     '''def _sigmoid(self, x):\n             return 1 / (1 + np.exp(-x))'''\n\n\n\nWe define a function called predict that takes a parameter X(the independent variables). Within the function\n\n\n\n\nWe assign the mathematical estimation for the linear regression model to the variable name linear_model\n\n\napply the sigmoid function to the linear_model and assigned it the variable name y_predicted\n\n\nwe define the threshold within our sigmoid function for returning values of 0 and ! assigned to the variable name y_predicted_cls and return the array of the resulting value.\n    '''def predict(self, X):\n                linear_model = np.dot(X, self.weights) + self.bias\n                y_predicted = self._sigmoid(linear_model) \n  \n                y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n                return np.array(y_predicted_cls)'''\n\n\n\n\nA function called fit is created. Within this function\n\n\n\nwe assign the number of samples(n_samples) and the number of features(n_features) values equivalent to the number rows and columns of our independent variable(X) respectively.\n\n\nthe weights are initialised using an array of zeroes following the dimension of n_features and bias is initialised with zero.\n    '''def fit(self, X, y):\n         n_samples, n_features = X.shape\n         self.weights = np.zeros(n_features)\n         self.bias = 0'''\n\n\n\na for loop is created following the iterative process of the gradient descent of the cost function. The number of times the loop is set to run given by the value  n_iters.\n\n\nWithin the for loop we have the following:\n\n\nWe assign the mathematical estimation for the linear regression model to the variable name linear_model\n\n\napply the sigmoid function to the linear_model and assigned it the variable name y_predicted\n     '''for _ in range(self.n_iters):\n                linear_model = np.dot(X, self.weights) + self.bias\n                y_predicted = self._sigmoid(linear_model)'''\n\n                \n                - compute the gradients of both weights and bias, given as **dw** and **db** given by the equation.\n                - update the weights and bias using.\n                \n           '''dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n      db = (1 / n_samples) * np.sum(y_predicted - y)\n      \n      self.weights -= self.lr * dw\n      self.bias -= self.lr * db'''\n\n\n\n\nFinally, we test our model on the breast cancer data set from sklearn package.\n\n""], 'url_profile': 'https://github.com/Adbe-El', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'Porto Alegre, Brazil', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Programming Exercise 1: Linear Regression\nThis repository provides a Python implementation that solves both the univariate and multivariate linear regression excercises proposed in ""Programming Exercise 1: Linear Regression"" from Stanford\'s Professor Andrew Ng at Coursera.\nA brief analysis of the results is provided in Portuguese. It was submited as an assignment of a graduate course named Connectionist Artificial Intelligence at UFSC, Brazil.\n'], 'url_profile': 'https://github.com/fredericoschardong', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'Hyattsville, Maryland', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Movie_analysis\nConsists java codes for Naive Bayesian and linear regression for prediction.\n'], 'url_profile': 'https://github.com/kavyap10', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'Corvallis, Oregon', 'stats_list': [], 'contributions': '348 contributions\n        in the last year', 'description': ['st-412-final-project\nMy final project for ST 412 - Methods of Data Analysis II (focused on linear regression modeling).\n'], 'url_profile': 'https://github.com/jackrwoods', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'NC', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['BreastCancer-in-R\nLogistic Regression for determining if Breast Cancer data is malignant or benign.\nData/Packages\nPackages used were ""mlbench"" and ""caret""\nmlbench contains in house data built in R\n'], 'url_profile': 'https://github.com/alx1056', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TimoteosOzcelik', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '1,016 contributions\n        in the last year', 'description': ['King County House Price Prediction\nThis dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n\nid: This is unique identification number given to each house in the dataset.\ndate: This is the date the house is added into the dataset and is in the format YYYY-MM-DD.\nprice: Price of each house in US dollars.\nbedrooms: Number of bed rooms available in each house and the dataset has houses having number of bedrooms ranging from 0 to 33.\nbathrooms: Number of bed rooms available in each house and the dataset has houses having number of bedrooms ranging from 0 to 8.\nsqft_living: Area size of living room in square feet.\nsqft_lot: Total area of size of lot in square feet.\nfloors: Number of floors available in house.\nwaterfront: This is an indicator if the house is located on or beside a lake or beach. Possible values are 0 – indicating No and 1 – indicating Yes.\nview: Rating of view of city or lake or beach from house and is rated from 0 to 5.\ncondition: Overall condition of house rated in the range 1 to 5.\ngrade: Overall grade of house ranging from 1 to 12.\nsqft_above: The surface area of house in square feet above ground level.\nsqft_basement: The surface area of house in square feet below ground level or basement.\nyr_built: The year of house in which it is constructed.\nyr_renovated: Year in which house is renovated or remodeled.\nzipcode: It is a 5 digit zip code in which the house is located.\nlat: Geographical Latitude position of the house.\nlong: Geographical longitude position of the house.\nsqft_living15: It is the average house square footage of the 15 closest houses.\nsqft_lot15: It is the average lot square footage of the 15 closest houses.\n\nLinear Regression\n\nTest MSE: 0.08219485949521853\nTest R2: 0.7048872421004834\nCross Validation Score 0.08169201394360087\n\nLasso\n\nTest MSE: 0.08221273491321496\nTest R2: 0.704823062126992\nCross Validation Score 0.0816374135527322\n\nRandom Forest\n\nTest MSE: 0.07173376185107747\nTest R2: 0.7424468096376502\nCross Validation Score 0.07341217104404467\n\n'], 'url_profile': 'https://github.com/esharma3', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['linear_regression_model\nconsists of linear regression model for predicting the chance of the admit based on cgpa\n'], 'url_profile': 'https://github.com/geethikreddy11', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Facebook-Comment-Prediction-with-Linear-Regression\nPredicting the number of comments on a post with Linear Regression,\nCOMPLETE ALGORITHM BUILT FROM SCRATCH IN PYTHON\nDataset\nThe dataset that we’re going to use is the “facebook-comment-volume-prediction” dataset, which involves the problem of predicting comment volume traffic or the number of comments on a given post. Several input variables are provided, along with the target variable.\n'], 'url_profile': 'https://github.com/nikitaa30', 'info_list': ['Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 17, 2020', 'Java', 'Updated Apr 17, 2020', 'HTML', 'Updated Apr 17, 2020', 'R', 'Updated Apr 16, 2020', 'R', 'Updated Apr 18, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'HTML', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Credit_Fraud_Detection\nhandle the imbalance labels and train model with Logistic Regression, Decision Tree, Random forest, SVM\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['HR_Analytics\nUsing logistic regression to predict employees retention(i.e whether the employees stays or leaves.\n'], 'url_profile': 'https://github.com/Mustaphayinka', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Deeplearning_basics\n'], 'url_profile': 'https://github.com/ragul165', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'Saint Paul, MN', 'stats_list': [], 'contributions': '254 contributions\n        in the last year', 'description': ['handwritten-digit-recognition\nMultinomial Logistic Regression on the MNIST dataset to predict the handwritten digit.\n'], 'url_profile': 'https://github.com/bradylange', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'delhi', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khareji', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '390 contributions\n        in the last year', 'description': ['World_Weather_Analysis\nProject Overview\nThe project consisted of:\n\nRetrieving world weather data through API calls;\nAnalyzing the data using Linear Regression to find the relationship between variables (temperature, humidity, cloudiness, wind speed, latitude); and\nThe creation of an app including heatmaps (using Google API) and functionality that allows customers to know the weather in the cities when they click on a pop-up marker, as wel as directions between multiple cities for travel.\n\nResources\n\nData source:\n\nOpenweatherMap API\n\n\nTechnologies: Python, Pandas, Matplotlib, SciPy, Citipy, Jupyter Notebook, Visual Studio Code 1.43.1\n\n'], 'url_profile': 'https://github.com/alineschneider', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'Victoria, BC', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/suseelapattamatta', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['Implementing-Logistic-regression-from-scratch\nThis is Logistic Regression model implemented from scratch on a dataset\n'], 'url_profile': 'https://github.com/DevMachTech', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': [""Kaggle-Competition-Regression-Analysis-for-US-Patents\nThis repository contains the code used to implement ridge regression. Used only 50% of data for model evaluation due to lack of computational capability.\nTask: Perform Regression analysis and predict forward citations a patent will receive.\nData: US Patent Data - USPTO\nObjective: Predict the number of forward citations a patent will receive.\nModel: Ridge Regression with hyper-parameter tuning\nBest Hyperparameters: {'alpha': 10, 'copy_X': True, 'fit_intercept': True, 'max_iter': 10, 'normalize': False}\nTest Score of Model: 0.23\n""], 'url_profile': 'https://github.com/SiddharthHareendran', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}","{'location': 'Amsterdam, the Netherlands', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['WVS Data Analysis\nThis project is an assignment for Statistics and Methodology course in Master of Science in Data Science and Society program, Tilburg University. In this project, we perform numerous data wrangling task as well as statistical inference and prediction on Wave 6 of the World Values Survey data. The data could be downloaded directly from WVS website. We include the preporcessed data in data directory.\nTable of Directory\n\nCode\nData\nFigure\nReport\n\nProject Summary\nThe aim of this project is to investigate the effect of particular country on feelings of happiness of its citizen - before and after controlling for health condition, and try to predict satisfaction with financial situation using plausible variable in the dataset. Our result is dependent to the dataset we are using.\n\nBefore controlling for health condition, we found that the model with country variable explains 4% of the feeling of happiness. We also found that US is the country with the happiest citizen. Furthermore, we gain more statistical power after controlling for health condition: the R squared of our model increases to 20.6%. The country with happiest citizen after controlling for health also change to India.\n\nMoreover, we also build 3 models to predict statisfaction with financial situatuon. Details of each model are as follows:\n\n\n\n\nModel\nVariable\nRationale\nCross-Validation Error (CVE)\n\n\n\n\n1\nSatisfaction with Life, Country, Sex\nMen and women have different perceptions of financial satisfaction according to their country and to their perceived satisfaction with life. (e.g. unsatisfied woman in Germany vs. satisfied man in India)\n3.536\n\n\n2\nNumber of Children, Marital Status, Sex\nMen and women have different perceptions of financial satisfaction at the presence of children and given their marital status. (e.g. single man with no children vs. divorced woman with several children)\n5.213\n\n\n3\nAge, Level of Education, Sex\nMen and women have different perceptions of financial satisfaction according to their age and to their education level. (e.g. older highly educated man vs. younger uneducated woman)\n5.262\n\n\n\nAfter 10-fold cross-validation, our experiment shows that model with individual perception of life, the country in which the respondent resides, and the biological sex has the lowest croess-validation error with CVE = 3.536. Full report of this project is in here.\nEnvironment and Library\nWe deploy our code in R using several libraries, such as:\n\nMASS\nMLmetrics\n\nWe also use Student Function provided by the teacher.\n'], 'url_profile': 'https://github.com/miftahulridwan', 'info_list': ['Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'R', 'Updated Jun 10, 2020']}"
"{'location': 'Guadalajara, México', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ManuelHiguera1', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Boosted-Decision-Stump\nUse of decision stumps (DS) and boosted decision stumps (BDS) for regression.\n'], 'url_profile': 'https://github.com/sgarciba', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Rajkot', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Linear-Regression\nWe Estimate the scores of student based on their total hours of study using linear regression.\nWe implement linear regression using OLS method, Gradient Descent and using Sklearn API.\n'], 'url_profile': 'https://github.com/d2Kapuriya', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Popayán, Colombia', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['Python-Notebooks\nHere we got some python notebooks used for data analysis, regression models, and evaluation of models.\n'], 'url_profile': 'https://github.com/juandag97', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '325 contributions\n        in the last year', 'description': ['BanknoteAuthentication_LogisticRegression_ML\nThe following program is a python implementation of a logistic regression from Scratch. In order to work, it requires the pandas, numpy, matplot and scikit libraries. This program works with the Banknote Authentication dataset obtained from the UCI repository for machine learning. In order to execute the program, you must have the csv attached in the repository, which converted to .txt of the original data into a .csv. Doing so, one can run the code by running the following command : python logisticRegression.py .\nThe program has the MIT license.\n'], 'url_profile': 'https://github.com/QuiquePosada', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '210 contributions\n        in the last year', 'description': ['Email_Text_Classification\nBy: Matthew Walter, Eric Allen, Murugesan Somasundaram\nLibraries: os, sys, re, math, glob, collections, pandas, numpy,\nSummary: Implement and evaluate Naive Bayes, Perceptron, and Logistic Regression for text classification.\nRequires 5-7 additional arguments:\npython main.py  <path_ham_train> <path_spam_train> <path_ham_test> <path_spam_test> <OPTIONAL: lambda> <OPTIONAL: iterations>\nprogram: BAYES | MCAP | PERCEPTRON\npath_ham_train: Directory Path\npath_spam_train: Directory Path\npath_ham_test: Directory Path\npath_spam_test: Directory Path\nlambda: Float Value (0.01 or 0.1 is recommended)\niterations: Integer Value\nTo Run\n\npython main.py BAYES <path_ham_train> <path_spam_train> <path_ham_test> <path_spam_test>\n\n\npython main.py MCAP <path_ham_train> <path_spam_train> <path_ham_test> <path_spam_test>  \n\n\npython main.py PERCEPTRON <path_ham_train> <path_spam_train> <path_ham_test> <path_spam_test> \n\nExample Run\n\npython main.py BAYES hw2datasets/dataset1/train/ham/ hw2datasets/dataset1/train/spam/ hw2datasets/dataset1/test/ham/ hw2datasets/dataset1/test/spam/\n\n\npython main.py MCAP hw2datasets/dataset1/train/ham/ hw2datasets/dataset1/train/spam/ hw2datasets/dataset1/test/ham/ hw2datasets/dataset1/test/spam/ 0.01 100\n\n\npython main.py PERCEPTRON hw2datasets/dataset1/train/ham/ hw2datasets/dataset1/train/spam/ hw2datasets/dataset1/test/ham/ hw2datasets/dataset1/test/spam/ 1000\n\n'], 'url_profile': 'https://github.com/smwalter96', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['SERL-project\nMulti-label learning deals with the problem where one object may be associated with one or more labels\n\nThis project deals with a framework named SERL.\n\nSERL learns better representation of the original features in the supervision of softmax regression.\nTo show the efficiency of this model, it has been compared with the efficiencies of some of the most commonly used multi-label classification algorithms.\nIt uses softmax regression whose class is defined in this project.\nThe softmax regression class has all the important functions such as fit, predict, etc.\nhis project also includes a research paper related to this softmax regression based multi-label classification.\n\n\n\n'], 'url_profile': 'https://github.com/KaranrajMokan', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Ajmer, rajasthan', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khannariya9', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['Its good to do correlation to understand multiocollinearity.\nThis is usually done during the preprocessing phase.\nUse the built in formual df.corr(indepentvariables) to calculate the multicollinearity.\nLater on use seaborn heatmap to plot the same.\nWhile ding preprocessing its better to check if all independent variables are in same magnitude.\nIf not in same magnitude,one indepenet variable with higher magniture will dominate the reg model.\nTwo ways to do the feature scaling.\na.Standard scalar.\nConvert the values to Z scores by using standard techniques -> (x-mean)/(std.deviation)\nbewre of the speelling ->StandardScaler and not StandardScalar\nb.Why do we perform fit_transform on train data set and transfrom on x_test set ->\nBcause we want to centre the test data set on same mean/standard deviation on train data set.After all this is subset of\noriginal data set.\nc.Always keep random_state with same value to replicate the results.\n'], 'url_profile': 'https://github.com/rameshbabulakshmanan84', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Optimized_Logistic_Regression\nAn optimized version of Logistic Regression using Torch with support of CPU and GPU instances.\nthis version has been benchmarked against SKlearn Logistic Regression on the MNIST dataset (both running on CPU), with the same accuracy to that of SKlearn Logistic Regression and more than 100% performance enhancement.\nthis is an optimized version of SKLearn Logistic Regression using Pytorch, the API is similiar to that of SKlearn, where an instance of LogisticRegression_T is initialized, fit is called to fit the model to the data (the class handles the conversion of numpy arrays to torch arrays and constructing the dataloader). predict method can then be called and works the same way as SKlearn API.\nN.B. Pytorch needs to be installed to use that version.\nto use the GPU support(optional) CUDA has to be installed .\n'], 'url_profile': 'https://github.com/mmohamed93', 'info_list': ['TypeScript', 'Updated Apr 19, 2020', 'R', 'Updated Apr 19, 2020', '1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Nov 20, 2020', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'Python', 'Updated Sep 4, 2020', '1', 'Python', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}"
"{'location': 'Chennai', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vickeydreamss', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Tusker-Brain\nPHP library for various machine learning task, including classisification, regression, and clustering\n'], 'url_profile': 'https://github.com/fikrihasani', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '184 contributions\n        in the last year', 'description': ['House-Price\nProblema de la competencia Kaggle\n'], 'url_profile': 'https://github.com/MaldoAlberto', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'Aachen', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Regression-with-Keras-on-CalCOFI-data\nA Neural Network will be set to apply regression will be applied using Keras\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includes abundance data on the larvae of over 250 species of fish; larval length frequency data and egg abundance data on key commercial species; and oceanographic and plankton data. The physical, chemical, and biological data collected at regular time and space intervals quickly became valuable for documenting climatic cycles in the California Current and a range of biological responses to them. CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term “El Niño” into the scientific literature.\nThe California Cooperative Oceanic Fisheries Investigations (CalCOFI) are a unique partnership of the California Department of Fish & Wildlife, NOAA Fisheries Service and Scripps Institution of Oceanography. The organization was formed in 1949 to study the ecological aspects of the sardine population collapse off California. Today our focus has shifted to the study of the marine environment off the coast of California, the management of its living resources, and monitoring the indicators of El Nino and climate change. CalCOFI conducts quarterly cruises off southern & central California, collecting a suite of hydrographic and biological data on station and underway. Data collected at depths down to 500 m include: temperature, salinity, oxygen, phosphate, silicate, nitrate and nitrite, chlorophyll, transmissometer, PAR, C14 primary productivity, phytoplankton biodiversity, zooplankton biomass, and zooplankton biodiversity.\nThe data is from Kaggle.\n'], 'url_profile': 'https://github.com/adurukan', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'UK', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['model-stats-extension\nmodel-stats-extension is a Python module built on top of scikit-learn that\nprovides a more thorough statistical analysis for the linear regression\nmodule from sci-kit learn.\nTable of Contents\n1. Installation and Use \n2. Project Motivation \n3. File Descriptions \n4. Testing \n5. Licensing, Authors, and Acknowledgements\nInstallation and dependencies \nDependencies\nThe package requires:\njoblib==0.14.1 \nnumpy==1.18.2 \npandas==1.0.3 \nscikit-learn==0.22.2\nscipy==1.4.1 \nsix==1.14.0 \nnumpy==1.17.4\n(or later)\nWorks for Python versions 3.5 or newer.\nUser Installation\npip install model-stats-extension\nor can clone this repository, navigate to source directory, and run\npip install ..\nUse\nTo use the module, first import: \\ from linear_model_extension import RegressionStats. \nRegressionStats is the working module which can be applied to an scikit-learn LinearRegression() model. \\\nTo run, must have a fitted scikit-learn LinearRegression() object as well as an X and y object\n(Pandas dataframe or numpy array) e.g.:\nfrom sklearn.linear_model import LinearRegression \nfrom linear_model_extension import RegressionStats \nlr_model = LinearRegression() \nlr_model.fit(X,y) \nstats = RegressionStats() \nstats.summary(lr_model, X, y)\n\nThis will return the following output:\n\n\n\n\ncoef\nstd err\nt\np-value\n[0.025\n0.975]\n\n\n\n\nIntercept\n4.130904\n0.094040\n43.927304\n1.896759e-174\n3.946148\n4.315660\n\n\ncrim\n-0.092946\n0.010088\n-9.213458\n8.519949e-19\n-0.112765\n-0.073126\n\n\n\nProject motivation\nAfter using both sklearn.linear_model.LinearRegression as well as statsmodels.api.OLS() linear regression\nmethods, I decided it would be useful to have some quick summary statistics, as produced in statsmodels, but\nfor use with the scikit-learn package, which many people may be more accustomed to. \nSo this module aims to provide quick statistical analysis functions for sci-kit learn LinearRegression\n(and others to come) objects without having to fit the model with the statsmodels module. The functions were tested against statsmodel output. I also wanted to compute the summary statistics from scratch to solidify my understanding\nof the statistics.\nNotes on Statistics\nStatistics were computed based on well established practices, in particular using the textbook: \nRencher, A. C., & Schaalje, G. B. (2008). Linear models in statistics. Hoboken, N.J: Wiley-Interscience.\nAlso consulted Python statsmodels source code, and\noutput from this module was tested against statsmodel.api output.\nFile descriptions\nThe code for the main module, RegressionStats, is located within the linear_model_extension folder in the file\nregression_stats.py. The more general super class of RegressionStats is located in model_stats.py. Tests are located\nin the linear_model_extension/tests folder along with the data used for the tests.\nTesting\nAfter installing, (with pytest >= 5.4.1 installed), you can launch the\ntest suite when located outside the source:\npytest linear_model_extension\nLicensing, authors, and acknowledgements \nThe package was written by Katherine Edgley in 2020. \nThe package is based off of scikit-learn and refers to code and output\nfrom statsmodels. The mathematical foundation is based off of\nthe textbook, as previously mentioned:\\ Rencher, A. C., & Schaalje, G. B. (2008). Linear models in statistics. Hoboken, N.J: Wiley-Interscience.\n'], 'url_profile': 'https://github.com/katherineedgley', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LucasLaughlin', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Analyze-the-Salary-and-year-experience\nAnalyze the Salary and year experiences by Linear regression from scratch\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['""# PredictUSflightsJan2015""\n'], 'url_profile': 'https://github.com/salma-samar', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'FCT, Nigeria', 'stats_list': [], 'contributions': '132 contributions\n        in the last year', 'description': ['COVID19-Linear-Regression-Model-Visualization\nContains visualizations for Linear Regression models (Bayesian Ridge and Polynomial Features)\n'], 'url_profile': 'https://github.com/datascientist-kenn', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['Datasets\nWe have split the wildfires.csv dataset into a training dataset (wildfires_train.csv) and test dataset (wildfires_test.csv). They are contained in the data subdirectory along with a codebook.\nExercise 1\nThe total area burned by a wildfire is of great concern to government planners. This is captured by the variable burned in the wildfires dataset, which is a continuous variable. In this exercise, you will train models to predict burned using other variables in the data (exclude wlf as a predictor ). Train the following candidate models:\n\nboosting\nbagging\nrandom forests\nlinear regression\nridge regression\n\nBe sure to properly tune all of these models, including any primary tuning parameters (e.g., mtry) or tuning parameters that appear relevant.\nCompare the estimated test errors for each model to determine which is best.\n\nExercise 2\nLocated in the northeast of the wilderness area is a wildlife protection zone. It is home to several rare and endangered species, and thus conservationists and park rangers are very interested in whether a given wildfire is likely to reach it. In the data, fires that reach the wildlife protection zone are denoted by the indicator variable wlf.\nIn this exercise, you will train models to predict wlf using other variables in the data (there is no exclusion on which varibles to use as predictors). Train the following candidate models:\n\nboosting\nbagging\nrandom forests\nlogistic regression\nridge logistic regression\n\nBe sure to properly tune all of these models, including any primary tuning parameters (e.g., mtry) or tuning parameters that appear relevant.\nCompare the estimated test errors for each model to determine which is best.\n'], 'url_profile': 'https://github.com/christian-montes', 'info_list': ['1', 'MATLAB', 'Updated Apr 14, 2020', 'PHP', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Python', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'R', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'HTML', 'Updated May 7, 2020']}"
"{'location': 'India', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mihirkumar02', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'Egypt', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Titanic-Machine-Learning-From-The-Disaster\nA scikit-learn logistic regression model applied to Kaggle Titanic Dataset.\nIn this jupyter notebook, I used nothing but a combination of numpy, pandas, and scikit-learn for the sake of cleaning data,\nperforming some exploratory data analysis and using a logistic regressor for predicting the survival of the passengers of Titanic.\n'], 'url_profile': 'https://github.com/Muhammad-Yousef', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Logistic-Regression-on-IRIS-Dataset\nA simple Logistic Regression implementation on IRIS Dataset using the Scikit-learn library.\n'], 'url_profile': 'https://github.com/GautamVijay', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Weather-Prediction\nData Mining techniques used such as linear regression and K means clsutering\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/niikkita', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['Machine-Learning-series---Linear-Regression\nIn the Series of Machine learning will share the Linear regression\nsklearn.model_selection is the library\ntrain_test_split\n1.Error \t:\nSyntaxError: invalid syntax\n1.CodeSnippet:\nfrom sklearn.linear_model import LinearRegression()\n1.Root cause :\nLinearRegression() is not correct rather it shoud be LinearRegression\n\n\n\nError:\nValueError: Expected 2D array, got 1D array instead:\nCodeSnippet:\nregressoragent.fit(X_Train,Y_Train)\nFix: We need to reshape the data.use column.values.reshape() and also assign it back to\nCommon Mistake  : X.values.reshape(-1,1) and not asigning back will retain the same shape.\nto correct, do X=X.values.reshape(-1,1)\nFit and Predict requires 2-D array rather than 1 D array.\n'], 'url_profile': 'https://github.com/rameshbabulakshmanan84', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['W5-Tonga-moviereview\nPersonal submit for week 5 homework, predict sentiment of movie review with logistic regression\n'], 'url_profile': 'https://github.com/TuanAnh-Pham', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'Cork, Ireland', 'stats_list': [], 'contributions': '252 contributions\n        in the last year', 'description': ['CS4618_ReviewPredictor\nCS4618 assignment to create a linear regression model to predict ratings of clothing items\n'], 'url_profile': 'https://github.com/LorcanMcVeigh', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'Dallas, United States', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Text-Classification\nClassifies Spam and Ham emails using Naive Bayes and Logistic Regression\nThis Repo contains 3 Python scripts, Logistic Regression with and without stop words. The stop words like ""I, The, am etc."" are removed.\nWithout removing the stop words the Naive Bayes produced an accuracy of 92.47 %, whislt with removing the stop words the accuracy dropped down to 90.38%.\nIn Logistic Regression, the iteration was set to 50 with regularization parameter and learning rate set to 0.01. It yielded 92.25 % accuracy.\n'], 'url_profile': 'https://github.com/shashanksathish', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['In-Depth_Machine_Learning\nDeep dive into the concept of Supervised Linear Regression and Classification.\n'], 'url_profile': 'https://github.com/SajidAnwar93', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['\n\n\nEvaluate information criteria for multivariate model selection\nmvIC extends the standard the standard Akaike or Bayesian Information Criterion (AIC, BIC) to the case of multivariate regression.  The model fit across many response variables is evaluated and the criterion explicitly considers correlation between reponses.  mvIC is applicable to linear and linear mixed models.\nForward stepwise regression with the mvIC criterion enables automated variable selection for high dimensional datasets.\nInstallation\ndevtools::install_github(""GabrielHoffman/mvIC"", repos=BiocManager::repositories())\nThis automatically installs dependencies from Bioconductor\nExamples\nMultivariate linear model\n# Predict Sepal width and Length given Species\n# Evaluate model fit\nfit1 = lm( cbind(Sepal.Width, Sepal.Length) ~ Species, data=iris)\nscore = mvIC( fit1 )\n# add Petal width and length\n# smaller mvIC means better model\nfit2 = lm( cbind(Sepal.Width, Sepal.Length) ~ Petal.Width + Petal.Length + Species, data=iris)\nmvIC( fit2 )\nForward stepwise regression\n# Combine respones on *rows*\nY = with(iris, rbind(Sepal.Width, Sepal.Length))\n\n# variables to consider in the model\nvariables = c(""Petal.Length"", ""Petal.Width"", ""Species"")\n\n# fit forward stepwise regression starting with model: ~1. \nbestModel = mvForwardStepwise( Y, ~ 1, data=iris, variables=variables)\nCategorical variables can be modeled as random effects using the (1|x) syntax.\n# model Species as a random effect\nvariables = c(""Petal.Length"", ""Petal.Width"", ""(1|Species)"")\n\nbestModel = mvForwardStepwise( Y, ~ 1, data=iris, variables=variables)\nIf a random effect is specified, a linear mixed model is fit and the number of parameter equal to the effective degrees of freedom of the model fit.  Note that using a linear mixed model is more computationally demanding, but\n\nprevents overcorrection with variables with many categories\nregularizes the effect of estimate for each category\ngracefully handles colinearity between categorical variables\n\nDocumentation\nSee manual for examples and documentation.\n'], 'url_profile': 'https://github.com/GabrielHoffman', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', '1', 'Python', 'Updated Oct 14, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'R', 'Updated Oct 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Cippa', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'West Lafayette, IN', 'stats_list': [], 'contributions': '518 contributions\n        in the last year', 'description': ['Here, I try to implement logistic regression using numpy. I have created functions for computing cost function and gradient descent. Please have a look at my personal notes below.\n# computing cost function\ndef compute_cost(X, y, theta):\n    m = len(y) # no of obs\n    h = sigmoid(X.dot(theta)) # h = g(z) where z = theta.X \n    epsilon = 1e-5 # for computing non-zero log\n    cost = (1/m)*(((-y).T @ np.log(h + epsilon))-((1-y).T @ np.log(1-h + epsilon))) # cost function to minimize\n    return cost\n    \n# computing gradient descent\ndef gradient_descent(X, y, params, learning_rate, iterations):\n    m = len(y)\n    cost_history = np.zeros((iterations,1)) # for simultaneous updation\n\n    for i in range(iterations): \n        params = params - (learning_rate/m) * (X.T @ (sigmoid(X @ params) - y)) \n        cost_history[i] = compute_cost(X, y, params)\n\n    return (cost_history, params)\n\n\n'], 'url_profile': 'https://github.com/akshay-madar', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['house-price-prediction\nin this project house price is predict by using (ML tool) linear regression algorithm.\n'], 'url_profile': 'https://github.com/pratik123963', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Tokyo, Japan', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""Start here if...\nYou have some experience with R or Python and machine learning basics. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.\nCompetition Description:\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nPractice Skills\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting\nAcknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\n""], 'url_profile': 'https://github.com/anindya-sanyal', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Drinking-Age-Mortality-Rates-through-R\n'], 'url_profile': 'https://github.com/Jie-Huang-Github', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sanu3007', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['COVID-19-Data-Analysis\nCOVID-19 Data Analysis using different regression algorithms\n'], 'url_profile': 'https://github.com/chandanaUdupa', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Santa Monica, CA', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['SparkClassification\nLogistic Regression, Gradient Boosted Trees, and Random Forrest Classification with PySpark\nSummary\nThe purpose of this project is to display knowledge and proficiency with Spark and PySpark. The project aims at exploring Spark Datasets & Dataframes and some Spark SQL. The project builds several classification models using Spark ML and Spark MLlib.\nImportant files\n\npdf of the databricks notebook with truncated output for readability: Spark DF, SQL, ML Exercise - Databricks.pdf\n\nhttps://github.com/AHalarewicz/SparkClassification/blob/master/Spark%20DF%2C%20SQL%2C%20ML%20Exercise%20-%20Databricks.pdf\n\nJupyter notebook for executing the databricks file on a local machine: Spark DF, SQL, ML Exercise.ipynb\n(contains complete output of queries and tables)\n\nhttps://github.com/AHalarewicz/SparkClassification/blob/master/Spark%20DF%2C%20SQL%2C%20ML%20Exercise.ipynb\n'], 'url_profile': 'https://github.com/AHalarewicz', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Ethnomusicology\nThis project aims to predict the continent of origin of ""world"" music by implementing three machine learning techniques:\n\nLogistic regression\nSVM\nNeural network\n\nRequirements\n\npandas=1.0.3\nscikit-learn=0.22.1\nnumpy=1.18.1\npytorch=1.4.0\nseaborn=0.10.0\nmatplotlib=3.1.3\n\nMethods\nThe dataset used for this project comes from a 2014 publication titled ""Predicting the Geographical Origin of Music"", and was accessed via the UC Irivine Machine Learning Repository. In order to tackle this as a classification problem, each song\'s geographic coordinates were mapped to its continent of origin, which is performed by ""get_song_continent.ipynb"".\nInstructions\nEach method, as outlined above, runs on a standalone Jupyter Notebook script. Simply run each script to obtain the accuracies of each model.\nResults\nAcross the three models, we found that a support vector machine using an RBF kernel was the most accurate at 63% accuracy.\n'], 'url_profile': 'https://github.com/mkirsch17', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Data\nkaggle competitions download -c house-prices-advanced-regression-techniques\n\n'], 'url_profile': 'https://github.com/vvnt', 'info_list': ['Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '260 contributions\n        in the last year', 'description': [""Module 2 Final Project\nThe task is to use data from the King County housing market to make a model that can predict housing prices. In order to determine the most valuable ways to use this model I broke the questions of this project into three groups.\nHousing Valuation\nThis model will be able to assist real estate professionals assess the value of homes, marketing efforts, and even undeveloped land. To demonstrate the potential of this model, this analysis will focus on:\n\nThe impact of month on price of sale\nThe impact of number of interested buyers on price of sale\n\nRenovation Advice\nThis model will be able to assist homeowners and real estate developers predict the efficacy of remodeling. To demostrate this component of the tool, this analysis will also investigate:\n\nWhether renovation has significant impact overall, and if so to what extent\nIf making an addition to the home, what is the impact of increased square footage on the price?\nIf making an addition, what kind of rooms would be most beneficial to add?\n\nInvestment Guidance\nThis model will also be able to assist investors in targeting homes that are most likely to increase in value. To demonstrate this capacity, this analysis will investigate:\n\nthe impact of age on the price of a house\n\nDescription of Data\nThe data contained 21597 entries with data on houses sold in King County during 2014 and 2015. Sale prices in the set range from $78,000 to $7,700,000 with a mean of $540,296, median of $450,000 and standard deviation of $367,368.\nDuring data exploration I found outlier values and tested for linearity of the relationship between the independent variables and the dependent variables.\nNotable Findings:\n\nA relatively small number of houses with extremely large prices were skewing the sample.\nLot Square footage does not appear to have a significant relationship with price after accounting for outliers and will be removed from the analysis.\nCondition has no apparent relationship with price and will be dropped from the analysis.\nHouses East of -121.783 degrees longitude were dropped from this analysis because they accounted for only 1% of the data and were causing skew. All houses are within 32.16 miles of Seattle city center.\n\n*--Note-- There is a very large sample of data with large lot sizes that has a noticably different relationship with price. Potentially, this is farm land or something like it. For future projects this slice of data could be useful for predicting more rural home values. Because these houses are so different I dropped these outliers even though I will  not be using lot size data in this analysis.\nAdded Features\nBased on the questions posed in the analysis, I chose to add these features to the data:\n\nMonth the house was sold\nAge of the house at point of sale\nNumber of years since the house has been renovated\nWhether the house had been renovated at all\nDistance from house to Seattle city center\nNearest city data for the 5 largest cities in the county\nSquare footage relative to nearest fifteen neighbors\n\nUltimately, for this analysis the data contained 16261 entries. Sale prices in the set range from $80,000 to $974,350 with a mean of $451,089 median of $420,000 and standard deviation of $185,546.\nThe data was split into a train test split with a test size of .2. To allow for reproducablity, the random state used for the split was 37.\nEstablish a Baseline Model\nAfter standardizing the variables, the baseline model recorded an r-squared of (.7500) against the training set along with a MSE of (.2499). In 10-Fold KFold crossvalidation, it recorded an r-squared of (.7487). Against the test set, r-squared was (.7657) and the MSE was (.2348). The difference of (.0151) for the MSE of the training and test groups suggests that the model is not overfit.\nHigh impact variables included: dist_to_Seattle and WaterFront\nLow impact variables included: floors\nModel 1 Residuals Against Test Data\n\nBased on the plot of residuals, it is clear that the model could be more generalizable, especially for high value homes.\nTrain the Model\nIn order to train the model I used the following steps:\n\nFind and included interactions.\nFind and included polynomial relationships.\nSelected for statistically significant variables.\n\nInteraction Features\nThrough an iterative process designed to find the interactions that most highly impacted the MSE of the model I determined 7 interaction features were worthy of inclusion in the final model. Full details can be found in the model training notebook. The general trend was geographic interactions. (e.g. as dist_to_Seattle and lat, which describes the fact that prices rise when approaching Seattle from the North or the South)\nAfter including the 7 interaction features, the model(model2)  recorded an r-squared of (.8009) against the training set along with a MSE of (.1990). In 10-Fold KFold crossvalidation, it recorded an r-squared of (.7993). Against the test set, r-squared was (.8147) and the MSE was (.1857). The difference of (.01331) for the MSE of the training and test groups suggests that the model is not overfit.\nHigh impact Variables included: dist_to_Seattle, WaterFront, Federal Way, and long * Seattle\nLow impact Variables included: long * dist_to_Seattle\nModel2 Residuals Against Test Data\n\nBased on the plot of residuals, there is improvement for high values homes, and residuals appear to be reduced.\nPolynomial Features\nThrough an iterative process designed to find the polynomial relationships between the independent and dependent variables, I determined polynomial factors to include as features in the model. Full details can be found in the model training notebook.\nAfter including the new features, the model(model3)  recorded an r-squared of (.8337) against the training set along with a MSE of (.1605). In 10-Fold KFold crossvalidation, it recorded an r-squared of (.8296). Against the test set, r-squared was (.8398) and the MSE was (.1605). The difference of (.0057) for the MSE of the training and test groups suggests that the model is not overfit.\nModel3 Residuals Against Test Data\n\nBased on the plot of residuals, there is improvement for high values homes, and residuals appear to be reduced.\nVariable Selection By P-Values\nThrough stepwise selection, I determined features that had statistically significant relationships at an alpha of .05.\nAfter excluding insignificant features, the model(model4)  recorded an r-squared of (.8315) against the training set along with a MSE of (.1684). In 10-Fold KFold crossvalidation, it recorded an r-squared of (.8294). Against the test set, r-squared was (.8389) and the MSE was (.1614). The difference of (.0070) for the MSE of the training and test groups suggests that the model is not overfit.\nModel4 Residuals Against Test Data\n\nBased on the plot of residuals, there is improvement for high values homes, and residuals appear to be reduced.\nAnalysis\nTo conduct analysis on each variable I isolated it's impact within the model by taking the average of every other category to fill the dataset, and looking at what happens to predicted price when only changing the target independent variable.\nHousing Valuation\n\nThe model predicts highest prices in the month of March, with a difference of $12,530 over the average prices of homes in the sample. The model predicts lowest prices in the month of October with prices falling a predicted $13,417.\nRenovation Advice\n\nThe model predicts that renovation of any kind improves the value of a home by $18,824. Not renovating leads to a prediction $529 below the average price of homes in the sample.\n\nThe model predicts that adding only 100 square feet of living space adds approximately $12,913 of value to a home, flattening off and peaking at 4738 square feet.\n\nThe model predicts that adding a second bedroom will increase the value of the home by $29,478. Adding additional bedrooms will contribute to the value of the home significantly less, and adding a fifth bedroom will actually decrease the value of the home. Adding an additional half-bathroom to any size home is predicted to increase the value by $6,781.\nInvestment Guidance\n\nThe model predicts that houses lose value for their first 24 years after being built. At that point, the prices then increase until peaking at 94 years old.\n""], 'url_profile': 'https://github.com/shsobieski', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['Image Inpainting\nInpainting is a process of predicting the corrupted pixels, for which we’ll use RBF regression. To use this code go ahead and clone the repsitory or downlad the file.\nRequirement: Numpy\nInstructions\nSTEP 1: Go ahead and open rbf_image_inpainting.py in a text editor. In the main\n          function you will see image_name. Make sure your image is of tif type.\n\nSTEP 2: You may choose to play-around with hyper parameters. It will make the \n          script either run slow or fast depending on your choice.\n\nSTEP 3: Run the file bf_image_inpainting.py in the terminal or by double click.\n\n'], 'url_profile': 'https://github.com/abhin-ch', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Movie-Review-Sentiment-Analysis-\nTechniques: Bad of words BOW, stop word, TF IDF, Logistic Regression, F1 score, presicion and recall\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Chennai, India', 'stats_list': [], 'contributions': '230 contributions\n        in the last year', 'description': ['House-Prices-Regression-Analytics-Club\nHouse prices regression task for Driver Performance Analysis Project, Analytics Club\n'], 'url_profile': 'https://github.com/NishantPrabhu', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'West Lafayette, IN, USA', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Identifying frames in social media text\nSocial media posts (tweets, facebook posts) often have a hidden underlying bias. Because of this bias, the same topic is potrayed in different lights (positive, negative or neutral). Identifying this hidden bias, commonly known as framing in natural processing tasks is a challenging task. In this project, we examine how a linear classifier such as logistic regression performs compared to a neural network in identifying frames in twitter discourse.\n'], 'url_profile': 'https://github.com/parvez018', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/divyanshutw', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['TitanicDataset\nAnalysing one of the most popular datasets about the Titanic survivors using Logistic Regression\n'], 'url_profile': 'https://github.com/mauwazahmed', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Provo, UT', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/natelant', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Weather Predict for Hanoi Using Machine Learning model Polynomial Regression with K-Fold Cross Validation\n1. Requirement:\n\nPython 3.x\nPandas\nScipy, Numpy\nScikit-Learn\nMatplotlib\n\n2. Run:\n\nExec main.py file\n\n'], 'url_profile': 'https://github.com/duyminhnguyen97', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['AutomatedCoal\nAutomated Coal System menggunakan metode multiple linear regression untuk mendapatkan gross caloric value\n'], 'url_profile': 'https://github.com/rizqiadhie00', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 5, 2020', 'Python', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020']}"
"{'location': 'Noida, India', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/singh-manvinder', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Torino', 'stats_list': [], 'contributions': '545 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danielegenta', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['About project\nSmall project to demonstrate regression, after which junit5 stop excuting tests, which executed before.\nMotivation\nWe have a quite big project with a lot of modules.\nWe discovered this regression:\nWe upgraded junit to version 5.6.2 and after few days we luckily found\n, that tests in one module were not executed anymore! :(   What a pity :(\nHow to use\n'], 'url_profile': 'https://github.com/bugs84', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Imperial College London', 'stats_list': [], 'contributions': '161 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashm97', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EbsHirani', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sivaaambati', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mihirkumar02', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Bank-Customer-Churn-Prediction\nUsed Logistic Regression, KNN, Decision Tree and Random Forest to make predictions.\nThe problem statement is there in the doc file.\n'], 'url_profile': 'https://github.com/spandanpal22', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Cambridge, MA', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arnavinator', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Arequipa', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Linear-Regression-with-Gradient-Descent\n'], 'url_profile': 'https://github.com/dantecarlo', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 18, 2020', 'Kotlin', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['SB-Mini-Project-Logistic-Regression\n'], 'url_profile': 'https://github.com/triffidia', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MahaSundar', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/urvins03', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Definition:\nWhat is Linear Regression?\nIt is a relationship between 2 variables x and y where x = input or independent variables y = output or dependent value This relationship is in the form of a straight line that best tries to fit each variable.\nMathematically it can be defined as y = mx + c where x and y are as defined above, m = slope or coefficient of regression c = constant\nGeneral view of a straight line : y = mx + c\n\nBest fit line\nLet us see a real world graphical representation of Linear Regression. For this we are using SARS 2003 dataset provided by Kaggle.\nHere, we are showing the data in the form of a scatter plot where the relationship between x and y variables is shown. To get better understanding of the relationship, we draw a line called as - best fit line. It is a line that is drawn through the scatter plot points. The idea of this line is to best fit all the points.\n\nEvaluation metrics:\n1. Mean Absolute Error (MAE):\nThe distance between any data point and the best fit line is called as prediction error. The absolute mean of it is called as Mean Absolute Error (MAE).\nValue range:\n0 to ∞ Best value = 0.0 or we can also say that lower the value, better it is.\nFormula:\n\n2. Mean Squared Error (MSE):\nThe summation of suqare of all the distances between any data point and the best fit line divided by the total number of data points (mean) gives us a value called as Mean Squared Error (MSE).\nValue range:\n0 to ∞ Best value = 0.0 or we can also say that lower the value, better it is.\nFormula:\n\n3. Root Mean Squared Error (RMSE):\nFor calculating RMSE, we do square root of MSE.\nValue range:\n0 to ∞ Best value = 0.0 or we can also say that lower the value, better it is.\nFormula:\n\nNote: If MAE or MSE or RMSE value is 0 then it means that all the data points are on the best fit line. Higher the value means that the data points are away from the best fit line.\n4. R2 Score - Coefficient of Determination:\nR2 Score is defined as regression sum of squares divided by the total sum of squares.\nFormula  and all the parameters needed to calculate the value of R-square:\n\nOR\n\n\nConsider the above example with one new line - horizontal added. So, now we can describe the formula components as -\na) SS (Res) = Regression sum of squares and it quantifies how far is the regression slope is from the horizontal (no relationship line) the sample mean or y¯.\nb) SS (Tot) = Regression total sum of squares and it quantifies how much the data points, yi, vary around their mean, y¯.\n'], 'url_profile': 'https://github.com/darshan-jain-29', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': [""Analyzing Pet Adoptions\nI completed this project as part of Metis Data Science Bootcamp.\nAccording to aspca.org 6.5 million dogs/cats end up in shelters each year in US, 1.5 million of them get euthenized each year. There are 300 million stray dogs and cats worldwide.\nOnly very small number of pets gets to go home to their forever home. Most of the pets stay in the shelters, if they are lucky.\nThe goal of this project was to predict the number of days it takes for a pet to get adopt\ned.\nData Source and Scraping\nI scraped the data from petfinder.my website. I only focused on dogs and cats. I used Python library Beautiful soup to scrape the pet profiles, which included information on the pet's type, gender, age, health, vaccination, body size, color, region of country, price, the date posted and updated, pet description, and so on (the pet profile also contained a picture of a pet, which I didn't scrape). All of this information is saved in a csv file\nData Cleaning\nMost of the columns needed some kind of cleaning:\n\nsome pet information was put together under one column, those needed to be separated into individual columns\nsome of the columns had redundant words and punctuations whcich were removed\nposted and updated dates were converted into datetime format\nadoption length was found using the posted and updated dates (we assumed the updated date was when the pet got adopted)\noutliers were removed\nI created some categorical features from continous features\nCategorical variables were converted into dummy variables for modeling purposes\n\nModeling\nI used Linear, Ridge and Laso Regression with k-fold cross validation. However, none of them had good results.\nChecking the linear regression assumptions, the residuals weren't normally distributed and the Q-Q plot was skewed which means that the relationship between the features and the target might not be linear.\nIn the future, I would like to explore this project more and try to imporve the results:\n\napply non linear models\nuse NLP to analyize the pet descriptions to see if that could help with increasing the model accuracy\nscrape the actual pictures of the pets and use that in the prediction since I think picture of the pet is one of the important factors of pet adoption\napproach this as a classification problem instead of regression\noptimize the code\n\n""], 'url_profile': 'https://github.com/LusineKamikyan', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '469 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/omamazainab', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Derek-Funk', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'Barcelona', 'stats_list': [], 'contributions': '1,319 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/theUniC', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prasadkachawar', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Predictive-analytics-using-regression-model\n'], 'url_profile': 'https://github.com/rnidhiry', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'R', 'Updated Apr 17, 2020', 'PHP', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}"
"{'location': 'Bengaluru', 'stats_list': [], 'contributions': '1,073 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Santosh051985', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Nairobi Kenya', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['Linear Regression on the Titanic Dataset\n'], 'url_profile': 'https://github.com/BrightonOtieno', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NHTSA---Logistic-Regression-and-SVM\nPerform predictive analysis (classification) upon a data set: model the dataset using\nlogistic regression and support vector machines, and making conclusions from the analysis. Follow the CRISP-DM framework in your analysis\nSVM and Logistic Regression Modeling\n\nCreate a logistic regression model and a support vector machine model for the classification task involved with your dataset.\nAssess how well each model performs using 80/20 training/testing split for the data\nAdjust parameters of the models to make them more accurate.\nDiscuss the advantages of each model for each classification task\nUse the weights from logistic regression to interpret the importance of different features for the classification task\n\n'], 'url_profile': 'https://github.com/daedwards06', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Perú', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/williamccondori', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Buffalo, New York', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Logistic-Regression-WDBC-Dataset\nIn this project binary logistic regression algorithm is employed to determine the malignancy status of the patients in the Wisconsin Diagnostic Breast Cancer (WDBC) dataset.\n'], 'url_profile': 'https://github.com/SamboDutta20', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Arequipa', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Linear-Regression-with-Normal-Equation\n'], 'url_profile': 'https://github.com/dantecarlo', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'INDIA', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ravite9', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': [""Predicting House Prices - Advanced-Regression\nIntroduction\nIn this repository, our goal is to predict the monetary value of homes in Boston suburbs\n\nWe will use several ML algorithms and evaluate their performance and predictive power in order to come up with a 'best fit' model trained on data collected from homes located in the suburbans of Boston.\n\nAbout the Dataset\nThe dataset is sourced from UCI Machine Learning Repository. The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. Note that every record will describe a Boston suburb or a town\n\nSource : https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names\n\nLanguages Used\nPython\n\nMachine Learning Pipeline\nImporting all required libraries > Loading/Importing the dataset > Data Exploration > Splitting the data into training and test datasets > Model Development > Model Comparison/Interpreting the results\n""], 'url_profile': 'https://github.com/aashishjosh1', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MohKu311', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['LinearRegression-on-Gapminder-Dataset\n'], 'url_profile': 'https://github.com/Ally0601', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Python', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Predictive-analytics-using-regression-model\n'], 'url_profile': 'https://github.com/rnidhiry', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['htw-data-mining-2020\nFrame the problem and look at the big picture.\n\nDefine the objective in business terms.\nFigure out who will leave the telecommunication company in the future?\nHow will your solution be used?\nCall the people who were clustered into ""will leave soon"" and talk to them.\n(In our example it is: Buy the ticket or do not buy the ticket)\nWhat are the current solutions/workarounds (if any)?\nHow should you frame this problem (supervised/unsupervised, online/offline, etc.)\nHow should performance be measured?\nIs the performance measure aligned with the business objective?\nWhat would be the minimum performance needed to reach the business objective?\nWhat are comparable problems? Can you reuse experience or tools?\nIs human expertise available?\nCustomers do have their day to day job so they might not be available for the data collection\nHow would you solve the problem manually?\nList the assumptions you or others have made so far.\nVerify assumptions if possible.\n\nExplore the data\nNote: try to get insights from a field expert for these steps.\n\nCreate a copy of the data for exploration (sampling it down to a manageable size if necessary).\nCreate a Jupyter notebook to keep record of your data exploration.\nStudy each attribute and its characteristics:\n\n\nName\nType (categorical, int/float, bounded/unbounded, text, structured, etc.)\n% of missing values\nNoisiness and type of noise (stochastic, outliers, rounding errors, etc.)\nPossibly useful for the task?\nType of distribution (Gaussian, uniform, logarithmic, etc.)\n\n\nFor supervised learning tasks, identify the target attribute(s).\nVisualize the data.\nStudy the correlations between attributes.\nDo not only use pearson correlation. We could have strong correlations between non correlating indicators\nStudy how you would solve the problem manually.\nIdentify the promising transformations you may want to apply.\nIdentify extra data that would be useful (go back to ""Get the Data"" on page 502).\nDocument what you have learned.\n\nPrepare the data\n\nTimeseries plot and look at how the flights change\nEngineer new features!\n\n'], 'url_profile': 'https://github.com/jonasfaehrmann', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'New Jersey', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Coursera-Linear-Regression-and-Modeling\nThis folder contains the answer keys to the Coursera course Linear Regression and Modeling (part of the Statistics with R Specialization) by Duke University, slides and the weekly lab R code.\n'], 'url_profile': 'https://github.com/ctzhou86', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/osinkolu', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Regression-Predicitive-Modeling-Apprentice-Chef\nObjective: Build a machine learning model to predict revenue over the first year of each customer’s life cycle.\nMachine Learning methods:\ni. Predict the TARGET variable REVENUE. There are a number of variables provided to support the prediction, i.e: Total Meals Order, Weekly Meal Plan, etc. Out of 29 columns, 4 are categorical and 25 are numerical.\nii. Feature engineering with collinearity by finding relationships between multiple predictors at once or called as multi-collinearity. Other techniques was by setting a certain threshold from analyzing the shape of the data.\niii. Data scaling and model tuning to optimize accuracy.\nChoosing between models:\ni. Start with several models that are interpretable and most flexible.\nii. Consider using the simplest model that reasonably approximates the best performance.\niii. Intepret the result.\n'], 'url_profile': 'https://github.com/ivanmanurung89', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['Applied Linear Regression & Web Scrapping\nTo see the workbook for the Applied Linear Regression project go to : https://github.com/EldaPiedade/Applied-Linear-Regression-Web-Scrapping/blob/master/AP_Project_Group1.pdf\nThe Data On this  Repository was gathered at https://texas.hometownlocator.com/tx/houston/ for use at Scholars Academy TTC Program by Elda Piedade.\nWeb Scraping Notebook: https://github.com/EldaPiedade/Applied-Linear-Regression-Web-Scrapping/blob/master/WEBSCRAPPING_Texas_Gazette.ipynb\nAbstract\nThe objective of this research is to implement linear regression to predict the median home value for the\nhouses in Houston zip codes. Data was gathered from 96 zip codes in\nHouston by utilizing python web scrapping resources to collect data\nfrom the Texas Hometown Locator website (owned by HTL, Inc.).With\nthe dataset extracted and cleaned, exploratory data analysis and\nstatistical analysis were performed to understand the relationship\nbetween the median home value and other variables, such as diversity\nindex, per capita income, and average household size. Based on the\nanalysis, data was modeled with linear regression. Initial\nresults from the linear regression model indicate that the diversity\nindex and average household size are not significant predictors. On the\nother hand, the per capita income identifies as the best predictor.\nAdditionally, although previous assumptions, the diversity index is\nonly moderately negatively correlated with the median home value. In\nother words, the more diverse a zip code is, the slightly smaller is the\nmedian home value. Further, into this research, the population in focus\nwill extend to other Texas cities, such as Austin. With more data, the\nrelationship between variables and median home value can be better\nevaluated as well as the prediction capacity of each model.\n'], 'url_profile': 'https://github.com/EldaPiedade', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Lab_4\nLab 4\n'], 'url_profile': 'https://github.com/Harsh-Panchal01', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '708 contributions\n        in the last year', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/nadinezab', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'Kas, Antalya, Turkey', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': ['As a capstone project for Data Science with Python course on Codecademy, I built a Linear Regression ML Model on Yelp Data based on the restaurant features.\n'], 'url_profile': 'https://github.com/cereniyim', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '404 contributions\n        in the last year', 'description': ['Regularized-Linear-Regression-Bias-Varianace\n\nUse regularized linear regression to\nstudy models with different bias-variance properties.\nUtilized learning curves to identify bias/variance relationship.\nIdentified bias as the model underfit training data, added additional features to better fit data and reduce bias.\nPrevented overfitting by optimizing lambda (regularization factor)\n\n'], 'url_profile': 'https://github.com/JJwilkin', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'MATLAB', 'Updated Apr 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Objective: \nCreate a linear regression model to predict profit (after opening weekend) of a movie based on different features.\nMethodology: \nData from 1000 films was acquired through webscraping from Box Office Mojo.\nAnalyses: \nVarious linear regression models with cross validation and regularization were run in order to determine best model to fit data.\nLasso regularization was chosen due to highest R^2 score.\n'], 'url_profile': 'https://github.com/sodas32', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kt50', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['rNeighborGWAS\nThis is a developer version of the rNeighborGWAS package. Please see vignette(""rNeighborGWAS"") for usage.\nCRAN version is available at https://cran.r-project.org/package=rNeighborGWAS.\nInstallation\nPlease install the package via GitHub using the devtools library as devtools::install_github(""yassato/rNeighborGWAS"", repo=""master"").\nDependency\nNote that the rNeighborGWAS requires the following R packages.\n\ngaston\nparallel\n\nRelease Notes\nversion 1.2.3 (developer version): rebuild using R version 4.0.3; asymmetric neighbor effects are implemented.\nversion 1.2.2 (CRAN version): partial PVEs provided by calc_PVEnei(); nei_lm() added.\nversion 1.2.1: testthat files fixed.\nversion 1.2.0: nei_lmm() and gaston2neiGWAS() added; nei_coval() and neiGWAS() refactored.\nversion 1.0.0: Initial version registered in CRAN.\nReference\nSato Y, Yamamoto E, Shimizu KK, Nagano AJ (2021) Neighbor GWAS: incorporating neighbor genotypic identity into genome-wide association studies of field herbivory. Heredity https://doi.org/10.1038/s41437-020-00401-w\n'], 'url_profile': 'https://github.com/yassato', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neelesh2k', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/calinacopos', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Córdoba, Argentina', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['kaggle-house-prices-advanced-regression-techniques\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\n'], 'url_profile': 'https://github.com/tofredi', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['IBM Introduction to Deep Learning & Neural Networks with Keras Course Project\n\nThis repository is a submission of Aisa Mijeno for Introduction to Deep Learning & Neural Networks with Keras course project.\n\nProject Description\n\nIn this course project, you will build a regression model using the deep learning Keras library, and then you will experiment with increasing the number of training epochs and changing number of hidden layers and you will see how changing these parameters impacts the performance of the model.\nDataset\n\nFor your convenience, the data can be found here again: https://cocl.us/concrete_data. To recap, the predictors in the data of concrete strength include:\n\nCement\nBlast Furnace Slag\nFly Ash\nWater\nSuperplasticizer\nCoarse Aggregate\nFine Aggregate\n\nInstructions\n\nPART A\nA. Build a baseline model.\nUse the Keras library to build a neural network with the following:\n\nOne hidden layer of 10 nodes, and a ReLU activation function\nUse the adam optimizer and the mean squared error as the loss function.\n\n\nRandomly split the data into a training and test sets by holding 30% of the data for testing. You can use the train_test_split helper function from Scikit-learn.\nTrain the model on the training data using 50 epochs.\nEvaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\nRepeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\nReport the mean and the standard deviation of the mean squared errors.\n\nSubmit your Jupyter Notebook with your code and comments.\nPART B\nB. Normalize the data.\nRepeat Part A but use a normalized version of the data. Recall that one way to normalize the data is by subtracting the mean from the individual predictors and dividing by the standard deviation.\nHow does the mean of the mean squared errors compare to that from Step A?\nPART C\nC. Increate the number of epochs\nRepeat Part B but use 100 epochs this time for training.\nHow does the mean of the mean squared errors compare to that from Step B?\nPART D\nD. Increase the number of hidden layers\nRepeat part B but use a neural network with the following instead:\n\nThree hidden layers, each of 10 nodes and ReLU activation function.\n\nHow does the mean of the mean squared errors compare to that from Step B?\n'], 'url_profile': 'https://github.com/aisamijeno', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ThejasviniVunnam', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. You will learn about p-values later, but for now, you can remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 6, 2020', 'Updated Apr 15, 2020', 'R', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Prediction-Of-Sales-using-Linear-Regression\nSales for a particular product as a function of advertising budgets for TV, radio, and newspaper media.\n'], 'url_profile': 'https://github.com/varalakshmiarcot', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aliweka2020', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'Russia, Saint-Petersburg', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Проект из обучающих курсов Яндекс. Практикум\nLine_regression-with-matrix-and-vector\nСоздание модели линейной регрессии при помощи библиотеки numpy\nОписание проекта\nНужно защитить данные клиентов страховой компании. Разработать такой метод преобразования данных, чтобы по ним было сложно восстановить персональную информацию. Обосновать корректность его работы.\nНужно защитить данные, чтобы при преобразовании качество моделей машинного обучения не ухудшилось.\n'], 'url_profile': 'https://github.com/EgoVed', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'Sialkot', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['Non-Linear-Regression-China-GDP-Trends\n'], 'url_profile': 'https://github.com/mrqasimasif', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Salary_Prediction-using-multiple-linear-regression\nI have created simple salary prediction model based on the multiple linear regression algorithm.\nit has multiple attributes such as experience, test_score and interview_score\n'], 'url_profile': 'https://github.com/KrishnaNarwade', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Life-Expectancy-Predictor-Using-Linear-Regression\n'], 'url_profile': 'https://github.com/mahakporwal02', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Prediction-of-House-s-Price-Using-Logistic-Regression-\nAn accurate prediction of the house price is important to prospective homeowners, developers, investors, appraisers, tax assessors and other real estate market participants, such as, mortgage lenders and insurers. Traditional house price prediction is based on cost and sale price comparison lacking of an accepted standard and a certification process. Therefore, the availability of a house price prediction model helps fill up an important information gap and improve the efficiency of the real estate market. In our project we use different attributes to predict house price.\n""# Prediction-of-House-Price-Using-Logistic-Regression-""\n""# Prediction-of-House-Price-Using-Logistic-Regression-""\n""# Prediction-of-House-Price-Using-Logistic-Regression-""\n'], 'url_profile': 'https://github.com/ykchethan', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', '1', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Apr 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '708 contributions\n        in the last year', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Boston Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Boston Housing Data again!\nThis time, let's only include the variables that were previously selected using recursive feature elimination. We included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nb = boston_features['B']\nlogdis = np.log(boston_features['DIS'])\nloglstat = np.log(boston_features['LSTAT'])\n\n# Min-Max scaling\nboston_features['B'] = (b-min(b))/(max(b)-min(b))\nboston_features['DIS'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features['LSTAT'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nX = boston_features[['CHAS', 'RM', 'DIS', 'B', 'LSTAT']]\ny = pd.DataFrame(boston.target, columns = ['target'])\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Importing and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n\n<matplotlib.legend.Legend at 0x1a24d6cef0>\n\n\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 100 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n\n<matplotlib.legend.Legend at 0x1a26e93438>\n\n\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/nadinezab', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['advertising-response-prediction-via-logistic-regression\nThis project is a part of Python for data science and machine learning bootcamp.\nProject Summary\nIn this project we will be working with an advertising data set, indicating whether or not a particular internet user clicked on an Advertisement. We will create a logistic regression classification model that will predict whether or not users will click on an ad based off the features collected of them.\nResults\nClassification accuracy: 0.91\nConfusion Matrix\n\n'], 'url_profile': 'https://github.com/zeglam', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aakansham20', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jejrhom', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Linear-Regression-Using-Boston-Housing-Data-Set\nThis is a mini project for Springboard online study of Data Science\n'], 'url_profile': 'https://github.com/jacquelineguo', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Indonesia', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nicholascandra1996', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}"
"{'location': 'Waterloo', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Wine Quality Prediction using Logistic Regression\nIn this mini-project we took UCI dataset of Red wine and white wine and used Logistic Regression to predict the quality of the wine\n'], 'url_profile': 'https://github.com/dhavalthakur', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '708 contributions\n        in the last year', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Boston Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Boston Housing Data\nWe pre-processed the Boston Housing data again. This time, however, we did things slightly different:\n\nWe dropped 'ZN' and 'NOX' completely\nWe categorized 'RAD' in 3 bins and 'TAX' in 4 bins\nWe transformed 'RAD' and 'TAX' to dummy variables and dropped the first variable to eliminate multicollinearity\nWe used min-max-scaling on 'B', 'CRIM', and 'DIS' (and log transformed all of them first, except 'B')\nWe used standardization on 'AGE', 'INDUS', 'LSTAT', and 'PTRATIO' (and log transformed all of them first, except for 'AGE')\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nboston_features = boston_features.drop(['NOX', 'ZN'],axis=1)\n\n# First, create bins for based on the values observed. 3 values will result in 2 bins\nbins = [0, 6, 24]\nbins_rad = pd.cut(boston_features['RAD'], bins)\nbins_rad = bins_rad.cat.as_unordered()\n\n# First, create bins for based on the values observed. 4 values will result in 3 bins\nbins = [0, 270, 360, 712]\nbins_tax = pd.cut(boston_features['TAX'], bins)\nbins_tax = bins_tax.cat.as_unordered()\n\ntax_dummy = pd.get_dummies(bins_tax, prefix='TAX', drop_first=True)\nrad_dummy = pd.get_dummies(bins_rad, prefix='RAD', drop_first=True)\nboston_features = boston_features.drop(['RAD','TAX'], axis=1)\nboston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\nage = boston_features['AGE']\nb = boston_features['B']\nlogcrim = np.log(boston_features['CRIM'])\nlogdis = np.log(boston_features['DIS'])\nlogindus = np.log(boston_features['INDUS'])\nloglstat = np.log(boston_features['LSTAT'])\nlogptratio = np.log(boston_features['PTRATIO'])\n\n# Min-Max scaling\nboston_features['B'] = (b-min(b))/(max(b)-min(b))\nboston_features['CRIM'] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\nboston_features['DIS'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features['AGE'] = (age-np.mean(age))/np.sqrt(np.var(age))\nboston_features['INDUS'] = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\nboston_features['LSTAT'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nboston_features['PTRATIO'] = (logptratio-np.mean(logptratio))/(np.sqrt(np.var(logptratio)))\nboston_features.head()\nRun a linear model in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nInterpret the coefficients for PTRATIO, PTRATIO, LSTAT\n\nCRIM: per capita crime rate by town\nINDUS: proportion of non-retail business acres per town\nCHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\nRM: average number of rooms per dwelling\nAGE: proportion of owner-occupied units built prior to 1940\nDIS: weighted distances to five Boston employment centres\nRAD: index of accessibility to radial highways\nTAX: full-value property-tax rate per $10,000\nPTRATIO: pupil-teacher ratio by town\nB: 1000(Bk - 0.63)^2 where Bk is the proportion of African American individuals by town\nLSTAT: % lower status of the population\n\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nCRIM: 0.15\nINDUS: 6.07\nCHAS: 1\nRM:  6.1\nAGE: 33.2\nDIS: 7.6\nPTRATIO: 17\nB: 383\nLSTAT: 10.87\nRAD: 8\nTAX: 284\n\nSummary\nCongratulations! You pre-processed the Boston Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Boston Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/nadinezab', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '635 contributions\n        in the last year', 'description': ['Santander-Value-Prediction-Challenge\nThe project is based on a Kaggle competition Santander Value Prediction Challenge.\nSantander Bank is asking Kagglers to help them identify the value of transactions for each potential customer.\n.\nDetailed project description is available on Medium:\nhttps://medium.com/@xDCT/santander-value-prediction-challenge-a5a6d260e561\n.\nProject Workflow Diagram:\n\n'], 'url_profile': 'https://github.com/DarioCT', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Ahmedabad', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Regression-Analysis-on-Stream-and-Batch-Data\nGradientDescent.ipynb\nMultimple Linear Regression Analysis on Boston Housing Dataset using Gradient Descent algorithm\nNormalEquation.ipynb\nMultimple Linear Regression Analysis on Boston Housing Dataset using Normal Equation Method\nICMR_and_ASR.ipynb\nMultimple Linear Regression Analysis on Boston Housing Dataset using Incremental Mathematical Stream Regression Anlysis and Approximate Stream Regression Approach.\nBoth methods use a window based approach to prevent multiple scans of the entire data set, nevertheless they are able to maintain the accuracy of the regression model.\nIMSR recalculates the regression function parameters based on the latest data in the current window and the synopsis of previous data.\nASR approximately estimates the regression parameters by using aweighted average of the parameters obtained from current window data and the regression parameters of pervious windows.\n'], 'url_profile': 'https://github.com/akshaygopani', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['Customer-churn-with-Logistic-Regression.ipynb\nA telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why.\n'], 'url_profile': 'https://github.com/shivamvns', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i=Pr(Y_i=1|X_i=x_i)=\\dfrac{\\text{exp}(\\beta_0+\\beta_1 x_i)}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '708 contributions\n        in the last year', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Boston Housing Data once more\nWe pre-processed the Boston Housing data the same way we did before:\n\nWe dropped \'ZN\' and \'NOX\' completely\nWe categorized \'RAD\' in 3 bins and \'TAX\' in 4 bins\nWe transformed \'RAD\' and \'TAX\' to dummy variables and dropped the first variable\nWe used min-max-scaling on \'B\', \'CRIM\', and \'DIS\' (and logtransformed all of them first, except \'B\')\nWe used standardization on \'AGE\', \'INDUS\', \'LSTAT\', and \'PTRATIO\' (and logtransformed all of them first, except for \'AGE\')\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nboston_features = boston_features.drop([\'NOX\', \'ZN\'],axis=1)\n\n# First, create bins for based on the values observed. 3 values will result in 2 bins\nbins = [0,6,  24]\nbins_rad = pd.cut(boston_features[\'RAD\'], bins)\nbins_rad = bins_rad.cat.as_unordered()\n\n# First, create bins for based on the values observed. 4 values will result in 3 bins\nbins = [0, 270, 360, 712]\nbins_tax = pd.cut(boston_features[\'TAX\'], bins)\nbins_tax = bins_tax.cat.as_unordered()\n\ntax_dummy = pd.get_dummies(bins_tax, prefix=\'TAX\', drop_first=True)\nrad_dummy = pd.get_dummies(bins_rad, prefix=\'RAD\', drop_first=True)\nboston_features = boston_features.drop([\'RAD\', \'TAX\'], axis=1)\nboston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\n\nage = boston_features[\'AGE\']\nb = boston_features[\'B\']\nlogcrim = np.log(boston_features[\'CRIM\'])\nlogdis = np.log(boston_features[\'DIS\'])\nlogindus = np.log(boston_features[\'INDUS\'])\nloglstat = np.log(boston_features[\'LSTAT\'])\nlogptratio = np.log(boston_features[\'PTRATIO\'])\n\n# Min-Max scaling\nboston_features[\'B\'] = (b-min(b))/(max(b)-min(b))\nboston_features[\'CRIM\'] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\nboston_features[\'DIS\'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features[\'AGE\'] = (age-np.mean(age))/np.sqrt(np.var(age))\nboston_features[\'INDUS\'] = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\nboston_features[\'LSTAT\'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nboston_features[\'PTRATIO\'] = (logptratio-np.mean(logptratio))/(np.sqrt(np.var(logptratio)))\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this function provided on your preprocessed Boston Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nThe stepwise procedure mentions that \'INDUS\' was added with a p-value of 0.0017767, but our statsmodels output returns a p-value of 0.000. Use some of the stepwise procedure logic to find the intuition behind this!\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.742981  \n# adjusted_r_squared is 0.740411\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Boston Housing dataset!\n'], 'url_profile': 'https://github.com/nadinezab', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/andrzejmalota', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}"
"{'location': 'Hong Kong', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': [""Regression-Analysis-of-second-hand-cars\nObjective: to predict the price of a second-hand car depending on its specifications\nLinear regression modeling was implemented on second-hand cars dataset to predict the price of a second-hand car depending on its specifications. Data preprocessing; data and error analysis; cleaning and validating data for uniformity and accuracy; exploratory data analysis were performed.\nThere are two files. One is 'Second-hand cars.csv' and the other one is 'Regression Analysis of second-hand cars.ipynb'\n'Second-hand cars.csv' is the Excel data file which contains the dataset\nIn case the ipynb file cannot be opened, you can access the programme on https://nbviewer.jupyter.org/github/waynechanccw01/Regression-Analysis-of-second-hand-cars/blob/master/Regression%20Analysis%20of%20second-hand%20cars.ipynb\n""], 'url_profile': 'https://github.com/waynechanccw01', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aish-warya', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. \\\nThe company is looking at prospective properties to buy to enter the market.\nYou are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\nWhich variables are significant in predicting the price of a house, and\nHow well those variables describe the price of a house.\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal\nYou are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for the management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/varunick', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'Ottawa, Ontario', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['Logistic_regression_and_lda_from\nImplemented a logistic regression and linear discriminant analysis from scratch to test on sonar and parkinson data\nHow to run code (using Colab):\n1. Open up Colab\n2. Upload any one of the "".pyinb"" files (File > Upload notebook...)\n3. When the file is uploaded, select ""Run All"" (Runtime > Run all, OR just use Ctrl+F9)\n4. Look at code box #4/5 (There are comments that say pretty much exactly what will be\nwritten here but nonetheless). You\'ll want to have the 3 .py files located somewhere\non your computer that you can readily access for this step.\na) When this box is being run, there will pop up a small box that says ""Choose File"".\nClick it, and navigate to where you have stored the .py files. The first file to\nupload is ""logisticregression.py"". Select it and upload/open it.\nb) When the file above is done uploading, there will be a second ""Choose File"" option.\nClick it and upload ""kfold.py"".\nc) When the file above has completed uploading, there will be a 3rd ""Choose File"" option.\nClick it and upload ""lda.py"".\n   IMPORTANT NOTE:\n\t   If you run into any errors, usually it\'s because you either waited to long to select \n\t   the choose file option, or Colab just had a hissy-fit. Just return to step 3 and try \n\t   again (ie. Run the program again). It\'ll fix it.\n\nIf the url for the dataset isn\'t working for whatever reason, you\'ll have to upload the dataset\nprovided somewhere public (like github) and switch the link in code box 2 with the link of\nwherever you\'ve uploaded the dataset.\n'], 'url_profile': 'https://github.com/ducongo', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '1,774 contributions\n        in the last year', 'description': [""Data Analysis on Adventure Works\nFinal Challenges from Microsoft: DAT275x. Data Analysis on  Adventure Works customer data to perform classification and regression.\nChallenge Overview\nIn 1998, the Adventure Works Cycles company collected a large volume of data about their existing customers, including demographic features and information about purchases they have made. The company is particularly interested in analyzing customer data to determine any apparent relationships between demographic features known about the customers and the likelihood of a customer purchasing a bike. Additionally, the analysis should endeavor to determine whether a customer's average monthly spend with the company can be predicted from known customer characteristics.\nIn this project, you must tackle three challenges:\n\nChallenge 1: Explore the data and gain some insights into Adventure Works customer characteristics and purchasing behavior.\nChallenge 2: Build a classification model to predict customer purchasing behavior.\nChallenge 3: Build a regression model to predict customer purchasing behavior.\n\nYou can explore the customer data using tools of your choice. Potential tools that you could use include:\n\nMicrosoft Excel\nR\nPython\nMicrosoft Azure Machine Learning\n\n""], 'url_profile': 'https://github.com/VasuGoel', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Machine_Learning_Image_Classifier\nIn this,I have implement one-vs-all logistic regression to recognize hand-written digits.\n'], 'url_profile': 'https://github.com/Ashish-Arya-CS', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'West Lafayette, IN', 'stats_list': [], 'contributions': '518 contributions\n        in the last year', 'description': ['This is a bare bones implementation of Ordinary Least Squares Regression in Python. A brief summary of the underlying mathematical concept is given below.\n\n'], 'url_profile': 'https://github.com/akshay-madar', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Analyse-the-Media-Buyers\nLinear Regression from scratch with loss function used: MSE and using gradient decent to minimize the loss function (error)\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 17, 2020', 'MATLAB', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020']}"
"{'location': 'Maryland', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Introduction\nTo observe how vacant homes and crime rate are co-related I used Python to do Regression Analysis.\nGetting Started\nimport pandas as pd\nimport string\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nCalling the crime dataset\ncrime=pd.read_csv(\'Crime_and_Safety__2015__data.csv\')\ncalling the vacant homes dataset\nvacant=pd.read_csv(\'vacanthome1.csv\')\nprinting the crime dataset to understand the attributes\ncrime\n\n\n\n\nthe_geom\nOBJECTID\nCommunity Statistical Areas (CSAs)\nPart 1 Crime Rate per 1,000 Residents\nViolent Crime Rate per 1,000 Residents\nProperty Crime Rate per 1,000 Residents\nJuvenile Arrest Rate per 1,000 Juveniles\nJuvenile Arrest Rate for Violent Offenses per 1,000 Juveniles\nJuvenile Arrest Rate for Drug Offenses per 1,000 Juveniles\nRate of 911 Calls for Service for Shootings per 1,000 Residents\nRate of Gun Homicides per 10,000 Residents\nRate of 911 Calls for Service for Common Assaults per 1,000 Residents\nRate of 911 Calls for Service for Narcotics per 1,000 Residents\nRate of 911 Calls for Service for Auto Accidents per 1,000 Residents\nAdult Arrest Rate per 1,000 Adults\nShape__Area\nShape__Length\n\n\n\n\n0\nMULTIPOLYGON (((-76.598249582828 39.3541564623...\n20\nGreater Govans\n48.965453\n11.328527\n37.262429\n17.316017\n2.597403\n4.329004\n3.183223\n0.748994\n66.473177\n81.546672\n47.841962\n32.581764\n2.269850e+07\n22982.125715\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n54\nMULTIPOLYGON (((-76.623112211246 39.3109806791...\n40\nPenn North/Reservoir Hill\n64.232520\n15.825403\n46.959040\n28.484232\n8.138352\n3.051882\n6.102607\n0.930906\n77.161771\n144.807613\n79.747621\n35.924054\n4.105980e+07\n29078.838498\n\n\n\nData-preprocessing step - checking for null values\ncrime.isna().any()\nthe_geom                                                                 False\nOBJECTID                                                                 False\nCommunity Statistical Areas (CSAs)                                       False\nPart 1 Crime Rate per 1,000 Residents                                    False\nViolent Crime Rate per 1,000 Residents                                   False\nProperty Crime Rate per 1,000 Residents                                  False\nJuvenile Arrest Rate per 1,000 Juveniles                                 False\nJuvenile Arrest Rate for Violent Offenses per 1,000 Juveniles            False\nJuvenile Arrest Rate for Drug Offenses per 1,000 Juveniles               False\nRate of 911 Calls for Service for Shootings per 1,000 Residents          False\nRate of Gun Homicides per 10,000 Residents                               False\nRate of 911 Calls for Service for Common Assaults per 1,000 Residents    False\nRate of 911 Calls for Service for Narcotics per 1,000 Residents          False\nRate of 911 Calls for Service for Auto Accidents per 1,000 Residents     False\nAdult Arrest Rate per 1,000 Adults                                       False\nShape__Area                                                              False\nShape__Length                                                            False\ndtype: bool\nData Pre-processing step - Rename for convinience\ncrime.rename(columns={\'Part 1 Crime Rate per 1,000 Residents\': \'CrimeRate\'}, inplace=True)\ncrime.rename(columns={\'Violent Crime Rate per 1,000 Residents\': \'ViolentCrimeRate\'}, inplace=True)\ncrime.rename(columns={\'Property Crime Rate per 1,000 Residents\': \'PropertyCrimeRate\'}, inplace=True)\ncrime.rename(columns={\'Juvenile Arrest Rate per 1,000 Juveniles\': \'JuvenileArrestRate\'}, inplace=True)\ncrime.rename(columns={\'Juvenile Arrest Rate for Violent Offenses per 1,000 Juveniles\': \'JuvenileViolentOffenses\'}, inplace=True)\ncrime.rename(columns={\'Juvenile Arrest Rate for Drug Offenses per 1,000 Juveniles\': \'JuvenileDrugOffenses\'}, inplace=True)\ncrime.rename(columns={\'Rate of 911 Calls for Service for Shootings per 1,000 Residents\': \'911Shootings\'}, inplace=True)\ncrime.rename(columns={\'Rate of Gun Homicides per 10,000 Residents\': \'GunHomicides\'}, inplace=True)\ncrime.rename(columns={\'Rate of 911 Calls for Service for Common Assaults per 1,000 Residents\': \'911CommonAssaults\'}, inplace=True)\ncrime.rename(columns={\'Rate of 911 Calls for Service for Narcotics per 1,000 Residents\': \'911Narcotics\'}, inplace=True)\ncrime.rename(columns={\'Rate of 911 Calls for Service for Auto Accidents per 1,000 Residents\': \'911AutoAccidents\'}, inplace=True)\ncrime.rename(columns={\'Adult Arrest Rate per 1,000 Adults\': \'AdultArrestRate\'}, inplace=True)\nData Pre-processing step - Dropping the columns which null values\ncrime.drop([\'the_geom\', \'OBJECTID\', \'Shape__Area\', \'Shape__Length\'],axis=1, inplace=True)\nPrinting the crime dataset after the updates\ncrime.head()\n\n\n\n\nCommunity Statistical Areas (CSAs)\nCrimeRate\nViolentCrimeRate\nPropertyCrimeRate\nJuvenileArrestRate\nJuvenileViolentOffenses\nJuvenileDrugOffenses\n911Shootings\nGunHomicides\n911CommonAssaults\n911Narcotics\n911AutoAccidents\nAdultArrestRate\n\n\n\n\n0\nGreater Govans\n48.965453\n11.328527\n37.262429\n17.316017\n2.597403\n4.329004\n3.183223\n0.748994\n66.473177\n81.546672\n47.841962\n32.581764\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4\nDorchester/Ashburton\n50.313932\n12.557271\n36.738503\n10.933558\n2.523129\n2.523129\n2.630239\n0.593925\n55.913796\n23.502461\n50.229085\n13.747564\n\n\n\nChecking if there are no more null values\ncrime.isna().any()\nCommunity Statistical Areas (CSAs)    False\nCrimeRate                             False\nViolentCrimeRate                      False\nPropertyCrimeRate                     False\nJuvenileArrestRate                    False\nJuvenileViolentOffenses               False\nJuvenileDrugOffenses                  False\n911Shootings                          False\nGunHomicides                          False\n911CommonAssaults                     False\n911Narcotics                          False\n911AutoAccidents                      False\nAdultArrestRate                       False\ndtype: bool\nData Preprocessing step - Splitting the Community column which has two places separated by \'/\' and then\nc = crime[\'Community Statistical Areas (CSAs)\'].str.split(\'/\').apply(pd.Series, 1).stack()\nc.index = c.index.droplevel(-1)\nc.name = \'Neighborhood\'\ndel crime[\'Community Statistical Areas (CSAs)\']\ncrime = crime.join(c)\ncrime=crime.reset_index()\ndel crime[\'index\']\nMaking sure the characters match correctly while performing a join\ncrime[\'Neighborhood\'] = crime[\'Neighborhood\'].str.lower()\ncrime.head()\n\n\n\n\nCrimeRate\nViolentCrimeRate\nPropertyCrimeRate\nJuvenileArrestRate\nJuvenileViolentOffenses\nJuvenileDrugOffenses\n911Shootings\nGunHomicides\n911CommonAssaults\n911Narcotics\n911AutoAccidents\nAdultArrestRate\nNeighborhood\n\n\n\n\n0\n48.965453\n11.328527\n37.262429\n17.316017\n2.597403\n4.329004\n3.183223\n0.748994\n66.473177\n81.546672\n47.841962\n32.581764\ngreater govans\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4\n72.435553\n26.114393\n44.978518\n53.972104\n10.309278\n14.554275\n9.667025\n1.208378\n116.071429\n220.797530\n76.194952\n72.268452\nharlem park\n\n\n\ncrime.shape[0]\n96\nData Exploratory Step - Understanding the vacant home dataset\nvacant[\'Neighborhood\'].value_counts()\nOutput\nBROADWAY EAST                      749\nCARROLLTON RIDGE                   515\nSANDTOWN-WINCHESTER                481\nHARLEM PARK                        453\nCENTRAL PARK HEIGHTS               367\nUPTON                              260\nPARKVIEW/WOODBROOK                  83\n                                  ...\n\nFOUR BY FOUR                         1\nLAURAVILLE                           1\nBURLEITH-LEIGHTON                    1\nWALTHERSON                           1\nGUILFORD                             1\nLOCUST POINT                         1\nHamilton Hills                       1\nName: Neighborhood, Length: 179, dtype: int64\n\nData Pre-processing step - dropping the columns which will not be used for the project\nvacant.drop([\'ReferenceID\',\'Block\',\'Lot\',\'NoticeDate\'],axis=1, inplace=True)\nMaking sure the characters match correctly while performing a join\nvacant[\'Neighborhood\'] = vacant[\'Neighborhood\'].str.lower()\nPrinting the crime dataset after the updates\nvacant\n\n\n\n\nBuildingAddress\nNeighborhood\nPoliceDistrict\nCouncilDistrict\nLatitude\nLongitude\n\n\n\n\n0\n1945 W NORTH AVE\neasterwood\nWESTERN\n7\n39.309473\n-76.649536\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8877\n5112 N FRANKLINTOWN ROAD\nfranklintown\nSOUTHWESTERN\n8\n39.303825\n-76.709725\n\n\n\n8878 rows × 6 columns\nChecking if there are no more null values\nvacant.isna().any()\nBuildingAddress    False\nNeighborhood       False\nPoliceDistrict     False\nCouncilDistrict    False\nLatitude           False\nLongitude          False\ndtype: bool\nJoining the datasets based on Neighborhood attribute\nfinal = pd.merge(crime, vacant, on = \'Neighborhood\')\nfinal\nfinal\n\n\n\n\nCrimeRate\nViolentCrimeRate\nPropertyCrimeRate\nJuvenileArrestRate\nJuvenileViolentOffenses\nJuvenileDrugOffenses\n911Shootings\nGunHomicides\n911CommonAssaults\n911Narcotics\n911AutoAccidents\nAdultArrestRate\nNeighborhood\nBuildingAddress\nPoliceDistrict\nCouncilDistrict\nLatitude\nLongitude\n\n\n\n\n0\n72.435553\n26.114393\n44.978518\n53.972104\n10.309278\n14.554275\n9.667025\n1.208378\n116.071429\n220.797530\n76.194952\n72.268452\nsandtown-winchester\n1807 W NORTH AVE\nWESTERN\n7\n39.309628\n-76.646765\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\n\n2384\n64.232520\n15.825403\n46.959040\n28.484232\n8.138352\n3.051882\n6.102607\n0.930906\n77.161771\n144.807613\n79.747621\n35.924054\nreservoir hill\n916 WHITELOCK ST\nCENTRAL\n7\n39.314631\n-76.633180\n\n\n\n2385 rows × 18 columns\nExploratory Analysis - Plotted a heat map to understand the crime with respect to district\nneighbor_heatmap = final[[\'PoliceDistrict\', \'CrimeRate\',\n       \'PropertyCrimeRate\',        \'JuvenileDrugOffenses\', \'911Shootings\', \'GunHomicides\',\n       \'911CommonAssaults\', \'911Narcotics\']]\n\nneighbor_heatmap = neighbor_heatmap.groupby(\'PoliceDistrict\').mean()\n\nneighbor_heatmap_cpy = neighbor_heatmap.copy()\n\ncol_names2 = [\'CrimeRate\',\n       \'PropertyCrimeRate\',        \'JuvenileDrugOffenses\', \'911Shootings\', \'GunHomicides\',\n       \'911CommonAssaults\', \'911Narcotics\']\n\nfeatures2 = neighbor_heatmap[col_names2]\n\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,100))\n\nscaler2 = min_max_scaler.fit(features2)\n\nfeatures2=scaler2.transform(features2)\n\nneighbor_heatmap_cpy[col_names2]= features2\n\nfig, ax = plt.subplots(figsize=(10,20))\n\nsns.heatmap(neighbor_heatmap_cpy, cmap=\'cividis\', linewidths=0.05,ax=ax)\n\nplt.show()\nOutput\n\nExploratory Analysis - Plotted a heat map to understand the crime with respect to neighbhorhood\nneighbor_heatmap = final[[\'Neighborhood\', \'CrimeRate\',\n       \'PropertyCrimeRate\',        \'JuvenileDrugOffenses\', \'911Shootings\', \'GunHomicides\',\n       \'911CommonAssaults\', \'911Narcotics\']]\n\nneighbor_heatmap = neighbor_heatmap.groupby(\'Neighborhood\').mean()\n\nneighbor_heatmap_cpy = neighbor_heatmap.copy()\n\ncol_names2 = [\'CrimeRate\',\n       \'PropertyCrimeRate\',        \'JuvenileDrugOffenses\', \'911Shootings\', \'GunHomicides\',\n       \'911CommonAssaults\', \'911Narcotics\']\n\nfeatures2 = neighbor_heatmap[col_names2]\n\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,100))\n\nscaler2 = min_max_scaler.fit(features2)\n\nfeatures2=scaler2.transform(features2)\n\nneighbor_heatmap_cpy[col_names2]= features2\n\nfig, ax = plt.subplots(figsize=(10,20))\n\nsns.heatmap(neighbor_heatmap_cpy, cmap=\'cividis\', linewidths=0.05,ax=ax)\n\nplt.show()\nOutput\n\nData Preprocessing - assigning numeric values which allows us to apply machine learning models\nfrom sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\nfinal[\'Neighborhood\'] = LE.fit_transform(final[\'Neighborhood\'])\nfinal[\'PoliceDistrict\'] = LE.fit_transform(final[\'PoliceDistrict\'])\nfinal[\'BuildingAddress\'] = LE.fit_transform(final[\'BuildingAddress\'])\noutput after label encoder is implemented\nfinal.head()\n\n\n\n\nCrimeRate\nViolentCrimeRate\nPropertyCrimeRate\nJuvenileArrestRate\nJuvenileViolentOffenses\nJuvenileDrugOffenses\n911Shootings\nGunHomicides\n911CommonAssaults\n911Narcotics\n911AutoAccidents\nAdultArrestRate\nNeighborhood\nBuildingAddress\nPoliceDistrict\nCouncilDistrict\nLatitude\nLongitude\n\n\n\n\n0\n72.435553\n26.114393\n44.978518\n53.972104\n10.309278\n14.554275\n9.667025\n1.208378\n116.071429\n220.79753\n76.194952\n72.268452\n35\n826\n10\n7\n39.309628\n-76.646765\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\n\n4\n72.435553\n26.114393\n44.978518\n53.972104\n10.309278\n14.554275\n9.667025\n1.208378\n116.071429\n220.79753\n76.194952\n72.268452\n35\n902\n10\n7\n39.309585\n-76.647336\n\n\n\nimplementing linear regression\nfrom sklearn.model_selection import train_test_split\nx = final.drop([\'CrimeRate\'],axis=1)\ny = final[\'CrimeRate\']\nfrom sklearn import linear_model\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state=42)\nlm = linear_model.LinearRegression()\nmodel=lm.fit(x_train,y_train)\npredictions=lm.predict(x_train)\nplt.figure(figsize=(10, 10))\nplt.scatter(y_train, predictions)\nplt.xlabel(\'Actual Values of Crime Rate\')\nplt.ylabel(\'Predictions of Crime Rate\')\nplt.xlim(0, 300)\nplt.ylim(0, 300)\nplt.show()\n\nfrom sklearn import metrics\nrms=np.sqrt(metrics.mean_squared_error(y_train, predictions))\ncalculating root mean square error\nrms\nOutput\n0.13086185933064995\ncalculating the accuracy of the model\nerrors = abs(predictions - y_train)\nmape = 100 \\* (errors / y_train)\naccuracy = 100 - np.mean(mape)\nprint(\'Accuracy:\', round(accuracy, 2), \'%.\')\nOutput\nAccuracy: 99.87 %.\nx_train[0:5]\n\n\n\n\nViolentCrimeRate\nPropertyCrimeRate\nJuvenileArrestRate\nJuvenileViolentOffenses\nJuvenileDrugOffenses\n911Shootings\nGunHomicides\n911CommonAssaults\n911Narcotics\n911AutoAccidents\nAdultArrestRate\nNeighborhood\nBuildingAddress\nPoliceDistrict\nCouncilDistrict\nLatitude\nLongitude\n\n\n\n\n275\n26.114393\n44.978518\n53.972104\n10.309278\n14.554275\n9.667025\n1.208378\n116.071429\n220.797530\n76.194952\n72.268452\n35\n375\n10\n9\n39.299836\n-76.639660\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\n1711\n30.264939\n66.718236\n71.992976\n14.925373\n15.803336\n6.768517\n0.966931\n109.649971\n175.401276\n74.453684\n69.323144\n39\n2146\n0\n11\n39.296348\n-76.631368\n\n\n\ny_train[0:5]\nOutput\n275           72.435553\n2107          61.644317\n1406          67.045962\n360           72.435553\n1711          98.336879\nName: CrimeRate, dtype: float64\npredictions[0:5].tolist()\nOutput\n[72.4553458293416,\n61.63029975502762,\n67.37324848098874,\n72.40258274909624,\n98.27466457270418]\nimplementing K-Means Clustering\nfrom sklearn.cluster import KMeans\nX = final[[\'CrimeRate\', \'Neighborhood\']]\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform( X )\ncluster_range = range( 1, 10 )\ncluster_errors = []\ncalculating the k value using elbow method\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters )\n  clusters.fit( X_scaled )\n  cluster_errors.append( clusters.inertia_ )\n\nclusters_df = pd.DataFrame( { ""num_clusters"":cluster_range, ""cluster_errors"": cluster_errors } )\n\nclusters_df[0:10]\nplt.figure(figsize=(16,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = ""o"" );\n\nplt.plot()\nx1=final[\'CrimeRate\']\nx2=final[\'Neighborhood\']\nX = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\ncolors = [\'blue\', \'green\', \'red\']\n\nkmeans*model = KMeans(n_clusters=3).fit(X)\ncenters = np.array(kmeans_model.cluster_centers*)\n\nplt.title(\'CrimeRate vs Neighborhood\')\nplt.xlabel(\'CrimeRate\')\nplt.ylabel(\'Neighborhood\')\n\nfor i, l in enumerate(kmeans*model.labels*):\nplt.plot(x1[i], x2[i], color=colors[l], marker= \'o\')\n\nplt.scatter(centers[:,0],centers[:,1], marker=\'\\*\', color=\'black\', s=400)\n\nplt.show()\n\n'], 'url_profile': 'https://github.com/NeeharikaKusampudi', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Classification\nClassification using Machine Learning models like Logistic Regression, KNN, Support Vector Machine, Naive Bayes, Decision Tree and Random Forest\nModels\nLogistic Regression\nfrom sklearn.linear_model import LogisticRegression\nKNN\nfrom sklearn.neighbors import KNeighborsClassifier\nNaive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nSupport Vector Machine\nfrom sklearn.svm import SVC\nDecision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nRandom Forest\nfrom sklearn.ensemble import RandomForestClassifier\nLibraries installation\nscikit Learn\npip install -U scikit-learn\nmatplotlib\npip install matplotlib\nPandas\npip install pandas\nNumpy\npip install numpy\n'], 'url_profile': 'https://github.com/Anish-AV', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""Classifying-Chicago-Car-Accidents\nThis README.md lists the project scenario, goals, methodology, and a summary of the files in the repository.\nProject File Summary\n\nREADME.md - a summary of all contents in this repository.\n/Data - The csv files downloaded for the cityofchicago data portal as well as the cleaned csv files used for modelling\n/DataCleaning - Jupyter Notebook containing all code used for the data cleaning steps\n/EDA - Jupyter Notebook containg all exploratory data anlysis and visualizations\n/FeatEng - Jupyter Notebook containing all code used for feature engineering\n/Modeling - Jupyter Notebook containing all code used for the logistic regression\n/Presentation - A pdf of the powerpoint presentation\n\nProject Scenario:\nIn 2017 the city of Chicago unveiled its Vision Zero plan to eliminate all traffic fatalities and serious injuries by 2026. This came following a jump in the number of deaths and serious injuries from traffic crashes by 8% between 2010-2014. One of the action steps of Vision Zero is the stricter enforcement of traffic laws, focusing on dangerous driving behaviors that cause most severe crashes. In Chicago, any accident that results in a fatality requires a mandatory investigation, but this creates a problem: Dangerous drivers are only investigated after a person is killed. How can dangerous drivers be identfied and taken off the street before someone is killed or seriously injured? To solve this problem the city of chicago has hired a freelance data scientist to create a model that identifies dangerous drivers after all accidents, including minor ones.\nProject Goals:\n\nTo be able to predict, following a car accident, whether or not a dangerous driver was the cause.\nTo fulfill this objective, a classification model will be built that identifies whether a car accident was caused by criminal driving behavior, or non-criminal driving behavior.\n\nMethodology\n\nDownload and clean data from cityofchicago data portal.\nBin primary_contributory_cause column into binary class: 'Criminal Driving' / 'Non-Criminal Driving'.\nFeature engineer additional columns.\nPerform Exploratory Data Analysis (EDA)\nSplit the data into a training set and test set.\nFit a logistic regression model to the training set.\nPerform hyperparameter tuning using RandomSearchCV / GridSearchCV.\nTest the optimized training model on the test data and evaluate score.\nCreate a presentation to translate findings into actionable insights for the Chicago Police Department.\n\n""], 'url_profile': 'https://github.com/Joe-Bit-lab', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'Bielefeld', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Feature-Importance-Calculation\nJuoyter notebook to elaborate a bit on how to calculate feature importance for regression and classification problmes.\n'], 'url_profile': 'https://github.com/RishikMani', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Joshurino', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'Tbilisi , Georgia', 'stats_list': [], 'contributions': '506 contributions\n        in the last year', 'description': ['.\n├── api_test.py\n├── app.py\n├── data\n│\xa0\xa0 └── credit_data.csv\n├── docker-compose.yml\n├── Dockerfile\n├── figures\n│\xa0\xa0 ├── Figure_1.png\n│\xa0\xa0 ├── Figure_cv_2.png\n│\xa0\xa0 └── Figure_split_3.png\n├── README.md\n├── requirements.txt\n├── run.py\n├── utils\n│\xa0\xa0 ├── data.py\n│\xa0\xa0 ├── __init__.py\n│\xa0\xa0 ├── model.py\n│\xa0\xa0 └── plot.py\n└── weights\n    ├── LogReg-e9xrh.pkl\n    └── LogReg-mpi0q.pkl\n\n4 directories, 17 files\n\nusage: run.py [-h] [--load LOAD] [--method {split,cv}]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --load LOAD          True: Load trained model False: Train model default:\n                       False\n  --method {split,cv}  Training methods: cv- Cross-Validation default\n                       10-Foldsplit- default 70 perc train, 30 perc test\n\nTrain the model\nTrain Test Split method\npython run.py --load no --method split\nConfusion Matrix\n\nAccuracy Score:  0.926\nDo you want to save the model weight? yes\nModel saved at weights/LogReg-z1f6y.pkl\n\nCross-Validation method\npython run.py --load no --method cv\nConfusion Matrix\n\nAccuracy Score:  0.948\nDo you want to save the model weight? yes\nModel saved at weights/LogReg-n1u9q.pkl\n\nLoad the trained model\npython run.py --load yes\nPlot data\nFeatures: Income, Age, Loan\nLabels: Default or not\n\nNote: Data is very inbalance\n\nDocker for the model\nYou can build Docker image by following:\ndocker-compose build\nRun Docker container\nYou can launch a container from the Docker image by following:\ndocker-compose up\n'], 'url_profile': 'https://github.com/mysterio42', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'Odisha, India', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Loan-Repayment-Prediction-using-Machine-Learning-and-Python\nAim:-\nOur aim from the project is to make use of numpy, pandas, & seaborn libraries from python to extract insights from the data & scikit-learn libraries for machine learning.\nAnd in the end, to predict whether the loan applicant can repay the loan or not using logistic regression machine learning algorithm.\nAttributes in the dataset:-\nLoan id,\nGender,\nMarried,\nDependents,\nEducation,\nSelf Employed,\nApplicant income,\nCoapplicant income,\nLoan Amount,\nCredit History,\nProperty_Area,\nLoan_Status.\nPrediction Accuracy : 83.76 %\n'], 'url_profile': 'https://github.com/adityatripathy0000', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Loan-Repayment-Prediction-using-Machine-Learning-and-Python\nAim\nOur aim from the project is to make use of numpy, pandas, & seaborn libraries from python to extract insights from the data & scikit-learn libraries for machine learning.\nAnd in the end, to predict whether the loan applicant can repay the loan or not using logistic regression machine learning algorithm.\nAttributes in the dataset\nLoan id,\nGender,\nMarried,\nDependents,\nEducation,\nSelf Employed,\nApplicant income,\nCoapplicant income,\nLoan Amount,\nCredit History,\nProperty_Area,\nLoan_Status\nPrediction accuracy=83.76\n'], 'url_profile': 'https://github.com/DebabrataTripathy', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Telecom-Customer-Churn-Analysis\nWhat is Churn rate?\nChurn rate is the percentage of customers who ceased their relationship with a company\nWhy Churn analysis?\nAcquiring a new customer is anywhere from five to 25 times more expensive than retaining an existing one. So, the goal of this project is to analyse all relevant customer data and develop focused customer retention programs for the Telecom company\nData Overview\nIt is a data with the following information for 3,333 customers:\nChurn = 1 if customer cancelled service, 0 if not\nAccount Weeks = number of weeks the customer has had active account\nContract Renewal = 1 if customer recently renewed contract, 0 if not\nData Plan = 1 if customer has data plan, 0 if not\nData Usage = gigabytes of monthly data usage\nCustomer Service Calls = number of calls into customer service\nDay Mins = average daytime minutes per month\nDay Calls = average number of daytime calls\nMonthly Charge = average monthly bill\nOverage Fee = largest overage fee in last 12 months\nRoam Mins = average number of roaming minutes\nApproach\n•\tDetailed Exploratory Data Analysis report of the dataset along with the missing value treatment\n•\tMulti-collinearity check and summarization of problem statement for business stakeholders\n•\tLogistic Regression Model: creation and interpretation of the results\n•\tComparing the model performances using confusion matrix\n•\tActionable Insights for the business stakeholders\nResults\nWho churned and why?\n•\tChurned customers have used very minimal data\n•\tHigher the Customer Service calls and Daytime calls, higher is the Churn rate. This is needs to be looked in detail.\n•\tCustomers who have renewed their contract do not seem to churn\n•\tAs the Defendant variable is Binary in nature, we can go ahead and try out    Logistic Regression of model building\nBusiness Recommendations\n•\tCustomer Service Support Calls to be addressed &improved. Customer centric measures to be taken to provide FCR(First Call Resolution)\n•\tImproved Training & Transaction calls monitoring to be improved to understand why customer are calling repeatedly\n•\tSelf-help Options can be introduced for customer to limit wait time in service operation calls & addressed through IVR or Customer Portal to reduce Churn\n•\tOverage Fees to be rechecked by Business teams along with market trend & initiate the survey along with Customers of expectations and revision\n•\tProactive Customer Offers to be included to customers with the tenurity and usage pattern of the services\n•\tNon-Data plans customers should be clustered and attractive data plans to be introduced with combo-plans for them to start off\n•\tCustomer using reduced data plans again to be targeted by rationalising the Data and call plans\n•\tTo Summarize attracting calling plans & improved calling(reception) Improved Customer centric measures like Quick resolution, Improved CSAT would make the churn rate under control\n'], 'url_profile': 'https://github.com/Tanukrishna', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AminaNaaz', 'info_list': ['Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Python', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2019', 'Python', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Python', 'MIT license', 'Updated Apr 16, 2020', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['rts\nGeneral definition for safe Regression Test Selection (RTS) algorithms, instantiated with an algorithm over JVM using the JinjaDCI JVM semantics\nThis repository gives a general definition for safe RTS algorithms and CollectionSemantics (the combination of a semantics with a collection function), including instantiations with class-collection-based RTS algorithms running over JVM.\nThis repository relies on that found at https://github.com/susannahej/jinja-dci, whose contents are expected to be in a folder named JinjaDCI (instead of jinja-dci) placed in the same folder as one containing this repository. Files in this repository are compatible with at least Isabelle2019  and Isabelle2020 (the current version as of this update).\nTo run this development, a setup accessing the related (non-Jinja) AFP entries is required. See https://www.isa-afp.org/using.html for directions on how to install the AFP.\nCurrent AFP dependencies:\n(only those required by JinjaDCI)\nQuestions related to installing this repository can be directed to sjohnsn2 (at) illinois (dot) edu.\n'], 'url_profile': 'https://github.com/susannahej', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'Dortmund', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['CoxBVS-SL\nCox model with Bayesian Variable Selection and Structure Learning (CoxBVS-SL)\nR source code and data associated with the publication Madjar K, Zucknick M, Ickstadt K, and Rahnenführer J (2020): Combining heterogeneous subgroups with graph-structured variable selection priors for Cox regression. arXiv: 2004.07542.\nOur method is focused on the situation of predefined, possibly heterogenous subgroups of patients with available survival endpoint and continuous molecular measurements such as gene expression\ndata, with the aim of obtaining a separate risk prediction model for each subgroup.\nFor this purpose, we propose a Cox regression model with a graph-structured variable selection prior that incorporates information on the relationships among the covariates and encourages the joint selection of linked variables.\nThese links correspond to variables either being conditionally dependent within each subgroup (e.g. functional or regulatory pathways) or being simultaneously prognostic across different subgroups.\nWe compare our approach to Cox regression models with an independent Bernoulli prior for variable selection as proposed by Treppmann et al. (2017).\nWe evaluate all models through simulations and a case study with Glioblastoma protein expression data.\nThe main file to run the simulation study is Run_Simulation.R and the main file for the case study is Run_CaseStudy.R.\nOverview of R files:\nRun_Simulation.R\nSetup and running of the simulations.\nAll simulations are run using the R package batchtools for parallelization.\nRun_CaseStudy.R\nSetup and running of the case study.\nAll settings are run using the R package batchtools for parallelization.\nData_Simulation.R\nHelper functions for generation of simulated training and test data sets, including simulated gene expression data and survival outcome.\nOnly needed in the simulation study, not in the case study.\nWrapperSimCoxBVSSL.R\nWrapper function for the inference of a specific type of Cox model with MCMC sampling based on simulated data.\nFirst, the training data is prepared (standarize covariates for Pooled model), then the hyperparameters for all priors are defined and starting values\nfor the MCMC sampler are set.\nWrapperCaseCoxBVSSL.R\nAnalogous to WrapperSimCoxBVSSL.R but for case study data, wrapper function for the inference of a specific type of Cox model with MCMC sampling.\nfunc_MCMC.R\nMain function for performing MCMC sampling of all parameters and storage of posterior results.\nfunc_MCMC_cox.R\nHelper functions for MCMC sampling of Cox model parameters and joint posterior distribution.\nfunc_MCMC_graph.R\nHelper function for MCMC sampling of graph and precision matrix (structure learning, adapted code from Wang (2015).\nWeibull_param.RData\nRData object with parameters of the Weibull distribution used for data simulation (see file ""Data_Simulation.R"")\nGBM_Data.RData\nRData object with preprocessed Glioblastoma protein expression data for case study, including original expression data of 20 selected proteins,\nsimulated survival endpoint, subgroup membership for each patient.\nStart with file Run_Simulation or Run_CaseStudy.R.\n'], 'url_profile': 'https://github.com/KatrinMadjar', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'Delhi NCR, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['#House_Price_Prediction\nMultiple Linear Regression is used to predict the price of the houses based on different 14 independent variables.\nThis repository is directly pushed from pycharm (Python IDE), It use flask to deploy it.\nThis repo consists of:\n1)Flask(Web framework) in:app.py\n2)Dataset: House_modified.csv\n3)Machine learning model : house_price_prediction.py\n4)Template:index.html\n4)CSS & fonts in : static folder.\nThese 4 directories will help you to create and deploy a finished machine learning model onto the website.\n'], 'url_profile': 'https://github.com/viraatsworld', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'Katowice', 'stats_list': [], 'contributions': '539 contributions\n        in the last year', 'description': ['BIGDATA-ANALYSIS\n1st project\nPreprocessing program\nProgram reads points (x,y,z,i) from the .txt file and put them into sorted list (sorted by X,Y). Then program saves points structure as binary file.\nRun the program with parameters:\ninput_file   - input file with raw ASCII points data \npreprocessed_file  - output file where points are preprocessed\nHistogram program\nProgram read file chunk by chunk. Chunk size is definied by user as M parameter. Then program adds points that exsists in bounding box definied in parameters by user. Program uses binary search to find first point from bounding box inside chunk. By Histogram class program calculates every needed parameter to display statistics.\nRun the program with parameters:\nM                - maximum size of memory usage \nminX              - minimum X value of points of interest\nmaxX              - maximum X value of points of interest\nminY              - minimum Y value of points of interest\nmaxY              - maximum Y value of points of interest\nbin_size          - size of histogram bin used in statistics calculation\nselection         - enable switching of which point parameter should be used for statistics summary, either intensity (i) or point height (z).\nInput file format\n394372.82 39157.52 217.57 61 \n394372.82 39165.22 218.13 39\n394372.82 39186.13 221.59 46\nExample\nRun with parameters:\noutput 1 394364 394374 39150 39160 5 i\nOutput\nNumber of points inside given bounding box: 1031\nCalculated average: 37,3346\nCalculated deviation: 19,7108\nCalculated skewness: 1,22782\nCalculated kurtosis: 4,83795\nNumber of bins: 26\nNumber of data reads from the input file: 37\n2nd project\nRegression program\nProgram creates linear regression for:\n\nLinear function - single variable\nGeneral linear function - multiple variables\nPolynomialfunction\n\nProgram detects if points inside .txt file belongs to polynomial function or general linear function with multiple variables. Program also calculates degree of the function if the function is polynomial.\nRun the program\nYou can run this console application without any parameters.\nInput file format (1st format)\nX Y\n10.6888 49264.7\n12.3991 39068.8\n33.5362 -1.25108e+006\nInput file format (2nd format)\nX1 X2 X3 X4 Y\n391.675 776.33 263.619 891.171 13950.1\n200.781 813.898 492.538 970.763 15888.7\n289.987 681.57 126.347 431.013 10149.2\nOutput example for linear function (single variable)\nc: 7,1594446690634115\nb1 : 3,1000057728440162\nOutput example for general linear function (4 variables)\nc: 1,0441290229898121 \nb1 : 0,10002800516499116\nb2 : 11,100030011339015\nb3 : 5,200130397807429\nb4 : 4,400084763095325\nOutput example for polynomial function (degree: 4)\nc: 61005,01752105124\nb1 : 5,609798431396484\nb2 : -0,031225020997226238\nb3 : 2,096800643252209\nb4 : -1,0999521179519434\nFINAL ESSAY\nTopic: Data Visualization\nAuthor: Mateusz Piwowarski\nREAD ESSAY\n'], 'url_profile': 'https://github.com/matpiwowarski', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ashish-Arya-CS', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'West Lafayette, IN', 'stats_list': [], 'contributions': '518 contributions\n        in the last year', 'description': [""Here, I have tried to implement linear regression using Normal Equation, ie. using matrix forms.\nMethod to solve for theta analytically:\ntheta = inv(X'X).(X'Y)                , where...\n\ntheta is coefficient vector\nY is dependent variable vector\nX is matrix containing independent variables, with an additional row of 1's for intercept\n\nComparison with Gradient Descent:\n\nno need to choose alpha -> learning rate\nno need to iterate\nno need to normalize the data\nslow if n is too large (~10000)\n\nWhat if (X'X) is non-invertible?\n\nRedundant features: some are linearly dependent on each other\nToo many features: (m < n)\n\ndelete some features\nuse regularization\n\n\n\n""], 'url_profile': 'https://github.com/akshay-madar', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '1,128 contributions\n        in the last year', 'description': ['IRIS Flower Data Analysis\nTask: Flower Classification\nWe are tasked to produce a machine learning model that classifies flowers. We have been given a dataset with 4 variables that are used to classify the flowers, but for graphical representation of the predictive regions it is ideal to produce a model with 2 variables. We will therefore be using ""Dimensionality Reduction"", alongside ""K-Fold Cross Validation"" and ""Grid Search"" for model evaluation and parameter tuning in order to find the optimal model.\nWe will be classifying the flowers using Logistic Regression, K-Nearest Neighbours, Support Vector Machine, Naive Bayes, Random Forest and XGBoost to find the optimal model.\nAdditional Reading\nClassification Models: https://github.com/MohitGoel92/Predicting-Customer-Purchases\nDimensionality Reduction, K-Fold Cross Validation and Grid Search: https://github.com/MohitGoel92/Wine-Classification-Analysis\nReferences\nDataset: https://archive.ics.uci.edu/ml/datasets/Iris\n'], 'url_profile': 'https://github.com/MohitGoel92', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Analyzing product sentiment\nAnalyzing product sentiment by Logistic regression and using techniques : word_tokenize, CountVectorizer. Using ROC curve to evaluate the model\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Mazda_Bayesian_LR\nAnalyzing the relationship of the age of Mazda cars against price using a Bayesian linear regression model\n'], 'url_profile': 'https://github.com/derpnallday', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['AICropPricePredictor\nPredicts the price of crops using Decision Tree Regressor.\n'], 'url_profile': 'https://github.com/shwethar99', 'info_list': ['Isabelle', 'GPL-3.0 license', 'Updated Oct 29, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 18, 2020', 'C#', 'Updated Jul 9, 2020', 'MATLAB', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 15, 2020', 'HTML', 'Updated Apr 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nCovariance and Correlation\nWe start the section by covering covariance and correlation, both of which relate to how likely two variables are to change together. For example, with houses, it wouldn\'t be too surprising if the number of rooms and the price of a house was correlated (in general, more rooms == more expensive).\nStatistical Learning Theory\nWe then explore statistical learning theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nBoston Housing dataset - again!\nThe dataset is available in the file \'boston.csv\'.\n\nImport the dataset and print its .head() and dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Boston housing dataset \ndata = None \n\n# Print the first five rows \n\n\n# Print the dimensions of data\nIdentify features and target data\nIn this lab, we will use three features from the Boston housing dataset: \'RM\', \'LSTAT\', and \'PTRATIO\':\nFeatures\n\n\'RM\' is the average number of rooms among homes in the neighborhood\n\'LSTAT\' is the percentage of homeowners in the neighborhood considered ""lower class"" (working poor)\n\'PTRATIO\' is the ratio of students to teachers in primary and secondary schools in the neighborhood\n\nTarget\n\n\nMEDV\',the median value of the home\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents for validity\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return the r-squared score and MSE for two equal-sized arrays for the given true and predicted values\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. """"""\n    \n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n    \n    \n    # Calculate the mean squared error between \'y_true\' and \'y_predict\'\n    \n    \n    # Return the score\n    \n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.4719999999999998]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.47097115950374013, 38.795686274509805]  - R2, MSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate MSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max-features to find a more optimal version of the model\n\nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Boston Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': [""Research_on_university_choice-HK-SH-\nLogistic regression on the data_set.xlsx file， main regresson is in the Logistic REG file.\ninput.xlsx(catagorical data) output.xlsx(dummy variable) are used by dummy_generator.py\nIt uses catagorical data to generate dummy variables, and output it to a new xlsx file\nPlot.ipynb is used to visualise some of the data\nFactors Influencing Mainland Chinese Student's Choice of University.pdf\nis our research paper.\nMore plots(made by pyplot) are in the paper.\nClassOfCity.py is used to converted name of the cites students lived in to the class of the city, according to China's tier system of cities in the file  'NicerFormat.txt', which originated from '线.txt'.\nI did all the coding and visualition\n""], 'url_profile': 'https://github.com/tonytian98', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'HTML', 'Updated Apr 13, 2020', 'HTML', 'Updated Apr 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'GURGAON ', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': [""Predicting-House-Prices-with-Regression-using-TensorFlow\nIt's a guided project using tensorflow to predict the prices of house using feilds  :\n\nYear of sale of the house\nThe age of the house at the time of sale\nDistance from city center\nNumber of stores in the locality\nThe latitude\nThe longitude\n\nThen for the model we used keras Sequential Model i.e. which is a linear stack of Layers.\nThen we visualize it for trained and untrained model it will follow the straight linear path for trained model so it shows that our predictions are accurate.\n""], 'url_profile': 'https://github.com/Hritikkounsal', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets. Key takeaways include:\n\nThe Pearson Correlation (range: -1 -> 1) is a standard way to describe the correlation between two variables\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nIntroduce Statsmodels for multiple regression\nPresent alternatives for running regression in Scikit Learn\n\nStatsmodels for multiple linear regression\nThis lecture will be more of a code-along, where we will walk through a multiple linear regression model using both Statsmodels and Scikit-Learn.\nRemember that we introduced single linear regression before, which is known as ordinary least squares. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(""auto-mpg.csv"") \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[""acceleration""]\nlogdisp = np.log(data[""displacement""])\nloghorse = np.log(data[""horsepower""])\nlogweight= np.log(data[""weight""])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[""acc""]= scaled_acc\ndata_fin[""disp""]= scaled_disp\ndata_fin[""horse""] = scaled_horse\ndata_fin[""weight""] = scaled_weight\ncyl_dummies = pd.get_dummies(data[""cylinders""], prefix=""cyl"")\nyr_dummies = pd.get_dummies(data[""model year""], prefix=""yr"")\norig_dummies = pd.get_dummies(data[""origin""], prefix=""orig"")\nmpg = data[""mpg""]\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 26 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_3     392 non-null uint8\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_70     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_1    392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(21)\nmemory usage: 23.4 KB\n\nThis was the data we had until now. As we want to focus on model interpretation and still don\'t want to have a massive model for now, let\'s only inlude ""acc"", ""horse"" and the three ""orig"" categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis= 1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_1\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n1\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n1\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n1\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n1\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n1\n0\n0\n\n\n\n\nA linear model using Statsmodels\nNow, let\'s use the statsmodels.api to run our ols on all our data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$, where, with $n$ predictors, X is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = ""mpg ~ acceleration+weight+orig_1+orig_2+orig_3""\nmodel = ols(formula= formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable ""mpg"" out of your data frame, and use the a ""+"".join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nOr even easier, simply use the .OLS-method from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors dataframe so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nInterpretation\nJust like for single multiple regression, the coefficients for our model should be interpreted as ""how does Y change for each additional unit X""? Do note that the fact that we transformed X, interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed X, the actual relationship is ""how does Y change for each additional unit X\'"", where X\' is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit learn\nYou can also repeat this process using Scikit-Learn. The code to do this can be found below. The Scikit-learn is generally known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit learn compared to Statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of Scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -0.71140721, -0.29903267,  1.01043987])\n\nThe intercept of the model is stored in the .intercept_-attribute.\n# intercept\nlinreg.intercept_\n21.472164286075383\n\nWhy are the coefficients different in scikit learn vs Statsmodels?\nYou might have noticed that running our regression in Scikit-learn and Statsmodels returned (partially) different parameter estimates. Let\'s put them side to side:\n\n\n\n\nStatsmodels\nScikit-learn\n\n\n\n\nintercept\n16.1041\n21.4722\n\n\nacceleration\n5.0494\n5.0494\n\n\nweight\n-5.8764\n-5.8764\n\n\norig_1\n4.6566\n-0.7114\n\n\norig_2\n5.0690\n-0.2990\n\n\norig_3\n6.3785\n1.0104\n\n\n\nThese models return equivalent results!\nWe\'ll use an example to illustrate this. Remember that minmax-scaling was used on acceleration, and standardization on log(weight).\nLet\'s assume a particular observation with a value of 0.5 for both acceleration and weight after transformation, and let\'s assume that the origin of the car = orig_3. The predicted value for mpg for this particular value will then be equal to:\n\n16.1041 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 6.3785 = 22.0691 according to the Statsmodels\n21.4722 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 1.0104 = 22.0691 according to the Scikit-learn model\n\nThe eventual result is the same. The extimates for the categorical variables are the same ""up to a constant"", the difference between the categorical variables, in this case 5.3681, is added in the intercept!\nYou can make sure to get the same result in both Statsmodels and Scikit-learn, by dropping out one of the orig_-levels. This way, you\'re essentially forcing the coefficient of this level to be equal to zero, and the intercepts and the other coefficients will be the same.\nThis is how you do it in Scikit-learn:\npredictors = predictors.drop(""orig_3"",axis=1)\nlinreg.fit(predictors, y)\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -1.72184708, -1.30947254])\n\nlinreg.intercept_\n22.482604160455665\n\nAnd Statsmodels:\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    22.4826     0.789    28.504  0.000    20.932    24.033\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1    -1.7218     0.653    -2.638  0.009    -3.005    -0.438\n\n\norig_2    -1.3095     0.688    -1.903  0.058    -2.662     0.043\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               9.59\n\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in both Scikit-Learn and Statsmodels. Before we discuss the model metrics in detail, let\'s go ahead and try out this model on the Boston Housing Data Set!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Boston Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Boston Housing Data again!\nThis time, let's only include the variables that were previously selected using recursive feature elimination. We included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nb = boston_features['B']\nlogdis = np.log(boston_features['DIS'])\nloglstat = np.log(boston_features['LSTAT'])\n\n# Min-Max scaling\nboston_features['B'] = (b-min(b))/(max(b)-min(b))\nboston_features['DIS'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features['LSTAT'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nX = boston_features[['CHAS', 'RM', 'DIS', 'B', 'LSTAT']]\ny = pd.DataFrame(boston.target, columns = ['target'])\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Importing and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n\n<matplotlib.legend.Legend at 0x1a24d6cef0>\n\n\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 100 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n\n<matplotlib.legend.Legend at 0x1a26e93438>\n\n\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jvstyna', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tirth8128', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Boston Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Boston Housing Data\nWe pre-processed the Boston Housing data again. This time, however, we did things slightly different:\n\nWe dropped 'ZN' and 'NOX' completely\nWe categorized 'RAD' in 3 bins and 'TAX' in 4 bins\nWe transformed 'RAD' and 'TAX' to dummy variables and dropped the first variable to eliminate multicollinearity\nWe used min-max-scaling on 'B', 'CRIM', and 'DIS' (and log transformed all of them first, except 'B')\nWe used standardization on 'AGE', 'INDUS', 'LSTAT', and 'PTRATIO' (and log transformed all of them first, except for 'AGE')\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nboston_features = boston_features.drop(['NOX', 'ZN'],axis=1)\n\n# First, create bins for based on the values observed. 3 values will result in 2 bins\nbins = [0, 6, 24]\nbins_rad = pd.cut(boston_features['RAD'], bins)\nbins_rad = bins_rad.cat.as_unordered()\n\n# First, create bins for based on the values observed. 4 values will result in 3 bins\nbins = [0, 270, 360, 712]\nbins_tax = pd.cut(boston_features['TAX'], bins)\nbins_tax = bins_tax.cat.as_unordered()\n\ntax_dummy = pd.get_dummies(bins_tax, prefix='TAX', drop_first=True)\nrad_dummy = pd.get_dummies(bins_rad, prefix='RAD', drop_first=True)\nboston_features = boston_features.drop(['RAD','TAX'], axis=1)\nboston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\nage = boston_features['AGE']\nb = boston_features['B']\nlogcrim = np.log(boston_features['CRIM'])\nlogdis = np.log(boston_features['DIS'])\nlogindus = np.log(boston_features['INDUS'])\nloglstat = np.log(boston_features['LSTAT'])\nlogptratio = np.log(boston_features['PTRATIO'])\n\n# Min-Max scaling\nboston_features['B'] = (b-min(b))/(max(b)-min(b))\nboston_features['CRIM'] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\nboston_features['DIS'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features['AGE'] = (age-np.mean(age))/np.sqrt(np.var(age))\nboston_features['INDUS'] = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\nboston_features['LSTAT'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nboston_features['PTRATIO'] = (logptratio-np.mean(logptratio))/(np.sqrt(np.var(logptratio)))\nboston_features.head()\nRun a linear model in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nInterpret the coefficients for PTRATIO, PTRATIO, LSTAT\n\nCRIM: per capita crime rate by town\nINDUS: proportion of non-retail business acres per town\nCHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\nRM: average number of rooms per dwelling\nAGE: proportion of owner-occupied units built prior to 1940\nDIS: weighted distances to five Boston employment centres\nRAD: index of accessibility to radial highways\nTAX: full-value property-tax rate per $10,000\nPTRATIO: pupil-teacher ratio by town\nB: 1000(Bk - 0.63)^2 where Bk is the proportion of African American individuals by town\nLSTAT: % lower status of the population\n\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nCRIM: 0.15\nINDUS: 6.07\nCHAS: 1\nRM:  6.1\nAGE: 33.2\nDIS: 7.6\nPTRATIO: 17\nB: 383\nLSTAT: 10.87\nRAD: 8\nTAX: 284\n\nSummary\nCongratulations! You pre-processed the Boston Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Boston Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}"
"{'location': 'IIT Kharagpur', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['Covid-19-Research\nThe repository consist of our codes and relevant datasets for developing markov chain models to predict the Covid 19 conditions in the world.\n'], 'url_profile': 'https://github.com/prerit1998jain', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tirth8128', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 30, 2020', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Boston Housing Data once more\nWe pre-processed the Boston Housing data the same way we did before:\n\nWe dropped \'ZN\' and \'NOX\' completely\nWe categorized \'RAD\' in 3 bins and \'TAX\' in 4 bins\nWe transformed \'RAD\' and \'TAX\' to dummy variables and dropped the first variable\nWe used min-max-scaling on \'B\', \'CRIM\', and \'DIS\' (and logtransformed all of them first, except \'B\')\nWe used standardization on \'AGE\', \'INDUS\', \'LSTAT\', and \'PTRATIO\' (and logtransformed all of them first, except for \'AGE\')\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\nboston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\nboston_features = boston_features.drop([\'NOX\', \'ZN\'],axis=1)\n\n# First, create bins for based on the values observed. 3 values will result in 2 bins\nbins = [0,6,  24]\nbins_rad = pd.cut(boston_features[\'RAD\'], bins)\nbins_rad = bins_rad.cat.as_unordered()\n\n# First, create bins for based on the values observed. 4 values will result in 3 bins\nbins = [0, 270, 360, 712]\nbins_tax = pd.cut(boston_features[\'TAX\'], bins)\nbins_tax = bins_tax.cat.as_unordered()\n\ntax_dummy = pd.get_dummies(bins_tax, prefix=\'TAX\', drop_first=True)\nrad_dummy = pd.get_dummies(bins_rad, prefix=\'RAD\', drop_first=True)\nboston_features = boston_features.drop([\'RAD\', \'TAX\'], axis=1)\nboston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\n\nage = boston_features[\'AGE\']\nb = boston_features[\'B\']\nlogcrim = np.log(boston_features[\'CRIM\'])\nlogdis = np.log(boston_features[\'DIS\'])\nlogindus = np.log(boston_features[\'INDUS\'])\nloglstat = np.log(boston_features[\'LSTAT\'])\nlogptratio = np.log(boston_features[\'PTRATIO\'])\n\n# Min-Max scaling\nboston_features[\'B\'] = (b-min(b))/(max(b)-min(b))\nboston_features[\'CRIM\'] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\nboston_features[\'DIS\'] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n\n# Standardization\nboston_features[\'AGE\'] = (age-np.mean(age))/np.sqrt(np.var(age))\nboston_features[\'INDUS\'] = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\nboston_features[\'LSTAT\'] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\nboston_features[\'PTRATIO\'] = (logptratio-np.mean(logptratio))/(np.sqrt(np.var(logptratio)))\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this function provided on your preprocessed Boston Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nThe stepwise procedure mentions that \'INDUS\' was added with a p-value of 0.0017767, but our statsmodels output returns a p-value of 0.000. Use some of the stepwise procedure logic to find the intuition behind this!\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.742981  \n# adjusted_r_squared is 0.740411\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Boston Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models. Moreover, you also got a brief introduction to data ethics. Remember that throughout your data work it is essential to consider personal privacy and the potential impacts of the data you have access to.\nRegression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preperation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\nEthics\nAside from regression, you also took a look at data privacy and ethics. You probably had already heard some of these ideas, but may have not been familiar with GDPR or privacy advocacy groups like the Electornic Frontier Foundation. The digital age has brought a slew of political and philosophical questions to the arena, and there are always fascinating (and disturbing) conversations to be had. Be sure to keep these and other issues at the forefront of your thought process, and not simply be dazzled by the power of machine learning algorithms. Ask yourself questions like, ""What is the algorithm being used for?"" or ""What are the ramifications or impact of this analysis/program/algorithm?"".\nWhen Einstein released his theory of relativity, its impact had tremendous benefit in advancing the field of physics yet the subsequent development of the Manhattan project was arguably a great detriment of humanity. To a similar vain, be thoughtful of which planes of thought you are operating on, and always be sure to include an ethical and philosophical perspective of the potential ramifications of your work.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': [""Identifying-potential-customers-for-loans\nIdentified potential loan customers for Thera Bank using classification techniques. Compared models built with Logistic Regression and KNN algorithm in order to select the best performing one.\nData Description:\nThe file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.\nDomain:\nBanking\nContext:\nThis case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing  department to devise campaigns with better target marketing to increase the success ratio with minimal budget.\nAttribute Information:\n\nID : Customer ID\nAge : Customer's age in completed years\nExperience : #years of professional experience\nIncome : Annual income of the customer ($000)\nZIP Code : Home Address ZIP code.\nFamily : Family size of the customer\nCCAvg : Avg. spending on credit cards per month ($000)\nEducation : Education Level. 1: Undergrad; 2: Graduate; 3: Advanced/Professional\nMortgage : Value of house mortgage if any.\nPersonal Loan : Did this customer accept the personal loan offered in the last campaign?\nSecurities Account : Does the customer have a securities account with the bank?\nCD Account : Does the customer have a certificate of deposit (CD) account with the bank?\nOnline : Does the customer use internet banking facilities?\nCredit card : Does the customer use a credit card issued by UniversalBank?\n\nLearning Outcomes:\n\nExploratory Data Analysis\nPreparing the data to train a model\nTraining and making predictions using a classification model\nModel evaluation\n\nObjective:\nThe classification goal is to predict the likelihood of a liability customer buying personal loans.\nSteps :\n\nRead the column description and understand each attribute well\nStudy the data distribution in each attribute\nGet the target column distribution\nSplit the data into training and test set in the ratio of 70:30 respectively\nUse different classification models (Logistic, K-NN and Naïve Bayes) to predict the likelihood of a customer buying personal loans.\nConfusion matrix for all the above models\nReason on which is the best model in this case and why it performs better?\n\n""], 'url_profile': 'https://github.com/CHYaswanth', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['conquer\nConvolution-type smoothed quantile regression\nDescription\nThe conquer library performs fast and accurate convolution-type smoothed quantile regression (Fernandes, Guerre and Horta, 2019) implemented via Barzilai-Borwein gradient descent (Barzilai and Borwein, 1988) with a Huber regression warm start. The package can also construct confidence intervals for regression coefficients using multiplier bootstrap.\nInstallation\nconquer is available on CRAN, and it can be installed into R environment:\ninstall.packages(""conquer"")\nCommon errors or warnings\nA collection of error / warning messages we received from issues or e-mails and their solutions:\n\n\nError: smqr.cpp: \'quantile\' is not a member of \'arma’. Solution: \'quantile\' function is added into RcppArmadillo version 0.9.850.1.0 (2020-02-09), so reinstalling / updating the library RcppArmadillo will fix this issue.\n\n\nError: unable to load shared object.. Symbol not found: _EXTPTR_PTR. Solution: This issue is common in some specific versions of R when we load any Rcpp-based libraries. It is an error in R caused by a minor change about EXTPTR_PTR. Upgrading R to 4.0.2 will solve the problem.\n\n\nExamples\nLet us illustrate conquer by a simple example. For sample size n = 5000 and dimension p = 70, we generate data from a linear model yi = β0 + <xi, β> + εi, for i = 1, 2, ... n. Here we set β0 = 1, β is a p-dimensional vector with every entry being 1, xi follows p-dimensional standard multivariate normal distribution (available in the library MASS), and εi is from t2 distribution.\nlibrary(MASS)\nlibrary(quantreg)\nlibrary(conquer)\nn = 5000\np = 70\nbeta = rep(1, p + 1)\nset.seed(2020)\nX = mvrnorm(n, rep(0, p), diag(p))\nerr = rt(n, 2)\nY = cbind(1, X) %*% beta + err\nThen we run both quantile regression using package quantreg, with a Frisch-Newton approach after preprocessing (Portnoy and Koenker, 1997), and conquer (with Gaussian kernel) on the generated data. The quantile level τ is fixed to be 0.5.\ntau = 0.5\nstart = Sys.time()\nfit.qr = rq(Y ~ X, tau = tau, method = ""pfn"")\nend = Sys.time()\ntime.qr = as.numeric(difftime(end, start, units = ""secs""))\nest.qr = norm(as.numeric(fit.qr$coefficients) - beta, ""2"")\n\nstart = Sys.time()\nfit.conquer = conquer(X, Y, tau = tau)\nend = Sys.time()\ntime.conquer = as.numeric(difftime(end, start, units = ""secs""))\nest.conquer = norm(fit.conquer$coeff - beta, ""2"")\nIt takes 0.1955 seconds to run the standard quantile regression but only 0.0255 seconds to run conquer. In the meanwhile, the estimation error is 0.1799 for quantile regression and 0.1685 for conquer. For readers’ reference, these runtimes are recorded on a Macbook Pro with 2.3 GHz 8-Core Intel Core i9 processor, and 16 GB 2667 MHz DDR4 memory.\nGetting help\nHelp on the functions can be accessed by typing ?, followed by function name at the R command prompt.\nFor example, ?conquer will present a detailed documentation with inputs, outputs and examples of the function conquer.\nLicense\nGPL-3.0\nSystem requirements\nC++11\nAuthors\nXuming He xmhe@umich.edu, Xiaoou Pan xip024@ucsd.edu, Kean Ming Tan keanming@umich.edu and Wen-Xin Zhou wez243@ucsd.edu\nMaintainer\nXiaoou Pan xip024@ucsd.edu\nReferences\nBarzilai, J. and Borwein, J. M. (1988). Two-point step size gradient methods. IMA J. Numer. Anal. 8 141–148. Paper\nFernandes, M., Guerre, E. and Horta, E. (2019). Smoothing quantile regressions. J. Bus. Econ. Statist., in press. Paper\nHe, X., Pan, X., Tan, K. M., and Zhou, W.-X. (2020). Smoothed quantile regression with large-scale inference. Preprint.\nHorowitz, J. L. (1998). Bootstrap methods for median regression models. Econometrica 66 1327–1351. Paper\nKoenker, R. (2005). Quantile Regression. Cambridge Univ. Press, Cambridge. Book\nKoenker, R. (2019). Package ""quantreg"", version 5.54. CRAN\nKoenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica 46 33-50. Paper\nPortnoy, S. and Koenker, R. (1997). The Gaussian hare and the Laplacian tortoise: Computability of squared-error versus absolute-error estimators. Statist. Sci. 12 279–300. Paper\nSanderson, C. and Curtin, R. (2016). Armadillo: A template-based C++ library for linear algebra. J. Open Source Softw. 1 26. Paper\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['ML-Prediction-using-VueJS-Flask\nA web application that predicts a target variable from a dataset uploaded by the user and also selecting the algorithms he wishes to use (eg. regression)\n'], 'url_profile': 'https://github.com/benitarego', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '924 contributions\n        in the last year', 'description': ['hate_speech_classifier\nDataset Link: https://github.com/aitor-garcia-p/hate-speech-dataset\nGoal: Classify hate speech dataset from white supremacist forum\n\nGenerate dataset insights (ex. Class Distribution, Most Common Words, Highest TF-IDF Weightings, etc.)\nClean Data (ex. Standardize numbers/dates/links, perform stemming)\nBenchmark various models, and compare them to the Majority Class\n\nResults/Observations:\n\nLogistic regression appears to perform better than the svm\nIn general, the incorrectly classified posts don\'t have any of the preprocessing symbols (except number), which suggests the preprocessing is helping\n\nOften, the language used in these are less specific (ex. dark skinned beings)\nIn general this stuff is still subjective: Test index 157: Guess: 0.0 Act: 1 Post: ""it s just the way they are .""\nSnowball stemmer helped a fair bit (makes sense since theres not much data, and some of the words could be hidden in their plural forms)\n\nLogistic Regression Results (It changes depending on what split you get, but the performance improvement is often there):\n\nWith Stemmer: 0.7594142259414226\nWith stemmer but removing stopwords: 0.7196652719665272\nNo Stemmer: 0.7238493723849372\n\n\n\n\nNLTK lemmatizer didnt work as well... probably because it doesnt do that well with handling unknown words, and its results depend on context\n\n\n\n'], 'url_profile': 'https://github.com/Sumobot1', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Bank-Data\nLogistic regression model that predicts if a client will subscribe a term deposit based on the marketing campaign efforts of a Portuguese banking institution, using Pandas, Numpy, Statsmodels, Matplotlib, Seaborn, and SciPy.\n'], 'url_profile': 'https://github.com/sujaysdesai', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/priyankmodiPM', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Boston, USA', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Lending_Club_Interest_Rate_Prediction\nAbout Lending Club:\nLending Club is a US peer-to-peer lending company. The company claims that $15.98 billion in loans had been originated through its platform up to December 31, 2015.\nLending Club enables borrowers to create unsecured personal loans between $1,000 and $40,000.\nThe standard loan period is three years. Investors can search and browse the loan listings on Lending Club website and select loans that they want to invest in based on the information supplied about the borrower, amount of loan, loan grade, and loan purpose.\nInvestors make money from interest. Lending Club makes money by charging borrowers an origination fee and investors a service fee\nInstructions:\n\nThe Notebook folder has three files: The Jupyter notebook namely ""Exploratory Data Analysis.ipynb"", ""Data Cleaning and Feature Engineering.ipynb"" and ""Data Model-Regression.ipynb""\nDownload the folder and run the .ipynb files using Jupyter Notebook\nThe source file is too large, you can access it via below link:\nhttps://lending-club-loan-dataset.s3.amazonaws.com/dataset/loan.csv\nThe LC Dictinary file gives a detailed explanations of the dataset columns\n\n'], 'url_profile': 'https://github.com/nileshnerkar', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Apr 20, 2020', '1', 'C++', 'Updated Aug 28, 2020', '1', 'Vue', 'MIT license', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'R', 'Updated Jan 29, 2021', 'HTML', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}"
"{'location': 'Rajkot', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/d2Kapuriya', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'Visakhapatnam,AndhraPradesh,India', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['MachineLearnigthruPython\n1.LinearRegression:\na sample example of linear regression which can predict health insurance by giving age as an input.\n2.ImageProcessing:\na sample example of image processing :face identification from background and chessboard creation.\n'], 'url_profile': 'https://github.com/kiransaiabhishek', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['AdvertisingBecomesSuccess\nIn this project we predict whether a person will click on a particular advertisement or not using Logistic Regression on the existing dataset\n'], 'url_profile': 'https://github.com/mauwazahmed', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/andrzejmalota', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/malharb', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '233 contributions\n        in the last year', 'description': [""Always Be Closing\nGraduate Course: Quantitative Analysis for Business\nThis was an individual project from a course on Linear Regression (a supervised machine learning technique for quantitative analysis). Below is the prompt from the client:\n\nMy name is Jim Armilay, and I am a real estate broker in the state of Virginia. I have been moderately successful in the past five years selling large high-end homes, but recently more rival brokers have been moving into this market. I think one way I can gain an advantage in this market is if I start to use data to study the home prices, but I'm not sure how to start. I recorded some of my recent sales with the Price in dollars and Square feet (Sqft) for each of the sales. The data is included in the separate attachment.\nI know that bigger properties sell for more, but I'm not sure how much it affects the price. Would you be able to help me to explore the data so that I can better understand what the data means; what it looks like. I also would like to better understand how the house prices are related to the square feet of the properties, in a general sense. Ultimately, I would like to use this data to help me make better decisions. For example, a client came to me trying to sell their 1500 square foot house for $300,000, which I think is a low price. Can this data help me to convince my client that the house should be sold for more, so that I can get a bigger commission? I am also talking to another client who is considering buying this massive 3750 sqft house! This client wants to know if the house price listed, $825,000, is reasonable among houses that size; can the model help me decide if the property is a good value?\n\nAnalysis Summary\nI did some numerical and visual exploration to know how the data looks like and what does it mean in general sense. I found that the predictor (Sqft) has a minimum and maximum value of 850 and 3262 with the average value of 1876.56 (Figure 1 & Table 1). Similarly, response variable (Price) ranges from $307,500 to $840,000 with a mean value of $507502.78 (Figure 2 & Table 1). In addition, I can say that the square foot is positively correlated with the Price (Figure 3) i.e., for every single unit increase in the square foot, the average price would be approximately $179.14 higher (Table 3).\nFigure 1: Box plot - Sqft\n\nFigure 2: Box plot - Price\n\nFigure 3: Scatter plot - Price vs Sqft\n\nTable 1: Descriptive Statistics - Price, Sqft\n\nTable 3: Coefficients\n\nYes, this data can be used as an evidence to persuade your client that the estimated price of $300,000 is less for a house of 1500 square foot (Table 4). Although the possible individual prediction estimates would range roughly between $259,563.88 and $620,527.28 (which includes your client’s estimates), the average estimate range is roughly between $404,926.84 and $475,164.32 (which is more than your client’s estimate). This means that on average houses of 1500 square foot area are sold in the range of $404,926.84 and $475,164.32. Hence, your client should consider re-evaluating his/her judgement.\nTable 4: 95% Confidence and Prediction Intervals for House Area of 1500 Sqft\n\nUnfortunately, the model is restricted to make predication for houses of area that ranges from 850 to 3262. The house in question with the area of 3750 square foot is larger than houses in the provided data. Conceptually, there could be other factors involved in predicting house prices of this big area such as house layout, useable space and potential future. For instance, the buyer of a large house may want to check the layout so that he/she doesn’t need to spend more on renovating the place which can be a really huge amount. In contrast, buyer of a small house may not need to worry as any modification won’t be that expensive given the house area. This might alternate the linear correlation that I see in the data currently. Thus, the model may mislead your second client.\nStatistical Analysis:\nI visualized the predictor and response in individual box plots to get the five-number summary (Minimum, First quartile, Median, Third quartile, Maximum). I discovered that the five number summaries of Price and Square foot are (307500, 394250, 502500, 585625, 840000) and (850, 1419, 1739.5, 2306.25, 3262), respectively. As the variables are linearly correlated (Figure 3), I defined the linear relationship between the two as the following:\n Y = β0 + β1X\n\nwhere Y is the price in USD and X is the house area in square foot. β0 and β1 are the intercept and slope of the predictor, respectively.\nUpon fitting the model, I got the estimated co-efficient β0 and β1 as 171331.47 and 179.14, respectively (Table 3). From table 2, I can note that the R square for this model came out to be .604 which means that 60.4% of the variability in this response is explained by this model. Although it is not ideal, I can still use the model to make the predictions. Similarly, the standard error of the estimate is 87111.55 which is not great either but still allowable relative to the mean of the house price.\nTable 2: Model Summary\n\nI then did hypothesize testing for both the intercept and slope at 5% significance levels (Table 3). For β0, the null and alternative hypothesis were as follows: H0: β0 = 0; HA: β0 ≠ 0. Since the P-value (0.001) is less than 0.05 at 5% significance level, I reject the null hypothesis i.e., β0 is statistically significant and cannot be equated to zero. For β1, the null and alternative hypothesis were as follows: H0: β1 = 0; HA: β1 ≠ 0. Since the P-value for β1 (0.000) is less than 0.05 at 5% significance level, I reject the null hypothesis i.e., β1 is statistically significant and there is some correlation between Square foot and Price.\nIt is important to note that this model is only limited to predictor values ranging from 850 to 3262 i.e., making predictions for anything less than or greater than that would be an attempt of extrapolation. Furthermore, only 60.4% of the variability in the model is explained by this model i.e., I may need to add more features to the analysis. Some of the additional variables to consider could be: 1) Distance from frequently visited places such as Supermarket, Hospital and Airport, 2) Age of the property and 3) Current state of local real estate market.\n""], 'url_profile': 'https://github.com/SagarBansal7', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['Comparison of Machine Learning Algorithms\nComparison of machine learning models built from scratch, trained and tested on the MNIST dataset.\nIntroduction\nThe MNIST dataset consists of 70,000 examples of handwritten digits, split into 60,000 training examples and 10,000 test examples. Classification on this dataset involves labeling each handwritten digit as an integer from 0 to 9. Numerous machine learning models exist for multi-class classification problems like this. This project covers 5 different approaches, from linear regression to convolutional neural nets, using various optimization, regularization, and  hyperparameter tuning techniques.\nMethods\nK-Nearest Neighbours (KNN)\nA hyperparameter of k=3 resulted in a test error of 3.19%. Being non-parametric, a KNN implementation with such a large dataset resulted in relatively long prediction times (> 20 mins for k=1 and k=2) due to the calculation, and especially sorting, of Euclidean distances. While doing these computations locally, both memory and disk space frequently maxed out. Nevertheless, the test error is rather low, even without other preprocessing techniques.\nLinear Regression\nSoftmax loss was used for multiclass logistical linear regression. L2 regularization resulted in the best accuracy out of L0, L1, and L2 regularization techniques. A bias variable was also added to improve accuracy. Optimization with SGD resulted in a test error of 19.97 with a batch size of 5000 and alpha of 0.1. Smaller batch sizes and lower alphas resulted in very long training times. Only 2 epochs were used, as loss stopped decreasing past 2 epochs.\nSupport Vector Machine (SVM)\nA support vector machine using L2 regularization and optimized with SGD was implemented to achieve a test error of 9.13%. Hinge loss was calculated naively in python without the use of vectors. SGD was implemented with step decay such that alpha was halved every 5 epochs. This greatly reduced training time and improved test accuracy, as alpha started out at 0.1 and ended at 0.0125. The model was trained for a total of 20 epochs, with a batch sizes of 2500. Loss plateaued around epoch 18.\nMulti-Layer Perceptron (MLP)\nA hidden layer size of 500 was used, which took about 3 hours to train. An alpha of 0.001 performed best, with both 0.0005 and 0.005 increasing test error. A batch size of 2500 helped to reduce overfitting, as smaller batch sizes resulted in higher test error. 250 epochs worked well. More epochs may yield better test errors, but training time is already significantly lengthy. SGD was used to decrease training time for hyperparameter tuning, while traditional gradient descent was used to achieve the best accuracy.\nConvolutional Neural Network (CNN)\nAdam optimization, a combination of momentum and RMSProp, was used to optimize the CNN. Each learning rate alpha depends on the gradient, and is adjusted with every learning step. ReLU activation was used, with 4 epochs and batch sizes of 64. The different between 2 and 4 epochs were not significant. Batch sizes of 64 clearly outperformed batch sizes of 32, but result in longer training times. Weights were randomly initialized to avoid 0 gradients. Increasing the number of epochs and batch sizes further may result in better accuracy, but training is lengthy and costly.\nResults\n\n\n\nModel\nTest Error (%)\nMNIST Lowest Test Error (%)\n\n\n\n\nKNN\n3.19\n0.52\n\n\nlinear regression\n19.97\n7.6\n\n\nSVM\n9.13\n0.56\n\n\nMLP\n1.57\n0.35\n\n\nCNN\n1.63\n0.23\n\n\n\nDiscussion\nThe results from the various models implemented are compared with the best classification results\nfrom various studies, which can be found at http://yann.lecun.com/exdb/mnist/.\nIn the best KNN approach by Keysers et. al (2007), they used a preprocessing technique called P2DHMDM, a type of nonlinear deformation. This shifts the edges of the handwritten digits, creating augmented training data, which significantly improves test accuracy. My KNN implementation contains no data preprocessing, and uses a naive Euclidean distance computation. Nevertheless, I was rather impressed by the accuracy of this basic model.\nThe linear classifier implemented by Yann LeCun (1998) also preprocesses the training data by deskewing. Additionally, instead of a traditional linear classifier, the best implementation uses a pairwise approach to train each unit in the layer to recognize one class. This significantly outperforms traditional regression approaches.\nThe best SVM approach, by DeCoste and Scholkopf (2002), uses a 9-degree polynomial kernel, along with deskewing preprocessing. My approach, however, relies on the naive optimization of hinge loss without the use of kernels. Additionally, they use virtual support vectors instead of classic support vectors. Virtual support vectors are generated from prior knowledge using traditional support vectors found in the first training run. This may explain the drastic difference in test errors.\nThe best neural net approach for the MNIST classification problem was implemented by Ciresan et. al (2010). In order to achieve such a low error rate, they used 6 hidden layers with many neurons per layer, numerous deformed training images, and GPU computation. Without a GPU to speed up training, I was only able to implement a single hidden layer with 500 neurons, which took 3 hours to train. As the model developed by Ciresan et. al is fantastically simple, any ML engineer with adequate graphics cards should be able to achieve similar results.\nThe lowest test error on the MNIST classification problem by far was achieved by Ciresan et. al in 2012 using a committee of 35 convolutional neural networks, with width normalization preprocessing. Their algorithm imposes a winner-takes-all strategy for selecting neurons to result in several deep neural columns that are experts on inputs preprocessed in different ways. They also use GPUs to speed up training. My implementation uses a single CNN with traditional forwards and backwards convolutions, pooling, and ReLU activation. Additionally, without a GPU, hyperparameter tuning was inconvenient.\n'], 'url_profile': 'https://github.com/ricky-ma', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '334 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gladishd', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '458 contributions\n        in the last year', 'description': ['salary-prediction-flask-flutter\nImplementation of a simple salary prediction model created using linear regression , deployed on web using flask and deployed to heroku . Android App Implemented Using Flutter\nDeployed\nLive on https://predict-salary-flask.herokuapp.com/\nWeb Screenshots\n\n\n\n\nApp Screenshots\n\n\n\n'], 'url_profile': 'https://github.com/IamAnkitSharma', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['\ngqrf\n\n\n‘gqrf’ is an R package for implementing geographic quantile regression\nforest method for spatial interpolation as described in Maxwell et al,\n(2020).\nInstallation\nYou can install the released version of project from\nCRAN with:\n#install.packages(""gqrf"")\nAnd the development version from GitHub with:\n# install.packages(""devtools"")\ndevtools::install_github(""Gitmaxwell/gqrf"")\nExample\nThis is a basic example which shows you how to solve a common problem:\n#library(gqrf)\n## basic example code\nReferences\nMaxwell, K., Rajabi, M., Esterle, J. (2020). Spatial interpolation of\ncoal geochemical properties using geographic quantile regression forest.\nManuscript submitted for publication.\n'], 'url_profile': 'https://github.com/Gitmaxwell', 'info_list': ['1', 'HTML', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Python', 'Updated Aug 6, 2020', 'Updated Sep 20, 2020', 'Python', 'Updated Apr 27, 2020', 'HTML', 'Updated Oct 11, 2020', 'Python', 'Updated Apr 14, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Oct 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['LoanPrediction\nIdentified potential loan customers for Thera Bank using classification techniques. Compared models built with Logistic Regression and KNN algorithm in order to select the best performing one.\n'], 'url_profile': 'https://github.com/KarthikRajagopalan-25', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'Accra', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Do Poor Countries Grow Faster than Rich Countries\nIntroduction\nWe provide an empirical example of using partialling-out with Lasso to\nestimate the regression coefficient —1 in the high-dimensional linear\nregressionmodel:\nY = —1D + —2W +‘.\n\nSpecifically we are interested in how the rates at which economies of\ndifferent countries grow, denoted by Y , are related to the initial\nwealth levels in each country, denoted by D, controlling for country’s\ninstitutional, educational, and other similar characteristics, denoted\nby W. The relationship is captured by the regression coefficient —1. In\nthis repo, this coefficient is called the “speed of\nconvergence/divergence”, as it measures the speed at which poor\ncountries catch up or fall behind wealthy countries, controlling for W .\nInference\nOur inference question here is: do poor countries grow faster than rich\ncountries, controlling for educational and other characteristics? In\nother words, is the speed of convergence negative:\n—1 <0?\n\nThis is the Convergence Hypothesis predicted by the Solow Growth Model\nBarro-Lee Growth Data\n\nThe outcome (Y) is the realized annual growth rate of a country’s\nwealth (Gross Domestic Product per capita).\nThe target regressor (D) is the initial level of the country’s\nwealth\nThe target parameter —1 is the speed of convergence, which measures\nthe speed at which poor countries catch up with rich countries.\nThe controls (W) include measures of education levels, quality of\ninstitutions, trade openness, and political stability in the\ncountry.\n\nHigh-Dimensional Setting\nThe sample contains 90 countries and about 60 controls. Thus\np ~ 60, n = 90\n\nand p/n is not small. We expect the least squares method to provide a\npoor/noisy estimate of —1. We expect the method based on partialling-out\nwith Lasso to provide a high quality estimate of —1.\nResult\n                          Estimate Std. Error\nLeast Squares              -0.0094      0.030\nPartialling-out via lasso  -0.0498      0.014\n\nAs expected, least squares provides a rather noisy estimate of the speed\nof convergence, and does not allow us to answer the question about the\nconvergence hypothesis. In sharp contrast, partialling-out via Lasso\nprovides a more precise estimate. The lasso based point estimate is ~4%\nand the 95% confidence interval for the (annual) rate of convergence is\n~7.5% to ~1.5%. This empirical evidence does support the convergence\nhypothesis.\nSummary\nIn this repo, we have examined an empirical example in the\nhigh-dimensionalsetting. Using least squares in this setting gives us a\nvery noisy estimate of the target regression coefficient and does not\nallow us to answer an important empirical question. In sharp\ncontrast,using the partialling-out method with Lasso does give us a\nprecise estimate of the regression coefficient and does allow us to\nanswer that question. We have found significant empirical evidence\nsupporting the convergence hypothesis of Solow.\n'], 'url_profile': 'https://github.com/bizmaercq', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'Edinburgh', 'stats_list': [], 'contributions': '293 contributions\n        in the last year', 'description': ['MLMC-MIMC-SGD\nWARNING work in progress\nCode for the numerical experiments in the paper Multi-Index Antithetic Stochastic Gradient Algorithm\nThis repository contains Stochastic MCMC methods for Bayesian regression with variance reduction (MLMC with antithetic samples, MIMC with antithetic samples, control variate using MAP estimate) using Pytorch:\n\n\nMASGA (Multi-index Antithetic Stochastic Gradient Algorithm). Multi-index and antithetic samples at the level of the time discretisation and subsampling on the Stochastic Langevin SDE.\nFor mathematical details, see\n  @misc{majka2020multiindex,\n      title={Multi-index Antithetic Stochastic Gradient Algorithm},\n      author={Mateusz B. Majka and Marc Sabate-Vidales and Łukasz Szpruch},\n      year={2020},\n      eprint={2006.06102},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n  }\n\n\n\nStochastic Langevin Dynamics using Control Variate to reduce the variance a MAP estimator. This algorithm is taken from the paper Control Variates for Stochastic Gradient MCMC and is used for benchmarking purposes.\n\n\nAcknowledgements\nMLMC and MIMC abstract classes initially based on https://bitbucket.org/pefarrell/pymlmc/src/master/.\nFiles\nRunning the code\n\nBayesian logistic regression with Gaussian prior on covertype dataset (the code is already prepared to download it). The code samples from the posterior using antithetic MLMC on the discretised Stochastic Langeving SDE and approximates E(F(X)), with F(X) = |X|^2. The code returns various plots specifying computational costs necessary to achieve different Mean Squared Errors.\n\npython logistic_masga_mlmc_subsampling.py --prior Gaussian --T 5 --n_steps 100 --device 1 --s0 32 --type_data covtype --Lmin 0 --N 10000\n\n\nBayesian logistic regression with Mixture of two Gaussians as the prior on a synthetic dataset:\n\npython logistic_masga_mlmc_subsampling.py --prior MixtureGaussians --T 5 --n_steps 100 --device 1 --s0 2 --dim 2 --data_size 512 --type_data synthetic --Lmin 0 --N 10000\n\n'], 'url_profile': 'https://github.com/msabvid', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""Capstone project: Climate Change & European politics\nAdam Storek, Paul Marande\nSociometric analysis (R + tidyverse) of the European Social Survey: Illustrating the relationship between party allegiance and opinions on climate change.\nWithin the last decade, environmental policies especially pertaining to the topic of climate change have flooded the political, social, and even cultural discourse. It is no surprise that in the latest round of the European Social Survey, for the first time ever, one entire module is dedicated to the “public attitudes to climate change, energy security and energy preferences”. We have decided to use this opportunity to examine the dynamics of the climate change discourse among European countries, and to establish a link between the public attitudes to climate change and the sphere of European politics, for which we will use the module Politics from the same round of European Social Survey.\nAccording to the United State’s National Aeronautics and Space Administration (NASA), Climate is defined as the average weather conditions in an area determined over a period of years. Hence climate change can be described as the abnormal variations in weather patterns, usually pushing weather to new extremes. Climate change has a very strong impact on wildlife and ecosystems. For instance, towards the end of the cretaceous period, climate change led a change in ecosystems which eventually brought upon the extinction of dinosaurs.\nIn the past decades, scientists have started to realize that climate change is being accelerated by mankind. The scientific community has also anticipated that without an active commitment of mankind to reduce its impact on climate, climate change will bring about a severe damage to or even a destruction of ecosystems such as the Great Barrier Reef and rising sea levels to name a few, posing a significant threat to the humanity itself. The scientific call for action resonated within most states as well as supranational entities like the European Union or the United Nations, and the issue has become increasingly politicized. The issue of man-made climate change is particularly problematic, because it embodies a negative externality of markets, an exploitation of public goods also known as “The tragedy of the commons”. Man-made climate change represents one of  the inefficiencies of unregulated markets; in addition, it is a supranational problem which can only be addressed by strong international cooperation. Resolving climate change is especially difficult because it is a prisoner's dilemma: although all individuals agree that limiting man-made climate change is necessary, one would prefer the other to do something about instead of him/herself.\nInherently, climate change fits well into the left and right divide, because it represents a market failure that needs regulation. We would therefore expect that political movements and parties, especially but not exclusively left-leaning, have incorporated measures to reduce human impact on climate change into their programmes or have centered their agenda around the issue of climate change and environment completely (such as the far-left Green Party). On the other hand, other political parties, predominantly right-leaning, have been increasingly sceptical towards the notion that climate change is man-made or to the climate change regulations themselves, often focusing on the fact that climate change regulations such as green tax are costly and disrupt the markets. That the climate change question has infused the national-level political discourse in the United States has been documented by Antonio and Brulle.\nHowever, as we have noted above, the issue of climate change is as much a national issue as it is international. The European Union and its member states, as experts suggest, “have adopted policies and programs that have put it at the forefront of international efforts to address climate change.” We have seen in the media that the split between the proponents of the European Union and Eurosceptics has also involved the topic of climate change. Especially in this regard, it would be unreasonable to think that climate change scepticism has fuelled Euroscepticism or vice versa, but the relationship is worthy of examination. All in all, it is our hypothesis that climate change has to some extent become a matter of political partisanship. Essentially, we will analyze how can the political actions of voters across Europe be related to their environmental concerns and subsequently the views on climate change.\nTo show this we will be crossing the European Social Survey (ESS8)-2016 on Climate Change with the ESS8-2016 on Politics. We will be looking at if individuals in European countries who report being very concerned about Climate Change actually vote for parties with hardline climate policies, take action in politics to limit climate change, or even push for a more integrated supranational system (i.e.: the European Union). In brief the ESS8-2016 on Climate Change focuses on 4 broad areas, namely: beliefs on climate change; concerns about climate change and energy security; personal norms, efficiency and trust; energy preferences. We will mainly be focusing on “Concerns about climate change and energy security”, as well as “personal norms, efficiency and trust” and “energy preferences”. We will cross those variables with those of ESS8-2016 on Politics (see below) to test our hypothesis.\nAs for the ESS8-2016 on Politics as described by the ESS “The politics questions of the core section focus on political interest, trust, electoral and other forms of participation, party allegiance as well as socio-political orientations.” This of course entails that we will have to do research on the categorization of individual European parties in terms of both their attitudes concerning environment as well as the position on the left-right political axis and the views concerning the European Union. These positions and views, assuming rationality of the respondents, can be partially assessed by comparing the parties they voted for or they feel are the closest to them to their other views as far as positioning on the left-right axis, trust in the European Parliament, and the question on European Unification go further or gone too far.\nList of questions on Climate Change provided by the ESS:\n""], 'url_profile': 'https://github.com/adamstorek', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '1,162 contributions\n        in the last year', 'description': [""Data Mining Cup 2020\nFiles and codes with HU Team solution to the Data Mining Cup 2020 competition. A detailed walkthrough of our solution is provided  in this blogpost.\nProject summary\nForecasting demand is an important managerial task that helps to optimize inventory planning. The optimized stocks can reduce retailer's costs and increase customer satisfaction due to faster delivery time. This project uses historical purchase data to predict future demand for different products.\nProject structure\nThe project has the following structure:\n\ncodes/: notebooks with codes for different project stages:\n\ndata preparation\nfeature engineering\nmodeling\nmeta-parameter tuning\nensembling (blending and stacking)\nhelper functions\n\n\ndata/: input data. The folder is not uploaded to Github due to size constraints. The raw data can be downloaded here.\noutput/: output files and plots exported from the notebooks.\ndocumentation/: task documentation provided by the competition organizers\noof_preds/: out-of-fold predictions produced by the train models within cross-validation.\nsubmissions/: test sample predictions produced by the trained models.\n\nRequirements\nYou can create a new conda environment using:\nconda create -n dmc python=3.7\nconda activate dmc\n\nand then install the requirements:\nconda install -n dmc --yes --file requirements.txt\npip install lightgbm\npip install imblearn\npip install catboost\n\nAlternatively, you can install the packages in your base environment:\nconda install --yes --file requirements.txt\npip install lightgbm\npip install imblearn\npip install catboost\n\n""], 'url_profile': 'https://github.com/kozodoi', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'Sylhet', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Sales-Revenue-Prediction\nIn this project I have built and evaluated multiple linear regression models using Python. I used scikit-learn to calculate the regression, while used pandas for data management and seaborn for data visualization. The data for this project consists of the very popular Advertising dataset to predict sales revenue based on advertising spending through media such as TV, radio, and newspaper.\n'], 'url_profile': 'https://github.com/Jack-Devil', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TriziaLeal', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'Umeå, Sweden', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Istiak1992', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '627 contributions\n        in the last year', 'description': ['LinkAgainstMyLibs\nLink against my libraries (Forth, OpenGL, Logger ...) to check if standalone project using them can compile.\nDownload\ngit clone git@github.com:Lecrapouille/LinkAgainstMyLibs.git --recurse-submodules\nlibSimForth\nhttps://github.com/Lecrapouille/SimForth\nClassic Forth interpreter:\ncd Forth\nmake -j8\n./build/Forth\nExtended Forth interpreter:\ncd ExtendedForth\nmake -j8\n./build/ExtendedForth\nlibOpenGLCppWrapper\nhttps://github.com/Lecrapouille/OpenGLCppWrapper\ncd OpenGL\nmake -j8\n./build/OpenGL\nliblogger\nhttps://github.com/Lecrapouille/MyLogger\ncd Logger\nmake -j8\n./build/Logger\n'], 'url_profile': 'https://github.com/Lecrapouille', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}","{'location': 'Pune, Maharashtra, India', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/himanshu1403', 'info_list': ['Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2021', 'HTML', 'MIT license', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 18, 2020', 'C++', 'Updated Nov 22, 2020', 'HTML', 'Updated Apr 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['kaggle competition projects\nIntroduction\nContains the implementation of four kaggle projects:\n\nTitanic: Machine Learning from Disaster\nHouse Prices: Advanced Regression Techniques\nDaily news for stock market prediction\nHome Depot Product Search Relevance\n\n'], 'url_profile': 'https://github.com/nick0126', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Reddit-flair-classifier\nA reactjs app with a flask backend which classifies reddit posts using logistic regression from r/india into 6 flair categories -\n\n\'Coronavirus\'\n\'Science/Technology\'\n\'Policy/Economy\'\n\'Politics\'\n\'Non-Political\'\n\'AskIndia\'\n\nCodebase\nThe data collection, evaluation and model training was done using Google Colab. The backend is being served using Flask and the frontend was developed using React.js. The backend is hosted using heroku and the frontend is hosted on github pages.\nRespository Structure\n\n\nThe base directory of the repo consists of the Flask API files, the Heroku Procfile, the Reddit Connector class and the Text classification helper class. These are all needed to run the API, load\nthe model and to connect with reddit. It also consists of the logistic regression model, the count vectorizer file, the tfidf transformer file, the tokenizer file and a trained word2vec LSTM model.\n\n\nThe src consists of the reactjs code which is used to design and run the webapp.\n\n\nThe notebooks folder consists of all the 3 jupyter notebooks used to collect the data, evaluate the data and train the necessary classifiers.\n\n\nRunning this project\n\nClone this repository using\n\ngit clone https://github.com/dh1105/Reddit-flair-classifier.git\ncd Reddit-flair-classifier\n\n\nEnsure that Python 3.x is installed on your local system. Install all dependencies using\n\npip install -r requirements.txt\n\n\n\nIn order to run the Flask API, you need to have a Reddit account with an api key to access data. Modify the RedditConnector.py class and add your details to it.\n\n\nYou can now run the Flask API using python app.py. This API will be listening at http://localhost:5000/. You can send requests to the respective endpoints.\n\nA sample call to the \'/predict\' endpoint would be:\n\nPOST /predict HTTP/1.1\nHost: localhost:5000\nContent-Type: application/json\ncache-control: no-cache\nPostman-Token: 685cb700-05d5-49cd-8663-bb3d83abb4c8\n{\n   ""url"": ""https://www.reddit.com/r/india/comments/g1v3cn/what_are_you_watching/?utm_source=share&utm_medium=web2x""\n}------WebKitFormBoundary7MA4YWxkTrZu0gW--\n\n\nA sample call to the \'/automated_testing\' endpoint would be:\n\nPOST /automated_testing HTTP/1.1\nHost: localhost:5000\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW\ncache-control: no-cache\nPostman-Token: 87036eae-7239-41e4-a045-c6e8d77a1ae5\n\nContent-Disposition: form-data; name=""upload_file""; filename=""C:\\Reddit_flair_flask_app\\example.txt\n\n\n------WebKitFormBoundary7MA4YWxkTrZu0gW--\n\nTake note that this expects a file with a link to a Reddit post on every line. The file can have any name but the form data key must be \'upload_file\'.\n\n\nTo start the frontend, ensure that you have npm installed. You can start the app by issuing the following command.\n\n\nnpm start\n\n\nThe backend and frontend can be run independently as well. By default, the front-end will be making API calls to the Heroku backend. In order to allow calls to be made to the local Flask app, uncomment line 1 and comment line 2 in baseURL.js.\n\nDataset\nThe dataset consists of 36000 posts, 6000 from each flair, all stored in a .csv file. It is available for download here.\nMethodology\nThe main logic behind the project is in the three jupyter notebooks.\n\n\nReddit_data_scraper: Using the pushshift.io API, I downloaded some of the latest posts from Reddit r/india. Each flair I considered has 6000 posts in the dataset.\nIn addition to this, it also contains some baseline traditional ML models trained on the dataset. The data collected consisted of a variety of fields but only the \'Title\', \'Selftext\' and \'link_flair_text\' were used further in the problem.\n\n\nExploratory_data_analysis: Analysis of the dataset to find frequent words corresponding to each flair, see the distribution of invalid text in posts [NaN, [deleted], [removed]] and to find prominent features to be used as training data.\n\nAn evaluation of the \'selftext\' revealed that a majority of posts either did not contain any text or had the text removed. As a result, the body itself would not serve as a suitable feature. Therefore, the \'title\' and \'selftext\', if any, were combined to make the feature to be considered.\nThis data was cleaned by removing punctuations, stopwords and URLs. The cleaned data was tokenized and displayed in seperate word clouds for each flair. This gave an insight about the significant overlap of key words between classes.\nFour models were trained as baseline models using a combination of the \'title\' and \'selftext\' as a feature. The Logistic regression model performed the best out of these four.\n\n\n\nFlair_classification: An attempt to enhance the traditional ML algorithms by using LSTMs to try and classify posts. The evaluation was done using simple LSTMs with both pre-trained embedding layers and trainable embedding layers.\n\nData cleaning was the same as mentioned earlier and the input consisted of the \'title\' and \'selftext\', if any, concatenated. Four embedding types were considered for the LSTM embedding layer - pre-trained word2vec, pre-trained Fasttext, pre-trained GLoVe and a trainable embedding layer.\nFour models were trained with each respective embedding layer. Each model also had EarlyStopping as a callback to prevent overfitting.\nThe word2vec model outperformed the other three models. However, I was unable to use that with the Flask API due to Heroku\'s slug size constraints.\n\n\n\nResults\nTraditional ML models\n\n\n\nModel\nTesting accuracy\nTraining accuracy\n\n\n\n\nLogistic regression\n0.5944\n0.7848\n\n\nSupport Vector Machine\n0.5971\n0.9352\n\n\nMultinomial Naive Bayes\n0.5833\n0.7348\n\n\nStocastic Gradient Descent classifier\n0.5758\n0.7306\n\n\n\nAs evident from the table above, although the SVM does have a marginaly high accuracy than logistic regression, it seems to be overfitting.\nA more detailed analysis of the models comprising of the classification report and confusion matrix can be seen in the Exploratory_data_analysis notebook.\nDeep learning models\n\n\n\nModel\nTesting accuracy and loss\nValidation accuracy and loss in the final epoch\n\n\n\n\nWord2Vec LSTM\n0.521, 1.273\n0.5177, 1.2955\n\n\nGloVe LSTM\n0.382, 1.576\n0.3729, 1.5764\n\n\nFasttext LSTM\n0.518, 1.282\n0.5177, 1.3147\n\n\nTrainable embedding LSTM\n0.551, 1.552\n0.5625, 1.5041\n\n\n\nAs evident from the table above, the GLoVe did not perform well. The model with the trainable embedding layer seems to be performing the best in terms of accuracy. However, it started overfitting very early and\nhas one of the highest losses. You can see the plot of epoch vs acc and loss in the Flair_classification notebook.\nInference\nThe logistic regression model with the \'title\' and \'selftext\' combined as a feature seems to be performing the best on the dataset which I had made. It seems to outperform the DL models due to scarcity of data. Hence, as a result\nthis was the model which is being used as part of the API. I also wanted to deploy the word2vec LSTM with the Flask API but could not do so due to slug size constraints of Heroku.\nReferences\n\nhttps://pushshift.io/api-parameters/\nhttps://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\nhttps://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\nhttps://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge\nhttps://medium.com/the-andela-way/deploying-a-python-flask-app-to-heroku-41250bda27d0\nhttps://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n\n'], 'url_profile': 'https://github.com/dh1105', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Cancer-Mortality-Rate-Tree-Model-based\nRefer to the cancer data and withhold data from the Cancer Mortality repository. For that project we tried Linear Regression and KNN. We will explore performance improvement from more flexible methods in this project.\nInstructions\nDeveloped Decision tree (with pruning), Random Forest, Boosting based models to predict cancer mortality.\nPerformed hyper parameter tuning using 10-fold Cross Validation (CV).\nReported performance of best model on the holdout data.\n'], 'url_profile': 'https://github.com/Hirenkr', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Machine-Learning-Algorithms\nThis repository contains some common Machine Learning Alogrithms with their working using Python and R. The algorithms implemented are : Decision Trees, K-Nearest Neighbor, Logistic Regression, Naive Bayes, Support Vector Machine and Random Forest.\n'], 'url_profile': 'https://github.com/HiteshSoneji', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['house_price_prediction\nA Python program to predict the house prices using univariate linear regression with linear algebra method. The hypotheses used for prediction are from an example problem in the Machine Learning course by Andrew Ng on Coursera.\nTest Run:\n\n'], 'url_profile': 'https://github.com/rithika-hebbar', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Techniques used Feature Scaling, Feature engineer, one-hot encoding and train model by Linear Regression, Random Forest, Decision tree. Evaluate the model by cross validation, MSE. Using GridSearchCV, randomizedSearchCV  to find the best hyperparameter\n'], 'url_profile': 'https://github.com/trantntran', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': [""SMS Spam Classifier: Binary Classification of SMS Spam Messages\nSMS messages are ubiquitous and a major form of communication for billions of people. The spam messages that affect other communication channels, like email, similarly affect SMS. Filtering SMS messages and marking them as spam, and possibly removing them, can not only give the end user a better experience, but it can also free up network bandwidth, as hundreds of millions, if not billions, of messages could prevented from being sent every year.\nThis issue will not go away soon, with the upcoming adoption of the rich communication services protocol, end user devices will only become more advanced and have increasing ability to receive and respond to highly interactive and content full messages. This will be exploited both by corporate actors seeking profit and malicious actors.\nThis analysis aims to classify SMS spam messages using a variety of techniques, so that spam messages can be filtered.\nTechniques Employed\nA variety of techniques were employed in an attempt to find the data set's most important attributes, and in turn, attempt to use these specifying features to classify the messages. The following techniques were used:\n\nWord Cloud Analysis\nBag of Words - Frequency Vectorizer\nPrincipal Component Analysis\nMultinomial Naive Bayes\nLogistic Regression\nSupport Vector Machine\nFiltering for key words, punctuation\n\nData Set\nThe data set we are working with is the SMS Spam Collection Data Set, which is available at the following link: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\nThe data set has 5,572 unique records, and each record contains two different fields, the message field itself and a binary classification of either being spam or legitimate, where 'ham' denotes the latter.\nThe First 10 Rows of the Data Set:\n\n\n\nClassification\nText\n\n\n\n\nham\ngo until jurong point, crazy.. available only ...\n\n\nham\nok lar... joking wif u oni...\n\n\nspam\nfree entry in 2 a wkly comp to win fa cup fina...\n\n\nham\nu dun say so early hor... u c already then say...\n\n\nham\nnah i don't think he goes to usf, he lives aro...\n\n\nspam\nfreemsg hey there darling it's been 3 week's n...\n\n\nham\neven my brother is not like to speak with me. ...\n\n\nham\nas per your request 'melle melle (oru minnamin...\n\n\nspam\nwinner!! as a valued network customer you have...\n\n\nspam\nhad your mobile 11 months or more? u r entitle...\n\n\n\nFindings and Results\nWord Cloud\nThe first step we did was to generate a word cloud, this allowed us to simply and trivially see the most common words, as they will be the largest in the graphic.\n\nFrom the Word Cloud above, the most common words included:\n\nnow\nwill\ngot\nit\nlove\nur\nok\n\nPrincipal Component Analysis\nWithout filtering any of the words and using all the words present in the initial data set, we did a Principal Component Analysis on the two most common attributes, which were essentially the two most common words, and got PC1 and PC2, though the variance of both only account for 0.304% and 0.293%, respectively. For the two most common features, they have little influence over whether the message is spam or not.\n\nFiltering the data set for specific words and symbols, we added additional fields for each record in the data set depending on whether each word or symbol was present in that record's text, and ignored the original fields that corresponded to the original set of words, unless they were one of the specific words that we added an additional field for. The specific words and symbols we looked for were:\n\nfree\nwin\ninterested\nbuy\n$\noffer\n!\n?\n\nOur new data frame looked like:\n\n\n\n\nFree\nWin\nInterested\nBuy\nDollarSign\nOffer\nExclamation\nQuestion\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\nWith these additional fields in place, and the original fields being ignored, we again did a Principal Component Analysis on the two most common attributes, and got PC1 and PC2, though the variance of both now accounted for 15.412% and 14.176%, respectively. This is an improvement, and means that the fields we added were more significant than the words alone at determining whether a given SMS message was spam or not. This is not surprising because the most common words, like 'the' and 'hi' may be present in both spam and legitimate messages.\n\nMachine Learning Algorithms\nPCA provided useful clues on what is important in the messages, not the most common words as much as specific words. However, PCA is only a tool to reduce dimensionality and provide some useful insights on the most common attributes in the process. Seeing how well certain machine learning classification algorithms fit the data also provides insight.\nLooking at three machine learning algorithms:\n\nMultinomial Naive Bayes\nLogistic Regression\nSupport Vector Machine\n\nWe saw how well each of the algorithms fit and predicted the data. The results are as follows:\n\nMultinomial Naive Bayes --> 98.15\nLogistic Regression -- > 98.27\nSupport Vector Machine --> 87.14\n\nClearly, Multinomial Naive Bayes and Logistic Regression did the best, whereas Support Vector Machine did the worst. This is most likely because though the data is in a very high dimension, given that the number of columns is established over the entire vocabulary of the 'text' column, it is not easily separable, regardless of the kernel used. Also, because an optimum soft margin had to be found, Support Vector Machine was the slowest of the methods. Multinomial Naive Bayes did very well considering its speed and simplicity. It was particularly well suited for the bag-of-words technique on the text data.\nResults of over 98% are quite good, however, since many words sent over SMS text messages are slang, not spelled correctly, or abbreviated, these altered words can dilute the amount of valid English words that we can attempt to derive meaning from. To investigate this, we filtered out all non-valid English words, but kept two attribute fields, looking at whether exclamation and questions marks were present. It is worth noting that some of the words present still are not valid English, such as 'dun', but they are present in the dictionary that we checked against. Our data frame, for our X values, before the text has been turned into a frequency vector, now look like this:\n\n\n\n\nText\nExclamationMark\nQuestionMark\n\n\n\n\n0\ngo until point crazy.. available only in great...\n0\n0\n\n\n1\n... joking ...\n0\n0\n\n\n2\nfree entry in a wkly comp to win fa cup final ...\n0\n0\n\n\n3\ndun say so early ... already then say ...\n0\n0\n\n\n4\nnah i do think he goes to he lives around here...\n0\n0\n\n\n5\nhey there darling it been week now and no word...\n1\n1\n\n\n6\neven my brother is not like to speak with me t...\n0\n0\n\n\n7\nas per your request has been set as your for a...\n0\n0\n\n\n8\nwinner as a valued network customer you have b...\n1\n0\n\n\n9\nhad your mobile 11 months or more entitled to ...\n1\n1\n\n\n\nRe-running out tests, though excluding Support Vector Machine, we now get:\n\nMultinomial Naive Bayes --> 97.67\nLogistic Regression -- > 98.03\n\nOur accuracy actually went down, for Multinomial Naive Bayes, by .478%, and for Logistic Regression, by .24%. That means we eliminated some key columns when filtering out non-valid English words. In fact, this makes sense, certain expressions and abbreviations, such as <3 or (:, might be very indicative of personal messages over more formal messages, whose purpose can be more ambiguous.\nGoing back to the second PCA analysis that we performed, where we only looked at certain key words and symbols:\n\nfree\nwin\ninterested\nbuy\n$\noffer\n!\n?\n\nLet's see the accuracy of our machine learning algorithms when only performed on these specific attributes. Instead of using Multinomial Naive Bayes, we will be using Bernoulli Naive Bayes, as that is better suited for binary features.\nOur results are:\n\nBernoulli Naive Bayes --> 90.13\nLogistic Regression -- > 89.71\n\nIt seems that though the key words and punctuation we included do account for more variability than many other words, they still do not account for enough variability to provide higher accuracy than when training on the full data set, and this makes sense given the limited scope of the attributes.\n""], 'url_profile': 'https://github.com/samsuri100', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '504 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aryabhatta22', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['reactFlaskMl\nThis is an ML application developped using ReactJs and Python. It uses Logistic regression model to predict wheather the automobile insurance claims are fraudlent or not. Flask API is used to get the request from the client side and send back the predicted response.\nGetting Started\nClone the repository by entering git clone https://github.com/chris287/reactFlaskMl.git\nThe scripts directory contains all the python scripts and ui directory consists of the ReactJs code for frontend UI.\nNavigate inside scripts directory and run command pip install -r requirements.txt in cmd to install all the required libraries for python.In the same way navigate inside ui and run npm i to install node modules.\nTo start the flask API server,navigate scripts directory and run python main.py.This will start a development server on http://127.0.0.1:5000/ for python flask API.Run the server by opening the link in browser.Next step is to load the ui.To do this run the command npm start inside ui directory. This will start a react server and you can access the ui.\n'], 'url_profile': 'https://github.com/chris287', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': [""E-commerce website A/B test results\nIn this project, we analyze the results of an A/B test performed by an e-commerce website. The objective is to determine\nthe probability of converting after seeing the old webpage or the new page. For that purpose, we tested the feature on\ntwo groups: a control group and a treatment (or experiment) one. The control group viewed the old page while the treatment\ngroup viewed the new one.\nAfter setting our null and alternative hypotheses to be:\n$$H_0: p_{new} - p_{old} \\leq 0$$\n$$H_1: p_{new} - p_{old} > 0$$\nWe simulated the test 10,000 times on a sample distribution of ab_test population.\nBy plotting the distribution from the null and comparing against the actual observed difference in conversion rates, the\nresults didn't prove to be statistically significant enough to justify making changes to the website. With a p-value of\napprox. 0.9, we would fail to reject the null and keep the old webpage in place. We also used the built-in Python module\nstatsmodels and arrived at the same conclusion.\nBy fitting a logistic regression model, we seek to predict two possible outcomes: conversion or no conversion. We see from\nthe results summary table above that the likelihood of converting given that an individual was in the treatment group isn't\nstatistically significant with a p-value of 0.19 (two-sided), confirming once again that we should stick with the old webpage.\nLastly, we added a country variable to our regression model to see whether the region where individuals were located affected\nthe conversion rate. With a p-value over the conventional threshold of 0.05, we would conclude here that the region isn't a\ndetermining factor contributing to the conversion rate.\n""], 'url_profile': 'https://github.com/chloelubin', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 13, 2020', 'Python', 'Updated Apr 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Python', 'Updated Apr 16, 2020', 'JavaScript', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated May 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\npoissonreg\n\n\n\n\n\npoissonreg enables the parsnip package to fit various types of Poisson\nregression models including ordinary generalized linear models, simple\nBayesian models (via rstanarm), and two zero-inflated Poisson models\n(via pscl).\nInstallation\nYou can install the released version of poissonreg from\nCRAN with:\ninstall.packages(""poissonreg"")\nInstall the development version from GitHub with:\nrequire(""devtools"")\ninstall_github(""tidymodels/poissonreg"")\nExample\nA log-linear model for categorical data analysis:\nlibrary(poissonreg)\n#> Loading required package: parsnip\n\n# 3D contingency table from Agresti (2007): \npoisson_reg() %>% \n  set_engine(""glm"") %>% \n  fit(count ~ (.)^2, data = seniors)\n#> parsnip model object\n#> \n#> Fit time:  6ms \n#> \n#> Call:  stats::glm(formula = count ~ (.)^2, family = stats::poisson, \n#>     data = data)\n#> \n#> Coefficients:\n#>               (Intercept)               marijuanayes  \n#>                    5.6334                    -5.3090  \n#>              cigaretteyes                 alcoholyes  \n#>                   -1.8867                     0.4877  \n#> marijuanayes:cigaretteyes    marijuanayes:alcoholyes  \n#>                    2.8479                     2.9860  \n#>   cigaretteyes:alcoholyes  \n#>                    2.0545  \n#> \n#> Degrees of Freedom: 7 Total (i.e. Null);  1 Residual\n#> Null Deviance:       2851 \n#> Residual Deviance: 0.374     AIC: 63.42\nContributing\nThis project is released with a Contributor Code of\nConduct.\nBy contributing to this project, you agree to abide by its terms.\n\n\nFor questions and discussions about tidymodels packages, modeling,\nand machine learning, please post on RStudio\nCommunity.\n\n\nIf you think you have encountered a bug, please submit an\nissue.\n\n\nEither way, learn how to create and share a\nreprex (a minimal, reproducible example),\nto clearly communicate about your code.\n\n\nCheck out further details on contributing guidelines for tidymodels\npackages and how to get\nhelp.\n\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Predicting-the-Strength-of-high-performance-concrete\nThis project involved feature exploration and selection to predict the strength of high-performance concrete. Used Regression models like Decision tree regressors to find out the most important features and predict the strength. Cross-validation techniques and Grid search were used to tune the parameters for best model performance.\nData Description:\nThe actual concrete compressive strength (MPa) for a given mixture under a specific age (days) was determined from laboratory. Data is in raw form (not scaled).The\ndata has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations).\nDomain:\nCement manufacturing\nContext:\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include\ncement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\nAttribute Information:\n\nCement : measured in kg in a m3 mixture\nBlast : measured in kg in a m3 mixture\nFly ash : measured in kg in a m3 mixture\nWater : measured in kg in a m3 mixture\nSuperplasticizer : measured in kg in a m3 mixture\nCoarse Aggregate : measured in kg in a m3 mixture\nFine Aggregate : measured in kg in a m3 mixture\nAge : day (1~365)\nConcrete compressive strength measured in MPa\n\nLearning Outcomes:\n\nExploratory Data Analysis\nBuilding ML models for regression\nHyper parameter tuning\n\nObjective:\nModeling of strength of high performance concrete using Machine Learning\nSteps:\n\n\nExploratory data quality report reflecting the following:\na. Univariate analysis\ni. data types and description of the independent attributes,\nii. analysis of the body of distributions/tails,\niii. missing values, outliers\nb. Multivariate analysis\ni. analysis between the predictor variables\nii. analysis between the predictor variables and target column in terms of their relationship and degree of relation if any.\niii. Visualize the analysis using boxplots and pair plots, histograms or density curves.\nSelect the most appropriate attributes\nc. Pick one strategy to address the presence outliers and missing values and perform necessary imputation\n\n\nFeature Engineering techniques\na. Identify opportunities (if any) to create a composite feature, drop a feature\nb. Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help?\nc. Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and present your findings in terms of the independent\nattributes and their suitability to predict strength\n\n\nCreate the model\na. Obtain feature importance for the individual features\n\n\nTuning the model\na. Algorithms that will be suitable for this project\nb. Techniques employed to squeeze that extra performance out of the model without making it overfit or underfit\nc. Model performance range at 95% confidence level\n\n\nReferences:\nMedium article on hyper parameter tuning\n'], 'url_profile': 'https://github.com/CHYaswanth', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NikitaKirane', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '804 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dalessioluca', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['property-management-website\nThis is a project based on website for property management with frontend and backend both and also machine learning concept.\nWe have used HTML, CSS and Javascript in frontend and python framework django in backend. Also we have used sqlite for handling our database.\nWe have also used the concept of machine learning specifically linear regression for the prediction of price using python.\nHence it is a complete project based on all different domains.\n'], 'url_profile': 'https://github.com/rohanmodi2810', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshsingh97', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '779 contributions\n        in the last year', 'description': ['FlaskApp\nMachine Learning Model Deployed on Flask Web App\nScreenshot\n\n'], 'url_profile': 'https://github.com/kudoabhijeet', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Machine-Learning-Assignments\nClustering , Numpy basics and Comparison of Supervised & Unsupervised :\nProblem Statement 1 : Say you are standing at the bottom of a staircase with  a dice. With each throw of the dice you either move down one step (if you get a 1 or 2 on the dice) or move up one step (if you get a 3, 4 or 5 on the dice). If you throw a 6 on the dice, you throw the dice again and move up the staircase by the number you get on that second throw. Note if you are on the base of the staircase you cannot move down! What is the probability that you will reach more than 60 steps after 250 throws of the dice. Change the code so that you have a function that takes as parameter, the number of throws Add a new parameter to the function that takes a probability distribution over all outcomes from a dice throw. For example (0.2,0.3,0.2,0.1,0.1,0.1) would suggest that the probability of getting a 1 is 0.2, 2 is 0.3 etc. How does that change the probability of reaching a step higher than 60?\nProblem Statement 2 : . Generate random data for for Multiple Linear Regression, Logistic Regression, K-mean Clustering\nProblem Statement 3 : Implement the following algorithms from scratch using numpy only and using the data from Question 1 “for loop” a. Linear Regression using Gradient Descent b. Logistic Regression using Gradient Descent c. Linear Regression with L1 and L2 Regularization d. Logistic Regression with  L1 and L2 Regularization e. K-Means\nProblem Statement 4 : Change code into an Object Oriented Project reusing part of the code where possible\n'], 'url_profile': 'https://github.com/chaitanya1812', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amansingh1701', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,851 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['R', 'Updated Oct 28, 2020', 'HTML', 'Updated Apr 19, 2020', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 19, 2020', '3', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Updated Apr 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '293 contributions\n        in the last year', 'description': [""COVID_19_Prediction\nMore detailed explanation can be found in reports/COVID-19_Prediction.pdf\nIdea\nModels like SIR are good at explaining the general trend of an epidemic like COVID 19. However, due to its small degree of freedom, these models has limited power when capturing smaller fluctuations from the curve and therefore is not good at prediction in short-term. Our hypothesis is that these smaller fluctuations are usually caused by people's reactions to the News and Social Media. Therefore, we combine these extra information from the News and Social Media together with SIR model to construct a model with better predictive power.\nMethodology\nWe first generate the SIR curve using Least Squared Estimate, and consider it as the baseline. We then collect unigrams and bigrams from COVID 19 related Tweets and count their frequency at a daily level. After that, we rank the keywords by their absolute correlation to the derative of the cumulative cases curve. Finally, we use the baseline curve and the top 20 keywords' daily trend as explanatory variables of a Linear Regression, number of confirmed case (leading 4 days) as the response variable, using L2 regularization to avoid overfitting. Note that the problem of autocorrelation is elimiated because the SIR curve itself is a non-linear time series function.\nData\nCOVID-19 data are found from www.canada.ca. This data set includes all informations to generate the SIR curve.\nTweets are downloaded from kaggle. This data set contains all Tweets that are tagged as COVID19 (or its synonyms), including non English ones. Hence, preprocessing is needed. Because this data set is very large (~40M records), it is processed in a distributed environment using Spark.\nResult\nWe use data in March to train the model and the first 4 days in April to evaluate its predictive performance. The predictive power of our model is much better than the baseline model (using SIR only)\n\n\n""], 'url_profile': 'https://github.com/tingwei758', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['rNeighborGWAS\nThis is a developer version of the rNeighborGWAS package. Please see vignette(""rNeighborGWAS"") for usage.\nCRAN version is available at https://cran.r-project.org/package=rNeighborGWAS.\nInstallation\nPlease install the package via GitHub using the devtools library as devtools::install_github(""yassato/rNeighborGWAS"").\nDependency\nNote that the rNeighborGWAS requires the following R packages.\n\ngaston\nparallel\n\nRelease Notes\nversion 1.2.2 (developer version): partial PVEs provided by calc_PVEnei(); nei_lm() added.\nversion 1.2.1: testthat files fixed.\nversion 1.2.0: nei_lmm() and gaston2neiGWAS() added; nei_coval() and neiGWAS() refactored.\nversion 1.0.0: Initial version registered in CRAN.\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CHYaswanth', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'karachi, pakistan', 'stats_list': [], 'contributions': '161 contributions\n        in the last year', 'description': ['TimeSeriesForecasting\nIn this project ""SalesForecasting"", we have forecast sales of different warehouses through ARIMA and Seasonal Arima model. The forecasted sales is of 9 months. There are some screenshots as well which clarifies how well we forcasted the sell.\nForecast of WareHouse #01\n\nForecast of WareHouse #02\n\nForecast of WareHouse #03\n\nForecast of WareHouse #04\n\n'], 'url_profile': 'https://github.com/Affanamin', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['house_rate_prediction\nThis project uses different prediction techniques like linear regression,Decision Tree and Random forest and predict the future rate of the house using the best fitted algorithm having least RMSE error(i.e. random forest).Stratified sampling,Correlation Coefficient, imputer class,Feature scaling, sklearn  and Cross-Validation are also some of the features used in this project\n'], 'url_profile': 'https://github.com/pavass', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '233 contributions\n        in the last year', 'description': [""Employee Absenteesim\nGraduate Course: Intermediate Statistical Modeling for Business\nThis was an individual project from a course on Logistic Regression (a supervised machine learning technique for statistical modeling). Below is the prompt from the client:\n\nHi, I'm Pam Poovey, the director of Human Resources for Ingels Sherman International Shipping. Recently I have been concerned about how often our workers have been taking days off for either personal or family health reasons (i.e. Family and Medical Leave). The company has a very generous leave policy, but I have noticed that many more people are using the leave policy than they used to. I suspect that at least some of the employees have been taking advantage of the company's policy but I would like to see if there is some sort of pattern in what kind of employee is more likely to take days off. Our department has recorded whether or not each employee took a leave day last quarter along with some basic information that we think may be related for each employee. We recorded their currentsalary, whether the employee is full time or part time, age, and whether or not the employee got a raise or promotion in the last five years.\nBecause this project is sensitive, we would like you to be an external consultant to help us analyze this data. Primarily, we would like to know if any of the factors we have access to can help us identify employees who are most likely to use a leave day. Secondarily, we would like to know how accurately the data can be used to predict whether or not an employee is likely to take a leave day during a given three-month period. As you might imagine, we want to avoid talking to employees to further investigate this issue unless we are very confident that they are likely to use the leave policy in three months. For that reason, before we use the model you develop to start investigating possible abuse the policy, is there a way to tune the model so that at most 10% of the people who don't use the leave policy are incorrected predicted by the model to use the leave policy? If the model can be tuned that way, how good will the model be at accurately identifying people who actually > will use the leave policy in three months if we do that?\n\nAnalysis Summary\nI discovered that Salary and whether a raise was given or not to the employee are crucial in identifying employees who are most likely to use a leave day. To a great surprise, employment status and age does not significantly affect the likelihood of the response. It is in contrast to the general belief that part-time employees are more likely to abuse leave policy as they are less loyal/dependent on the company.\nA good estimation of accuracy for prediction based on the data will be 62.4%. Although I can modify the model to reach up to 80% accuracy, it would drastically reduce the percentage of accurately identified employees who actually will use the leave policy. Yes, I can tune the model so that at most 10% of the people who don't use the leave policy are incorrectly predicted by the model to use the leave policy. In that case, the people that will be identified to use policy who actually will use the leave policy in three months will range from 0.0% to 17.5%.\nBesides this, client should keep in mind that response may be affected by the weather and season of that quarter. The tendency to take leave may have a seasonal pattern. Also, data was highly imbalanced which could give a false sense of accuracy. Finally, additional features such as Married or not and Reason for leave may have added more to the analysis.\nStatistical Analysis\nWhen I summarized the given data, I found that the average value of Salary and Age variables in the given data are roughly 47519.64 USD and 35, respectively (see Table 1 to 4). I made a bar graph on the response variable i.e., tookLeave (see Figure 1). I found that 20% of the employees took a leave which is significantly high considering the short period length i.e., three months. In addition, the box plot of age categorized by tookLeave didn’t show any considerable difference between the two categories (see Figure 3). Although the minimum and maximum age is different for both categories, the mean and median are roughly same. Hence, I decided to not include age as a predictor in the model.\nFigure 1: Bar Graph - Frequency vs tookLeave\n\nFigure 3: Box Plot - tookLeave vs Age\n\nFigure 4: Box Plot - tookLeave vs Salary\n\nTable 1 & 2: Numerical Summary of Salary\n\nTable 3 & 4: Numerical Summary of Age\n\nSince the response variable is a qualitative variable and I had to classify whether each employee is likely to take a leave or not, I used Logistic Regression in this case. The model had tookLeave as dependent variable and salary, employmentStatus & raiseOrPromo as independent variables. I found that the model is useful as the ROC curve is always above the diagonal line (random guessing) and AUC value is 0.64 which is greater than 0.5 (see Figure 2). For the model validity, I used Hosmer and Lemeshow Goodness-of-Fit Test (see Table 8) to test the null hypothesis that the logistic model built accurately describes the data at 5% significance level and found that the p-value is 0.7827 which is greater than 0.05. Hence, I fail to reject the null hypothesis i.e., there is no evidence that the model does not fit well. Moreover, I didn’t notice any pattern in Pearson Chi-Square Residuals vs Case Number plot (see Figure 5). Thus, I can say that there is no dependence issue in this model.\nFigure 2: ROC Curve - Sensitivity vs Specificity\n\nTable 8: Hosmer and Lemeshow Goodness-of-Fit test\n\nFigure 5: Pearson Chi-Square Residual vs Case Number Model and Outlier Diagnostics\n\nTable 5 & 6: Analysis of Effects and Analysis of Maximum Likelihood Estimation\n\nTable 7: Classification Table\n\nUsing Wald Chi-squared test (see Table 5 & 6), I revealed that Salary (0.0001 < 0.05) and RaiseorPromo (0.0047 < 0.05) are statistically significant whereas employment status (0.2515 > 0.05) does not significantly affect the likelihood of the response at 5% significance level. incorrectly predicted by the model to use the leave policy. For the model accuracy, I chose pprob cut-off of 0.22 which results in both Specificity (64.2) and Sensitivity (55.3) greater than 50% (see Table 7).\n""], 'url_profile': 'https://github.com/SagarBansal7', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'Winter Springs, FL', 'stats_list': [], 'contributions': '487 contributions\n        in the last year', 'description': ['DataMiningFinalProject\nThe data set PHY_TRAIN used in this project came from 2004 KDD CUP competition.    It is a perfect  data  set  that  can  be  used  to  build  predictive  models  using  tools  including  “logistic regression” (with some interactions), “Gradient Boosting” and “Random Forest”.    To save your time on model development, a file with useful information on all predictor variables is included as well.\n'], 'url_profile': 'https://github.com/darkhark', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['CORONAVIRUS-SPREAD-ANALYSIS-INDIA-\nTHE REPOSITORY CONTAINS OF THE DATA OF THE TOTAL SPREAD OF CORONAVIRUS ACROSS VARIOUS PACES ACROSS INDIA TILL DATE 13 APRIL 2020.A LINEAR REGRESSION MODEL HAS BEEN USED TO DERIVE RELATIONSHIP BETWEEN NUMBER OF CASES,RECOVERED AND DEATHS ,ACCURACY COMES OUT TO BE LESS BECAUSE THE DATA IS NOT UNIFORM\n'], 'url_profile': 'https://github.com/prateekpr', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'Leeds, United Kingdom', 'stats_list': [], 'contributions': '402 contributions\n        in the last year', 'description': ['Education-Funding-Analyser\nTool which is able to predict through a simple Linear Regression algorithm in Python the effects of bigger fundings spent on education in a country. The GUI is implemented using Frontend Tech (HTML5, CSS3, JS + JQuery). The link between Python and JS is realised through the Eel framework.\nFrontend :\n\n[HTML5]\n[CSS3]\n[JS] ( + JQuery )\n\nBackend :\n[Python]\n\n[Eel Framework]\n\nRequired to Run :\n\nLatest version of Python\nThe Eel framework\n\nCheers !\n'], 'url_profile': 'https://github.com/RazvanBerbece', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ratul003', 'info_list': ['Jupyter Notebook', 'Updated Sep 16, 2020', 'R', 'Updated Nov 25, 2020', 'HTML', 'Updated May 31, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', '1', 'SAS', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', '1', 'Python', 'Updated Apr 29, 2020', 'MIT license', 'Updated Apr 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Twitter_Sentimental_Analysis\nImplementation is done in Google Colab.\na)Tweets Pre-processing and Cleaning\n-Data Inspection\n-Data Cleaning\nb)Story Generation and Visualization from Tweets\nc)Extracting Features from Cleaned Tweets\n-Bag-of-Words\n-TF-IDF\n-Word Embeddings\nd)Model Building\n-Logistic Regression\n-Support Vector Machine\n-RandomForest\n-XGBoost\ne)Model Fine-tuning\n'], 'url_profile': 'https://github.com/likhitha-pallerla', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Dhavales', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhsaur', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Bayesian_vs_Frequentist_exercise_example\nCompares Bayesian vs Frequentist models on an exercise dataset obtained by kaggle (https://www.kaggle.com/fmendes/exercise-and-calories). Explains drawbacks and advantages of models and interprets results on data. Performs a t-test on gender and a simple linear regression on exercise duration to predict calories burned.\n'], 'url_profile': 'https://github.com/derpnallday', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Việt nam', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': [""machine learning\nTable of contents\n\nPre-Requisites\nHow to run\nWhat's included\nCreators\n\nPre-Requisites\n\nInstall Anaconda\n\nHow to run\n\nType and run these command\n\n\nconda env create -f env.yml\n\n\nconda activate machine-learning\n\n\njupyter notebook\n\n\nSome data files is too large to be uploaded so get more detail in notebook\n\nWhat's included\nmachine-learning\n├── README.md\n├── asmnt\n│\xa0\xa0 ├── MLP301x\n│\xa0\xa0 │\xa0\xa0 ├── asm1\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── Manipulating the US - Baby Names dataset using pandas.ipynb\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── data\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── desktop.ini\n│\xa0\xa0 │\xa0\xa0 └── asm2\n│\xa0\xa0 │\xa0\xa0     ├── Assignment2\n│\xa0\xa0 │\xa0\xa0     └── desktop.ini\n│\xa0\xa0 ├── MLP302\n│\xa0\xa0 │\xa0\xa0 ├── asm1\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── assigment\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── desktop.ini\n│\xa0\xa0 │\xa0\xa0 └── asm2\n│\xa0\xa0 │\xa0\xa0     ├── assigment\n│\xa0\xa0 │\xa0\xa0     └── desktop.ini\n│\xa0\xa0 ├── MLP303\n│\xa0\xa0 │\xa0\xa0 ├── asm1\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── assigment\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── desktop.ini\n│\xa0\xa0 │\xa0\xa0 └── asm2\n│\xa0\xa0 │\xa0\xa0     ├── assigment\n│\xa0\xa0 │\xa0\xa0     └── desktop.ini\n│\xa0\xa0 ├── MLP304\n│\xa0\xa0 │\xa0\xa0 ├── asm1\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── assigment\n│\xa0\xa0 │\xa0\xa0 └── asm2\n│\xa0\xa0 │\xa0\xa0     └── assigment\n│\xa0\xa0 └── MLP305\n│\xa0\xa0     ├── MLP305x_asm1_anhlvse00534x@funix.edu.vn\n│\xa0\xa0     │\xa0\xa0 ├── assigment\n│\xa0\xa0     │\xa0\xa0 └── desktop.ini\n│\xa0\xa0     ├── MLP305x_asm2_anhlvse00534x@funix.edu.vn\n│\xa0\xa0     │\xa0\xa0 ├── assigment\n│\xa0\xa0     │\xa0\xa0 └── desktop.ini\n│\xa0\xa0     ├── MLP305x_asm3_anhlvse00534x@funix.edu.vn\n│\xa0\xa0     │\xa0\xa0 ├── assigment\n│\xa0\xa0     │\xa0\xa0 └── desktop.ini\n│\xa0\xa0     └── desktop.ini\n├── build.sh\n├── desktop.ini\n├── env.yml\n├── example_project\n│\xa0\xa0 ├── Spotify-Song-Recommendation-ML\n│\xa0\xa0 │\xa0\xa0 ├── EDA.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── GMF.png\n│\xa0\xa0 │\xa0\xa0 ├── Neural-Collaborative-Filtering.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Spark-MLib-ALS.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── SpotifyProject-WriteUp.pdf\n│\xa0\xa0 │\xa0\xa0 ├── Test.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── data\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── mpd.slice.0-999.json\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── mpd.slice.1000-1999.json\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── mpd.slice.2000-2999.json\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── mpd.slice.3000-3999.json\n│\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 ├── restructureData.py\n│\xa0\xa0 │\xa0\xa0 └── src\n│\xa0\xa0 │\xa0\xa0     ├── check.py\n│\xa0\xa0 │\xa0\xa0     ├── deeper_stats.py\n│\xa0\xa0 │\xa0\xa0     ├── descriptions.py\n│\xa0\xa0 │\xa0\xa0     ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0     ├── print.py\n│\xa0\xa0 │\xa0\xa0     ├── show.py\n│\xa0\xa0 │\xa0\xa0     └── stats.py\n│\xa0\xa0 ├── deep-feature-for-images-classification\n│\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 ├── using-sklearn\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── Deep Features for Image Classification.ipynb\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── Deep Features for Image Retrieval.ipynb\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── data\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── desktop.ini\n│\xa0\xa0 │\xa0\xa0 └── using-turicreate\n│\xa0\xa0 │\xa0\xa0     ├── FND06-NB01.ipynb\n│\xa0\xa0 │\xa0\xa0     ├── data\n│\xa0\xa0 │\xa0\xa0     └── desktop.ini\n│\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 ├── document-retrieval\n│\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 ├── using-sklearn\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── Document retrieval.ipynb\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── data\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── desktop.ini\n│\xa0\xa0 │\xa0\xa0 └── using-turicreate\n│\xa0\xa0 │\xa0\xa0     ├── FND04-NB01.ipynb\n│\xa0\xa0 │\xa0\xa0     ├── Untitled.ipynb\n│\xa0\xa0 │\xa0\xa0     ├── data\n│\xa0\xa0 │\xa0\xa0     ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0     └── practice-document-retrieval.ipynb\n│\xa0\xa0 └── song-recommendation-system\n│\xa0\xa0     ├── desktop.ini\n│\xa0\xa0     ├── using-sklearn\n│\xa0\xa0     │\xa0\xa0 ├── Song recommender 1.ipynb\n│\xa0\xa0     │\xa0\xa0 ├── Song recommender new.ipynb\n│\xa0\xa0     │\xa0\xa0 ├── data\n│\xa0\xa0     │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0     │\xa0\xa0 └── practice.ipynb\n│\xa0\xa0     └── using-turicreate\n│\xa0\xa0         ├── FND05-NB01.ipynb\n│\xa0\xa0         ├── Untitled.ipynb\n│\xa0\xa0         ├── data\n│\xa0\xa0         └── desktop.ini\n├── lab_ex\n│\xa0\xa0 ├── classification\n│\xa0\xa0 │\xa0\xa0 ├── Analyzing_product_sentiment.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Analyzing_product_sentiment2.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_10_Exploring_Precision_and_recall.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_11_Training_Logistic_Regression_via_Stochastic_Gradient_Ascent.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_11_Training_Logistic_Regression_via_Stochastic_Gradient_Ascent[Conflict].ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_1_Predicting_sentiment_from_product_reviews.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_2_Logistic_Regression_with_L2_regularization.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_3_Predicting_sentiment_from_product_reviews_with_SVM.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_4_Predicting_sentiment_from_product_reviews_with_Naive_Bayes.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_5_Predicting_sentiment_from_product_reviews_with_Feed_forward_neural_network.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_6_Identifying_safe_loans_with_decision_trees.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_7_Decision_Trees_in_Practice.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_8_Exploring_Ensemble_Methods.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Exercise_9_Identifying_safe_loans_with_Random_Forest.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── FND03-NB01.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Lab_1_Implementing_logistic_regression_from_scratch.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Lab_2_implement_2_layers_neural_network_for_image_classification.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Lab_3_Implementing_binary_decision_trees.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Lab_4_Boosting_a_decision_stump.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── cs231n\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── __init__.py\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── __pycache__\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── classifiers\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── data_utils.py\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── datasets\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── features.py\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── gradient_check.py\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── vis_utils.py\n│\xa0\xa0 │\xa0\xa0 ├── data\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── amazon_baby.csv\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── amazon_baby_subset.csv\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── cs231n.zip\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── dir_archive.ini\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── important_words.json\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── lending-club-data.csv\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── m_bfaa91c17752f745.0000\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── m_bfaa91c17752f745.frame_idx\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── m_bfaa91c17752f745.sidx\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── module-10-assignment-numpy-arrays.npz\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── module-3-assignment-numpy-arrays.npz\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── objects.bin\n│\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 ├── practice-analyzing-sentiment-classification.ipynb\n│\xa0\xa0 │\xa0\xa0 └── utils\n│\xa0\xa0 │\xa0\xa0     ├── __init__.py\n│\xa0\xa0 │\xa0\xa0     ├── __pycache__\n│\xa0\xa0 │\xa0\xa0     └── desktop.ini\n│\xa0\xa0 ├── clustering_retrieval\n│\xa0\xa0 │\xa0\xa0 ├── Fitting Gaussian Mixture Models with EM.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Fitting a diagonal covariance Gaussian mixture model to text data.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Hierarchical_Clustering.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Latent Dirichlet Allocation for Text Data.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Latent Dirichlet Allocation for Text Datablank.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Locality Sensitive Hashing.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Nearest Neighbors.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── Untitled.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 ├── em_utilities.py\n│\xa0\xa0 │\xa0\xa0 ├── ex-DBSCAN.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── images\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── cloudy_sky\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── rivers\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── sunsets\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── trees_and_forest\n│\xa0\xa0 │\xa0\xa0 ├── k-means with text data.ipynb\n│\xa0\xa0 │\xa0\xa0 ├── people_wiki.csv\n│\xa0\xa0 │\xa0\xa0 └── student.gshortcut\n│\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 ├── intro\n│\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 ├── implement-one-dimensional-numpy\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── Implementing_One_Dimensional_Numpy.ipynb\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── desktop.ini\n│\xa0\xa0 │\xa0\xa0 ├── implement-two-dimensional-numpy\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── Implementing_Two_Dimensional_Numpy.ipynb\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0 │\xa0\xa0 └── proof-2019-12-31_20.25.43.mp4\n│\xa0\xa0 │\xa0\xa0 └── outlier-treatment\n│\xa0\xa0 │\xa0\xa0     ├── desktop.ini\n│\xa0\xa0 │\xa0\xa0     ├── outliers_detection.ipynb\n│\xa0\xa0 │\xa0\xa0     └── outliers_treatment_impact.ipynb\n│\xa0\xa0 └── regression\n│\xa0\xa0     ├── Assessing Fit (polynomial regression).ipynb\n│\xa0\xa0     ├── Feature Selection and LASSO (Interpretation).ipynb\n│\xa0\xa0     ├── LASSO (coordinate descent) blank.ipynb\n│\xa0\xa0     ├── LASSO (coordinate descent).ipynb\n│\xa0\xa0     ├── Module+1.ipynb\n│\xa0\xa0     ├── Multiple Regression (Interpretation).ipynb\n│\xa0\xa0     ├── Multiple Regression (gradient descent).ipynb\n│\xa0\xa0     ├── Overfitting_Demo_Ridge_Lasso.ipynb\n│\xa0\xa0     ├── PhillyCrime.ipynb\n│\xa0\xa0     ├── Predicting house prices using k-nearest neighbors regression.ipynb\n│\xa0\xa0     ├── Predicting house prices.ipynb\n│\xa0\xa0     ├── Ridge Regression (gradient descent) blank.ipynb\n│\xa0\xa0     ├── Ridge Regression (gradient descent).ipynb\n│\xa0\xa0     ├── Ridge Regression (interpretation).ipynb\n│\xa0\xa0     ├── Simple Linear Regression.ipynb\n│\xa0\xa0     ├── Untitled1.ipynb\n│\xa0\xa0     ├── data\n│\xa0\xa0     │\xa0\xa0 ├── Philadelphia_Crime_Rate_noNA.csv\n│\xa0\xa0     │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0     │\xa0\xa0 ├── dir_archive.ini\n│\xa0\xa0     │\xa0\xa0 ├── m_1ce96d9d245ca490.0000\n│\xa0\xa0     │\xa0\xa0 ├── m_1ce96d9d245ca490.frame_idx\n│\xa0\xa0     │\xa0\xa0 ├── m_1ce96d9d245ca490.sidx\n│\xa0\xa0     │\xa0\xa0 └── objects.bin\n│\xa0\xa0     ├── desktop.ini\n│\xa0\xa0     ├── house_images\n│\xa0\xa0     │\xa0\xa0 ├── bill_gates.png\n│\xa0\xa0     │\xa0\xa0 ├── desktop.ini\n│\xa0\xa0     │\xa0\xa0 ├── house1.png\n│\xa0\xa0     │\xa0\xa0 └── house2.jpg\n│\xa0\xa0     └── predicting-house-price-using-turicreate.ipynb\n└── tutorial\n    ├── desktop.ini\n    ├── numpy-100-exercise\n    │\xa0\xa0 ├── 100_Numpy_exercises.ipynb\n    │\xa0\xa0 ├── 100_Numpy_exercises_no_solution.ipynb\n    │\xa0\xa0 ├── 100_Numpy_exercises_with_hint.ipynb\n    │\xa0\xa0 ├── LICENSE.txt\n    │\xa0\xa0 ├── desktop.ini\n    │\xa0\xa0 └── requirements.txt\n    └── numpy-tutorial-py3.ipynb\n\n71 directories, 160 files\n\nCreators\n\n\nGithub\n\n\nMake friend FACEBOOK\n\n\nMy LinkIn\n\n\nEnjoy 🤘\n""], 'url_profile': 'https://github.com/leanh153', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'West Lafayette, IN', 'stats_list': [], 'contributions': '518 contributions\n        in the last year', 'description': ['Amazon - Customer and Retail Analytics Solution for ECommerce\nMotivation:\nAmazon sells millions of products through thousands of sellers on its platforms. This portfolio of product varies widely across different product categories and even different times of the year. To boost visibility and popularity of products, Amazon hosts a New Releases ranking of top 100 products of all categories, a list which is updated hourly. To enable sellers insert their products into this list and move them further up the order, it is crucial to gain an understanding of which factors influence their ranks and how.\nBut why Amazon?\nBecause it is the world\'s largest online marketplace, with more than 310 active user base, and sells more than 12 millions products across different categories.\n\n\n\nProject Flow:\n\nObjective\nComputational Steps: data collection | data preparation | sentiment of reviews | modeling\nInsights and Value Proposition\nDetailed overview of steps to follow\n\n1. Objective:\nThe objective is to build statistical model on publicly available data from Amazon so that sellers and ultimately Amazon can leverage our insights to push their products up the trending ladder.\n\n\n\n2. Computational Steps:\nTo examine the impact of various variables on rank of sellers, Amazon - ""Hot New Releases"" is leveraged as the platform to obtain data points for few vital attributes in our solution – rank, prime delivery status, discount offered status, price, top 10 customer reviews, average rating, rating count, number of answered questions, and number of product images. Data is scraped rigourously from Amazon using BeautifulSoup, for seven different categories – Electronics, Computers and Accessories, Clothing, Pet Supplies, Tools and Home Improvement, Toys and Games, and Grocery – that catalyze most of the sales for Amazon.\n\n\n\n\n\n\nThis is followed by necessary data preparation to account for skewness, and a check on correlation amongst variables.\n\n\n\n\n\n\nTo ensure a comprehensive analysis, top 10 customer reviews are collected which captured the overall product sentiment adequately. Thereafter, Microsoft Azure Text Analytics API is used to generate averaged sentiment scores for each product.\n\n\n\nAfter requisite data preparation to account for skewness in certain variables, log-log regression model was deployed to measure elasticity of rank with respect to given variables. We also accounted for synergy effect between variables and included interaction terms. The full model encompassing all product categories was run to give significant variables that affected ranking of products. For a deeper analysis into how different product categories behaved, a subsampled regression analysis for each category was also performed to see how significant the variables were for these categories.\n3. Insigths and Value Proposition:\nThere are multiple parameters which have considerable effect on the ranks of new releases. Also, the extent of impact of different parameters is different across the categories. Amazon and its sellers can benefit from the model by identifying which areas help in increasing the rank of their newly released products. The probability of higher sales increases massively due to higher visibility, if the rank increases. Also, while it is essential that sellers manage to enter this trending list of new releases, they would be able to reap maximum benefit if they move from the rank bucket of 51-100 on second page to the first page of top 50 product releases. We believe our analysis and accompanying insights would help them achieve this critical objective to stay on the trending list for longer, which would result in improved sales for their products, and edge out competing brands\n4. Detailed overview of steps to follow:\n\t\t\t\t\t\tAmazon Hot New Releases   \n\n\n1.) Run the “product and review links.ipynb” file to get the links of individual categories. Make sure to change the link for each category. The category links will be dynamic and updates \n\t    every hour. After exporting the results to CSV file, the product links in the csv file have to be considered only till ""ProducID"". This needs to be done because, the link gets changed \n\t    dynamically every hour and the hyperlink changes. \t\tOutput file obtained: cumulative links.csv\n\n2.) Use the product links and review links obtained from the previous step and run the “Reviews and # of images.ipynb” to scrape the top reviews of each of the product. \n        This file also gives number of images displayed for each product.     Output file obtained: cumulative review.csv\n\n\n3.) Run the “Sentiment Analysis.ipynb” file to get the sentiment score of each review. Azure API is used to get the sentiment score. \tAfter appending the sentiment scores to the file\n        generated in step 2 we obtain cumulative sentiments.csv\n\n4.) Run the “Product Attributes.ipynb” file to get the attributes of each product. Following attributes can be obtained (price, average rating, number of ratings, number of reviews,\n        number of answered questions, prime category, Discount) cumulative sentiments.csv\n\n5.) Use “Data cleaning.ipynb” to merge all the datasets obtained above. output file obtained: modelfile.csv\n\n6.) Use “LinearRegression.R” to get the regression results.\n\nData Dictionary for the csv file : modelfile.csv\nRank -> Rank of the product (for the specific category)\nCategory -> Product Category field\nProduct Link -> Link of the product needed to access the data\nName - > Product Name\nAvg_Rating -> Rating of the product\nReview_link -> Hyperlink \nrating_count -> Number f ratings (in number) given to the product and its different from average rating\nprime -> prime delivery or not\nImages -> Number of images for each product\nsentiment score -> sentiment score extracted from the top 10 reviews\nAnswered Questions - > Number of answered questions present for the product\nPrice -> Price of the product\nDiscount -> Discount (in percentage) offered per product\nCustomer_review_count -> Number of customer reviews for each product\n\nData Dictionary for the csv file : cumulative reviews.csv\nRank -> Rank of the product (for the specific category)\nCategory -> Product Category field\nreview -> review of the product\nsentiment score -> sentiment score extracted from the top 10 reviews\n\nData Dictionary for the csv file : cumulative attributes.csv\nRank -> Rank of the product (for the specific category)\nProduct Link -> Link of the product needed to access the data\nAnswered Questions - > Number of answered questions present for the product\nPrice -> Price of the product\nDiscount -> Discount (in percentage) offered per product\nCustomer_review_count -> Number of customer reviews for each product\nCategory -> Product Category field\n\nData Dictionary for the csv file : cumulative links.csv\nRank -> Rank of the product (for the specific category)\nCategory -> Product Category field\nProduct Link -> Link of the product needed to access the data\nName - > Product Name\nAvg_Rating -> Rating of the product\nReview_link -> Hyperlink \nrating_count -> Number f ratings (in number) given to the product and its different from average rating\nprime -> prime delivery or not\nImages -> Number of images for each product\n\n'], 'url_profile': 'https://github.com/akshay-madar', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['Neural-Networks-from-Scratch\nBuilding Neural Networks from Scratch in raw Python\n'], 'url_profile': 'https://github.com/MrAsimZahid', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['humanpose-rnn-action-recognition\nWe implemented a deep neural network to predict human actions by modifying the gait recognition model by Marian Margeta (https://github.com/marian-margeta/gait-recognition). We extracted the spatial features of the human poses of each frame and extract their temporal features by running them through a Recurrent Neural Network. At last, we classify the human actions with a Logistic Regression Model using the temporal features as input.\nDatasets\n\n\nhttps://www.crcv.ucf.edu/data/UCF_YouTube_Action.php\nUCF11 Dataset (or UCF Youtube Action Dataset): contains 11 action categories: basketball shooting, biking/cycling, diving, golf swinging, horse back riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog.\n\n\nhttps://www.crcv.ucf.edu/data/UCF101.php\nUCF101 Dataset: ""With 13320 videos from 101 action categories, UCF101 gives the largest diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc, it is the most challenging data set to date.""\n\n\nhttp://dipersec.kingston.ac.uk/G3D/G3D.html\nD3G Dataset: G3D dataset contains a range of gaming actions captured with Microsoft Kinect. The Kinect enabled us to record synchronised video, depth and skeleton data. The dataset contains 10 subjects performing 20 gaming actions, from which we selected 7 categories: punch right, punch left, kick right, kick left, wave, flap arms and clap.\n\n\nResults\nWe were able to achieve a 94.87% accuracy on D3G dataset, 74.24% accuracy on UCF11 dataset, and 64.55% accuracy on UCF101 dataset.\nJupyter Notebook\nGo to humanpose-rnn-action-recognition.ipynb for our main works.\nBasic information about architecture\n\nExtract features\nThe dummy code bellow shows how to generate the identification vector form the input data video_frames. For the best results, all frames should include the whole person visible from the profile view. The person should be located approximately in the center of each frame.\n# Initialize computational graphs of both sub-networks\nnet_pose = HumanPoseIRNetwork()\nnet_gait = GaitNetwork(recurrent_unit = \'GRU\', rnn_layers = 2)\n\n# Load pre-trained models\nnet_pose.restore(\'path/to/pose_checkpoint.ckpt\')\nnet_gait.restore(\'path/to/gait_checkpoint.ckpt\')\n\n# Create features from input frames in shape (TIME, HEIGHT, WIDTH, CHANNELS) \nspatial_features = net_pose.feed_forward_features(video_frames)\n\n# Process spatial features and generate identification vector \nidentification_vector = net_gait.feed_forward(spatial_features)\nPre-trained models\nHumanPoseNN: MPII + LSP\nDownload: MPII+LSP.ckpt\nThe checkpoint MPII+LSP.ckpt was trained on images from MPII and LSP database. In the graph below you can see the average distance between predicted and desired joints on a validation set of about 6 000 images.\nGaitNN: H3.6m-GRU-1\nDownload: H3.6m-GRU-1.ckpt\nThe checkpoint H3.6m-GRU-1.ckpt was trained to identify 305 people (100 training, 150 validation, 155 testing) using 10 gait sequences for each person. These sequences catch person in three different covariate conditions: Normal walk, walking with backpack and walking with coating shoes. However, the people on all of these video-sequences wear the same clothing.\nThe name describe used architecture (model-RNNcell-layers), so e.g. checkpoint H3.6m-GRU-1.ckpt should be loaded in this way:\nnet_pose = HumanPoseIRNetwork()\nnet_gait = GaitNetwork(recurrent_unit = \'GRU\', rnn_layers = 1)\n\n# Load pre-trained models\nnet_pose.restore(\'models/MPII+LSP.ckpt\')\nnet_gait.restore(\'models/H3.6m-GRU-1.ckpt\')\nData Preprocessing\n# process video as frames and save them as jpeg\ndef process_video_frames(index, root, file_path):\n    video = cv2.VideoCapture(file_path)\n    i = 0\n    while i < MAX_NUM_FRAME_PER_VIDEO and video.isOpened():\n        ret, frame = video.read()\n        if ret == False:\n            break\n        if not os.path.exists(root + \'/\' + str(index)):\n            os.makedirs(root + \'/\' + str(index))\n        cv2.imwrite(root + \'/\' + str(index) + \'/frame\'+str(i)+\'.jpg\', frame)\n        i+=1\n    video.release()\n    cv2.destroyAllWindows()\n\ndef process_ucf_11_dataset():\n    dataset_dir = os.path.join(UCF_11_DATA_DIR, ""train"")\n    (root, dirs, files) = next(os.walk(os.path.abspath(dataset_dir)))\n    print(""Processing UCF11 dataset containing"", len(dirs), ""classes:"", dirs)\n    for category in dirs:\n        temp_dir = os.path.join(dataset_dir, category)\n        (root, dirs, files) = next(os.walk(os.path.abspath(temp_dir)))\n        print(category, ""contains"", len(dirs), ""subfolders."")\n        for sub_folder in dirs:\n            sub_dir = os.path.join(temp_dir, sub_folder)               \n            (root, dirs, files) = next(os.walk(os.path.abspath(sub_dir)))\n            video_files = [i for i in files if i[-3:]==""mpg""]\n            i = 1\n            for video_file in video_files:\n                video_path = os.path.join(root, video_file) \n                process_video_frames(i, root, video_path)\n                i += 1\n    print(""Processing completed."")\n\n# Only have to run this once\nprocess_ucf_11_dataset()\nLoad Dataset and Extract Features\n# Load dataset. This will take some time for the first loading\nx, y = load_ucf_11_dataset()\nLoad Processed Data from .csv Files\n# Load train and test data from saved csv\ntrain_data = pd.read_csv(\'data/UCF11_train.csv\')\n\nx_train = train_data.drop(columns=\'label\')\nx_train = np.array(x_train)\ny_train = train_data[\'label\']\ny_train = np.array(y_train)\n\ntest_data = pd.read_csv(\'data/UCF11_test.csv\')\n\nx_test = test_data.drop(columns=\'label\')\nx_test = np.array(x_test)\ny_test = test_data[\'label\']\ny_test = np.array(y_test)\nLoad from saved model\n# load the models from disk\nlogreg = joblib.load(UCF_11_LOGREG_MODEL_PATH)\n\n# evaluate logistic regression model\ny_pred_log = logreg.predict(x_test)\nprint(""LogReg accuracy on D3G Dataset:"", metrics.accuracy_score(y_test, y_pred_log))\nVisualisation of Pose Estimation\n# sample frame from G3D dataset\nvisualise_pose_estimation(\'images/example_1.png\', \'images/example_1_pose.jpg\')\nGood example:\nThe pose estimation performs well on G3D dataset leading to a high classification accuracy.\n \nBad examples:\nAs you can see here, the pose estimation does not work as well in UCF11 dataset. This might explain the lower accuracy achieved in the final classification.\n \nThe pose estimation performs even worser when the subject doing the action is not in the center of the frame and when there are multiple subjects in the frame.\n \nPrediction\ndef predict_action(video_path, model):\n    video_frames = read_video_frames(video_path)\n    temporal_features = extract_features(video_frames)\n\n    sample_features = []\n    sample_features.append(temporal_features)\n    sample_x =  np.array(sample_features, dtype=\'float32\')\n\n    # Predict\n    sample_pred = model.predict(sample_x)\n    # change the category here for different datasets\n    print(UCF_11_CATEGORIES[sample_pred[0]]) \n\nsample_video_path = ""samples/test_7.mp4""\npredict_action(sample_video_path, logreg)\nvideo = Video.from_file(sample_video_path)\nvideo\nDiscussion\nOur project was a great example of Transfer Learning. The model trained previously for human gait recognition was extended here to recognize general human actions.\nOne advantage of our model for human action recognition was that it extracted only the spatial and temporal information of the human pose, thus excluding the background noises. It does not work well, as we had observed, on frames where the human subject was away from the center or partially blocked or when there were multiple subjects.\nTo further improve the classification accuracy especially on UCF11 and UCF101 datasets, we have to standardize the video frames so that the subject performing the action is centralized and at a profile view. As we have seen from the visualization of pose estimation, some frames were wrongly estimated leading to the inaccurate results.\nAnother reason that the classification accuracy is higher on D3G might be because that D3G comprises of basic actions while the actions from UCF11 dataset are more complex.\nWebcam.py\nFor future works, the natural extension of this project would be to enable real-time action recognition on video or camera stream. We would also be exploring the possibility of combining the current popular approaches with our model through Ensemble methods to see if better results could be achieved.\ncv2.namedWindow(""preview"")\nvc = cv2.VideoCapture(0)\n\n# Predict action every N frames\nN = 70\nlast_N_frame = []\n\nif vc.isOpened():\n    rval, frame = vc.read()\nelse:\n    rval = False\n\nwhile rval:\n    cv2.imshow(""preview"", frame)\n    rval, frame = vc.read()\n    if len(last_N_frame) > N:\n        temporal = extract_features(last_N_frame)\n        prediction = logistic_model.predict([temporal])\n        print(""Predict Action:"", G3D_CATEGORIES[prediction[0]])\n        last_N_frame.clear()\n    processed = imresize(frame, [299,299])\n    last_N_frame.append(processed)\n    key = cv2.waitKey(20)\n    if key==27:\n        break\n\ncv2.destroyWindow(""preview"")\nCurrently, the real-time action recognition on webcam does not perform as well as on videos.\nApplications\nAmidst the Covid-19 situation, most people are advised to stay home.\nWe intend to make a webcam game based on real-time action recognition, which could encourage people to exercise at home while having fun. The game would ask the players to do a certain sequeunce of actions and score them based on how well they did as captured on camera.\nDuring school closure where home-based learning takes place, teachers can also use this to teach Physical Education by getting students to perform certain actions such as stretching exercises, jumping jacks, running on the spot, etc. The webcam can capture their actions and a rating can be given according to a scoring, turning this into a form of gamification.\nThis can also be used for training specific moves, such as dance or martial arts. This can be set in a self-learning mode where a person can practise certain moves on his/her own repeatedly in order to perfect the action. The webcam captures the move and a scoring is provided for the action as a form of advice on how well the action matches the ideal move.\n'], 'url_profile': 'https://github.com/yuhaozhang94', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': [""SUV-Buyers-Classification-in-Python\nPerformed Classification on non-linearly separable datasets of SUV Buyers.\nModeled all the classification techniques available to find the best algorithm that classifies whether a person will buy a SUV or not.\nUsed k-Fold Validation for all the techniques.\nModel Accuracy on test set:\nLogistic Regression-\n89.00%\nKNN Classifier- 93.00% SVM-\n90.00%\nKernel SVM Classifier-\n93.00%\nNaive Bayer's Classifier-\n90.00%\nDecision Tree Classifier-\n91.00%\nRandom Forest Classifier-\n91.00%\n""], 'url_profile': 'https://github.com/FarheenB', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dimasuwandi', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""In this project, an attempt to make an early detection and analysis of the Alzheimer's disease using the ADNI data and the MMSE test data.\nThe brain MRI images used are obtained from : http://adni.loni.usc.edu/ MMSE Result and Oasis data (in csv format) have been uploaded.\nThe various analysis being performed are found under individual files as follows: 1.imageSegemntation.py - performs segmentation using thersholding of the Brain MRI. 2.MMSE_Analysis - performs multiple regression using MMScore. 3.AnalysisUsingLongitudinalMRI - Performs analysis and predictions using the oasis data. 4.EdgeDetection - Performs edge detection of the Brain MRI for better analysis. 5.knn - Alzheimer's analysis can be achieved by KNN classifying algorithm\n""], 'url_profile': 'https://github.com/nikkithags', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Sandyford, Dublin', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Snowfall-Prediction-using-Machine-Learning\nIn this research, Machine learning algorithms like Long-Short Term Model (LSTM), Decision tree, Random Forest and XG Boost were used as a classifier to improve the accuracy of Snowfall prediction for the region of Boston. The geographical parameters like Humidity, Temperature, Wind-speed, Precipitation, Sea-level, Dew-point and Visibility were used as independent variables. Before the modeling phase, Data lagging was performed for 2 step followed by Exploratory Data Analysis was using techniques like Multiple Linear Regression, Correlation Plot and variable importance plot. Feature Selection was also executed using Logistic Regression and Boruta algorithm. Experimental evaluations resulted in the highest accuracy shown by LSTM with an accuracy of 89.98%. In terms of sensitivity, Random Forest outperformed other classiﬁer models. Whereas, Decision tree and XG Boost resulted well in the overall performance of prediction with respect to other evaluation metrics. The results of this research added to the contribution of the knowledge in weather prediction in the domain of Snowfall for the machine learning industry.\n'], 'url_profile': 'https://github.com/atifferoz', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ratul003', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['DaLiBor_predict2019\nOnline software for species distribution modelling (SDM) especially aiming on bryophytes and lichens in Czechia. Using scripting platform R package caret (Kuhn 2008) software returns two SDMs based on linear regression (GLM) and random forest (ranger) algorithm. Based on models, software returns the prediction map of potential habitat suitability for target species. Predictions are based on environmental data form CzechGrids d-base created by Institute of botany in 2018. There are climatic, topographic, landscape and forest data available. Output maps have cell size of 10 m, covering whole Czechia or its subset. The map is grid/raster in GeoTiff format of coordinate reference system EPSG:32633. Output is ready to be used in GIS software. DaLiBor_predict2019 has online graphical user interface, iterative task communicates with a user via emails providing model evaluations and asking for user inputs in prediction.\nLink for GUI: https://predikce.ibot.cas.cz/\nThe software was developed with financial support of Technology Agency of the Czech Republic (TAČR) project no. TH02030644\n'], 'url_profile': 'https://github.com/manmatej', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shanukarda', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['HousePricePrediction\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.     The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.     The company wants to know:  Which variables are significant in predicting the price of a house, and  How well those variables describe the price of a house.\n'], 'url_profile': 'https://github.com/sakusuma', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Music-Analysis\nABSTRACT\nToday, several music platforms provide thousand of new songs; it becomes ambitious to find highly appealing or like-able songs\nwithin such loads of data. In our project we have considered various attributes like acousticness, danceability, energy,\ninstumentalness, key, loudness, mode, speechiness, valence and artist to find whether the audience will like the song or not.\n“1” is used to denote that the audience will like the song and “0” is used to denote that they won’t like that song.\nWe have built a classifier that could predict whether or not the audience will like the song. Whole data is divided into\ntwo parts. 80% of the data is used as training data and the rest 20% as test data. We have used three algorithms to reach to our\nresult.\n1.Decision Tree\n2. Random Forest\n3. Multiple Linear Regression\nEXPLANATION:\n1.First of all we have to import some python libraries like numpy (for handling numeric data), Pandas(for reading file from\nthe system), matplotlib(for plotting graphs) and sklearn (for using the already made training algorithms).\n2.Read the dataset using pandas libraby.\n3.Define the features and output from the imported dataset.\n\nNow split the dataset into two parts (here we took 80:20) using test_train_split.\n\n5.Now import the required Algorithm (Random Forest ,Multiple Regressor , Decision Tree)\n6.Train the model using the selected algorithm.\n7.Now predict the output using x_test dataset and store it in y_pred.\n8.Compare the y_pred output with the y_testand get the accuracy of the model by using inbuilt accuracy function in sklearn.\n9.Print the score.\n'], 'url_profile': 'https://github.com/riyasri03', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Bangalore,India', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Lalith-1', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ratul003', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Principal-Components-Analysis---PCA-analysis\nPrincipal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation which converts a set of correlated variables to a set of uncorrelated variables. PCA is a most widely used tool in exploratory data analysis and in machine learning for predictive models. Moreover, PCA is an unsupervised statistical technique used to examine the interrelations among a set of variables. It is also known as a general factor analysis where regression determines a line of best fit.a principal component can be defined as a linear combination of optimally-weighted observed variables. The output of PCA are these principal components, the number of which is less than or equal to the number of original variables. Less, in case when we wish to discard or reduce the dimensions in our dataset. The PCs possess some useful properties which are listed below:  The PCs are essentially the linear combinations of the original variables, the weights vector in this combination is actually the eigenvector found which in turn satisfies the principle of least squares. The PCs are orthogonal, as already discussed. The variation present in the PCs decrease as we move from the 1st PC to the last one, hence the importance.\n'], 'url_profile': 'https://github.com/kdmac', 'info_list': ['2', 'Python', 'Updated Apr 16, 2020', '1', 'R', 'Updated Apr 16, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'CC0-1.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'MIT license', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Machine-Learning-for-Thermoelectrics-Discovery\nPublication:-\nhttps://www.cambridge.org/core/journals/data-centric-engineering/article/machine-learning-approaches-to-identify-and-design-low-thermal-conductivity-oxides-for-thermoelectric-applications/7086514CABE816961AA8413206FD6977\nAbstract:-\nTransition metal oxides are attractive materials for high temperature thermoelectric applications due to their thermal stability, low cost bulk processing and natural abundance. Notwithstanding the high power factor, their high thermal conductivity is a roadblock in achieving higher efficiency. The search space for new thermoelectric oxides has been limited to the alloys of a few previously explored systems, such as ZnO, SrTiO3 and CaMnO3. The phenomenon of thermal conduction in crystalline alloys and its dependence on crystal properties is also poorly understood, which limits the ability to design new alloys. In this paper, we apply machine-learning models for discovering novel transition metal oxides with low lattice thermal conductivity (kL). A two-step process is proposed to address the problem of small datasets frequently encountered in materials informatics. First, a gradient boosted tree classifier is learnt to categorize unknown compounds into three categories of thermal conductivity: Low, Medium, and High. In the second step, we fit regression models on the targeted class (i.e. low kL) to estimate kL with an R2 value of 0.96. Gradient boosted tree model was also used to identify key material properties influencing classification of kL, namely lattice energy per atom, atom density, electronic energy band gap, mass density, and ratio of oxygen by transition metal atoms. Only fundamental materials properties describing the crystal symmetry, compound chemistry and interatomic bonding were used in the classification process, which can be readily used as selection parameters. The proposed two-step process addresses the problem of small datasets and improves the predictive accuracy.\n'], 'url_profile': 'https://github.com/Sid-darthvader', 'info_list': ['R', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020']}","{'location': 'Jaipur', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Wrapper-Method-Forward-and-backward-Selection\nWhat is Feature selection? As the name suggests, it is a process of selecting the most significant and relevant features from a vast set of features in the given dataset. For a dataset with d input features, the feature selection process results in k features such that k < d, where k is the smallest set of significant and relevant features. So feature selection helps in finding the smallest set of features which results in Training a machine learning algorithm faster. Reducing the complexity of a model and making it easier to interpret. Building a sensible model with better prediction power. Reducing overfitting by selecting the right set of features. Feature selection methods For a dataset with d features, if we apply hit and trial method with all possible combinations of features then total 2^d — 1 models need to be evaluated for a significant set of features. It is a time-consuming approach, therefore, we use feature selection techniques to find out the smallest set of features more efficiently. There are three types of feature selection techniques : Filter methods Wrapper methods Embedded methods Difference between Filter, Wrapper and Embedded methods  Filter vs. Wrapper vs. Embedded methods In this post, we will only discuss feature selection using Wrapper methods in Python. Wrapper methods In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The evaluation criterion is simply the performance measure which depends on the type of problem, for eg. for regression evaluation criterion can be p-values, R-squared, Adjusted R-squared, similarly for classification the evaluation criterion can be accuracy, precision, recall, f1-score, etc. Finally, it selects the combination of features that gives the optimal results for the specified machine learning algorithm.  Flow chart — Wrapper methods Most commonly used techniques under wrapper methods are: Forward selection Backward elimination Bi-directional elimination(Stepwise Selection)\n'], 'url_profile': 'https://github.com/NEERAJSHARMABSDU', 'info_list': ['R', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/askme01', 'info_list': ['R', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', 'Updated Apr 14, 2020']}",,,,,,,
