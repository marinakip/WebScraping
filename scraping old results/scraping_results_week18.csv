"{'location': 'Uppsala, Sweden.', 'stats_list': [], 'contributions': '298 contributions\n        in the last year', 'description': ['ebms_regression\n\nOfficial implementation (PyTorch) of the papers:\n\n\nEnergy-Based Models for Deep Probabilistic Regression, ECCV 2020 [arXiv] [project]. \nFredrik K. Gustafsson, Martin Danelljan, Goutam Bhat, Thomas B. Schön. \nWe propose a general and conceptually simple regression method with a clear probabilistic interpretation. We create an energy-based model of the conditional target density p(y|x), using a deep neural network to predict the un-normalized density from the input-target pair (x,y). This model of p(y|x) is trained by directly minimizing the associated negative log-likelihood, approximated using Monte Carlo sampling. Notably, our model achieves a 2.2% AP improvement over Faster-RCNN for object detection on the COCO dataset, and sets a new state-of-the-art on visual tracking when applied for bounding box regression.\n\n\nHow to Train Your Energy-Based Model for Regression, BMVC 2020 [arXiv] [project]. \nFredrik K. Gustafsson, Martin Danelljan, Radu Timofte, Thomas B. Schön. \nWe propose a simple yet highly effective extension of noise contrastive estimation (NCE) to train energy-based models p(y|x; theta) for regression tasks. Our proposed method NCE+ can be understood as a direct generalization of NCE, accounting for noise in the annotation process of real-world datasets. We provide a detailed comparison of NCE+ and six popular methods from literature, the results of which suggest that NCE+ should be considered the go-to training method. We also apply NCE+ to the task of visual tracking, achieving state-of-the-art performance on five commonly used datasets. Notably, our tracker achieves 63.7% AUC on LaSOT and 78.7% Success on TrackingNet.\n\n\nThis repository contains code for the experiments on object detection, age estimation (TODO!), head-pose estimation (TODO!) and 1D regression. Code for the visual tracking experiments is available at pytracking.\nIf you find this work useful, please consider citing:\n@inproceedings{gustafsson2020energy,\n  author={Gustafsson, Fredrik K and Danelljan, Martin and Bhat, Goutam and Sch{\\""o}n, Thomas B},\n  title = {Energy-Based Models for Deep Probabilistic Regression},\n  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n  month = {August},\n  year = {2020}\n}\n\n@inproceedings{gustafsson2020train,\n  author={Gustafsson, Fredrik K and Danelljan, Martin and Timofte, Radu and Sch{\\""o}n, Thomas B},\n  title = {How to Train Your Energy-Based Model for Regression},\n  booktitle = {Proceedings of the British Machine Vision Conference (BMVC)},\n  month = {September},\n  year = {2020}\n}\n\nAcknowledgements\n\nThe object detection code is based on maskrcnn-benchmark by @facebookresearch.\nThe object detection code utilizes PreciseRoIPooling by @vacancy.\n\nIndex\n\nUsage\n\n\n1D Regression\n\n\n\n\nObject Detection\n\n\nDocumentation\n\n\n1D Regression\n\n\n\n\nObject Detection\n\n\nPretrained model\n\n\n\n\n\n\n\nUsage\nThe code has been tested on Ubuntu 16.04. A docker image is provided (see below).\n\n1D Regression\nObject Detection\n\n1dregression\n\n$ docker pull fregu856/ebms_regression:ufoym_deepo_pytorch-py36-cu90_ebms_regression\nCreate start_docker_image_ebms_regression.sh containing (My username on the server is fregu482, i.e., my home folder is /home/fregu482. You will have to modify this accordingly):\n\n#!/bin/bash\n\n# DEFAULT VALUES\nGPUIDS=""0""\nNAME=""ebms_regression_GPU""\n\nNV_GPU=""$GPUIDS"" nvidia-docker run -it --rm --shm-size 12G \\\n        -p 7200:7200\\\n        --name ""$NAME""""0"" \\\n        -v /home/fregu482:/root/ \\\n        fregu856/ebms_regression:ufoym_deepo_pytorch-py36-cu90_ebms_regression bash\n\n\n(Inside the image, /root/ will now be mapped to /home/fregu482, i.e., $ cd -- takes you to the regular home folder)\n(To create more containers, change the lines GPUIDS=""0"", --name ""$NAME""""0"" and -p 7200:7200)\nGeneral Docker usage:\n\n\nTo start the image:\n\n\n\n\n\n\n$ sh start_docker_image_ebms_regression.sh\n\n\n\n\n\n\nTo commit changes to the image:\n\n\n\n\n\n\nOpen a new terminal window.\n\n\n\n\n\n\n\n\n$ docker commit ebms_regression_GPU0 fregu856/ebms_regression:ufoym_deepo_pytorch-py36-cu90_ebms_regression\n\n\n\n\n\n\nTo exit the image without killing running code:\n\n\n\n\n\n\nCtrl + P + Q\n\n\n\n\n\n\nTo get back into a running image:\n\n\n\n\n\n\n$ docker attach ebms_regression_GPU0\n\n\n\n\n\n\n\n\n\n\nExample usage:\n\n$ sh start_docker_image_ebms_regression.sh\n$ cd --\n$ python ebms_regression/1dregression/1/nce+_train.py \n\n\n\n\ndetection\n\n$ docker pull fregu856/ebms_regression:ufoym_deepo_pytorch-py36-cu90_ebms_regression\nCreate start_docker_image_ebms_regression.sh containing (My username on the server is fregu482, i.e., my home folder is /home/fregu482. You will have to modify this accordingly):\n\n#!/bin/bash\n\n# DEFAULT VALUES\nGPUIDS=""0""\nNAME=""ebms_regression_GPU""\n\nNV_GPU=""$GPUIDS"" nvidia-docker run -it --rm --shm-size 12G \\\n        -p 7200:7200\\\n        --name ""$NAME""""0"" \\\n        -v /home/fregu482:/root/ \\\n        fregu856/ebms_regression:ufoym_deepo_pytorch-py36-cu90_ebms_regression bash\n\n\n(Inside the image, /root/ will now be mapped to /home/fregu482, i.e., $ cd -- takes you to the regular home folder)\n(To create more containers, change the lines GPUIDS=""0"", --name ""$NAME""""0"" and -p 7200:7200)\nGeneral Docker usage:\n\n\nTo start the image:\n\n\n\n\n\n\n$ sh start_docker_image_ebms_regression.sh\n\n\n\n\n\n\nTo commit changes to the image:\n\n\n\n\n\n\nOpen a new terminal window.\n\n\n\n\n\n\n\n\n$ docker commit ebms_regression_GPU0 fregu856/ebms_regression:ufoym_deepo_pytorch-py36-cu90_ebms_regression\n\n\n\n\n\n\nTo exit the image without killing running code:\n\n\n\n\n\n\nCtrl + P + Q\n\n\n\n\n\n\nTo get back into a running image:\n\n\n\n\n\n\n$ docker attach ebms_regression_GPU0\n\n\n\n\n\n\n\n\n\n\n$ docker attach ebms_regression_GPU0\n$ cd ebms_regression\n$ git clone https://github.com/cocodataset/cocoapi.git\n$ cd cocoapi/PythonAPI\n$ python setup.py build_ext install\n$ cd ebms_regression\n$ git clone https://github.com/NVIDIA/apex.git\n$ cd apex\n$ python setup.py install --cuda_ext --cpp_ext\n$ cd ebms_regression/detection\n$ python setup.py build develop\nCtrl + P + Q\n$ docker commit ebms_regression_GPU0 fregu856/ebms_regression:ufoym_deepo_pytorch-py36-cu90_ebms_regression\n\n\n\n\n\n\nDownload the code from https://github.com/vacancy/PreciseRoIPooling and place in ebms_regression/detection/external/PreciseROIPooling.\n\n\n\n\n\n\nDownload the COCO dataset:\n\n\n$ docker attach ebms_regression_GPU0\n\n\n\n\n$ cd ebms_regression/detection/datasets/coco\n\n\n\n\nDownload annotations_trainval2017.zip ($ wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip) and unzip ($ unzip annotations_trainval2017.zip).\n\n\n\n\nDownload train2017.zip ($ wget http://images.cocodataset.org/zips/train2017.zip) and unzip ($ unzip train2017.zip).\n\n\n\n\nDownload val2017.zip ($ wget http://images.cocodataset.org/zips/val2017.zip) and unzip ($ unzip val2017.zip).\n\n\n\n\nDownload test2017.zip ($ wget http://images.cocodataset.org/zips/test2017.zip) and unzip ($ unzip test2017.zip).\n\n\n\n\nDownload image_info_test2017.zip ($ wget http://images.cocodataset.org/annotations/image_info_test2017.zip) and unzip ($ unzip image_info_test2017.zip).\n\n\n\n\n(detection/datasets/coco should now contain the folders annotations, train2017, val2017 and test2017)\n\n\n\n\n\n\n\n\nDownload a pretrained Faster-RCNN detector (e2e_faster_R-50-FPN_1x.pkl) from https://drive.google.com/open?id=1Ows6VAPH5i5Y-gL9uHDa1SASZ0WQmxOj and place in detection/pretrained_models.\n\n\n\n\n\n\nExample usage:\n\n$ sh start_docker_image_ebms_regression.sh\n$ cd --\n$ cd ebms_regression/detection\n$ python tools/train_net.py --config-file ""configs/nce+_train.yaml""\n$ python tools/test_net.py --config-file ""configs/nce+_eval_val.yaml""\n$ python tools/test_net.py --config-file ""configs/nce+_eval_test-dev.yaml""\n\n\n\n\n\n\n\nDocumentation\n\n1D Regression\nObject Detection\n\nDocumentation/1dregression\n\nExample usage:\n\n$ sh start_docker_image_ebms_regression.sh\n$ cd --\n$ python ebms_regression/1dregression/1/nce+_train.py \n\n\n1dregression/1 contains all code for the first dataset, 1dregression/2 all code for the second dataset.\n\n\n\n\n\n1dregression/1/model.py: Definition of the feed-forward DNN f_\\theta(x, y). Identical to 1dregression/2/model.py.\n1dregression/{1, 2}/datasets.py: Definition of the {first, second} dataset.\n1dregression/{1, 2}/{{mlis, mlmcmcL16, kldis, nce, sm, dsm, nce+}}_train.py: Train 20 models on the {first, second} dataset using {{ML-IS, ML-MCMC-16, KLD-IS, NCE, SM, DSM, NCE+}}.\n1dregression/{1, 2}/{{mlis, mlmcmcL16, kldis, nce, sm, dsm, nce}}_eval.py: Evaluate the KL divergence to the true p(y | x) for all 20 trained models, compute the mean for the 5 best models.\n1dregression/{1, 2}/{{mlis, mlmcmcL16, kldis, nce, sm, dsm, nce}}_viz.py: Visualize p(y | x; \\theta) for one of the 20 trained models (example plot for NCE+).\n\n\n\n\nDocumentation/detection\n\nExample usage:\n\n$ sh start_docker_image_ebms_regression.sh\n$ cd --\n$ cd ebms_regression/detection\n$ python tools/train_net.py --config-file ""configs/nce+_train.yaml""\n$ python tools/test_net.py --config-file ""configs/nce+_eval_val.yaml""\n$ python tools/test_net.py --config-file ""configs/nce+_eval_test-dev.yaml""\n\n\ndetection/configs contains all config files needed to train a model using ML-IS, ML-MCMC-8, KLD-IS, NCE, DSM or NCE+. It also contains all config files needed to evaluate such a trained model on 2017 val or 2017 test-dev.\ndetection/maskrcnn_benchmark/modeling/roi_heads/iou_head/iou_head.py: Definition of the training and prediction procedures.\ndetection/maskrcnn_benchmark/modeling/roi_heads/iou_head/loss.py: Definition of the loss for all training methods.\n\n\n\n\n\n\n\nPretrained model\n\nObject detection model trained with NCE+ on 2017 train: https://drive.google.com/open?id=1F7QoRJpbWQOWprD1Zefp050Ua8MYVFLW.\n\n\n\n\n\nEvaluate pretrained model on 2017 val:\n\n\nDownload the file nce+_model_0060000.pth from above and place in detection/pretrained_models.\n\n\n\n\n$ sh start_docker_image_ebms_regression.sh\n\n\n\n\n$ cd --\n\n\n\n\n$ cd ebms_regression/detection\n\n\n\n\n$ python tools/test_net.py --config-file ""configs/nce+_eval_pretrained_val.yaml""\n\n\n\n\nExpected output:\n\n\n\nAP, AP50, AP75, APs, APm, APl\n0.3936, 0.5799, 0.4263, 0.2220, 0.4257, 0.5188\n\n\n\n\n\nEvaluate pretrained model on 2017 test-dev:\n\n\nDownload the file nce+_model_0060000.pth from above and place in detection/pretrained_models.\n\n\n\n\n$ sh start_docker_image_ebms_regression.sh\n\n\n\n\n$ cd --\n\n\n\n\n$ cd ebms_regression/detection\n\n\n\n\n$ python tools/test_net.py --config-file ""configs/nce+_eval_pretrained_test-dev.yaml""\n\n\n\n\nDownload the file detection/checkpoints/nce+_eval_pretrained_test-dev/inference/coco_2017_test-dev/bbox.json (105.2 MB).\n\n\n\n\nRename this file to detections_test-dev2017_nce+_pretrained_results.json.\n\n\n\n\nCompress this file to create detections_test-dev2017_nce+_pretrained_results.zip.\n\n\n\n\nGo to https://competitions.codalab.org/competitions/20794. Click ""Participate"". Mark ""test-dev2019 (bbox)"". Choose a team name. Method name: nce+_pretrained. Upload the zip file (nothing happens for 1-2 mins after you upload the zip file, but then it appears in the table).\n\n\n\n\nWait for the evaluation to complete on the server (click on ""Refresh status"" until the status is ""Finished"").\n\n\n\n\nClick on ""Download output from scoring step"".\n\n\n\n\nscores.txt in the downloaded output_file.zip contains the results.\n\n\n\n\nExpected output:\n\n\n\nAP: 0.397\nAP_50: 0.587\nAP_75: 0.427\nAP_small: 0.221\nAP_medium: 0.420\nAP_large: 0.505\nAR_max_1: 0.331\nAR_max_10: 0.534\nAR_max_100: 0.564\nAR_small: 0.353\nAR_medium: 0.597\nAR_large: 0.717\n\n'], 'url_profile': 'https://github.com/fregu856', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '383 contributions\n        in the last year', 'description': ['Develop-Multi-Output-Regression-Models\n'], 'url_profile': 'https://github.com/krishnaik06', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': [""Countries-GDP-prediction \nData Source\nWe have 'Countries of The World' data set\n(from kaggle: Fernando Lasso: https://www.kaggle.com/fernandol/countries-of-the-world).\nData Description\nThis dataset have each country as a data point (227 countries in total), and for each, we have 20 columns, each column represents a different aspect or measure of the specific country.\nProject Goal\nThe goal of the project is to understand this dataset, get some insights from it, and finally to train a model that can predict GDP per capita for each country.\n\nConclusion\n4 different learning regressors (Linear Regression, SVM, Random Forest, and Gradient Boosting) were tested, and we have achieved the best prediction performance using Random Forest, followed by Gradient Boosting, then Linear Regression, while SVM achieved the worst performance of the four.\nThe best prediction performance was achieved with a Random Forest regressor, using all features in the dataset, and resulted in the following metrics:\n\nMean Absolute Error (MAE): 2142.13\nRoot mean squared error (RMSE): 3097.19\nR-squared Score (R2_Score): 0.8839\n\n(gdp_per_capita values in the dataset ranges from 500 to 55100 USD).\n\nDeployment\nI have created a web app using streamlit library to deploy the final model. In the app, the user can input the different attributes of a certain country, and click a button in order to get the estimated GDP per capita for that country.\nplease feel free to  try my app here, and send me an email if you have any questions or feedback.\n""], 'url_profile': 'https://github.com/zeglam', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '368 contributions\n        in the last year', 'description': ['LSTM-SVM-RF-time-series\nRegression prediction of time series data using LSTM, SVM and random forest. 使用LSTM、SVM、随机森林对时间序列数据进行回归预测，注释拉满。\n'], 'url_profile': 'https://github.com/stxupengyu', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Robyn MMM Open Source Project 2.0\n2021-03-03\nHow To Start\nThere are three .R script files:\n\nfb_robyn.exec.R # you only need this script to execute, it calls the\nother 2 scripts\nfb_robyn.func.R # this contains feature engineering, modelling functions and plotting\nfb_robyn.optm.R # this contains the budget allocator and plotting\n\nTwo .csv files:\n\nde_simulated_data.csv # this is our simulated data set\ngenerated_holidays.csv # this contains holidays of all countries from the\nlibrary prophet\n\nAll files should be placed in the same folder\nPlease make sure you've installed all library specified in\nfb_robyn.exec.R first\nTest run:\nAfter installing all libraries, if you select all and run in\nfb_robyn.exec.R, the script should run through and save some plots on your selected folder\nUsage Guidelines\n\nLatest script usage guideline: Please see comments in scripts within the source code in fb_robyn.exec.R\nGuidelines on the website to be updated soon: https://facebookexperimental.github.io/Robyn/docs/step-by-step-guide\n\nJoin the FB Robyn MMM community. Coming soon\nFB Contact\n\ngufeng@fb.com, Gufeng Zhou, Marketing Science Partner\nleonelsentana@fb.com, Leonel Sentana, Marketing Science Partner\naprada@fb.com, Antonio Prada, Marketing Science Partner\nigorskokan@fb.com, Igor Skokan, Marketing Science Partner\n\nSee the CONTRIBUTING file for how to help out.\nLicense\nFB Robyn MMM R script is MIT licensed, as found in the LICENSE file.\n\nTerms of Use - https://opensource.facebook.com/legal/terms\nPrivacy Policy - https://opensource.facebook.com/legal/privacy\n\n""], 'url_profile': 'https://github.com/facebookexperimental', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}","{'location': 'Surat, India', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': [""Cricket-Chirps-Prediction\nUsing Linear Regression, Decision Tree and Random Forest Algorithm\nA Machine Learning project to count the number of times an insect called 'Cricket' chirps in a specified temprature.\nIn this i have implemented three regression algorithm Linear Regression, Decision tree and Random Forest.\nWHY DO CRICKETS CHIRP?\nSo, how and why do crickets make that chirping sound, anyway? Chirping is a cricket’s way of communicating. Male crickets use chirping to attract females, scare off other males, or warn of danger.\nContrary to popular belief, crickets do not use their legs to chirp! In fact, crickets produce the iconic sound by rubbing the edges of their wings together. The male cricket rubs a scraper (a sharp ridge on his wing) against a series of wrinkles, or “files”, on the other wing. The tone of the chirping depends upon the distance between the wrinkles.\nThere are several reasons why crickets chirp. They may be:\nCalling to attract a female with a a loud and monotonous sound\nCourting a nearby female with a quick, softer chirp\nBehaving aggressively during the encounter of two males\nSounding a danger alert when sensing trouble\nCrickets are part of the family Orthoptera (grasshoppers and katydids).\n\nRun\nOpen Anaconda Prompt\nGo to the project directory\nwrite python app.py\n\nProject\n\nPrediction\n\nJust follow☝️ me and Star⭐ my repository\n""], 'url_profile': 'https://github.com/Snehal-Singh174', 'info_list': ['43', 'Jupyter Notebook', 'MIT license', 'Updated Aug 14, 2020', '10', 'Jupyter Notebook', 'Updated May 1, 2020', '13', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '19', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '48', 'R', 'MIT license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'HTML', 'Updated Oct 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['ggregplt\nThe package, ggregplt, uses ggplot and plotly to make interactive regression plots, diagnostic plots, and correlation matrices for regression.\nThis package can be downloaded using the following steps:\nlibrary(devtools)\ninstall_github(""jesprabens/ggregplt"")\n\nlibrary(ggregplt)\n\nint_reg - Interactive Regression Plot\nThis function allows the user to input a regression model and get an itneractive regression plot returned.  Moreover, if a simple linear model is plotted this function can also take in an x value to produce a predited point on the plot.  Multiple linear regression models can also be plotted, but not with an inputted x.\nExample: int_reg\nregression <- lm(Sepal.Width~Sepal.Length, data = iris)\n\nint_reg(mod = regression, x = 3)\n\nThis will prodcue an interactive plot with a predicted value of Sepal.Width when Sepal.Length is 5.\nVIF_table\nThis function  allows the user to plug in a multiple regression equation along with its\' data set and gives the corresponding VIF values with the correlation matrix in order to diagnose multicollinearity\nExample: VIF_table\nequation <- lm(Sepal.Width~Sepal.Length+Petal.Length, data = iris)\nVIF_Table(equation, 3)\n\nThis example will produce a dataframe of the VIF values of the predictor variables predicting Sepal.Width based on Sepal.Length, and Petal.Length from. the iris dataset and a note telling the user whether or not the VIF values are below their given threshold.\nMR - Multiple Regression\nThis function  allows the user to plug in a multiple regression equation and returns a corresponding interactive correlation matrix in order to diagnose which variables are causing multicollinearity.\nExample: MR\nequation <- lm(Sepal.Width~Sepal.Length+Petal.Length, data = iris)\nMR(iris, equation)\n\nThis example will produce an interactive correlation matrix of the predictor variables predicting Sepal.Width based on Sepal.Length, and Petal.Length from. the iris dataset.\n'], 'url_profile': 'https://github.com/jesprabens', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['PythonML-Logistic-Regression\nIGNORE THE LINEAR REGRESSION FILES.\nThe first file is the famous Titanic project. Here I have used logistic regression for analyzing and predicting the patterns for survival during the Titanic disaster.\nThe second file is data of an advertising company where the data consists of parameters by which we can decide whether the user clicks on the ad or not.\n'], 'url_profile': 'https://github.com/shaurya172', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Symbolic Regression from Scratch with Python\nThis repository contains Python code to accompany the article Symbolic Regression from Scratch with Python.\nThe code implements a basic symbolic regression system which uses genetic programming to\nfind a program that describes the relationship between some features and a target variable.\nMore specifically, this code uses the ""Auto MPG Data Set"" from UCI\nto find a relationship between vehicle characteristics and mpg (miles per gallon).\nDepedencies\n\nPandas\nauto-mpg.data\n\nUsage\npython symbolic_regression.py\nSetup/Installation\npip install python\nwget http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\n\nDevelopment and Contributing\nIf you\'d like to report an issue or bug, suggest improvements, or contribute code to this project, please refer to CONTRIBUTING.md.\nCode of Conduct\nThis project has adopted the Contributor Covenant for its Code of Conduct.\nSee CODE_OF_CONDUCT.md to read it in full.\nLicense\nLicensed under the Apache License 2.0.\nSee LICENSE to read it in full.\n'], 'url_profile': 'https://github.com/datarobot-community', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Annecy, France', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': [""Sparse robust linear regression with Huber's criterion in python\nThis code is an illustration of the use of Huber's criterion for various tasks. It consists in a toolbox provided in association with the paper:\nBlock-wise Minimization-Majorization Algorithm for Huber's Criterion: Sparse Learning and Applications, Esa Ollila and Ammar Mian\nSubmitted to MLSP 2020 conference.\n\nIt also helps with reproducibility of the results presented in the paper. It provides both matlab and python codes.\nWARNING: Python version is still under debug and isn't as trustworthy as the matlab one yet but is being worked on. The results in the paper have been obtained using the matlab version.\nFiles' organization\nThe repository is decomposed into two subdirectories:\n\nmatlab/ which contains the matlab code. To reproduce the results presented in the paper, please run:\n\nSimulation_1_Regression_example.m\nSimulation_1_Image_denoising_example.m\n\n\npython/ which contains the python code.\n\nThe main functions to execute Huber regression with the MM-framework are in the package mmhuber/.\nSome examples of it are provided in the form of Jupyter notebooks in the subfolder notebooks/.\n\n\n\nAuthors\nThe folder was created by:\n\nAmmar Mian, Postdoctoral researcher at Aalto University in the Department of Signal Processing and Acoustics.\n\nContact: ammar.mian@aalto.fi\nWeb: https://ammarmian.github.io/\n\n\nEsa Ollila, Professor at Aalto University in the Department of Signal Processing and Acoustics\n\nContact: esa.ollila@aalto.fi\nWeb: http://users.spa.aalto.fi/esollila/\n\n\n\n""], 'url_profile': 'https://github.com/AmmarMian', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sarathsurpur', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'MIT license', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '3', 'Jupyter Notebook', 'Updated May 3, 2020', '3', 'Python', 'Apache-2.0 license', 'Updated May 1, 2020', '3', 'MATLAB', 'Updated Aug 29, 2020', '2', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'Canada', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['This repository contains Python codes for Forecast of urban water consumption under the impact of climate change research. Bayesian statistics and clustering techniques for predictions of long-term urban water consumption are described.\nYou can reach me on LinkedIn\n'], 'url_profile': 'https://github.com/NioushaR', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Kolkata, India', 'stats_list': [], 'contributions': '240 contributions\n        in the last year', 'description': ['LinearRegressionDeploy\nDeploying a simple linear regression app built using Flask on Heroku. Available at https://linregdeploy.herokuapp.com/ .\n'], 'url_profile': 'https://github.com/Anpr1211', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-Real-estate-price-prediction\nMultiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable.\nInstallation\nRequiremenmts\n\nPython 3.3+ or Python 2.7\nscikit learn\nnumpy\npandas\nmatplotlib\n\nInstall the libraries from the pypi.org with the following commands.\npip install scikit-learn\npip install numpy\npip install pandas\npip install matplotlib\n\nPlease do refer stackoverflow for any errors during installation.\n'], 'url_profile': 'https://github.com/dharineeshramtp2000', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Nurmijärvi', 'stats_list': [], 'contributions': '2,372 contributions\n        in the last year', 'description': ['Logistic regression\n\n\nPackage author: Jukka Aho (@ahojukka5, ahojukka5@gmail.com)\nLogistic regression model\n'], 'url_profile': 'https://github.com/ahojukka5', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alimdsaif3', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Linear Regression Gradient descent\nThere is an example of simple gradient descent algorithm for the linear regression task\nDependencies\n\nPython ~ 3.7.6\npandas\nnumpy\nmatplotlib\nsklearn\njupyter (if you want use jupyter notebook)\n\nWhat you can do with this repository?\n\nLearn how to make an GD algorithm in python and use it on real dataset\nFind an example of sklearn Linear Regression\n\nHow to use this\nJupyter notebook\n\nInstall packages from the list above\nOpen linear_regression_algo.ipynb in jupiter notebook\n\nOR\nPython\n\nInstall packages from the list above\nRun linear_regression_algo.py\nIt reads data from dataset folder home_data.csv. You can change behaviour in datasets.py\nАfter completion of the algorithm you will see all epoch on plots\n\nExample dataset\nThe data set for an example is taken from here.\nOutput\n\n'], 'url_profile': 'https://github.com/Zaroymi', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Building a predictive model of stress vulnerability using sleepEEG features\nQuestion/Science Problem\nCan we use pre-stress sleep features to predict vulnerability to stress? Can sleep disturbances, prior to stress exposure, be predictors to future stress susceptibility and poor psychiatric outcomes?\nHere, we deployed Logistic Regression/SVC/RidgeClassifier models + Feature Engineering to deal with Multicolinearity to answer this question\nABSTRACT\nThere is a tight association between mood and sleep as disrupted sleep is a core feature of many mood disorders.\nThere is a paucity in available animal models for investigating the role of sleep in the etiopathogenesis of depression.\nTherefore, we aimed to assess whether prior sleep disturbances can predict susceptibility to future stress using the chronic social defeat (CSD) stress paradigm.\nPre-CSD, mice susceptible to stress displayed increased fragmentation of Non-Rapid Eye Movement (NREM) sleep, due to increased switching between NREM and wake and a higher percentage of NREM bouts with short duration, relative to resilient mice. The pre-CSD sleep features from both phenotypes were separable enough to allow prediction of susceptibility to stress with around 80% accuracy. Post-CSD, susceptible mice maintained the enhanced NREM fragmentation while resilient mice exhibited high NREM fragmentation, only in the dark. Our findings emphasized the putative role sleep plays in signaling vulnerability to stress.\nData Struture\nAfter scoring the vigilance states of 7 Susceptible and 7 Resilient mice (Balanced Classification Dataset) pre-exposure to chronic stress, 24 sleep features were extracted prior to exposure to stress:\nIt is worth mentioning that C57/B6J mice display a fragmented sleep pattern: they sleep in bouts, they spend around 60% of the time during the light cycle in sleep state versus 40% in the dark cycle as they are nocturnal and more active during the dark. Therefore, we extracted sleep EEG features in both Light and Dark phase. In rodents, such as mice and rats, sleep is made of two states: NREM and REM sleep.\n'], 'url_profile': 'https://github.com/basmarad', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['Machine-Learning-Regression\nRegression Model Implementation with sklearn\n\n\nSVR with gaussian RBF kernel implementation\n\n\nDecision Tree Regression\n\n\nRandom Forest Regression\n\n\nNot upload linear, multiple, and polynomial regression currently in this repository\n\n\n'], 'url_profile': 'https://github.com/hamdaankhalid', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': [""PythonML -  Linear Regression\nThis is a basic Linear Regression Exploratory data analysis exercise.\nWe are given a E-commerece company data and we need to predict what's good for the company, should they invest more time and resources in developing the app or the website?\nSuch questions are to be answered via this Predictive Data Analysis exercise.\nCredits- Jose Portilla,Udemy.\n""], 'url_profile': 'https://github.com/shaurya172', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Boston-Housing-Prices-Multiple-Regression\nGoing back to basics and trying to fit the famous Boston Housing Prices with our well known Multiple Regression.\nInstallation\nRequiremenmts\n\nPython 3.3+ or Python 2.7\nscikit learn\nnumpy\npandas\nmatplotlib\n\nInstall the libraries from the pypi.org with the following commands.\npip install scikit-learn\npip install numpy\npip install pandas\npip install matplotlib\n\nPlease do refer stackoverflow for any errors during installation.\n'], 'url_profile': 'https://github.com/dharineeshramtp2000', 'info_list': ['5', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Python', 'Updated May 6, 2020', '4', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Julia', 'MIT license', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 29, 2020', '2', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Oct 20, 2020', '2', 'Python', 'Updated May 2, 2020', '4', 'Jupyter Notebook', 'Updated May 1, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['regression\nRégression Logistique\n'], 'url_profile': 'https://github.com/sjaubert', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'Thiruvalla, Kerala', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinur1992', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'Daegu', 'stats_list': [], 'contributions': '182 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/Undecember', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ynmacias', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Bank-Problem\nImplementing Categorial ML algortihms (Decision Trees, Logistic Regression and KNN)\n'], 'url_profile': 'https://github.com/raghav1129', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\n'], 'url_profile': 'https://github.com/jaysatija7', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/srikant-ai', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'Trivandrum,Kerala', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/lekmeera', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['regression\nMultiple Linear Regression\n'], 'url_profile': 'https://github.com/priyankayslp', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}","{'location': 'Monroe, WI', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/Iron-Maiden-19', 'info_list': ['HTML', 'Updated Jan 4, 2021', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Regression\nRegression\n'], 'url_profile': 'https://github.com/bharadwajpatnala', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '173 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Reclocco', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Noida', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/betheman', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '276 contributions\n        in the last year', 'description': ['Regression Analysis\nThis repository contains real-data data analysis assignments completed with R for the UCSB course PSTAT 126 - Regression Analysis, featuring the following statistical techniques:\n\n\nSimple and multiple linear regression\n\n\nAnalysis of variance (ANOVA)\n\n\nTransformations\n\n\nVariable and model selection\n\n\nStepwise regression\n\n\nGeneralized linear modeling\n\n\nThe pdfs contain the analysis, and the code folder holds the R and R Markdown files used.\n'], 'url_profile': 'https://github.com/shaiyon', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gismiantony', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashutosh2909', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vidyarmenon', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': [""Robust-Geodesic-Regression\nCode for the numerical experiments in the paper 'Robust Geodesic Regression' by Ha-Young Shin and Hee-Seok Oh.\nfinding_c.R contains the code for the approximate AREs and their derivatives for the Huber and Tukey biweight estimators, used to find the tuning parameter c for 95% efficiency. Also included is the code for the ARE of the L_1 estimator.\nsphere_gradient_descent.R contains the code for performing the gradient descent algorithm to solve the geodesic regression problem on the k-sphere S^k, embedded in R(k+1). It includes the code for the exponential and logarithmic maps, parallel transport and Jacobi fields.\nsphere_karcher_mean.R contains the code for finding the intrinsic mean of a dataset on the k-sphere S^k, embedded in R(k+1). It requires sphere_gradient_descent.R to be executed first.\nsphere_simulations.R contains the code for the simulation experiments on S^k. It requires the finding_c.R, sphere_gradient_descent.R and sphere_karcher_mean.R files to be executed first. The output is the sample variance for p and v^j, used to calculate relative efficiency, and the MSEs for p and v^j in the G, T, and C scenarios. Under initializations, the dimension k, the number of independent variables n, and the type of M-type estimator can be changed.\nkendall_gradient_descent.R contains the code for performing the gradient descent algorithm to solve the geodesic regression problem on the Kendall's 2-dimensional shape space. It includes the code for the exponential and logarithmic maps, parallel transport and Jacobi fields.\nknendall_karcher_mean.R contains the code for finding the intrinsic mean of a dataset on Kendall's 2-dimensional shape space. It requires kendall_gradient_descent.R to be executed first.\nkendall_experiments.R contains the code for the experiments on Kendall's 2-dimensional shape space. It requires the finding_c.R, kendall_gradient_descent.R and kendall_karcher_mean.R files to be executed first. The type of M-type estimator can be changed under the initializations section. We have used the preprocessed data provided by Cornea and Zhu on their website http://www.bios.unc.edu/research/bias/software.html under the title 'Regression Models on Riemannian Symmetric Spaces'.\n""], 'url_profile': 'https://github.com/hayoungshin1', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Linear-Regression-OLS-vs-Gradient-Descent-\nStarting from the basics!!! Yes, Linear Regression is the first and simplest regression a ML enthusiast would have learnt. This repository is a very simple intuition on how OLS and Gradient Descent work for a simple Salary vs Experience Dataset\nInstallation\nRequiremenmts\n\nPython 3.3+ or Python 2.7\nscikit learn\nnumpy\npandas\nmatplotlib\n\nInstall the libraries from the pypi.org with the following commands.\npip install scikit-learn\npip install numpy\npip install pandas\npip install matplotlib\n\nPlease do refer stackoverflow for any errors during installation.\n'], 'url_profile': 'https://github.com/dharineeshramtp2000', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Machine-learning-regression\nLinear and logistic regression, Multivariate regression, Polynomial regression, Multivariate Logistic Regression, Nonlinear logistic regression, Regularized logistic regression, Insurances and regularization techniques\n'], 'url_profile': 'https://github.com/MehdiHmidi523', 'info_list': ['GPL-3.0 license', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', '3', 'R', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}"
"{'location': 'Rourkela, Mumbai', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Machine-Learning-mini-projects\nLinear Regression,Logistic Regression,KNN, Random Forests\n'], 'url_profile': 'https://github.com/srurora', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['automobile\n'], 'url_profile': 'https://github.com/lupitayusuf', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/openoffice97', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['House-Pricing\nAdvanced Regression\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price.\nThe company is looking at prospective properties to buy to enter the market. A regression model is to be built using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\n1.Which variables are significant in predicting the price of a house, and\n2.How well those variables describe the price of a house\n Business Goal \nThe goals is to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/Vijayapurani', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Ml_Regression\nRegression projects\n'], 'url_profile': 'https://github.com/mraman2512', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hakngrow', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/reedwindsnowterns', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MounikaKukudala', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/protyush16', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Prediction-of-car-prices\nLinear Regression\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n1.Which variables are significant in predicting the price of a car\n2.How well those variables describe the price of a car\n3.Based on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/Vijayapurani', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 19, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'Coimbatore', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Lead-Scoring\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.  The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%.\nTo make the lead conversion process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\nThe goal is to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.\n'], 'url_profile': 'https://github.com/Vijayapurani', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swannpichon', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Predict Titanic Survival Logistic Regression\nIn this project I was creating a Logistic Regression model that predicts which passengers survived the sinking of the Titanic, based on features like age and class. The data I was using for training the model is provided by Kaggle.\n\nI loaded the passenger data, manipulated it with Pandas and Numpy in order to prepare data in columns for further analysis.\nAfter I chose the features I was interested in and merged them, I split the data into the training and test sets.\nSince sklearn‘s Logistic Regression implementation uses Regularization, I normalised the feature data with StandardScaler.\nCreated the trained the model with LogisticRegression.\nMeasured the model’s score and analysed the features coefficients determined by the model.\nUsed the trained model to make predictions on the survival of a few fateful passengers and the find the probabilities that led to these predictions.\n\n'], 'url_profile': 'https://github.com/kate-del', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Regression-Forest\nRegression Forest\n'], 'url_profile': 'https://github.com/sicsempatyrannis', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'Greater Noida', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['logistic-regression\n'], 'url_profile': 'https://github.com/shreyanshsingh2107', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/999Tom', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Housing-Price-Predictor\nUsing the Housing Price Predictor Dataset to learn how to deal with datasets containing outliers to create more accurate regression models.\n'], 'url_profile': 'https://github.com/trueTRYtre3', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'nantes', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hushee69', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'FCT Abuja, Nigeria', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['The Logistic Regression Algorithm\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is binary.  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. statisticsolutions\nLogistic Regression is a method for classifying data into discrete outcomes. The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.\n\n\nThe Logistic regression algorithm was implemented in logistic_regression.py and you can reference this notebook for more practical details on how the linear regression algorithm works.\n\nThe class LogisticRegression which contains several variables & methods (public and private) to carry out the relationships modelled.\nclass LogisticRegression:\n    def __init__(self, x, y, alpha=0.01, num_iter=1000, verbose=False, lambd=0.0):\n        pass\nAt initilization of the Logistic Regression model:\n\nx will be the input feature which should be a (Xm, n) matrix.\ny will be the target feature which should be a (Ym, 1) matrix.\n\nalpha is the learning rate and num_iter the number of iterations used in gradient descent. verbose if True will produce the detailed output of the cost function for diagnostic purposes, and lambd is the parameter used to perform regularization (L2 Regularization) on the model, it is of no effect if lambd is set to 0.0.\nNote ""m"" is the number of training examples and ""n"" is the number of features.\nThe choice of numpy array was to perform vectorization on the data, thus avoiding the constant use of excessive for loops and thus optimizing the program.\nThe Hypothesis of a simple logistic regression is given as: sigmoid(hθ(x) = θo x + θ1x)\nThe sigmoid function being given as: 1 / (1 + ez)\nwhere x is the input variable.\nThe cost function or mean squared error is used to measure the accuracy of our hypothesis. This takes the average difference of all the result of the with the inputs from and the actual output y\'s.\nThe mean of the cost function is halved as a convenience for the computation of gradient descent.\ndef fit(self, timeit=True, count_at=100):\n    pass\nSo when we have our hypothesis function and we have a way of measuring how well it fits into the data. We then need to estimate the parameters in the hypothesis function and this is where Gradient Descent comes in and this process goes on for a period of time until the cost converge to a global minimum.\nFollow me on Twitter @GM_Olalekan\n'], 'url_profile': 'https://github.com/ganiyuolalekan', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Linear-regression\nLinear regression Python\nLinear regression implemented with NumPy and TensorFlow\n'], 'url_profile': 'https://github.com/sicsempatyrannis', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', '2', 'Python', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['Linear-Regression-Project\nLinear Regression Project\n'], 'url_profile': 'https://github.com/VikramVikrant', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['neural-networks-\n'], 'url_profile': 'https://github.com/CLozy', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '234 contributions\n        in the last year', 'description': ['Which factors correlate the most with the price of a house?\nThrough the KC housing prices data set provided, we are able to find out whether or not the variables such as the number of bedrooms and bathrooms etc, correlate with and affect the price of a house, negatively or positively.\nThrough data cleaning and using my own best judgement I was able to fill or drop missing values from the data set. This allowed me to then normalize the data which puts all of the variables on the same field so that the machine can run the regression.\nAfter going through the data and running train/test models, I have determined that every variable effects the housing price in a positive way and I can predict the price of a house with 77% accuracy.\nColumn Names and descriptions for Kings County Data Set\n\nid - unique identified for a house\ndateDate - house was sold\npricePrice -  is prediction target\nbedroomsNumber -  of Bedrooms/House\nbathroomsNumber -  of bathrooms/bedrooms\nsqft_livingsquare -  footage of the home\nsqft_lotsquare -  footage of the lot\nfloorsTotal -  floors (levels) in house\nwaterfront - House which has a view to a waterfront\nview - Has been viewed\ncondition - How good the condition is ( Overall )\ngrade - overall grade given to the housing unit, based on King County grading system\nsqft_above - square footage of house apart from basement\nsqft_basement - square footage of the basement\nyr_built - Built Year\nyr_renovated - Year when house was renovated\nzipcode - zip\nlat - Latitude coordinate\nlong - Longitude coordinate\nsqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors\nsqft_lot15 - The square footage of the land lots of the nearest 15 neighbors\n\n'], 'url_profile': 'https://github.com/dylankarman', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['Linear-Regression-Multiplevariables\nLinear Regression Multiplevariables.\n'], 'url_profile': 'https://github.com/Abhilashavadhanula', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['CreditCardFraud\nlogistic regression oversampling\n'], 'url_profile': 'https://github.com/RidaMalik', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['This is a predictive algorithm to estimate the CO2 emissions from vehicles based on features such as cylinder, engine size, etc. The model is built using simple regression, multiple regression, and polynomial regressions (degree 2 and 3) techniques and the model accuracy is evaluated.\nDatasets provide model-specific fuel consumption ratings and estimated carbon dioxide emissions for new light-duty vehicles\nfor retail sale in Canada available from Government of Canada website:\nhttps://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64\nThe .csv file used here is:\nhttps://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv\n'], 'url_profile': 'https://github.com/golpiraelmi', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Linear_Regression\nLinear Regression Mini Project\n'], 'url_profile': 'https://github.com/veraguzelsoy', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['linear_regression\nconcept of linear regression\n'], 'url_profile': 'https://github.com/geethikreddy11', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Trivandrum,Kerala', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Logistic-Regression\nProject using Logistic Regression\n'], 'url_profile': 'https://github.com/lekmeera', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['machine-learning-regression-\n'], 'url_profile': 'https://github.com/CLozy', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Linear-regression\nPython linear regression code\n'], 'url_profile': 'https://github.com/paulpreethi', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Athens, GA', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Code implements various linear regression algorithms. Code can be slightly modified for n data points.\n'], 'url_profile': 'https://github.com/jph83405', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['guth-betacov\nBoosted regression trees approach\n'], 'url_profile': 'https://github.com/viralemergence', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'New Delhi,India', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['ELL409_Assignment3\nSupport Vector Regression\nI. Implementations:\n1. SVR from scratch using CVXOPT library for optimization\n\n2. SVR using sklearn library  \n\nII. DataSet:\tBoston Housing DataSet (http://lib.stat.cmu.edu/datasets/boston)\nHence performed Support Vector Regression (SVR) on the given dataset using a general-purpose\nconvex optimization package (CVXOPT) as well as using a customized solver (sklearn).\nPredicting MEDV using the given 13 features.\n'], 'url_profile': 'https://github.com/a-blind-ant', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""ML-Regression\n\nApply Linear regression model using python .\nRegression analysis is one of the most important fields in statistics and machine learning .\nDataset source : https://www.kaggle.com/annetxu/health-insurance-cost-prediction\nDATA OVERVIEW:\nATTRIBUTES :\n•\tage: age of primary beneficiary\n•\tsex: insurance contractor gender, female, male\n•\tbmi: Body mass index\n•\tchildren: Number of children covered by health insurance / Number of dependents\n•\tsmoker: Smoking\n•\tregion: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n•\tcharges: Individual medical costs billed by health insurance\n""], 'url_profile': 'https://github.com/Yasmeenalaskar', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['KFCBackOffice\nBack Office Regression Automation\n'], 'url_profile': 'https://github.com/Bangthree', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/databrickshouse', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['gradient-boosting-\n'], 'url_profile': 'https://github.com/CLozy', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swasthikshetty', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': [""Logistic-Regression with Python\nWe will be working with the Titanic Data Set from Kaggle. This is a very famous data set for machine learning!\nWe'll be trying to predict a classification- survival or deceased with the help of Logistic Regression in Python for classification.\n""], 'url_profile': 'https://github.com/karthikmallikarjun', 'info_list': ['Jupyter Notebook', 'Updated Dec 2, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated May 19, 2020', 'Python', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Polynomial-Regression\nPolynomial Regression using sklearn\n'], 'url_profile': 'https://github.com/atusneem', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Ahmedabad, Gujarat, India', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Basic-linear-regression\nBasic Linear Regression model\nHey there!\nGet started with machine learning with a basic linear regression model to predict\nthe mean tempreture of the given data set of Tempretrures.\nThe data used from the data set is filtered for better understanding is reduced complexity.\nLearn!\n'], 'url_profile': 'https://github.com/vrajpatel17', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['multioutput-regression-using-pyhton\n'], 'url_profile': 'https://github.com/jitendradoble', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Gainesville, USA', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['Random Regression Analysis\nMaintainer: Rostam Abdollahi-Arpanahi\nDate:  May 2, 2020\n\nHere you can see an example file for running Random Regression Analysis using BLUPf90 family programs, generating Legendre Polynomials coefficients and computing the breeding values based on Test-day milk data in dairy cattle.\nAnalysis includes three steps as following:\n1. Download the materials from github repository\ngit clone https://github.com/Rostamabd/Random-Regression-Analysis.git\n\n2. Run renumf90\nDownload blupf90 family programs from here:  BLUPF90\necho ParRenum.par | ./renumf90 | tee ren.log\n\n3. Run blupf90\nThe renumf90.par file generated by renumf90 has been modified to meet the RR analysis requirements. Probably, the recent release of renumf90, there are some options to create the RR parameter file by renumf90. Blupf90 wiki page is here: BLUPf90 programs help\nDownload BLUPF90 family manual.\necho renf90.par | ./blupf90 | tee blup.log\n\nIt should be noticed that this is a toy example and the starting values for (co)variance components are arbitrary. In your analysis, I would suggest you run a program such as gibbs1f90, airemlf90 or thrgibbs1f90  for estimating the (co)variances.\n4. Post Random Regression Analysis\nThe estimates of random effects in the solution file generated by blupf90 program are the random regression coefficients. For instance, if the degree of Legendre polynomials is 2, we see three estimates per each individual in the output file. Basically, these three values are the coefficients of RR for intercept (b0), b1 and b2. So, if you are interested in prediction of breeding values, the RR coefficients must convert to breeding values. Interestingly, we can predict the breeding values for each time point or the entire time period based on these values. In the example file, we fitted Legendre polynomials with degrees 2 and 2 for permanent environmental and additive animal effects, respectively.\nWell, now its time to run R script for estimating the breeding values.\nThe main Rscript is Post_RR_ANAL.R but it has dependency to legendre_Coeff.R script for deriving the Legendre polynomials function. By default the program computes 6 degree of Legendre polynomials function.\nsource(""Post_RR_ANAL.R"")\n\nThe estimated breeding values are stored in all_RR_BV.txt file.\nContact information\nPlease send your comments and suggestions to rostam7474 at gmail dot com\n'], 'url_profile': 'https://github.com/Rostamabd', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Univariate_Linear_Regression\nThis Example is done using basic lib like Numpy, Seaborn, Pandas and Matplotlib only.\nIn this, I have tried to apply the Mathematical aspect of machine learning which makes the better understanding of the background and easily debug the progarm or model.\n'], 'url_profile': 'https://github.com/rishiagrawal2609', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['Writing a naive linear regression in Python\nPrerequisites\nBefore reading this tutorial, you should know a bit of Python. If you\nwould like to refresh your memory, take a look at the\n:doc:Python tutorial <python:tutorial/index>.  You should know how\nto start Python, how to use the print() function, and how to import\na library, but otherwise we hope this tutorial will be fairly\nself-explanatory.\nIf you want to be able to run the examples in this tutorial, you should also\nhave matplotlib <https://matplotlib.org/>_ available to the version of Python\nyou will be using.\nLearner profile\nThis tutorial is for people who have are jsut getting started with NumPy.  It\npresumes very little prior knowledge.  Its intent is to provide an introduction\nto basic NumPy functions using a concrete example.  It should be considered\na supplement to more systematic NumPy tutorials, but it should be accessible\nbefore or after you use one of those.\n'], 'url_profile': 'https://github.com/justbennet', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Warsaw', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Resolving classification and regression problems\n\nAdults\n\nBest model: XGBoost  ~87 % accuracy\nOur basic model achieved 87% accuracy with a quite stable learning curve (a bit overfitted but for the purpose of this exercise acceptable).\n\nAdults2\nIn this case the hyperparameters optimization (hyperopt and grid random search) was for no use at all.  Non of the them contibuted to a better performance. My  model got overfitted with the best loss: 0.8570989865902344.\nThat being said, I should probably develop a bit more feature engineering part or increase the amount of data. Perhaps, the model was too sophisticated for such a simple problem and small dataset.\n\nSummary:\nI’ve learned how to use Decision Tree, Random Forest, touch a bit of feature engineering and parameter optimization. After all, validation plays a huge role and can be very helpful in further estimations.\n\nAdults3\nI made an attempt to split dataset with StratifiedKFold and add some more features. The initial basic model achieved: 87,52%. After using optimization my model have decreased to 85,78 % (which is still less than initial model, but more than ""Adults2"").\n\n'], 'url_profile': 'https://github.com/kordusmonika', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Mumbai,India', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['Python-Tkinter\n\nThis is small code where it shows linear regression performed on manually entered data.\nUsing Tkinter Library the GUI is created.\nFollowing is the output Image 😄\n\n\n'], 'url_profile': 'https://github.com/Mmddzz3', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': [""Intro to ML\nGetting_Started\nHi bro, what's up.\nLet's meet at H3, 169\n""], 'url_profile': 'https://github.com/durgeshahire15', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Linear_Regression\nIndepth Analysis of Linear Regression\n'], 'url_profile': 'https://github.com/PuspakRout', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '2', 'R', 'Updated May 4, 2020', '2', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'Kyiv, Ukraine', 'stats_list': [], 'contributions': '236 contributions\n        in the last year', 'description': ['Haskell Logistic Regression\nIn this project I try (and achieve) to implement Logistic Regression in Haskell from scratch.\n'], 'url_profile': 'https://github.com/arjaz', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Konstantin-Iakovlev', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['ConvexRegression\nLarge-Scale Convex Regression\nCall active_set and active_set_limited_memory to get solutions to convex regression.\nSee Paper for the choice of parameters.\n'], 'url_profile': 'https://github.com/wenyuC94', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'Tartu, Estonia', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['This dataset contains information of customers in bank. Through given variables you should predict whether a person makes less than 50K $ in a year.\nNote: You should not use all variables. Define which parameters will be useful for you.\nDataset Desciption:\nage: continuous.\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\nfnlwgt: continuous.\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\neducation-num: continuous.\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\nsex: Female, Male.\ncapital-gain: continuous.\ncapital-loss: continuous.\nhours-per-week: continuous.\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n'], 'url_profile': 'https://github.com/Leyla-Hasanova', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': [""Intro to ML\nGetting_Started\nHi bro, what's up.\nLet's meet at H3, 169\n""], 'url_profile': 'https://github.com/durgeshahire15', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Telecom-Churn-Case-Study\nRandom Forest, XgBoost, Logistic Regression\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\nIn this project, customer-level data of a leading telecom firm is analysed, predictive models are built to identify customers at high risk of churn and the main indicators of churn are identified.\nThe dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively.\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months.\n Data Analysis \nUnderstanding the typical customer behaviour during churn will be helpful as customers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers).In churn prediction, we assume that there are three phases of customer lifecycle :\nThe ‘good’ phase:  In this phase, the customer is happy with the service and behaves as usual.\nThe ‘action’ phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\nThe ‘churn’ phase: In this phase, the customer is said to have churned. Churn is defined based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available for prediction. Thus, after tagging churn as 1/0 based on this phase, all data corresponding to this phase are discarded.\nIn this case, since data over a four-month window is analysed, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase.\n'], 'url_profile': 'https://github.com/Vijayapurani', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/CLozy', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['USHousing\nUS housing model - Linear regression\n'], 'url_profile': 'https://github.com/crazyphoton007', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Project-9\nCross-Validation with Linear Regression\n'], 'url_profile': 'https://github.com/harshitpaunikar1', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kaushalsingh584', 'info_list': ['Haskell', 'Updated Oct 15, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Julia', 'MIT license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jul 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 3, 2020']}"
"{'location': 'Krakow, Poland', 'stats_list': [], 'contributions': '162 contributions\n        in the last year', 'description': ['regression-models-course-project\nJHU regression models course project\n'], 'url_profile': 'https://github.com/GeorgyMakarov', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Sweden', 'stats_list': [], 'contributions': '469 contributions\n        in the last year', 'description': ['Regression\nAn assignment for Introduction to Machine Learning at Linnaeus University spring 2020.\nContains exercises in:\n\nMultivariate Regression\nCross validation\nGradient Descent, Normal equation\nData standardization\nPolynomial Regression\nMultivariate logistic Regression\nForward Selection\n\n'], 'url_profile': 'https://github.com/clundstrom', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kaushalsingh584', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Chemical Segregation\nChemical Segregation Challenge using Logistic Regression.\n'], 'url_profile': 'https://github.com/YatharthKaushik', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Binary_Classification_Logistic_regression\nBinary classification using logistic regression\nthis repo contains the notebook whioch has the model of logistic regression which is used for the problem of binary image classification\nthe model scans any image and classifies whether there is a cat in the image or not. the datasets used to train the model are present in form of h% file\nyou can use the ipynb notebook to work the noteboks pdf version is also present one can go through to take a grasp of how the model works\nyou can fork it with the google colaboratory but you have to upload the datset to the colab in order to use them\n'], 'url_profile': 'https://github.com/imZain448', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Forecasting concetration of PM10 particles based on weather data\nBuilding a complete Machine Learning pipeline for forecasting air pollution 12 hours ahead.\nDATA:\nThe data is collected from 8 different stations in Macedonia.\nLinks from where the data was collected:\nhttps://pulse.eco/restapi\nhttps://darksky.net/dev/docs\nMODELS:\n2 Models were built. In the first model each station has its own predictor, and in the second model (LSTM model) datasets are concatenated and universal predictor is made.\n1. model:\nPreprocessing - removing or interpolating missing values, transforming categorical data, dropping redundant features.\nFeature selection - creating lag, seasonal and statistical features and dropping features that do not have effect on air pollution.\nModel selection - 3 models were built for each station (Linear regressor, Extra treees regressor, XGBoost regressor)\n2. model:\nPreprocessing - removing or interpolating missing values, transforming categorical data, dropping redundant features.\nFeature selection - creating statistical and seasonal features and dropping features that do not have effect on air pollution,\nconcatenating datasets and handling different categorical features.\nModel selection - For creating lag features previous 7 days were observed, for each hour separately, by the LSTM layer. Data is splitted, schuffled, scaled and reshaped to have proper shape for LSTM layer. The network is trained on Google colaboratory.\nLoss:\n\nFinal evaluation:\n\nI could not reduce MAE more because the data is very noisy and same inputs are mapping to different outputs\n'], 'url_profile': 'https://github.com/Data-Science-kosta', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Islamabad, Pakistan', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['gcp-ai-platform-hyperparameter-tuning-tf2\nPerform Hyperparameter tuning on a small Regression problem using TensorFlow 2\nRun on Docker using AI platform\n\nSetup environment variables\n\nexport PROJECT_ID=$(gcloud config list project --format ""value(core.project)"")\nexport IMAGE_REPO_NAME=gcp_ai_platform_hyperparameter_tuning_tf2\nexport IMAGE_TAG=gcp_ai_platform_hyperparameter_tuning_tf2_image\nexport IMAGE_URI=us.gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\n\n\nBuild Docker image\n\ndocker build -f Dockerfile -t $IMAGE_URI ./\n\nPush the Docker image to GCP container registry\n\ndocker push $IMAGE_URI\n\nPrepare hyperparameter tuning config file\n\nIn the hptuning_config.yaml you add the parameters to optimize with your hyperparameter tuning, and provide arguments to your code as hyperparameters that will be used to optimize. You can also specifiy the job submission details like parallel jobs to run and how many jobs to run.\n\nTraining with hyperparameter tuning on AI platform using Docker\n\nInitialize input variabless\nexport REGION=us-central1\nexport JOB_NAME=gcp_ai_platform_hyperparameter_tuning_tf2_$(date +%Y%m%d_%H%M%S)\n\nSubmit the jobs\n\ngcloud ai-platform jobs submit training $JOB_NAME --scale-tier BASIC --region $REGION --master-image-uri $IMAGE_URI --config hptuning_config.yaml\n\n'], 'url_profile': 'https://github.com/imransalam', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Pridiction Of Purchase\nThis is a Purchase Prediction Machine Learning model created with the help of Logistic Regression to determine the puchase of Car on the basis of age and estimated salary.\nIt detects images using deep learning and python\nI will be using Logistic Regression in this project,in perticular it is trained on dataset provided of Kaggle.\nInstallation\n* pip3 install numpy\n* pip3 install flask \n* Docker\n\nWorking Process\nThe model is built on python program and later on it is connected to the flask which helps it to deploy on webserver.\nWebpages are built on HTML and CSS.\nDocker and Dockerfile is used to convert all of it into an OS(image).\nThat image is later on pushed on Docker Hub for public use.\nTo pull that image from Docker Hub\ndocker pull maverick6798/logistic\nAll the files later on are uploaded on github.\nTo Run The Project\n\nDownload the docker\nUse these command on terminal -\n\n\ndocker pull maverick6798/logistic\ndocker run -dit maverick6798/logistic\n\n\nGo to your browser and use URL 0.0.0.0:5000.\nEnter the age and estimated salary of that person.\n\nNOTE\nThere is a chance that docker might not open image. so use command setenforce 0 before running the image.\n'], 'url_profile': 'https://github.com/Maverick6798', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['heart_disease\ndata source: https://www.kaggle.com/ronitf/heart-disease-uci\n'], 'url_profile': 'https://github.com/luigivendetta', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Beijing', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Melbourne-house-price-prediction\nUsing Logistic regression and Bayesian regression to predict the hould price based on Tensorflow\nenvironment and packages\npython 3.6,\nnumpy 1.16.3,\npandas 0.22.0,\ntensorflow 1.13.1,\nmatplotlib 2.2.2\ndata\nDataset: https://www.kaggle.com/anthonypino/melbourne-housing-market\nThe dataset includes address, rooms, land size, distance from C.B.D., price, etc.\n'], 'url_profile': 'https://github.com/Kris4HU', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '2', 'Jupyter Notebook', 'Updated Jun 21, 2020', '2', 'Python', 'Updated Nov 13, 2020', '2', 'HTML', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}"
"{'location': 'Nepal', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['BASICS-IN-PYTHON-\nMATH BEHIND MACHINE LEARNING(LINEAR REGRESSION , LOGISTIC REGRESSION AND MANY MORE)\nIN THIS REPO YOU WILL FIND BASIC MATH FUNCTIONS WHICH ARE USED TO DEVELOP MACHINE LEARNING ALGORITHM.\n'], 'url_profile': 'https://github.com/rockerritesh', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['RegressionModels\nFirst order methods for regression models in Python\n'], 'url_profile': 'https://github.com/alodieboissonnet', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '155 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/togashidm', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Swedish Auto Insurance\nThis is a very simple dataset that has just two columns:\n\none is the number of claims made, and\nthe next is the payment made in thousands of Kronor for that claims.\n\nSince this has just two variables - one independent, and one dependent - and both continuous, it would make sense to get started with building some Regression Models on the same. This repository hosts the approach on this dataset using - Simple Linear Regression (Ordinary Least Squares Regression) Model :)\n\n\n\nDataset:\nhttp://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr06.html\n'], 'url_profile': 'https://github.com/akshaybhaskaran', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Powai, Mumbai, India', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SaharAlmahfouzNasser', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Real-Estate-Price-Regression\nThanks to https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard/notebook, which contributes a lot to the idea of stacking modeling. \nThe notebook involves a story of combining Lasso, XGBoost, Gaussian Process Regressor and KernelRidge by stacking method to test the performance of each base models and the whole stacking one. The problem is about regression for real estate price. Source: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/notebooks The result suggests that the stacking model defeat all three base models, as well as other ensemble models, and at the same time avoid the overfitting problem.\n'], 'url_profile': 'https://github.com/Hy-Zou', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,774 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juxsta', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alok526', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': [""Multiple-Linear-Regression-with-scikit-learn\nMultiple Linear Regression with scikit-learn\nCourse Objectives:\nIn this project, I built and evaluated multiple linear regression models using Python. I used scikit-learn to calculate the regression, while using pandas for data management and seaborn for plotting. The data for this project consists of the very popular Advertising dataset to predict sales revenue based on advertising spending through media such as TV, radio, and newspaper.\nBy the end of this project, I was able to:\n\nBuild univariate and multivariate linear regression models using scikit-learn\nPerform Exploratory Data Analysis (EDA) and data visualization with seaborn\nEvaluate model fit and accuracy using numerical measures such as R² and RMSE\nModel interaction effects in regression using basic feature engineering techniques\n\nProject Structure\n\n\nTask 1: Introduction and Overview\nWe will  introduce the model we will be building as well the Advertising dataset for this project.\n\n\nTask 2: Load the Data\n\n\nIn this task, we will load the very popular Advertising dataset about various costs incurred on advertising by different media such as through TV, radio, newspaper, and the sales for a particular product. Next, we will briefly explore the data to get some basic information about what we are going to be working with.\n\nTask 3: Relationship between Features and Target\n\nIt is good practice to first visualize the data before proceeding with analysis and model building. In this task, we will apply seaborn to create scatter plots of each of the three features and the target. This will allow to make a qualitative observations about the linear or non-linear relationships between the features and the target.\n\nTask 4: Multiple Linear Regression Model\n\nWe will extend the simple linear regression model to include multiple features. Our approach will give each predictor a separate slope coefficient in a single model. This way, we can avoid the drawbacks of fitting a separate simple linear model to each predictor. In this task, we use scikit-learn's LinearRegression( ) estimator to calculate the multiple regression coefficient estimates when TV, radio, and newspaper advertising budgets are used to predict sales revenue. Lastly, we will compare and contrast the coefficient estimates from multiple regression to those from simple linear regression.\n\nTask 5: Feature Selection\n\nDo all the predictors help to explain the target, or is only a subset of the predictors useful? We will address exactly this question in this task. We will use feature selection to determine which predictors are associated with the response, so as to fit a single model involving only those features. We will use R², the most common numerical measure of model fit and understand its limitations.\n\nTask 6: Model Evaluation Using Train/Test Split and Model Metrics\n\nAssessing model accuracy is very similar to that of simple linear regression. Our first step will be to split the data into a training set and a testing set using the train_test_split( ) helper function from sklearn.metrics. Next, we will create two separate models, one of which uses all predictors, while the other excludes newspaper. We fit the training set to the estimator and make predictions on the testing set. Model fit and the accuracy of the predictions will be evaluated using R² and RMSE. Visual assessment of our models will involve comparing the residual behaviors and the prediction errors using Yellowbrick. Yellowbrick is an open source, pure Python project that extends the scikit-learn API with visual analysis and diagnostic tools. It is commonly used inside of a Jupyter Notebook alongside pandas data frames.\n\nTask 7: Interaction Effect (Synergy) in Regression Analysis\n\nFrom our previous analysis of the residuals, we concluded that we need to incorporate interaction terms due to the non-additive relationship between the features and target. A simple method to extend our model to allow for interaction effects is to include a third feature by taking the product of the other two features in our model. This feature will have its separate slope coefficient which can be interpreted as the increase in the effectiveness of radio advertising for a one unit increase in TV advertising or vice versa.\n###CERTIFICATE DETAILS\n\nName: Multiple Linear Regression with scikit-learn\nIssuing Organization: Coursera\nIssue Date: May 2020\nExpiration Date: This certification does not expire\nCredential ID: 9XMUAVXTPJWJ\nCredential URL: https://www.coursera.org/account/accomplishments/certificate/9XMUAVXTPJWJ\n\n""], 'url_profile': 'https://github.com/mrsingh3131', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rajoelina', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated May 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['CreditCardFraud\nLogistic Regression to detect credit card fraud. Dataset can be found at https://www.kaggle.com/mlg-ulb/creditcardfraud.\n'], 'url_profile': 'https://github.com/brookemm', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Profit_Estimation_of_Company\nProfit estimation of company with linear regression\n'], 'url_profile': 'https://github.com/RinayShah', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Neuron_Network_Line_Regression\nSImple neeuron network used line regression method\n'], 'url_profile': 'https://github.com/kalinVn', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alecb2', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'Madison, South Dakota, USA', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Linear-Regression\nMachine Learning using SLR, MLR algorithms\n'], 'url_profile': 'https://github.com/RahatMahmud1', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['HousePrices\nPredicting Boston House prices using Multivariable regression\n'], 'url_profile': 'https://github.com/atrijo2001', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Pyplots_regression\nPython regression plots\nThis code is a demo on how to do ploynomial regression work on a random data set collected from Postgres for doing Java application performance analysis. This code uses numpy and matplotlib to curve fitting/regression and do the plotting.\n'], 'url_profile': 'https://github.com/sherif-abdelfattah', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Logistic-Regression\nSimple Logistic Regression using dummy data\n'], 'url_profile': 'https://github.com/jh729670', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': [""Logistic regression approximation of Covid19 death rate.\nSummary\nUsing two logit functions to approximate the Covid19 death rate provides a very good fit for many countries, while single logit falls behind.\nPython Jupyter notebook code can be found in src folder. Data is as of April 30, 2020.\nIntro\nhttps://en.wikipedia.org/wiki/Logistic_function\nLogistic growth can be used to model a number of processes, including the propagation of a virus. It describes the non-linear population growth:\n\nwith the phase portrait:\n\nand the first derivative (density):\n\nThe accumulated number of Covid deaths in Italy follows similar pattern:\n\nIt does not have much detail, but after taking the first derivative (convert to daily deaths) the structure appears:\n\nResults\nIf we try to fit the logit function in this curve (Italian Covid19 deaths), the fit will not be great:\n\nBut if we fit a sum of two logit functions, the fit becomes much better:\n\nSame with Spain. One logit model:\n\nTwo hump model:\n\nPortugal, on the other hand, doesn't fit well neither in one nor two hump models:\n\n\nGermany - one hump vs two hump (two hump is slightly better):\n\n\nFrance:\n\n\nNorway:\n\n\nBelow are the two-hump models for few more countries:\n\n\n\n\n\n\n\nIran's trajectory is quite complex for one or two humps:\n\nSame with Austria - the fit is not great:\n\nInterpretation of results\nIn many cases the two-logit model provides a much better fit than one-logit model. One possible explanation is that there are two parallel epidemics taking place either in different geographies (within the same country) or in two different population groups in the same location that do not interact with each other and posess different characteristics (mobility, infection rate etc).\nTaking Italy and Spain, for example, the first hump is tall and narrow, which indicates fast spread with high mortality rate - probably nursing homes. Second hump could be the general population. First wave was over by April 30, while the second is in decline:\n\nBut in some countries (like Sweden) the first hump is mild, while the second one is more prominent:\n\nConclusion\nEach country develops its own pattern of virus spread. In some cases it can be approximated by single logit function, in many cases it takes two logit functions to provide a good fit. Some countries deveop a unique pattern that is hard to model.\n""], 'url_profile': 'https://github.com/quantbin', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}","{'location': 'Bengaluru,India', 'stats_list': [], 'contributions': '496 contributions\n        in the last year', 'description': ['Titanic-Dataset\nAnalysing Titanic Dataset using Logistic Regression\nIn this dataset we will analyse whether a person will survive ar die in a ship(titanic) accident,by analysing various features which infuences our outcome\nIn analysing Dataset modules used:\npandas\nnumpy\nio\nmatplotlib\nseaborn\nsklearn\n\nThis dataset will give precision of 79%\n'], 'url_profile': 'https://github.com/Adityanagraj', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated Apr 30, 2020', 'R', 'Updated Apr 28, 2020', 'Python', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': [""NHL Prediction\n\nThe goal of this team project was create a Logistic Regression model to predict the winner of NHL hockey games. We recognized this as a challenging project with many obstacles and no data that was readily available to achieve our goal, but hockey is a major interest of our team and we wanted to explore logistic regression within a domain of interest.\nChallenges\nObviously to predict the winner of a hockey game you have to make sure that you only use information that was available prior to that game being played, but the data that is available only provides game by game (boxscore) information. This isn't very useful for prediction purposes.\nData\nWe contacted Natural Stat Trick for permission to use data from their website for our project. We did this because we wanted to use some of the more modern statistics (such as Corsi) that are in use today and these statistics are not readily available in boxscore data. The website graciously provided permission for non-commercial use. We suplemented this data with data that we had previously obtained from the Kaggle NHL dataset.\nWe obtained data for the 2014-15, 2015-16, 2016-17, 2017-18, and 2018-19 seasons.\nPreprocessing\nAs mentioned in the challenges section, now that we had the data we wanted we had to massage it into a format that we could use. Loosely these are the steps that we took.\n\nCreate running averages for each statistic for each individual team for Natural Stat Trick advanced statistics.\nAdd the NHL game ID to the Natural Stat Trick data.\nCreate running averages for each statistic for each individual team for NHL boxscore data.\nMerge these into a single dataset.\nFor each game look at the average statistics for all games prior for the Home and Away teams and subtract the Away team statistics from the Home team to create marginal statistics for each game. All data is in reference to the Home Team.\n\nWe now have data in a form appropriate for machine learning.\nAll preprocessing was performed in Python. Logistic Regression was performed using R as per the project requirements.\nResults\nWe learned two things during this project that we could use as a baseline to judge our model.\n\nThe home team wins approximately 54.7% of the time.\nThe theoretically upper limit for NHL predictions is approximately 62%. This result is from:\n\nWeissbock, J. (2014). Forecasting Success in the National Hockey League using In-Game Statistics and Textual Data (Doctoral dissertation, University of Ottawa).\n\n\n\nWith these two metrics in mind we had set a reasonable target of 57% accuracy for our model recognizing that there is a lot of additional work we could do to improve it in the future.\nWe achieved an accuracy of 57.4% over the entire 2018-2019 season; predicting 730 / 1271 correct results.\nBut, unfortunately this was not good enough to quite our day jobs as we would not have been able to beat the bookies:\n\nAlthough, that's not the result we wanted it's also not that bad. We would have lost ~$250 after wagering $12,710 so we are only losing about $0.02 for every $1 wagered and that's better than most games of chance at a Casino.\n""], 'url_profile': 'https://github.com/m-dodd', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['multiridge\nR package for multi-penalty ridge regression\nlibrary(devtools);\ninstall_github(""markvdwiel/multiridge"")\nYou may also also install multiridge by downloading the .zip or tar.gz file, and use in R (with correct file name):\ninstall.packages(filename, repos=NULL); install.packages(c(""penalized"", ""pROC"", ""risksetROC"", ""survival"", ""mgcv""))\nDemo script and data available from: https://drive.google.com/open?id=1NUfeOtN8-KZ8A2HZzveG506nBwgW64e4\nScripts and data used for manuscript, plus script to check results: https://drive.google.com/drive/folders/1hwQEezOQZATZb0hixg67KXxC3Rm-2zvq?usp=sharing\n'], 'url_profile': 'https://github.com/markvdwiel', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Meridian, ID', 'stats_list': [], 'contributions': '920 contributions\n        in the last year', 'description': ['comparinator\nSimple visual regression testing for websites with reasonable defaults.\nIf you need something more advanced, wraith is probably what you want. This tool will only crawl the base path and any pages on the alpha site that are directly linked from the base path (determined by domain).\nUsage\ncomparinator -help shows all available flags.\nFor the most common testing scenario:\ncomparinator -alpha-base-url=""https://www.yoursite.com"" -beta-base-url=""https://dev.yoursite.com""\nYou\'ll need to have webdriver running at localhost:4444 or specify the URL to your webdriver with the -webdriver-url flag.\nOnce it\'s done running, you\'ll have an output directory with all of the screenshots + diffs + a JSON file with a bunch of info in it about which screenshots belong to which url, how similar they are, and the overall similarity across all screenshots. Someday I\'ll build a little web UI around this.\nYou can optionally specify the path to a sitemap.xml file using the sitemap-url flag. If you specify a sitemap, the links contained in the sitemap will be used instead of the links found on the home page of the alpha site. The full URL to the sitemap.xml depends on what you specify for the alpha site base url -- essentially, it\'s alpha-base-url sitemap-url.\nBuilding\nmake\nReleasing\nPush a tag to the repo and Github CI will take over, build a release, and upload\nthe artifacts to the release page.\n'], 'url_profile': 'https://github.com/cweagans', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['LLR_GP\nLocal Linear Regression with Gaussian Process Regularization\nThis code is for regularizing local linear regression by modeling the parameters as a Gaussian Process, effectively smoothing them and in many cases reducing mean-square error of parameter estimates.\nOptimization and model specification are both done in Stan.\n'], 'url_profile': 'https://github.com/noskacj', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Brooklyn, NY', 'stats_list': [], 'contributions': '345 contributions\n        in the last year', 'description': [""Modeling King County House Prices\nFlatiron Mod 2 Housing Price Project\nAuthors: Andy Peng\nThe contents of this repository detail an analysis of the module 2 project. This analysis is detailed in hopes of making the work accessible and replicable.\nBusiness problem:\nThe task is to investigate housing prices for a company that focuses on finding affordable family houses in the Kings County area. For this project we will be using the Kings County Housing Sales dataset to investigate different features that might affect the housing prices. By using the kc_house_data.csv dataset we will be trying to find certain features that we should keep our eyes out to get the best prices for family houses.\nData\nData includes house prices and features of houses such as zip code, number of bedrooms, number of bathrooms, etc.\nMethods\n\nDescriptive Analysis\nChoices Made\nModeling (Future work)\nKey relevant findings from exploritory data analysis\n\nResults\nVisual 1\n\n> Box plot graph of House Prices with and without Water Front View\nVisual 2\n\n> Zip Code's Average Housing Prices\nVisual 3\n\n> Interactive Mapping of Zip Code's Average Housing Prices\nVisual 4\n\n> Footage of the house vs Housing Prices\nRecommendations:\nTo summarize everything above, we created a model that manages to predict housing prices within a $80,000 range. We chose to explore the features waterfront view, zipcode and sqft_living. Below shows the exploration results from the code above.\n\n\nHouses with no waterfront view have a lower average in housing prices.\n\n\nZipcodes around certain locations such as Mercer Island tend to cost more than houses away from Mercer Island.\n\n\nThe larger the footage of the home is the more expensive the housing price is.\n\n\nLimitations & Next Steps\nDue to time constraint, there are many features that we haven't considered. For example schools in the area, crime rate, time of sale and picture of the house.\nFor further information\nPlease review the narrative of our analysis in our jupyter notebook or review our presentation\nFor any additional questions, please contact andypeng93@gmail.com\nRepository Structure:\nHere is where you would describe the structure of your repoistory and its contents, for example:\n\n├── README.md                       <- The top-level README for reviewers of this project.\n├── Housing_Prices.ipynb             <- narrative documentation of analysis in jupyter notebook\n├── presentation.pdf                <- pdf version of project presentation\n└── images\n    └── images                          <- both sourced externally and generated from code\n\n\n""], 'url_profile': 'https://github.com/andypeng93', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/koileee', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/9597yashKulkarni', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashtanzk', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Mini_Project_Linear_Regression\nLinear Regression Using Boston Housing Data Set\n'], 'url_profile': 'https://github.com/mahmuterenkoca', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'San Francisco, USA', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['ML_Supervised_Regression\nSchool:     Hult International Business School\nCourse:     Machine Learning (DAT-5303)\nAssignment: Machine Learning Regression-Based Analysis (Supervised)\nThis assignment was to be completed in Python wherein the task was to first conduct an exploratory analysis of the given dataset, then use an OLS Regression to identify the statistically significant variables, and lastly explore different Regression Machine Learning algorithms (such as Ridge, Lasso, ARD) to find the model with the closest training and test scores.\nBusiness problem:\nThe executives at Apprentice Chef, Inc. have come to realize that over 90% of revenue comes from customers that have been ordering for 12 months or less, and want to understand how much revenue to expect from each customer within their first year of orders. The tasks involve: •analyzing data •developing the top insights• building a machine learning model to predict revenue.\n'], 'url_profile': 'https://github.com/dkmanutd', 'info_list': ['HTML', 'Updated May 8, 2020', '1', 'R', 'Updated Jan 22, 2021', '1', 'Go', 'Updated Dec 17, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2021', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}"
"{'location': 'Gothenburg, Sweden', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Titanic--Machile-Learning\nTitanic- Predict survival rate using Logistic Regression\n'], 'url_profile': 'https://github.com/sandeepthimmappa', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Air-Quality-Prediction\nAir Quality Prediction using Multi-Variate Regression.\n'], 'url_profile': 'https://github.com/YatharthKaushik', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Text-Classfication\nA Text Classifier based on logistic regression\n'], 'url_profile': 'https://github.com/KalraAkshay', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['Predicting-House-Prices-with-Regression-using-TensorFlow\nPredicting House Prices with Regression using TensorFlow\nCourse Objectives\nIn this course, I am going to focus on the following learning objectives:\n\nLearn to create, train and evaluate neural network models with TensorFlow and Keras.\nLearn to solve regression problems with the help of neural networks.\n\nBy the end of this course, I was able to create a neural network model which was able to predict price of a house given some input features with a high degree of accuracy.\nProject Structure\nThe project on Predicting House Prices is divided into following tasks:\n\n\nTask 1: Introduction\nIntroduction to the house pricing problem.\nPrerequisites for this course.\nIntroduction to the Rhyme interface.\nImporting libraries and helper functions.\n\n\nTask 2: The Dataset\nImporting the dataset.\nA quick look at a few examples.\nCheck missing data.\n\n\nTask 3: Data Normalization\nData normalization.\nConvert label values back to prices.\n\n\nTask 4: Training and Test Sets\nSelect features.\nSelect labels.\nConvert from Pandas data structures to numpy arrays.\nTrain and test split.\n\n\nTask 5: Creating the Model\nCreating a sequential model with Keras.\nModel architecture - hidden layers and hidden units.\nCompiling the model by specifying an optimizer and a loss function.\nComputing trainable parameters.\n\n\nTask 6: Model Training\nTraining the model to fit to training data.\nPlotting training and validation loss.\n\n\nTask 7: Predictions\nPlot and compare raw predictions.\nPlot and compare price predictions.\n\n\nCERTIFICATE DETAILS\n\nName: Predicting House Prices with Regression using TensorFlow\nIssuing Organization: Coursera\nIssue Date: May 2020\nExpiration Date: This certification does not expire\nCredential ID: 8VP6WQM65JUK\nCredential URL: https://www.coursera.org/account/accomplishments/certificate/8VP6WQM65JUK\n\n'], 'url_profile': 'https://github.com/mrsingh3131', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['movie_dataset_machine_learning\nClassify ratings and apply regression on revenue\n'], 'url_profile': 'https://github.com/rahilagrawal-university-assignments', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['HDG-COE\nProtecting Regression Models with Local Differential Privacy.\nintroduction\nThese contents support the paper HDG.\nIt aims to add High-Dimensional Gaussian on model COEfficients to prevent unauthorized model disclosure while meeting user-desired privacy requirement and maximizing utility.\nLicense\nGNU General Public License.\n'], 'url_profile': 'https://github.com/anslab-in-xdu', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['Linear-Regression-with-NumPy\nLinear Regression with NumPy and Python\nIn this course, I am going to focus on three learning objectives:\n\nImplement the gradient descent algorithm from scratch.\nPerform univariate linear regression with Numpy and Python.\nCreate data visualizations and plots using matplotlib.\n\nBy the end of this course, I was able to build linear regression models from scratch using NumPy and Python, without the use of machine learning frameworks such as scikit-learn and statsmodels.\nThe project on Linear Regression with NumPy and Python was divided into the following tasks:\n\n\nTask 1: Introduction and Import Libraries\nIntroduction to the data set and the problem overview.\nImport essential modules and helper functions from NumPy and Matplotlib.\n\n\nTask 2: Load the Data and Libraries\nLoad the dataset using pandas.\nExplore the pandas dataframe using the head() and info() functions.\n\n\nTask 3: Visualize the Data\nUnderstand the data by visualizing it.\nFor this dataset, I will use a scatter plot using Seaborn to visualize the data, since it has only two variables: the         profit and population.\n\n\nTask 4: Compute the Cost 𝐽(𝜃)\nA look at the machinery that powers linear regression: Gradient Descent.\nI want to fit the linear regression parameters 𝜃 to my dataset using gradient descent.\nThe objective of linear regression is to minimize the cost function J(𝜃).\nWe can think of the cost as the error my model made in estimating a value.\n\n\nTask 5: Implement Gradient Descent from scratch in Python\nThe parameters of my model are the 𝜃_j values.\nThese are the values I will adjust to minimize the cost J(𝜃).\nOne way to do this is to use the batch gradient descent algorithm.\nIn batch gradient descent, each iteration performs the following update.\nWith each step of gradient descent, the parameters 𝜃_j come closer to the optimal values that will achieve the lowest           cost J(𝜃).\n\n\nTask 6: Visualizing the Cost Function J(𝜃)\nTo better understand the cost function J(𝜃),I will plot the cost over a 2-dimensional grid of 𝜃_0 and 𝜃_1 values.\n\n\nTask 7: Plotting the Convergence\nPlotting how the cost function varies with the number of iterations.\nWhen I ran gradient descent previously, it returns the history of J(𝜃) values in a vector “costs”.\nI will now plot the J values against the number of iterations.\n\n\nTask 8: Training Data with Univariate Linear Regression Fit\nNow that I have correctly implemented and run gradient descent and arrived at the final parameters of my model, I    can use these parameters to plot the linear fit.\n\n\nTask 9: Inference using the optimized 𝜃 values\nIn this final task, I will use my final values for 𝜃 to make predictions on profits in cities of 35,000 and 70,000 people.\n\n\n##Certificate Details\n\nName: Linear Regression with NumPy and Python\nIssuing Organization: Coursera\nIssue Date: April 2020\nExpiration Date: This certification does not expire\nCredential ID: N5SLYJAUPNQF\nCredential URL: https://www.coursera.org/account/accomplishments/certificate/N5SLYJAUPNQF\n\n'], 'url_profile': 'https://github.com/mrsingh3131', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Predicting hose rent prices using regression techniques\nSteps to be followed:\na) Keep the dataset and code in the same folder\nb) Run the code with this dataset\nSummary:\nHousing prices fluctuate significantly based on myriad factors. As a result, completing a fair deal while purchasing or selling a property is still a challenge. This study aims to demystify the factors which go into objectively determining housing prices. The majority of property features shown by realtors demonstrate a linear relationship with housing price. Categorical features, in particular location-based features such as zip code, show a statistically significant relationship with price. However, due to the randomness associated with locations both linear and linear mixed effect models are considered in this model. The findings indicate that even though multiple factors are relevant in deciding housing prices, a small set of factors can be effectively used for predicting price. Furthermore, this study uncovers that pricing for certain types of properties deviate significantly from the norm in Kings Country, which are typically older and larger in lot size. The prediction accuracy of the final model indicates high efficacy in the Kings County region. Significant differences were not identified between the linear and a mixed effects model. The subset of identified predictors relevant to predicting housing price can be utilized as a starting point to build models for other regions.\nExploratory Data Analysis\nDistribution of data\nThis process is carried out to gain familiarity with the predictors involved. Histograms are effective for identifying data distributions. These distributions can be utilized to uncover patterns within the data, which can be used later to describe model behavior. Furthermore, histograms can also be utilized to  gather an understanding about the sparsity of data.\nFor instance The sparsity of the Sqft_basement variable required transformation into a factor form rather than the numeric form retained by all other square footage related variables. The sparse data for basement data was due to ~61% of  residential properties not including a basement level.\nCategorical variables\nBoxplots of categorical variables plotted against the response variable are an effective method to gain a sense of within-group and between-group variability. Box Plots are utilized to understand which categorical variables are similar and aggregated to reduce the total number of categories within a predictor.\nCorrelations\nCorrelation is measured between the predicting variables to detect (near) linear dependence and strengthen the interpretation of statistical statements. In particular, the presence of multicollinearity is evaluated to determine the precision of the estimate coefficients and quantify the statistical power of the final regression models.\nModel choices and Goodness of Fit\nFor this data set and analysis, multiple linear regression is chosen to predict housing prices in Kings County. In addition, the linear mixed effects model is utilized to model the random effects contributed by the high dimensional zip code variable. The variability introduced by treating zip code as a random effect is used for variable selection and prediction. Each zip code contains idiosyncratic characteristics capturing the behavior of the residents and local infrastructure. In order to accommodate such characteristics, the linear mixed effects model is used with the random effect variable as zip codes. The lme4 package provided by R statistical software has been used to model the mixed effects model, and the ‘glmmLASSO’ package has been used for variable selection.\nData Transformations\nUtilized Box-Cox transformations to ensure normality of response variable and constant variance amongst certain predicting variables\nVariable selection\nIn line with best practices, variable selection is performed to select the most relevant predictors and avoid overfitting the model. The methods employed in the project are Stepwise regression, Lasso and Elastic Net. Ridge regression is not utilized because it does not perform variable selection and there was no multicollinearity between predictors in the Kings County dataset.\nIn order to avoid overfitting and effectively validate the model, the data is split into training and test sets. 75% of the data points are randomly selected to form the training set and the rest is set aside for testing.\nZip codes are used a controlling variable in linear models. Zipcodes are also used as variables with random effect when utilizing the mixed effects model.\nModel Validation\nMean Square Prediction Error (MSPE) is utilized to ascertain which model performs the best amongst our selection of models.\nPrecision Measure(PM) gives a sense of the usefulness of the model in predicting the response.\nCategorical Variables\nPredicting variables consisting of a discrete number of categories are converted to factors and inspected with box plots and ANOVA. The ANOVA analysis performed on the Year variable did not show statistical significance. Hence, we decided to drop ‘Year’ as a predicting variable.\n'], 'url_profile': 'https://github.com/anierudhv', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Logit-model\nNaive implementation of linear regression model\n'], 'url_profile': 'https://github.com/Leilusen', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'São Paulo, Brazil', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['LogisticRegressor\nA Logistic Regression implementation with PyTorch\n\n'], 'url_profile': 'https://github.com/paulosestini', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iamvibinvijay', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Bhopal, MP, India', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['Stock Price Prediction with Different Algorithms\nDatasets from: Quandl Financial and Economic Data\n'], 'url_profile': 'https://github.com/krishnachourasia', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Iris-data-classification\nIris data classification using Logistic regression\n'], 'url_profile': 'https://github.com/sanu98', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['fifafinal\nfifa audience linear regression and decision tree\nThis project is an analysis of the impact that televised soccer by FIFA has in worldwide leveles, along with the data that shows the difference of this impact, according to the percentage of audience per country, the GDP of each country or the population density.\n'], 'url_profile': 'https://github.com/tonisbert', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['mini-project\nRegression on the Iowa house prices dataset.\nExplored XGBoost and Random Forest regression.\nHouse keeping\n\n\nthe file standard_ml.yml has the details of the conda environment used.\nYou can see how to create a conda environment from a yml file here.\n\n\nthe file results.csv has the results as they are submitted in the competition.\n\n\nAll the code is in the jupyter notebook TreeBasedRegression.ipynb\n\n\n'], 'url_profile': 'https://github.com/ltsaprounis', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Kaggle_HousePrices_Advanced_Regression_Techniques\nKaggle competition_House Prices: Advanced Regression Techniques\n'], 'url_profile': 'https://github.com/Jessie-Cheng', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Hamirpur, H.P.', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': [""Linear_Regression_0\nLinear Regression with Python and Scikit Learn\nPredicting the house price with Linear Regression.\nThe data contains the following columns:\n'Avg. Area Income': Avg. Income of residents of the city house is located in.\n'Avg. Area House Age': Avg Age of Houses in same city\n'Avg. Area Number of Rooms': Avg Number of Rooms for Houses in same city\n'Avg. Area Number of Bedrooms': Avg Number of Bedrooms for Houses in same city\n'Area Population': Population of city house is located in\n'Price': Price that the house sold at\n'Address': Address for the house\n""], 'url_profile': 'https://github.com/iamchetansharma8', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Shiny Regressor\nSimple shiny app for creating regression plot.\nThe page will show error at the beginning. Simply upload your file with 2 columns and it will show a plot.\nHow it works\nUsers can:\n\nUpload their own file in CSV format, choose separator and decimal.\nCustomize the plot with color, size, and transparency.\nAdd proper title and axis labels.\nChoose regression type from:\n\nLinear\nPolynomial 2 degree\nPolynomial 3 degree\n\n\n\nHosting\nApp is hosted on shinyapps here.\n'], 'url_profile': 'https://github.com/Jarartur', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swasthikshetty', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['multiple-regression\n'], 'url_profile': 'https://github.com/CLozy', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'GPL-3.0 license', 'Updated Jan 18, 2021', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NeelanM1992', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'Gießen, Germany', 'stats_list': [], 'contributions': '2,352 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CSchoel', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'nantes', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hushee69', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Simple Linear Regression\nThe goal of this project was to build a linear regression model from the ground up using Scikit Learn\n'], 'url_profile': 'https://github.com/nandakishorem777', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/arkgawel', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['linear_regression_test\nstudy of linear regression for laboratory works\n'], 'url_profile': 'https://github.com/Alekhina', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'Amsterdam', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': [""Reggie's+Linear+Regression\n""], 'url_profile': 'https://github.com/vivicenisilva', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'Bangladesh', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['Predicting-House-Prices\nPredicting House Prices with Regression using TensorFlow\n'], 'url_profile': 'https://github.com/dhrubapuc23', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayu-singh', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}","{'location': 'Roosendaal', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ThomasKoot', 'info_list': ['1', 'R', 'Updated May 4, 2020', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'JavaScript', 'Updated May 3, 2020']}"
"{'location': 'India', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['Linear-Regression\n1. Dataset : Honey Production in USA\nSource: Kaggle\nLibrary Used : Scikit - Learn\nObjective : Predict The Mean Production(y) given the year(x).\nResult ( Image)\n\n2. Dataset : StreetEasy Manhattan Dataset from Codeacademy\nSource: Codeacademy StreetEasy: StreetEasy is New York City’s leading real estate marketplace — from studios to high-rises, Brooklyn          Heights to Harlem.\nLibrary Used : Scikit Learn\nObjective : Predict the Rent\nModel : Multi Linear Regression\nResult :\n\n3. Dataset : ATP Tennis Players Stats\nSource : Codeacdemy Data Science Course (data uploaded on repo)\nLibrary Used : Scikit-Learn\nObjective : Predict the # of Aces of the Player\nModel : Multilayer Perceptron Model\nResult :\n\n'], 'url_profile': 'https://github.com/quantisedboy', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Regression-and-Bias-Variance\nPerformed polynomial regression and analyzed over-fitting.\nimport numpy as np\nimport math\nimport random as rand\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nx=np.random.rand(100)\nnoise=np.random.normal(0,np.sqrt(0.2),100)\ny = np.zeros(len(x))\nfor i in range(100):\ny[i]=np.exp(math.sin(2np.pix[i]))+x[i] + noise[i]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=1)\ndef func_true(x):\nreturn np.exp(math.sin(2np.pix)) + x\ndef rms_error(y_crrct, y_pred):\nreturn np.sqrt(np.mean((y_crrct - y_pred)2))\nx_target_plot = np.linspace(0, 1, 1e5)\ny_target_plot = np.vectorize(func_true)(x_target_plot)\ntrain_rms_error=np.zeros(4)\ntest_rms_error=np.zeros(4)\nplt.plot(x_target_plot, y_target_plot)\nplt.scatter(x, y, marker=\'*\')\nplt.show()\ntrain_rmse = []\ntest_rmse = []\ndegree_array = [1, 3, 6, 9]\ndegree = 1\nX_1_plot = np.vstack([x_target_plotdegree, x_target_plot0]).T\nX = np.vstack([x_traindegree, x_train0]).T\nX_1_test = np.vstack([x_testdegree, x_test**0]).T\nw = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y_train)\ny_pred_train = np.dot(X, w)\ny_plot = np.dot(X_1_plot, w)\ny_pred_test = np.dot(X_1_test, w)\nplt.scatter(x_train, y_train, marker=\'.\', c=\'g\', label=""Train Data"")\nplt.scatter(x_test, y_test, marker=\'.\', c=\'black\', label=""Test Data"")\nplt.plot(x_target_plot, y_target_plot, c=\'orange\', label=""Target Func"")\nplt.plot(x_target_plot, y_plot, c=\'blue\', label=""Estimated Func"")\nplt.legend()\nplt.title(\'Polynomial Regression for degree 1\')\nplt.xlabel(""x"")\nplt.ylabel(""y"")\nplt.grid(True)\nplt.show()\ntrain_rmse.append(rms_error(y_train, y_pred_train))\ntest_rmse.append(rms_error(y_test, y_pred_test))\nprint(""Test Error: "", rms_error(y_test, y_pred_test))\ndegree = 3\nX_3_plot = np.vstack([x_target_plotdegree, x_target_plot2, x_target_plot, x_target_plot0]).T\nX = np.vstack([x_traindegree, x_train2, x_train, x_train0]).T\nX_3_test = np.vstack([x_testdegree, x_test2, x_test, x_test**0]).T\nw = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y_train)\ny_pred_train = np.dot(X, w)\ny_plot = np.dot(X_3_plot, w)\ny_pred_test = np.dot(X_3_test, w)\nplt.scatter(x_train, y_train, marker=\'.\', c=\'g\', label=""Train Data"")\nplt.scatter(x_test, y_test, marker=\'.\', c=\'black\', label=""Test Data"")\nplt.plot(x_target_plot, y_target_plot, c=\'orange\', label=""Target Func"")\nplt.plot(x_target_plot, y_plot, c=\'blue\', label=""Estimated Func"")\nplt.legend()\nplt.title(\'Polynomial Regression for degree 3\')\nplt.xlabel(""x"")\nplt.ylabel(""y"")\nplt.grid(True)\nplt.show()\nplt.scatter(y_test, y_pred_test)\nplt.plot(np.linspace(0, 4, 100), np.linspace(0, 4, 100), c=\'g\')\nplt.show()\nplt.title(""Training estimate Vs Training target for degree 3"")\nplt.xlabel(""Training target"")\nplt.ylabel(""Training estimate"")\nplt.plot(rms_error(y_train, y_pred_train))\nprint(""RMS Error Train"", rms_error(y_train, y_pred_train))\ntrain_rmse.append(rms_error(y_train, y_pred_train))\ntest_rmse.append(rms_error(y_test, y_pred_test))\ndegree = 6\nX_6_plot = np.vstack([x_target_plotdegree,x_target_plot5,x_target_plot4,x_target_plot3,x_target_plot2,x_target_plot, x_target_plot0]).T\nX = np.vstack([x_traindegree, x_train5, x_train4, x_train3, x_train2, x_train, x_train0]).T\nX_6_test = np.vstack([x_testdegree,x_test5,x_test4,x_test3,x_test2, x_test, x_test0]).T\nw = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y_train)\ny_pred_train = np.dot(X, w)\ny_plot = np.dot(X_6_plot, w)\ny_pred_test = np.dot(X_6_test, w)\nplt.scatter(x_train, y_train, marker=\'.\', c=\'g\', label=""Train Data"")\nplt.scatter(x_test, y_test, marker=\'.\', c=\'black\', label=""Test Data"")\nplt.plot(x_target_plot, y_target_plot, c=\'orange\', label=""Target Func"")\nplt.plot(x_target_plot, y_plot, c=\'blue\', label=""Estimated Func"")\nplt.legend()\nplt.title(\'Polynomial Regression for degree 6\')\nplt.xlabel(""x"")\nplt.ylabel(""y"")\nplt.grid(True)\nplt.show()\ntrain_rmse.append(rms_error(y_train, y_pred_train))\ntest_rmse.append(rms_error(y_test, y_pred_test))\ndegree = 9\nX_9_plot = np.vstack([x_target_plotdegree,x_target_plot8,x_target_plot7,x_target_plot6,x_target_plot5,x_target_plot4,x_target_plot3,x_target_plot2,x_target_plot, x_target_plot0]).T\nX = np.vstack([x_traindegree,x_train8,x_train7,x_train6,x_train5, x_train4, x_train3, x_train2, x_train, x_train0]).T\nX_9_test = np.vstack([x_testdegree,x_test8,x_test7,x_test6,x_test5,x_test4,x_test3,x_test2, x_test, x_test**0]).T\nw = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y_train)\ny_pred_train = np.dot(X, w)\ny_plot = np.dot(X_9_plot, w)\ny_pred_test = np.dot(X_9_test, w)\nplt.scatter(x_train, y_train, marker=\'.\', c=\'g\', label=""Train Data"")\nplt.scatter(x_test, y_test, marker=\'.\', c=\'black\', label=""Test Data"")\nplt.plot(x_target_plot, y_target_plot, c=\'orange\', label=""Target Func"")\nplt.plot(x_target_plot, y_plot, c=\'blue\', label=""Estimated Func"")\nplt.legend()\nplt.title(\'Polynomial Regression for degree 9\')\nplt.xlabel(""x"")\nplt.ylabel(""y"")\nplt.grid(True)\nplt.show()\ntrain_rmse.append(rms_error(y_train, y_pred_train))\ntest_rmse.append(rms_error(y_test, y_pred_test))\nplt.plot(degree_array, train_rmse, label=""Train RMSE"")\nplt.plot(degree_array, test_rmse, label=""Test RMSE"")\nplt.legend()\nplt.ylim(0, 10)\nplt.show()\n'], 'url_profile': 'https://github.com/RITESH8055', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Portugal', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jeevora', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rohan1320', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['""ML""\n'], 'url_profile': 'https://github.com/python-plusplus', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nitin-22', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Indonesia', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['Regression-\n'], 'url_profile': 'https://github.com/dhava-stmkg', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Hayward', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': [""Machine-Learning-Projects\nThese are my projects for machine learning.\nI use regression analysis, logistic regression, hypothesis testing, time series and differnt models to train my data.\nHypothesis Analysis on Marketing Campaign and Driving Miles\n\nAnalyze the SFO and LAX data sets and determine if either marketing campaign was successful in raising the average miles driven per Uber driver.\n\nLogistic Regression on Customers Transaction Prediction\n\nUse logistic regression to predict when customers are going to transact\nDetermine the causes for a transaction\nEvaluated the performance the model\n\nLogistic Regression on Employee Turnover Prediction\n\nUse logistic regression to predict when people are going to leave a company\nDetermine the causes for attrition\nEvaluated the performance the model\n\nPredict Salary by Lasso & Ridge Regularization\n\nUse regularization to predict salaries for a sports player\nExplain the output of the regularized models\n\nRegression Analysis on Housing Price\n\nRemove/manipulate/transform features from the data set, remain only useful data\nGraphically and numerically describe model performance and find the relation between them\nApply regression analysis techniques and EDA principles to find out what features will influence the rental price\n\nTelecom Customer Churn Prediction By Using Different Machine Learning algorithms\n\nTrialed a list of different Machine Learning algorithms, such as Logistic Regression(with Lasso & Ridge), Decision Tree, KNN Classifier, and Random Forest Classifier, and Linear Regression to predict potential customer churn and customer life time value.\nProvided the best model that has achieved the highest AUC value with lowest MSE(Mean Squared Error).\nContructed the particial dependece plot to discover how the most 6 importance features related to the customer churn.\n\nTime Series Analysis Using ARIMA on Electro Data Prediction\n\nImplemented ARIMA model, analyzed 2 data sets to predict the values for the next 8 time periods and the subsequent 7 years (with confidence intervals), and make 3 observations about the data (i.e., describe its composition and characteristics).\n\nForecasting on Video CTR\n\nUsing Moving Average, Exponential smoothing, AR and ARIMA model to forecast video CTR (click through rate)\nSelect a performance measure for the model and pick the best performing model with lowest MSE.\n\nMultiple Regression Analysis on Civilization VI Game Players' Active Days\n\nDetermine the causes of active day\nUse multiple regression model to predict players' active day\n\n""], 'url_profile': 'https://github.com/Kristyyy', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Indonesia', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/FebrianiFR', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Trivandrum,Kerala', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Car-Price-Prediction\nCar Price Prediction dataset used along with RFE and KFold for Linear Regression since it is a regression based project.\n'], 'url_profile': 'https://github.com/lekmeera', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 3, 2020', 'MATLAB', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Jack\nDiscontinuous_Regression\n'], 'url_profile': 'https://github.com/JF11579', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['STLF_IRISH_RESIDENCES\nShort Term Load Forecasting of Load consumption data from Irish Residences using Polynomial Regression and Machine learning techniques\nFor this study we have executed all the 4 models and obtained the MAPE scores\ngiven in the model results benchmark table.The polynomial regression being simple model which has\nthe least complexity of parameters as well as it is lighter on the system to run. This\nmodel helps us achieve our goal of comparing the linear models with non-linear\nmodels. These experiments establish the fact that linear models although being the\nsimpler and less complex ones are not an optimal option for short term load\nforecasting of our smart energy meter dataset. Random forest algorithm is also a non-\nlinear model which we have implemented as a simple machine learning model which\ngives us a better result than the polynomial regression but the deep learning model we\nhave used which are LSTM and LSTM-stacked which gives us better results than the\nrandom forest ensemble of regression trees. The MAPE scores of LSTM and Multi-\nstacked LSTM are close to each other with the simpler version giving better score,\nreason being it Is simple in terms of complexity and as well it takes less time to\nexecute with hyper parameter optimization algorithm. The LSTM-multi stacked gives\na slightly higher MAPE with a difference of 0.14777. In theory Multi-stacked LSTM\nshould give better results although in this case we have limited number of computing\nresources and time to train the model. If we execute the model tuning more\nhyperparameters we are bound to achieve best results with the multi stacked LSTM\nhowever, LSTM multi stacked is too complex and computation intensive which does\nnot fit in our goal to establish a model which is less computation intensive and\naccurate. From the perspective of establishing a model with low complexity yet\nproviding a accurate result we come to a conclusion that the Single stacked LSTM is a\nbetter model for predicting short term load forecast.\n'], 'url_profile': 'https://github.com/pratikmoghe', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Pune, Maharashtra, India', 'stats_list': [], 'contributions': '323 contributions\n        in the last year', 'description': ['StockPrediction\nStock Prediction using machine learning model- Linear Regression and Decision Tree Regressor\n'], 'url_profile': 'https://github.com/LuffyAnshul', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Regression Week 3: Assessing Fit (polynomial regression)\nIn this notebook you will compare different regression models in order to assess which model fits best. We will be using polynomial regression as a means to examine this topic. In particular you will:\n\nWrite a function to take an SArray and a degree and return an SFrame where each column is the SArray to a polynomial value up to the total degree e.g. degree = 3 then column 1 is the SArray column 2 is the SArray squared and column 3 is the SArray cubed\nUse matplotlib to visualize polynomial regressions\nUse matplotlib to visualize the same polynomial degree on different subsets of the data\nUse a validation set to select a polynomial degree\nAssess the final fit using test data\n\nWe will continue to use the House data from previous notebooks.\nFire up Turi Create\nimport turicreate\nNext we\'re going to write a polynomial function that takes an SArray and a maximal degree and returns an SFrame with columns containing the SArray to all the powers up to the maximal degree.\nThe easiest way to apply a power to an SArray is to use the .apply() and lambda x: functions.\nFor example to take the example array and compute the third power we can do as follows: (note running this cell the first time may take longer than expected since it loads Turi Create)\ntmp = turicreate.SArray([1., 2., 3.])\ntmp_cubed = tmp.apply(lambda x: x**3)\nprint(tmp)\nprint(tmp_cubed)\n[1.0, 2.0, 3.0]\n[1.0, 8.0, 27.0]\n\nWe can create an empty SFrame using turicreate.SFrame() and then add any columns to it with ex_sframe[\'column_name\'] = value. For example we create an empty SFrame and make the column \'power_1\' to be the first power of tmp (i.e. tmp itself).\nex_sframe = turicreate.SFrame()\nex_sframe[\'power_1\'] = tmp\nprint(ex_sframe)\n+---------+\n| power_1 |\n+---------+\n|   1.0   |\n|   2.0   |\n|   3.0   |\n+---------+\n[3 rows x 1 columns]\n\nPolynomial_sframe function\nUsing the hints above complete the following function to create an SFrame consisting of the powers of an SArray up to a specific degree:\ndef polynomial_sframe(feature, degree):\n    # assume that degree >= 1\n    # initialize the SFrame:\n    poly_sframe = turicreate.SFrame()\n    # and set poly_sframe[\'power_1\'] equal to the passed feature\n    poly_sframe[\'power_1\'] = feature\n    # first check if degree > 1\n    if degree > 1:\n        # then loop over the remaining degrees:\n        # range usually starts at 0 and stops at the endpoint-1. We want it to start at 2 and stop at degree\n        for power in range(2, degree+1): \n            # first we\'ll give the column a name:\n            name = \'power_\' + str(power)\n            # then assign poly_sframe[name] to the appropriate power of feature\n            poly_sframe[name] = poly_sframe[\'power_1\'].apply(lambda x: x**power)\n    return poly_sframe\nTo test your function consider the smaller tmp variable and what you would expect the outcome of the following call:\nprint(polynomial_sframe(tmp, 3))\n+---------+---------+---------+\n| power_1 | power_2 | power_3 |\n+---------+---------+---------+\n|   1.0   |   1.0   |   1.0   |\n|   2.0   |   4.0   |   8.0   |\n|   3.0   |   9.0   |   27.0  |\n+---------+---------+---------+\n[3 rows x 3 columns]\n\nVisualizing polynomial regression\nLet\'s use matplotlib to visualize what a polynomial regression looks like on some real data.\nsales = turicreate.SFrame(\'home_data.sframe/\')\nAs in Week 3, we will use the sqft_living variable. For plotting purposes (connecting the dots), you\'ll need to sort by the values of sqft_living. For houses with identical square footage, we break the tie by their prices.\nsales = sales.sort([\'sqft_living\', \'price\'])\nLet\'s start with a degree 1 polynomial using \'sqft_living\' (i.e. a line) to predict \'price\' and plot what it looks like.\npoly1_data = polynomial_sframe(sales[\'sqft_living\'], 1)\npoly1_data[\'price\'] = sales[\'price\'] # add price to the data since it\'s the target\nNOTE: for all the models in this notebook use validation_set = None to ensure that all results are consistent across users.\nmodel1 = turicreate.linear_regression.create(poly1_data, target = \'price\', features = [\'power_1\'], validation_set = None)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 21613\nNumber of features          : 1\nNumber of unpacked features : 1\nNumber of coefficients    : 2\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 1.009580     | 4362074.696077     | 261440.790724                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n#let\'s take a look at the weights before we plot\nmodel1.coefficients\n\n\nname\nindex\nvalue\nstderr\n\n\n(intercept)\nNone\n-43579.08525145019\n4402.689697427721\n\n\npower_1\nNone\n280.6227708858474\n1.936398555132125\n\n\n[2 rows x 4 columns]\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(poly1_data[\'power_1\'],poly1_data[\'price\'],\'.\',\n         poly1_data[\'power_1\'], model1.predict(poly1_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7fe680c1d9e8>,\n <matplotlib.lines.Line2D at 0x7fe680c1db38>]\n\n\nLet\'s unpack that plt.plot() command. The first pair of SArrays we passed are the 1st power of sqft and the actual price we then ask it to print these as dots \'.\'. The next pair we pass is the 1st power of sqft and the predicted values from the linear model. We ask these to be plotted as a line \'-\'.\nWe can see, not surprisingly, that the predicted values all fall on a line, specifically the one with slope 280 and intercept -43579. What if we wanted to plot a second degree polynomial?\npoly2_data = polynomial_sframe(sales[\'sqft_living\'], 2)\nmy_features = poly2_data.column_names() # get the name of the features\npoly2_data[\'price\'] = sales[\'price\'] # add price to the data since it\'s the target\nmodel2 = turicreate.linear_regression.create(poly2_data, target = \'price\', features = my_features, validation_set = None)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 21613\nNumber of features          : 2\nNumber of unpacked features : 2\nNumber of coefficients    : 3\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.008588     | 5913020.984255     | 250948.368758                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\nmodel2.coefficients\n\n\nname\nindex\nvalue\nstderr\n\n\n(intercept)\nNone\n199222.4964446195\n7058.004835516299\n\n\npower_1\nNone\n67.99406406773976\n5.287872013161773\n\n\npower_2\nNone\n0.038581231278915384\n0.0008982465470323439\n\n\n[3 rows x 4 columns]\n\nplt.plot(poly2_data[\'power_1\'],poly2_data[\'price\'],\'.\',\n         poly2_data[\'power_1\'], model2.predict(poly2_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7fe6806f26a0>,\n <matplotlib.lines.Line2D at 0x7fe6806f2780>]\n\n\nThe resulting model looks like half a parabola. Try on your own to see what the cubic looks like:\npoly3_data = polynomial_sframe(sales[\'sqft_living\'], 3)\nmy_features = poly3_data.column_names() # get the name of the features\npoly3_data[\'price\'] = sales[\'price\'] # add price to the data since it\'s the target\nmodel3 = turicreate.linear_regression.create(poly3_data, target = \'price\', features = my_features, validation_set = None)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 21613\nNumber of features          : 3\nNumber of unpacked features : 3\nNumber of coefficients    : 4\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.015488     | 3261066.736008     | 249261.286346                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\nmodel3.coefficients\n\n\nname\nindex\nvalue\nstderr\n\n\n(intercept)\nNone\n336788.1179517966\n10661.015371317615\n\n\npower_1\nNone\n-90.14762361186747\n10.622289184419227\n\n\npower_2\nNone\n0.08703671508097557\n0.002966306231483158\n\n\npower_3\nNone\n-3.839852119597755e-06\n2.241749095900145e-07\n\n\n[4 rows x 4 columns]\n\nplt.plot(poly3_data[\'power_1\'],poly3_data[\'price\'],\'.\',\n         poly3_data[\'power_1\'], model3.predict(poly3_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7fe6806e8ba8>,\n <matplotlib.lines.Line2D at 0x7fe6806e8c88>]\n\n\nNow try a 15th degree polynomial:\npoly15_data = polynomial_sframe(sales[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names() # get the name of the features\npoly15_data[\'price\'] = sales[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data, target = \'price\', features = my_features, validation_set = None)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 21613\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.013692     | 2662308.584339     | 245690.511190                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\nmodel15.coefficients\n\n\nname\nindex\nvalue\nstderr\n\n\n(intercept)\nNone\n73619.75210522377\nnan\n\n\npower_1\nNone\n410.2874625479694\nnan\n\n\npower_2\nNone\n-0.23045071443460427\nnan\n\n\npower_3\nNone\n7.588405424472302e-05\nnan\n\n\npower_4\nNone\n-5.657018025607986e-09\nnan\n\n\npower_5\nNone\n-4.570281308121097e-13\nnan\n\n\npower_6\nNone\n2.6636020659609474e-17\nnan\n\n\npower_7\nNone\n3.385847693136091e-21\nnan\n\n\npower_8\nNone\n1.1472310407255558e-25\nnan\n\n\npower_9\nNone\n-4.6529358647462826e-30\nnan\n\n\n[16 rows x 4 columns]Note: Only the head of the SFrame is printed.You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7fe68065de48>,\n <matplotlib.lines.Line2D at 0x7fe68065df28>]\n\n\nWhat do you think of the 15th degree polynomial? Do you think this is appropriate? If we were to change the data do you think you\'d get pretty much the same curve? Let\'s take a look.\nChanging the data and re-learning\nWe\'re going to split the sales data into four subsets of roughly equal size. Then you will estimate a 15th degree polynomial model on all four subsets of the data. Print the coefficients (you should use .print_rows(num_rows = 16) to view all of them) and plot the resulting fit (as we did above). The quiz will ask you some questions about these results.\nTo split the sales data into four subsets, we perform the following steps:\n\nFirst split sales into 2 subsets with .random_split(0.5, seed=0).\nNext split the resulting subsets into 2 more subsets each. Use .random_split(0.5, seed=0).\n\nWe set seed=0 in these steps so that different users get consistent results.\nYou should end up with 4 subsets (set_1, set_2, set_3, set_4) of approximately equal size.\n(half_1, half_2) = sales.random_split(0.5, seed=0)\n(set_1, set_2) = half_1.random_split(0.5, seed=0)\n(set_3, set_4) = half_2.random_split(0.5, seed=0)\nFit a 15th degree polynomial on set_1, set_2, set_3, and set_4 using sqft_living to predict prices. Print the coefficients and make a plot of the resulting model.\npoly15_data = polynomial_sframe(set_1[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names() # get the name of the features\npoly15_data[\'price\'] = set_1[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data, target = \'price\', features = my_features, validation_set = None)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5404\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.014660     | 2195218.932305     | 248858.822200                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+-------------------------+-----------------------+\n|     name    | index |          value          |         stderr        |\n+-------------+-------+-------------------------+-----------------------+\n| (intercept) |  None |    223312.75024735887   |          nan          |\n|   power_1   |  None |    118.08612759009429   |          nan          |\n|   power_2   |  None |  -0.047348201136106494  |          nan          |\n|   power_3   |  None |  3.253103424735485e-05  |          nan          |\n|   power_4   |  None |  -3.323721525708726e-09 |          nan          |\n|   power_5   |  None |   -9.7583045756326e-14  |          nan          |\n|   power_6   |  None |  1.1544030339970148e-17 |          nan          |\n|   power_7   |  None |  1.0514586941310895e-21 | 9.834107444697464e-17 |\n|   power_8   |  None |  3.4604961652319043e-26 |          nan          |\n|   power_9   |  None | -1.0965445396033781e-30 |          nan          |\n|   power_10  |  None | -2.4203181214715775e-34 |          nan          |\n|   power_11  |  None | -1.9960120684331556e-38 | 7.530139806380401e-33 |\n|   power_12  |  None | -1.0770990387847978e-42 |          nan          |\n|   power_13  |  None |  -2.728628177229153e-47 |          nan          |\n|   power_14  |  None |  2.447826934581167e-51  |          nan          |\n|   power_15  |  None |  5.0197523270931016e-55 |          nan          |\n+-------------+-------+-------------------------+-----------------------+\n[16 rows x 4 columns]\n\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7fe6805d5358>,\n <matplotlib.lines.Line2D at 0x7fe6805d5438>]\n\n\npoly15_data = polynomial_sframe(set_2[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names() # get the name of the features\npoly15_data[\'price\'] = set_2[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data, target = \'price\', features = my_features, validation_set = None)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5398\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.020364     | 2069212.978547     | 234840.067186                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+-------------------------+----------------------+\n|     name    | index |          value          |        stderr        |\n+-------------+-------+-------------------------+----------------------+\n| (intercept) |  None |    89836.50773678801    |  1068153.0338730316  |\n|   power_1   |  None |    319.8069467541447    |  5161.370721704783   |\n|   power_2   |  None |   -0.10331539703285443  |  9.765764190591652   |\n|   power_3   |  None |  1.0668247603610658e-05 | 0.007986774844848023 |\n|   power_4   |  None |  5.7557709775688265e-09 |         nan          |\n|   power_5   |  None |  -2.54663464694922e-13  |         nan          |\n|   power_6   |  None | -1.0964134508136501e-16 |         nan          |\n|   power_7   |  None |  -6.364584415677271e-21 |         nan          |\n|   power_8   |  None |  5.5256041690461245e-25 |         nan          |\n|   power_9   |  None |  1.3508203898572387e-28 |         nan          |\n|   power_10  |  None |  1.1840818823275861e-32 |         nan          |\n|   power_11  |  None |  1.9834800062111657e-37 |         nan          |\n|   power_12  |  None |  -9.925335906219101e-41 |         nan          |\n|   power_13  |  None |  -1.608348470351073e-44 |         nan          |\n|   power_14  |  None |  -9.120060241709453e-49 |         nan          |\n|   power_15  |  None |  1.686366583204479e-52  |         nan          |\n+-------------+-------+-------------------------+----------------------+\n[16 rows x 4 columns]\n\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7fe68053de48>,\n <matplotlib.lines.Line2D at 0x7fe68053df28>]\n\n\npoly15_data = polynomial_sframe(set_3[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names() # get the name of the features\npoly15_data[\'price\'] = set_3[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data, target = \'price\', features = my_features, validation_set = None)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5409\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.022606     | 2269769.506523     | 251460.072754                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+-------------------------+------------------------+\n|     name    | index |          value          |         stderr         |\n+-------------+-------+-------------------------+------------------------+\n| (intercept) |  None |     87317.9795534432    |   1312239.9698394272   |\n|   power_1   |  None |    356.30491104641953   |   6366.570002421361    |\n|   power_2   |  None |   -0.16481744280817676  |   12.890135699293946   |\n|   power_3   |  None |  4.404249926875631e-05  |  0.014320866376643955  |\n|   power_4   |  None |   6.48234876349399e-10  | 9.596328147110607e-06  |\n|   power_5   |  None |  -6.752532265620608e-13 | 3.9261381225379545e-09 |\n|   power_6   |  None | -3.3684259273123967e-17 | 8.329476496006475e-13  |\n|   power_7   |  None |  3.609997042271464e-21  |          nan           |\n|   power_8   |  None |  6.469997256952375e-25  |          nan           |\n|   power_9   |  None |  4.2363938881230826e-29 |          nan           |\n|   power_10  |  None | -3.6214942566554345e-34 | 6.225887372764209e-28  |\n|   power_11  |  None |  -4.271195272909962e-37 | 8.913513084503276e-32  |\n|   power_12  |  None |  -5.614459718165853e-41 | 9.036816267021372e-36  |\n|   power_13  |  None |  -3.874527729174895e-45 | 7.859525708046308e-40  |\n|   power_14  |  None |   4.69430360106533e-50  | 3.867807896979562e-44  |\n|   power_15  |  None |  6.390458860118455e-53  | 7.755384726650742e-49  |\n+-------------+-------+-------------------------+------------------------+\n[16 rows x 4 columns]\n\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7fe680523780>,\n <matplotlib.lines.Line2D at 0x7fe680523860>]\n\n\npoly15_data = polynomial_sframe(set_4[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names() # get the name of the features\npoly15_data[\'price\'] = set_4[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data, target = \'price\', features = my_features, validation_set = None)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5402\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.020047     | 2314893.173826     | 244563.136754                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+-------------------------+------------------------+\n|     name    | index |          value          |         stderr         |\n+-------------+-------+-------------------------+------------------------+\n| (intercept) |  None |    259020.87944831268   |   1555657.2648813187   |\n|   power_1   |  None |    -31.72771619349126   |   10206.331532978036   |\n|   power_2   |  None |   0.10970276960581996   |   27.945340898358204   |\n|   power_3   |  None | -1.5838384727820846e-05 |  0.04233393833972426   |\n|   power_4   |  None |  -4.476606239124062e-09 | 3.961676931695559e-05  |\n|   power_5   |  None |  1.1397657348710987e-12 |  2.40253601675784e-08  |\n|   power_6   |  None |  1.9766912057280682e-16 | 9.444623626379352e-12  |\n|   power_7   |  None | -6.1578367894174755e-21 | 2.1424184602263764e-15 |\n|   power_8   |  None |  -4.880123041026591e-24 |          nan           |\n|   power_9   |  None |  -6.621867812721926e-28 |          nan           |\n|   power_10  |  None |  -2.706315833472915e-32 |          nan           |\n|   power_11  |  None |  6.723704116417694e-36  |          nan           |\n|   power_12  |  None |  1.7411564629955993e-39 |  9.88390219483489e-35  |\n|   power_13  |  None |  2.0918837568633785e-43 | 1.2401250941213685e-38 |\n|   power_14  |  None |  4.7801556582516095e-48 | 6.0982722216174955e-43 |\n|   power_15  |  None | -4.7453533306938734e-51 | 1.299544023952453e-47  |\n+-------------+-------+-------------------------+------------------------+\n[16 rows x 4 columns]\n\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7fe680496198>,\n <matplotlib.lines.Line2D at 0x7fe680496278>]\n\n\nSome questions you will be asked on your quiz:\nQuiz Question: Is the sign (positive or negative) for power_15 the same in all four models?\nNo\nQuiz Question: (True/False) the plotted fitted lines look the same in all four plots\nFalse\nSelecting a Polynomial Degree\nWhenever we have a ""magic"" parameter like the degree of the polynomial there is one well-known way to select these parameters: validation set. (We will explore another approach in week 4).\nWe split the sales dataset 3-way into training set, test set, and validation set as follows:\n\nSplit our sales data into 2 sets: training_and_validation and testing. Use random_split(0.9, seed=1).\nFurther split our training data into two sets: training and validation. Use random_split(0.5, seed=1).\n\nAgain, we set seed=1 to obtain consistent results for different users.\n(training_and_validation, testing) = sales.random_split(0.9, seed=1)\n(training, validation) = training_and_validation.random_split(0.5, seed=1)\nNext you should write a loop that does the following:\n\nFor degree in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] (to get this in python type range(1, 15+1))\n\nBuild an SFrame of polynomial data of train_data[\'sqft_living\'] at the current degree\nhint: my_features = poly_data.column_names() gives you a list e.g. [\'power_1\', \'power_2\', \'power_3\'] which you might find useful for turicreate.linear_regression.create( features = my_features)\nAdd train_data[\'price\'] to the polynomial SFrame\nLearn a polynomial regression model to sqft vs price with that degree on TRAIN data\nCompute the RSS on VALIDATION data (here you will want to use .predict()) for that degree and you will need to make a polynmial SFrame using validation data.\n\n\nReport which degree had the lowest RSS on validation data (remember python indexes from 0)\n\n(Note you can turn off the print out of linear_regression.create() with verbose = False)\ndef get_RSS(model, data, outcome):\n    # First get the predictions\n    predicted = model.predict(data);\n    # Then compute the residuals/errors\n    errors = outcome-predicted;\n    # Then square and add them up    \n    RSS = (errors*errors).sum();\n    return(RSS)  \nfrom heapq import heappush, heappop\ndef lowest_RSS_degree_model (train_data_set, validation_data_set, feature, output_feature, degrees):\n    if degrees>1 :\n        RSSs = []\n        models = []\n        heap = []\n        for degree in range (1, degrees+1):\n            poly_data = polynomial_sframe(train_data_set[feature], degree)\n            my_features = poly_data.column_names()\n            poly_data[output_feature] = train_data_set[output_feature]\n            model = turicreate.linear_regression.create(poly_data,\n                                                        target = output_feature,\n                                                        features = my_features,\n                                                        validation_set = None,\n                                                        verbose= False,\n                                                        l2_penalty=0., \n                                                        l1_penalty=0.)\n            RSS = get_RSS(model, polynomial_sframe(validation_data_set[feature], degree), validation_data_set[output_feature])\n            #save RSS into a min heap\n            heappush(heap, (RSS,degree))\n            RSSs.append(RSS)\n            models.append(model)\n        min_RSS = min(RSSs)\n        min_model = models[RSSs.index(min_RSS)]\n        print(heap)\n    return (min_model)\nQuiz Question: Which degree (1, 2, …, 15) had the lowest RSS on Validation data?\nbest_model = lowest_RSS_degree_model(training, validation, \'sqft_living\', \'price\', 15)\n[(592395859849004.5, 6), (592677914323034.9, 8), (598827152777892.9, 5), (598630662756922.8, 9), (609123922774459.5, 4), (616719668845846.8, 3), (605727492843186.8, 7), (676709739838073.2, 1), (607091004045995.0, 2), (5868658570329244.0, 10), (8.560309284673024e+16, 11), (2.1794004014702093e+17, 12), (3.259038639091958e+17, 13), (6.557335167248488e+17, 14), (7.322855169735261e+17, 15)]\n\nNow that you have chosen the degree of your polynomial using validation data, compute the RSS of this model on TEST data. Report the RSS on your quiz.\nQuiz Question: what is the RSS on TEST data for the model with the degree selected from Validation data?\nget_RSS(best_model, polynomial_sframe(testing[\'sqft_living\'], 6), testing[\'price\'])\n123989069495092.97\n\nbest_model.coefficients\n\n\nname\nindex\nvalue\nstderr\n\n\n(intercept)\nNone\n-138616.6434657832\n62226.86451772267\n\n\npower_1\nNone\n942.5229675125695\n124.71517714313305\n\n\npower_2\nNone\n-0.7182838465772793\n0.09107619749761804\n\n\npower_3\nNone\n0.000288023742327898\n3.101699716301387e-05\n\n\npower_4\nNone\n-5.220252694798691e-08\n5.221555393589805e-09\n\n\npower_5\nNone\n4.3921964392805715e-12\n4.1533770715441225e-13\n\n\npower_6\nNone\n-1.3597954959742855e-16\n1.2268199412649673e-17\n\n\n[7 rows x 4 columns]\n\n'], 'url_profile': 'https://github.com/garabaya', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Poland', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maticel', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cpratt0', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'São Paulo', 'stats_list': [], 'contributions': '389 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pedropadilha13', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['Sales_Prediction\nThe dataset is taken from kaggle (Reference: https://www.kaggle.com/sazid28/advertising.csv)\nI have implemented Linear Regression to predict the sales of a company based upon the expenses made over advertisement sources, such as TV, Newspaper, Radio. \nThe main aim is to analyse which advertisement source provides more profit for the sales.\nAdvertising.csv contains the dataset. \nSales_Prediction.ipynb is the jupyter notebook containing the implementation. \nComparision of Expenses vs TV: \n\nComparision of Expenses vs Three advertising sources: \n\n'], 'url_profile': 'https://github.com/DamnikJain1', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Seoul', 'stats_list': [], 'contributions': '524 contributions\n        in the last year', 'description': ['This project was bootstrapped with Create React App.\nAvailable Scripts\nIn the project directory, you can run:\nyarn start\nRuns the app in the development mode.\nOpen http://localhost:3000 to view it in the browser.\nThe page will reload if you make edits.\nYou will also see any lint errors in the console.\nyarn test\nLaunches the test runner in the interactive watch mode.\nSee the section about running tests for more information.\nyarn build\nBuilds the app for production to the build folder.\nIt correctly bundles React in production mode and optimizes the build for the best performance.\nThe build is minified and the filenames include the hashes.\nYour app is ready to be deployed!\nSee the section about deployment for more information.\nyarn eject\nNote: this is a one-way operation. Once you eject, you can’t go back!\nIf you aren’t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.\nYou don’t have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.\nLearn More\nYou can learn more in the Create React App documentation.\nTo learn React, check out the React documentation.\n'], 'url_profile': 'https://github.com/ganadara135', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}","{'location': 'Vila Velha-ES', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': [""machine_learning_on_stocks\nLinear Regression\nFeatures and Labels\nThe features are the descriptive attributes, and the label is what you're attempting to predict or forecast.\nimportant Packages\nnumpy module to convert data to numpy arrays, which is what Scikit-learn wants.\nPreprocessing is the module used to do some cleaning/scaling of data prior to machine learning, and model_selection is used in the testing stages.\nLinearRegression algorithm as well as svm from Scikit-learn, which we'll be using as our machine learning algorithms to demonstrate results.\nResults\ncomparing line is what happened in real life\nresults Disney Stocks\n\nresults Microsoft Stocks\n\nimportant links\nchoosing the right classifier scikit.learn\n""], 'url_profile': 'https://github.com/PedroFabriz2', 'info_list': ['R', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated May 25, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'R', 'Updated Jun 21, 2020', 'JavaScript', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 3, 2020', 'Python', 'Updated May 2, 2020']}"
"{'location': 'Canton, NY', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': [""no_co_wild_phase_1_models\nmultiple regression modeling for Kate's thesis data\n""], 'url_profile': 'https://github.com/erethizon', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Multiple-Functional-Brain-Networks-Related-to-Pain-Perception-Revealed-by-fMRI\nThis code carries out regression analyses to associate pain ratings with network activation intensities, both within and across individuals. They were applied in Damascelli, M., Woodward, T. S., Sanford, N., Zahid, H. B., Lim, R., Scott, A., & Kramer, J. K. Multiple Functional Brain Networks Related to Pain Perception Revealed by fMRI.\nAll functions can be run directly from ""RegressionAnalyses.m"".\n'], 'url_profile': 'https://github.com/MatteoDamascelli', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Titanic-survivors-prediction\nIt consists of Titanic survivors prediction using Logistic Regression\n'], 'url_profile': 'https://github.com/nikhilmshebannavar', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['En-Fin_word_classifier\nEnglish/Finnish word clasifier using ML (linear regression)\n'], 'url_profile': 'https://github.com/vlad-berlea', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['ConcreteStrength\npredicting the strength of concrete by regression (Keras and Python)\n'], 'url_profile': 'https://github.com/m4hmadi', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'PUNE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhaychougule', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""UWashingtonRegression\nCoursework from UWashington's Coursera course on Regression\n""], 'url_profile': 'https://github.com/AbhishekPaitya', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhydemi', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Linear-Regression\nImplementation of linear regression, a supervised learning algorithm\n'], 'url_profile': 'https://github.com/desais4', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['MultipleLinearRegression\nMultiple Linear Regression using scikit learn.\n'], 'url_profile': 'https://github.com/atusneem', 'info_list': ['HTML', 'Updated May 4, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Python', 'Updated May 3, 2020', 'R', 'MIT license', 'Updated May 2, 2020', 'Python', 'Updated Apr 28, 2020']}"
"{'location': 'Krakow', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['Car_price_predictions_ML_Regression\n'], 'url_profile': 'https://github.com/justynatrojniak', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AbhishekSuman25', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'Seoul, South Korea', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-Model-Using-R\nA multiple linear regression model based prediction done on suicide dataset\nA small research done for the course - Time Series Analysis and Forecasting\n'], 'url_profile': 'https://github.com/EsratMaria', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/boooooogey', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chaitanyahardikar', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['ML-Java-Weka-Linear-regression-model\nJava project to implement linear regression using weka source files\n'], 'url_profile': 'https://github.com/ShivamPatil27', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '183 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/savanismit', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['AI_PLA_Regression_and_Classification\nImplementation of Preception Learning Algorithm (PLA), Linear Regression and Classification\nThere are three parts to this implementation.\nI. Perceptron Learning Algorithm - PLA (problem1)\nII. Linear Regression (problem2)\nIII. Classification (problem3)\n•\tSVM with Linear Kernel.\n•\tSVM with Polynomial Kernel.\n•\tSVM with RBF Kernel.\n•\tLogistic Regression.\n•\tk-Nearest Neighbors.\n•\tDecision Trees.\n•\tRandom Forest.\nTo run the program use the following command:\n$ python3 problemX.py inputX.csv outputX.csv\nwhere X is the number of problem to execute.\n'], 'url_profile': 'https://github.com/IntsarSaeed', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'São Paulo, Brazil', 'stats_list': [], 'contributions': '291 contributions\n        in the last year', 'description': ['Enhanced House Price Predictions\nMachine learning - Regression - Enhanced house price predictions - Kaggle competition\nThis repository contains datasets, python code (.ipynb Jupyter notebook) and support files used in Kaggle\'s ""Housing Prices Competition for Kaggle Learn Users"" (April 2020).\n'], 'url_profile': 'https://github.com/pcbreviglieri', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '252 contributions\n        in the last year', 'description': ['Ylann Rouzaire, all rights reserved.\nKernel Regression Dynamics\nGoal of the project\nInvestigate the dynamics of learning in a Teacher/Student Kernel Regression framework\nStructure of the code\nThe project is small enough for the code to be organized as follows :\n\nAll the functions are defined in function_definitions.jl\nThe main.jl defines the keys arguments, among them the number of statistics to collect and distributes the work on different processors thanks to the function Run.\nThe data is saved in JLD files by the Run function so that the analysis can be performed later by the analysis.jl file.\nThe benchmark.jl is a test file and therefore may contain deprecated syntax.\n\nSome remarks\n\n\nThe Teacher/Student framework for supervised Regression is explained in details in the reportfile.\n\n\nThe kernels we use for Teachers are from the Matérn family : Read more. The Student kernel is kept fixed to Matérn[ν = 1/2]  (aka Laplace kernel or exponential kernel). These kernels are isotropic and translation invariant. They are coded in the function k(h) .\n\n\nThe data is generated uniformly on the unit hypersphere of dimension d (for clarity : d=1 means the unit circle and d=2 means the usual sphere embedded in the natural 3 dimensions) by normalizing (to 1) points from a multivariate random normal distribution in d+1 dimensions. From this data one can then extract the training sets and testing sets. Therefore, to constructs theses sets, one has to call successively generate_X(...) ,  generate_Z(...) , extract_TestSet(...) , extract_TrainSet(...) .\n\n\nFor prediction, the Student needs a Gram Matrix K defined as : (K)ij = k(x_i,x_j). This positive definite matrix is computed is the ConstructGramMatrices function. The pdness is mathematically guaranteed by the pdness of the Matérn kernels. However, some numerical issues sometimes imply that the matrix is not positive definite. Hence the EnforcePDness function that perturbs slightly the matrix by adding a tiny jitter on the diagonal.\n\n\nOptimization routines :\nWe investigate the dynamics of the test error during the optimization routine explained hereafter, guided by the knowledge of the behavior of the exact test error (≈ test error at infinite time of optimization) developed in Learning Curves of Kernel Methods, empirical data vs. Teacher/Student paradigm by Spigler, Geiger and Wyart.\nThe prediction of the student is computed within the predict method : the prediction at a given point X is a simple weighted average of similarities between X and all the training points. The weights are initially zeros but they are optimized during the training.\nThree optimization Algorithms are implemented : GradientDescent (GD), ConjugateGradientDescent (CGD) and ConjugateGradientDescentFixedEpochs\n\nGD : the easiest to implement, easy to work out analytically but very slow numerically. Fixed learning rate η, rescaled by the number of training points P.\nCGD : more complicated to work out analytically but very efficient numerically. Converges in approximately P epochs. It comes in two versions :\n\nThe default version, where the algorithm stops when the train loss reaches a given threshold.\nThe benchmark version, where the algorithm runs for a fixed number of epochs passed in argument. Also useful when one is interested in the beginning of the dynamics.\n\n\nThe leading term in runtime complexity is O(#number_epochs * P * max(P,Ptest))\n\nAdditional remarks :\n\nI strongly advise against preconditioning (other than EnforcePDness) since it modifies greatly the results, leading to false conclusions.\nSurprisingly, adding momentum to GD did not accelerate convergence so it is not implemented in the current version.\nThe code is designed to collect statistics by running independent realisations to emulate the expectation over the Teacher random process. A brute-force approach would be to run all simulations the same number of times but it would take way too long. Therefore, since at small ν and at large P (independently), the standard deviation of the results goes to zeros, one concentrates the efforts (= more realisations) for large ν and small P\n\nBibliography\n\nSpigler, Geiger and Wyart : Learning Curves of Kernel Methods, empirical data vs. Teacher/Student paradigm.\nJacot, Gabriel and Hongler : Neural Tangent Kernel Convergence and Generalization in Neural Networks\nBordelon, Canatar and Pehlevan : Spectrum dependent learning curves in kernel regression and wide neural networks\n\n'], 'url_profile': 'https://github.com/Rouzaire', 'info_list': ['Jupyter Notebook', 'Updated May 14, 2020', 'Updated May 2, 2020', 'Rebol', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Java', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Nov 10, 2020', 'Julia', 'Updated Jun 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/someshkr', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Gaurav-Singh11', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'Gurugram,India', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sahil005', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['mlmodel-linear-regression\nML Model Linear Regression demo run on cardio data\n'], 'url_profile': 'https://github.com/anjul', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '281 contributions\n        in the last year', 'description': ['MMA-867---Kaggle-NYC-Taxi-Trip-Duration\nPredicting NYC taxi trip duration using only regression techniques\n'], 'url_profile': 'https://github.com/noriegaian', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'Hamirpur, H.P.', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['Logistic_Regression_0\nLogistic Regression on Titanic dataset to predict survival of passenger.\nThis is a binary classification problem to predict whether the passenger is alive or not, using scikit learn.\nThe data set used is famous titanic dataset taken from kaggle. link-https://www.kaggle.com/c/titanic\n'], 'url_profile': 'https://github.com/iamchetansharma8', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""linear_regression\nlinear regression on data set using gradient descent\n\nRun the code through the driver class, which has an example of how to run this over a set of cubic data\nThe jave code does most of the work however at the end a python executable is called which generates the plot of the fit and data. To get this to work was a bit tricky as I had to setup the version of python being used inside the python script which is something I have never done before.\nFeel free to run this on your own data set, I recommed sticking to a system of only 2 variables i.e. x and y though I believe I wrote the code such that it works with any number of variables corresponding to a single y, i.e. y = x+z should work fine, but the graph wont display anything in 2D.\nAlso when doing the fitting be aware that the fit can fail and diverge giving you a useless result, to resolve this usually descreasing alpha works, if the fit is doing well but doesn't reach a minimum increase the number of iterations\n""], 'url_profile': 'https://github.com/reidwelty', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'Vancouver, Canada', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['multivariate_analysis\n'], 'url_profile': 'https://github.com/Jaskaran23', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'Kerala,India', 'stats_list': [], 'contributions': '223 contributions\n        in the last year', 'description': ['kaggle_house_prediction\nThis are the  solution for the ongoing knowledge competition in kaggle named House Prices: Advanced Regression Techniques\n\n'], 'url_profile': 'https://github.com/ashishshaji', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Regenplatz', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 27, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 3, 2020', '1', 'R', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Java', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Nov 27, 2020']}"
"{'location': 'new delhi', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Predict-Carbon-dioxide-Emission-of-cars-using-Multiple-Regression-Model\nUsing scikit-learn to implement Multiple linear regression.\nMultiple Regression Model\nIn reality, there are multiple variables that predict the Co2emission.\nWhen more than one independent variable is present, the process is called multiple linear regression.\nFor example, predicting co2emission using FUELCONSUMPTION_COMB, EngineSize and Cylinders of cars. The good thing here is that Multiple linear regression is the extension of simple linear regression model.\nExplained variance regression score:\nIf  𝑦̂   is the estimated target output, y the corresponding (correct) target output, and Var is Variance, the square of the standard deviation, then the explained variance is estimated as follow:\n\n\nE𝚡𝚙𝚕𝚊𝚒𝚗𝚎𝚍 V𝚊𝚛𝚒𝚊𝚗𝚌𝚎( 𝑦 , 𝑦̂ ) = 1 − 𝑉𝑎𝑟{ 𝑦 − 𝑦̂ } 𝑉𝑎𝑟{𝑦}\n\n\nThe best possible score is 1.0, lower values are worse.\n'], 'url_profile': 'https://github.com/antiksaini', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'San Diego, Ca', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""ML-Regression\nRegression model made to predict an abalone's age\nDataset used: https://archive.ics.uci.edu/ml/datasets/Abalone\n""], 'url_profile': 'https://github.com/Austin795', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vkehfdl1', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['pairs-trading-in_indian_equities\ni have created this project on pairs trading where i have two parts first is the reaserach notebook where i try and find tradable pairs and other one is the backtesting part where i backtest the selected pairs in blueshift a backtesting ide build based on zipline\nbrief on the steps done in research notebooks where I try and find congregated pairs\n\n\nSo the way research will go is I want to run back tests for [2016,2017,2018] separately sector by sector reason I am running back test year by year ex:I don’t want to add pairs in 2016 back test algo which were  found  in 2017 research notebook to avoid look ahead bias and also this running back test for every year separately will allow me to add pairs and remove pairs from back test algo which means 2016 backtest will have a bit different tradable pairs than 2017 back test tradable pairs  which is more realstick way of back testing as we add pairs wich are satisfying the conintigration adn remove pairs wich no more satisfyies the critiries\n\n\nSo I am have three research notebook [2015,2016,2017] all of them have same code but different dates so if I wanted to find pairs for 2016 back test I find pairs from stocks data from last six month of 2015 each research note book tries to find pairs for upcoming year.\n\n\nSo the main logic to find pairs will go like this  suppose I am in 2016 right now so I will run all the test on rolling basis for last so I have calculated rolling_adf_test and rolling_beta value and I have taken mean of  rolling adf_test_pavlue  for all pairs .so as I mentioned adf_test_pvalue in my code give 95% confidence for p values below -2.9 I have kept my rolling_adf_pvalue  mean condition to -2.5  will only select those pairs which have rolling_adf_pvalue_mean below -2.5\nso what exactly is rolling_adf_pvalue_mean it is the single mostimport figure in my research to find robust tradable pairs wich means pairs whos spread hold stationarity for longer durations .so as we know to find conintigrated pairs throu8gh linear regression method first we have to run linear regression on the pair and then calculate the spread through the beat coefficient adn run adf test on that spread and if the adf test pvlue is less than 5% this means we can say with 95% confidence that spread is stationary good eneough it this happens we get a cointigrted pair but think about this tells taday that the apir is tradable it might change tommoorw who knows how do you the pairs has a hihgher cahnce of holding sattionarity so thats why i use rolling_adf_test_pvlue_mean so its simple think about it like this say we run linear regression on last 90 day of data and calculated spread and ran adf_test and we got adf_pvalue wich staets wether or not series is stationary but the twist is we run all these test every day on the last 90 day data from that day so we are running all these test on rolling basis for six months so we get rolling_pvalue according to the test i am using in my research notebook 95% confidence for stationary is achieved when the adf_pvalue is below -2.8 . so if we take mean of the rolling adf_pvalue and the mean is below -2.5 mark we can say that there lot of days where the pair remained conintigrated so we select only the pairs wich are whose rolling_adf_pvalue is below -2.5 keeping it lower than this u will get very few pairs to trade\n\n\nto give u a glipse of the selectes pairs spread and rolling adf_pvalue i have plotted the spread of some tradable from my research notebook and thier respective rolling adf\n\n\n\nThis way I can check how robust the congregation is between the pairs  will they  stay cointegrated for week or two three months yes even the pairs whose rolling_adf_pvalue_mean is below -2.5 for 5 months will show non stationary spread but still they will be cointegrated almost close to around 50% on the time period on which I ran my test\n\n\nSo, I have created a user defined function to do above mention task so in short if I run function on 6month of stock data function calculates past 60 day spread on rolling bases and run adf_test on rolling basis.\n\n\nnote: pls know that adf pvalues below -2.9 signifies with 95% confidence that series is stationary .\nnow  the backtest part so the backtesting will be done in blueshift for every year seprately so i will run the backtest for every year seprately\n'], 'url_profile': 'https://github.com/YogeshTS', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Amir-Manafpour', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': [""Adi Ojha's Implementation of ML algorithms!\nIn this repo, I will implement various ML models. The purpose is to deepen my understanding of machine learning, and practice implementing theory and algorithms on paper to code.\nI hope to use these implementations, in the future, to create a user-dashboard where users can select the data they want to train or classify and a model to use.\n""], 'url_profile': 'https://github.com/adiojha629', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['linear_regression_basics\nLinear Regression basics, using sklearn library to fit the model\nData Set : UCI Machine Learning Datasets - Student grade data set\nDemonstrated basic linear Regression model, this could be the first step in builing machine learning models\n'], 'url_profile': 'https://github.com/vickeydreamss', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'Meyreuil', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""core-v-verif\nFunctional verification project for the CORE-V family of RISC-V cores. This project is under active development.\nNEWS FLASH\nThe OpenHW Group CV32E40P is now live!This repository no longer contains a local copy of the RTL.  The RTL is cloned from the appropriate core-v-cores repository as needed.  The specific branch and hash of the RTL is controlled by a set of variables in cv32/sim/Common.mk.\nGetting Started\nFirst, have a look at the OpenHW Group's website to learn a bit more about who we are and what we are doing.\nThe design and verification documentation for the various CORE-V cores is located in the OpenHW Group's CORE-V documentation repo.  Reading the Verification Strategy is strongly recommended.\nIf you want to run a simulation there are two options:\n\nTo run the CORE testbench (based on the RI5CY testbench), go to cv32/sim/core and read the README.\nTo run the CV32E40P UVM environment, go to cv32/sim/uvmt_cv32 and read the README.\n\nDirectory Structure of this Repo\nci\nExplainer for the CI flow used by CORE-V-VERIF.\ncore-v-cores\nEmpty sub-directory into which the RTL from one or more of the CORE-V-CORES repositories is cloned.\ncv32\nVerification Environments, testbenches, testcases and simulation Makefiles for the CV32E cores.\ncv64\nVerification Environments, testbenches, testcases and simulation Makefiles for the CV64A cores.\ndoc\nEmpty.  Please see the CORE-V-DOCS repository.\nlib\nCommon components for the CV32 and CV64 verification environments.\nContributing\nWe highly appreciate community contributions. You can get a sense of our current needs by reviewing the GitHub\nprojects associated with this repository.   Individual work-items\nwithin a project are defined as issues with a task label.\nTo ease our work of reviewing your contributions, please:\n\nReview CONTRIBUTING.\nSplit large contributions into smaller commits addressing individual changes or bug fixes. Do not mix unrelated changes\ninto the same commit!\nWrite meaningful commit messages.\nIf asked to modify your changes, do fixup your commits and rebase your branch to maintain a clean history.\n\n""], 'url_profile': 'https://github.com/JeanRochCoulon', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Metis_Project_2_Regression\nLinear Regression with S & P Stock Price Data\n'], 'url_profile': 'https://github.com/Bruce-Gergley', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['ML-andreng-ex1\nMachine Learning by Andre NG- Exercise 1(Linear Regression with one variable and Multiple Variables)\n'], 'url_profile': 'https://github.com/nslearn', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2021', '1', 'Python', 'Updated May 2, 2020', 'Assembly', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'MATLAB', 'Updated May 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Magggs16', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '184 contributions\n        in the last year', 'description': ['Linear-Regression\n\nImplement linear regression to predict profits for a food truck.\nUsing gradient descent to fit the linear regression parameters theta to the dataset.\n\n'], 'url_profile': 'https://github.com/sarehsoltani', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['House-Price-Prediction---regression\nKaggle competition : in top 25% of entries on leaderboard\nHouse Prices: Advanced Regression Techniques\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\nUsing tools and techniques introduced and practiced through this course (Predictive Modelling) and previous courses, we were able to build a robust linear regression model to help predict house prices based on the variables in the test dataset.  In order to achieve our final result, which helped us achieve a leaderboard score in the 25% percentile on Kaggle, we started off with a simple linear regression model and built on to it in order to minimize the error rate on our test set.\nPreliminary Analysis:\nWe first started off with some preliminary analysis on the dataset and did some background research on the real estate market and factors that are generally important. Generally, we know that things like location, # of rooms, quality of house, property age, and square footage are generally positively correlated with the sale price. We were able to confirm this using the dataset with basic graphs in tableau and R.\nData Management (Feature Engineering, Outlier Analysis, Missing Values):\nFirst of all, we start with importing the data into the tool of our choice, R. We are given two datasets: test and train. We merge these two and proceed with the data management piece where we will eventually work to complete this dataset for the purpose of our regression model.\nGoing through the dataset, we found several missing values (NA). To address these, we used data imputation methods such as mean imputation, mode imputation (for categorical variables) and also assigning new variables (based on data description: see Appendix). We also did some outlier analysis, density plots (look at distribution of columns) and other graphical analysis to view the data based on a variety of outlooks.\nOnce the missing data and outliers were addressed, we proceeded with the feature engineering piece to create new features that would help to better predict the outcome. We started with a few features such as PropertyAge and Years since Remodelled and then came back and constructed more to help decrease our error rate on the test data.\nModel Building:\nNow we get to the most important piece, building our linear regression model. We start off with creating dummy variables for the various categorical variables in our model. We also ensure the data types are correct based on context (this would have been done in the previous phase). Once we are comfortable, we split up the data into separate datasets to train and test our mod; and finally predict the Sales Price.  We revised the model through feature engineering, outlier analysis and leveraging techniques such as regularization to get to a better model. Here are the steps that were routinely done to get to our current model:\ni.\tUse the linear regression tool in R & input independent & dependent variables into the formula to build the regression model.\nii.\tPredict the values and compare with the actual Sales Price using MAPE.\niii.\tOnce we get an approximate score, we go back to identify improvements that can be made by either creating new meaningful features (using Feature Engineering) or fixing any issues with the underlying dataset.\niv.\tNote that we ended up using Lasso Regression to regularize the model and ensure ideal complexity to avoid overfitting.\nOnce we are satisfied with the model results on the test dataset, we use the regression model to predict the actual values for the test dataset (with no Sales Price). This was then exported and uploaded to the Kaggle competition page to retrieve our final results on the Leaderboard.\nAnalysis the quality of our results, we found the MAPE error to be approximately 9.36% on the test dataset, which was very good in my opinion. On the actual predict dataset, my score was ~12.6% (Root Mean Squared Logarithmic Error) which was impressively in the top 25% percentile in the public leaderboard on Kaggle!\n'], 'url_profile': 'https://github.com/palpanesar1', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'Montreal ', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['BIXI future trip prediction\nThe project is implemented by Chaoyang Zheng, Quan Hao and Gabriel Lainesse for master course: Data mining in HEC MONTREAL.\n\nI. Introduction\nCurrently, more than 1,000 cities around the world have established or plan to establish a bike sharing system, spreading from campus, subway stations and residential areas to commercial center, grand parks and organizations. As a great application of sharing economy and the first large-scale bike sharing system in North America, Bixi Montréal plays an essential role in promoting travel flexibility and transportation efficiency, while also positively impacting the environment and quality of life.\nThe primary goal of this project is to establish accurate models that will predict the number of bikes required in different areas of the city at different points in time, as the way to anticipate demand. To do so, we take into consideration a number of features, including weather conditions, weekday daytime, statutory holiday, Montreal festival data, city of Montréal geographical feature data and fuel price, through which pertinent suggestions about the demand could be provided to Bixi service to help them increase the operational efficiency. We apply and compare results obtained from linear regression and random forest models. Our result shows that the random forest with staring neighborhood gave us the best result.\nTo review the code, we show how we implemented data cleaning and feature engineering in ""Bixi_Pre-Processing_Step1"" and ""Bixi_Pre-Processing_Step2"", and ""Bixi_Final_Modeling"" explain the visualization and modeling process.\nFor more detaied information about report, please see ""report"" folder.\nII.Exploratory data analysis\nTo learn insights towards our dataset, we generated following data visualization:\n\nGraph: Ridgeline Plot of Trip Count per Hour of the Day per Neighborhood\nIntuition\n\nTop 2 neighborhood in Bixi usage: Le Plateau-Mont-Royal and Ville-Marie\n2 peak period: 7-9 and 16-18 (commuting time)\n\n\nGraph: Box plot for the number of Bixi trips in Montreal under each weather condition in a given day\nIntuition\n\nCompleted metrics for weather condition. e.g.: Temperature, Wind speed etc(Numerical data)\nDescriptions of observed weather.e.g.:  broken clouds, thunderstorm, light rain etc.\n\n\nGraph: Line chart for the number of Bixi trips in Montreal under different weekly temperature\nIntuition\n\nPositive correlation between weekly temperature and trip counts\n\n\nGraph: : Line chart for the number of Bixi trips under different Weekly Wind speed\nIntuition\n\nNegative correlation between weekly wind speed and trip counts\n\n\nGraph: : Line chart for the number of Bixi trips in Montreal under different weekly humidity\nIntuition\n\nNegative correlation between weekly humidity and trip counts (Rainy)\n\n\nGraph: : Line chart for the number of Bixi trips under different Weekly Wind speed\nIntuition\n\nNegative correlation between weekly wind speed and trip counts\n\n\nGraph: : Line chart for the number of Bixi trips under different Weekly pressure\nIntuition\n\nNo significant correlation between weekly wind speed and trip counts\n\n\nGraph: : City of Montreal geographical features dataset\nIntuition\n-The information on the distribution of each neighborhood\n\nLand use (affectation) based on the City’s urban planning\n\nIII.Models and results\nTo predict the Bixi future trip volume, two regression models, a multiple linear regression and a random forest regression were developed. For shorter route affectation, we record both the starting station and ending station. Before we started to build these models, we calculated the number of rows and the mean of the count of trips for each group of features, in order to get a sense of the quality of the grouping and to make sure our target variable had meaning on its own. The count of trips is 75,986 for the starting neighbourhood models, 135,384 for long route affectation models and 120,127 for the short route affectation models. The average trip count per group is 62.88 for the starting neighbourhood models, 35.014 for the long route affectation models and 39.46 for the short affectation models.\nPerformance metrics are detailed in the table below. All performance metrics were calculated on the test dataset, using the train-test split methodology for a 70% train / 30% test split of the data.\n\nTable:  Performance metric\nReference\n[1] Bixi Open Data. URL: https://www.bixi.com/en/open-data\n[2] Weatherstats.ca. URL : https://montreal.weatherstats.ca/download.html\n[3] Kaggle - Historical Hourly Weather Data. URL : https://www.kaggle.com/selfishgene/historical-hourly-weather-data\n[4] Montréal Données Ouvertes – Affectation du Sol. URL : http://donnees.ville.montreal.qc.ca/dataset/affectation-du-sol\n[5] Montréal Données Ouvertes – Arrondissements. URL : http://donnees.ville.montreal.qc.ca/dataset/polygones-arrondissements\n[6] Government of Ontario – Fuels price survey information. URL : https://www.ontario.ca/data/fuels-price-survey-information\n[7] Aubert Sigouin, BIXI Montreal (public bicycle sharing system) https://www.kaggle.com/aubertsigouin/biximtl\n[8] Borgnat, Pierre, et al. ""Shared Bicycles in a City: A Signal Processing and Data Analysis Perspective."" 26 2010. Scientific Commons. 1 February 2010 http://www.scientificcommons.org/58104633\n[9]Pablo Jensen, Jean-Baptiste Rouquier, Nicolas Ovtracht, Céline Robardet. Characterizing the speed and paths of shared bicycles in Lyon. Transportation Research Part D: Transport and Environment, Elsevier, 2010, 15 (8), pp.522-524. <10.1016/j.trd.2010.07.002>. \n[10]Froehlich, J., J. Neumann and N. Oliver. ""Measuring the pulse of the city through shared bicycle programs."" International Workshop on Urban, Community, and Social Applications of Networked Sensing Systems‐ UrbanSense08. 2008.\n[11]Patrick Vogel, Torsten Greiser, Dirk Christian Mattfeld, Understanding Bike-Sharing Systems using Data Mining: Exploring Activity Patterns, Procedia - Social and Behavioral Sciences, Volume 20, 2011, Pages 514-523, ISSN 1877-0428.\n[12]Nair, Rahul & Miller-Hooks, Elise & Hampshire, Robert & Busic, Ana. (2012). Large-Scale Vehicle Sharing Systems: Analysis of Vélib\'. International Journal of Sustainable Transportation - INT J SUSTAIN TRANSP. 7. 10.1080/15568318.2012.660115.\n[13]Rahul Nair, Elise Miller-Hooks, Robert C. Hampshire & Ana Bušić (2013) Large-Scale Vehicle Sharing Systems: Analysis of Vélib\', International Journal of Sustainable Transportation, 7:1, 85-106, DOI: 10.1080/15568318.2012.660115\n[14]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: machine learning in Python,” Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.\n'], 'url_profile': 'https://github.com/chaoyangzhengnash', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nithinb99', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Classification of normal and abnormal moving of a drone.\nThe program extracts relevant data from a multiple sensors and trains a logistic regression classifier.\nMATLAB\n'], 'url_profile': 'https://github.com/Data-Science-kosta', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['SpatialRegression\nSpatial Regression with INLA\nThis is data published in Jama 29/4/2020 on COVD-19 in New York. The New York borough shapefiles were obtained from New York Open Data at https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm. For those wishing to evaluate other datasets, there’s lung cancer data in SpatialEpi library, lip cancer, leukemia in DClusterm library.\nKey aspect of spatial regression is that neighbouring regions are similar and distant regions are less so. It uses the polyn2nb in spdep library to create the neighbourhood weight. This section uses Bayesian modeling for regression with fitting of the model by  INLA.\nhttps://www.r-bloggers.com/spatial-data-analysis-with-inla/. The Rmd file is contained within the NewYork folder.\nFirstly, we estimate the calculate the raw rate of disease (cases/total population at risk). The expected number of cases is the rate multiply by the population in each suburb. SMR is the number of cases divided by the expected number of cases.\nSpatial Regression with rstan\nThe Rmd file is contained within the NewYork folder along with the rstan files. This analysis is still in development as I ran into problems with rstan and trying to reinstall rstan. The rstan analysis takes a lot longer to perform than with INLA.\nSpatio-temporal regression with INLA\nGit Bash\necho ""# SpatialRegression"" >> README.md\ngit init\ngit add README.md\ngit commit -m ""first commit""\ngit remote add origin https://github.com/GNtem2/SpatialRegression.git\ngit push -u origin master\nwhen committing change on the same repository, the git push command led to this errror message ""Updates were rejected because the remote contains work that you do"".\ngit pull\nThis command git pull results in another message ""Please enter a commit message to explain why this merge is necessary.."" and the git bash screen changed. This can be resolved by typing ""i"""" and insert a message then press Esc and write "":wq"" and press Enter to return to git bash window.\n'], 'url_profile': 'https://github.com/GNtem2', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'Gandhinagar, India', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Simple Linear regression\nIt is a linear regression model with a single explanatory variable.\nIt has 2d sample points: one independent variable & one dependent variable (x and y in cartesian coordiante system).\nIt finds a linear relation between the two variables, which predicts the dependent variable values as a function of the independent variables as acurately as possible.\n'], 'url_profile': 'https://github.com/aishawariya-athawale', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Linear-Regression\nImplementing Linear Regression algorithm from scratch in python and comparing both scratch and sklearn approaches by comparing their scores.\n'], 'url_profile': 'https://github.com/Akhilesh015', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kristybell', 'info_list': ['Python', 'Updated May 3, 2020', '1', 'MATLAB', 'Updated May 1, 2020', '1', 'R', 'Updated Jul 12, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 1, 2020', 'HTML', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['Income Segmentation and Prediction\nPerformed K-means for income segmentation. Also, performed ANN(Artificial Neural Network), KNN and Linear Regression to predict income and put them in different classifications.\n'], 'url_profile': 'https://github.com/polly63', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ycc252', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['CannAnxi\nLink between cannabis and anxiety -- log/lin regression\n'], 'url_profile': 'https://github.com/erichenschel', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manicregression', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Exercises\nExercises for DataBootcamp Week 5 Day 2\n'], 'url_profile': 'https://github.com/lighthouse-labs', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/mohit0412', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/parool-karwat', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Cairo,Egypt', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AbdallahMamdouh', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ttaylorok', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Sweden', 'stats_list': [], 'contributions': '469 contributions\n        in the last year', 'description': ['Classification and Regression\nAn assignment for Introduction to Machine Learning at Linnaeus University spring 2020.\nContains two algorithm implementations, namely K-nearest neighbour and polynomial regression.\nThis library contains 3 notebooks, one for each exercise.\n\nEx1_Microchips.ipynb\nEx2_Polynomial.ipynb\nEx4_Microchips.ipynb\n\n'], 'url_profile': 'https://github.com/clundstrom', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 27, 2020', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'HTML', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}"
"{'location': 'Pune', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Linear-regression\n'], 'url_profile': 'https://github.com/mayurkuwar43', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Windsor, Ontario', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Harshal131', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Yangon', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['LinearRegression\nLinearRegression for Cake Dataset\n'], 'url_profile': 'https://github.com/theingithetthetzaw', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/madhugadi', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Linear-Regression\nWorked on the Boston data set and predicted the cost of houses by implementing Linear Regression algorithm from scratch. Gradient Descent was involved to minimize the overall cost of the predictions.\n'], 'url_profile': 'https://github.com/arunish711', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tawhid20', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Indore, India', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Linear_Regression\nFitting dataset into Linear Regression model\nDataset : https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\n\nLibraries used :\n\n\nNumpy Library\nPandas Library\nSeaborn Library\nSKlearn Library (sci-kit learn)\n\n\nThe jupyter notebook has following sections:\n\n\nData understanding and exploration\nData cleaning\nExploratory Data Analysis\nFitting to Linear Regression\nFitting to Linear Regression with Polynomial Features\n\n'], 'url_profile': 'https://github.com/HarshnaVerma', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/supratim1', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Cairo-Egypt', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hebaabdelwhab', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['linear_regression\nBy the end of the lecture students will be able to:\n\nRecall the characteristics of linear regression\nIdentify areas of their work they can apply linear regression to\nInterpret a linear regression function\nExplain what’s special about the line of best fit\nSolve problems with linear regression using python\nEvaluate the quality of a linear regression model\nDraft a plan of attack for solving one specific problem in your organisation using linear regression\n\nSlides\nExit ticket\n'], 'url_profile': 'https://github.com/DanSanz', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 2, 2020', 'MATLAB', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pragyakapoor', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Kota,Rajasthan,India', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Linear-Regression\nBasic\n'], 'url_profile': 'https://github.com/Samyak12345', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/matthewbburrell', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'hyderabad', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Avinash-Gangisetty', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Logistic-regression-\n'], 'url_profile': 'https://github.com/haniramezani', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Poojau22', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['logistic-regression-\n'], 'url_profile': 'https://github.com/Oumayma-mbarek', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kshitijved', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Analysis script used to produce regression models based on SR15 data used in the TROPICS project\nAll SR15 scenario data that is required for TROPICS-scenario_prep has been downloaded from the\nfollowing links:\nhttps://data.ene.iiasa.ac.at/iamc-1.5c-explorer/\nhttps://www.iea.org/reports/energy-technology-perspectives-2017\nIEA ETP 2017 data is optionally used to test physical intensity benchmarks in combination with\nSR15 data. These data can be acquired from the IEA, but are not necessary to run either of the\nR scripts included in the repository.\nhttps://www.iea.org/reports/energy-technology-perspectives-2017\n'], 'url_profile': 'https://github.com/CDPworldwide', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['logistic_regression\n\nHeart Disease Prediction dataset - https://data.world/informatics-edu/heart-disease-prediction\n\n'], 'url_profile': 'https://github.com/FahimSifnatul', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated Apr 27, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hassen-Bououni', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'hyderabad', 'stats_list': [], 'contributions': '310 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anjan111', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,854 contributions\n        in the last year', 'description': ['Coursera-Regression\n-Describe the input and output of a regression model.\n-Compare and contrast bias and variance when modeling data.\n-Estimate model parameters using optimization algorithms.\n-Tune parameters with cross validation.\n-Analyze the performance of the model.\n-Describe the notion of sparsity and how LASSO leads to sparse solutions.\n-Deploy methods to select between models.\n-Exploit the model to form predictions.\n-Build a regression model to predict prices using a housing dataset.\n-Implement these techniques in Python.\n'], 'url_profile': 'https://github.com/manpreet-kau-r', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': ['linear-regression\nBuild a linear regression model to predict data - IRONHACK Project #5\n'], 'url_profile': 'https://github.com/LudivineLacour', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Regression_Analysis\n\ncar-prices: Estimate car prices using linear  regression\n\n'], 'url_profile': 'https://github.com/Oren-Ben', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['logistic_regression\nBITS F464 ML Assignment\nApplying logistic regression to detect forged banknotes. Dataset has been used from the UCI Machine Learning Repository. The model has been trained in three ways:\n\nWithout regularisation\nWith L1 regularisation\nWith L2 regularisation\n\n'], 'url_profile': 'https://github.com/aman3599', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['linear_regression\n\nBangladesh GDP Growth Rate dataset - http://data.gov.bd/api/download/?id=3b304e3f-cdd7-4c0e-ad03-76873408722d\nUSA Birth Rate prediction dataset - https://catalog.data.gov/dataset/births-and-general-fertility-rates-united-states-1909-2013\n\n'], 'url_profile': 'https://github.com/FahimSifnatul', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['linear-regression\nsupervised learning\n'], 'url_profile': 'https://github.com/shivani-16', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/basheeraldajani', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anshulmahajan01', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 1, 2020', 'Updated Apr 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/Muftah-Elzawi', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/Chaitra09', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'pune', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PUNAM-CODE', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Erlangen, Germany', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mahadev1995', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'kerala', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['LinearRegression\nLinear Regression model to predict Adipose tissue value based on Waist circumference\n'], 'url_profile': 'https://github.com/NamithaRavy', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Vehicles price regression\nThis project is about predicting used cars sales prices using Random Forest Regressor model\nYou can visit this website to check it out live\n'], 'url_profile': 'https://github.com/ali-naji', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Lineal_Regression\nPractica del Diplomado de Ciencia de Datos\n'], 'url_profile': 'https://github.com/lijumtzc', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Logistic-Regression\nWorked on the Titanic Data set to predict whether the person survived or not. Used Logistic Regression to achieve this purpose.\n'], 'url_profile': 'https://github.com/arunish711', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/narayana8799', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Thiruvalla, Kerala', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinur1992', 'info_list': ['MATLAB', 'Updated May 3, 2020', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 10, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shub-coder', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vineeth356', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vineeth356', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': [""Advanced Regression Project\nHi! The reason I do this porject is to test my skills and also try to practice some regression methods I've learned(I need to keep practicing so I won't forget the knowledge I've learned). The detail is down below.\nDataset\nI got the dataset from Kaggle (an online community of data scientists and machine learning practitioners).\nThis one is about Airbnb data in New York city.\nThe link is down below:\nAribnb: housing\n\nProccess\nI import the data(data description included) and do some exploratory analysis first. I also do some data visualization to have more understanding of the data.\nModels\n1. Linear Regression\n2. Ridge Regression\n3. Lasso Regression\n4. Polynomial Regression\n5. SVM regression (linear)\n6. SVM regression (RBF)\n7. KNN regressor\nEnd\nThis wraped up my practice, hope it hlped you guys, thank you very much.\n""], 'url_profile': 'https://github.com/harry-hwang', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Electronic City, Phase-2, Bengaluru', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Logistic-Regression\nDiabetes dataset Prediction classification problem\n'], 'url_profile': 'https://github.com/yashpal-ml', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Marseille, France', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sellamiakrem', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '270 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pitaconsumer', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/narayana8799', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Linear-regression\nLinear algebra is essential to machine learning. By identifying relationships of points in vector space, patterns can be determined that can lead to accurate predictions.\nIn this tutorial, we’ll be calculating a best-fit line and using the equation of that line to model the linear relationship between the independent and dependent variables. In simpler terms, we’ll be finding an equation to represent the correlations present in our dataset.\nLinear regression is one of the simplest algorithms used in machine learning, and therefore it’s good to start here.\n'], 'url_profile': 'https://github.com/mahoneynomadic', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Logistic-Regression\nImplementing the basic Logistic model.\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist.\nIn statistics, the logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.\nBuilding a binary classification model in this repo.\n'], 'url_profile': 'https://github.com/Akhilesh015', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '254 contributions\n        in the last year', 'description': ['logistic_regression\nCode for logistic regression write-up (phbromley.github.io/blog/)\n'], 'url_profile': 'https://github.com/peterhbromley', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,626 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JacqKevin', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Linear_Regression-\n'], 'url_profile': 'https://github.com/sanu98', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nadaAlqahtani', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anshulmahajan01', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Regression-Models\n'], 'url_profile': 'https://github.com/lakshya5079', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'Milan, Italy', 'stats_list': [], 'contributions': '1,006 contributions\n        in the last year', 'description': ['Matrix Regression\n\n\n\nImplementation of the MatrixRegression (MR) algorithm for multi-label text classification that can be used in an online learning context. It is presented in the following paper:\nPopa, I. & Zeitouni, Karine & Gardarin, Georges & Nakache, Didier & Métais, Elisabeth. (2007). Text Categorization for Multi-label Documents and Many Categories. 421 - 426. 10.1109/CBMS.2007.108.\nAbstract:\n\nIn this paper, we propose a new classification method that addresses classification in multiple categories of textual documents. We call it Matrix Regression (MR) due to its resemblance to regression in a high dimensional space. Experiences on a medical corpus of hospital records to be classified by ICD (International Classification of Diseases) code demonstrate the validity of the MR approach. We compared MR with three frequently used algorithms in text categorization that are k-Nearest Neighbors, Centroide and Support Vector Machine. The experimental results show that our method outperforms them in both precision and time of classification.\n\n'], 'url_profile': 'https://github.com/nicoloverardo', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MDrogosz', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/veraguzelsoy', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/Intelligent-Systems-Phystech', 'info_list': ['Python', 'Updated May 2, 2020', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Python', 'Updated May 1, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2021', 'Python', 'Updated Dec 11, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/manish0718', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['LogisticRegression\nThis repository creates a logreg model on your custom data from scratch.\nTable Of Contents\n\nMotivation\nRequired Libraries\nRun\nOutput\n\nMotivation\nThis project is used to train a logistic regression model from scratch. I always believe learning through code is the best method to learn Machine Learning. Therefore, to all the beginners or even advanced users, checkout the code and see how gradient descent actually works in a logistic function.\nRequired Libraries\n\nPython\nNumpy\nPandas\nscikit-learn\n\nRun\npython run.py --train iris.csv --lr 0.01 --epochs 300 \nYou can use your dataset as well. Make sure the target column is names as labels and there are no ID columns. I am using the first column as index column, so adjust your file accordingly. A sample data is also provided.\nTo checkout other configurable options, run -\npython run.py --help\nOutput\nThis program outputs classification report and predictions over your test dataset.\n'], 'url_profile': 'https://github.com/ajain3982', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/mayurkuwar43', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'Fortaleza, Ceará', 'stats_list': [], 'contributions': '325 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MiqueiasMaia', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'Dubai', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Regression-Project\n'], 'url_profile': 'https://github.com/Divya668', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'Aurangabad', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Machine-Learning-Algorithms.\nHere is the list of 5 most commonly used machine learning algorithms. Linear Regression. Logistic Regression. Decision Tree. Naive Bayes. kNN & RandomForest.\n'], 'url_profile': 'https://github.com/pratikgarud', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['USA-Coronavirus\nThis project was designed to attempt a forecast of the cases and deaths attributable to the Covid 19 outbreak in the USA. It is a regression problem. The researchers compared forecast results from simple linear regression and polynomial regressions models. A 30 day forecast was performed for both the cases and deaths.\n'], 'url_profile': 'https://github.com/KenDaupsey', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Decision_Tree_Regression_R\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BRVishnu', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NBA-using-Linear-Regression\nMITx: 15.071x The Analytics Edge -- -Recitation\nIn this recitation we will apply some\nof the ideas from Moneyball to data from the National\nBasketball Association-- that is, the NBA.\nSo the first thing we\'ll do is read in the data\nand learn about it.\nThe data we have is located in the file NBA_train.csv\nand contains data from all teams in season since 1980,\nexcept for ones with less than 82 games.\nSo I\'ll read this in to the variable NBA,\nNBA = read.csv(""NBA_train.csv"").\nOK.\nSo we\'ve read it in.\nAnd let\'s explore it a little bit using\nthe str command, str(NBA).\nAll right.\nSo this is our data frame.\nWe have 835 observations of 20 variables.\nLet\'s take a look at what some of these variables are.\nSeasonEnd is the year the season ended.\nTeam is the name of the team.\nAnd playoffs is a binary variable for whether or not\na team made it to the playoffs that year.\nIf they made it to the playoffs it\'s a 1, if not it\'s a 0.\nW stands for the number of regular season wins.\nPTS stands for points scored during the regular season.\noppPTS stands for opponent points\nscored during the regular season.\nAnd then we\'ve got quite a few variables that\nhave the variable name and then the same variable\nwith an \'A\' afterwards.\nSo we\'ve got FG and FGA, X2P, X2PA, X3P, X3PA, FT, and FTA.\nSo what this notation is, is it means\nif there is an \'A\' it means the number that were attempted.\nAnd if not it means the number that were successful.\nSo for example FG is the number of successful field goals,\nincluding two and three pointers.\nWhereas FGA is the number of field goal attempts.\nSo this also contains the number of unsuccessful field goals.\nSo FGA will always be a bigger number than FG.\nThe next pair is for two pointers.\nThe number of successful two pointers and the number\nattempted.\nThe pair after that, right down here, is for three pointers,\nthe number successful and the number attempted.\nAnd the next pair is for free throws,\nthe number successful and the number attempted.\nNow you\'ll notice, actually, that the two pointer and three\npointer variables have an \'X\' in front of them.\nWell, this isn\'t because we had an \'X\' in the original data.\nIn fact, if you were to open up the csv\nfile of the original data, it would just say, 2P and 2PA,\nand, 3P and 3PA, without the \'X\' in front.\nThe reason there\'s an \'X\' in front of it\nis because when we load it into R,\nR doesn\'t like it when a variable begins with a number.\nSo if a variable begins with a number\nit will put an \'X\' in front of it.\nThis is fine.\nIt\'s just something we need to be\nmindful of when we\'re dealing with variables in R.\nSo moving on to the rest of our variables.\nWe\'ve got ORB and DRB.\nThese are offensive and defensive rebounds.\nAST stands for assists.\nSTL stands for steals.\nBLK stands for blocks.\nAnd TOV stands for turnovers.\nDon\'t worry if you\'re not a basketball expert\nand don\'t understand exactly the difference between each\nof these variables.\nBut we just wanted to familiarize you\nwith some common basketball statistics that are recorded,\nand explain the labeling notation\n'], 'url_profile': 'https://github.com/martyraturi', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Updated May 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['myLinearRegressionPackage\nHomework 12 repository\n'], 'url_profile': 'https://github.com/msalmon7', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Regression-Bootstrap-Techniques\nRepo for Statistics Comps SP 2020\n'], 'url_profile': 'https://github.com/nobuakimasaki', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '181 contributions\n        in the last year', 'description': [""Boston Housing Price Prediction Using Multiple Linear Regression\nI am using Housing data for Predicting Boston Housing Price usimg Multiple Linear Regression.\nTable of Contents\n\nInstalation\nProject Motivation\nFile Description\nLicensing, Authors, and Acknowledgements\n\nInstallation\nHere First install python 3 and then after install any IDE like pycharm,Anaconda,Atom,VS Code etc and then after install                 pandas,numpy,matlpotlib,sklearn etc.\nAfter installation Now, you are ready for doing yor work.\nProject Motivation\nThis is my own  project.\nI am using Boston Hpusing data set from sklearn\nyou can also download from Kaggle.\nIn this project i am using boston housing dat for predictin it's price.\nHere i am using Simple Linear Regression algorithm for predicting price.\nFile Descriptions\nCopy of 03-02-Multiple Regression.ipynb  :  This is a Jupyter Notebook containing the data for prediction.\nboston_data.data  :  This is a dataset taken from sklearn for prediction.\nLicensing, Authors, Acknowledgements\nI am using boston housing  data from sklearn. So here, You can find the Licensing for the data from sklearn and from kaggle also and other descriptive information. Otherwise, feel free to use the code.\n""], 'url_profile': 'https://github.com/rjabhi123', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-Model-\nHere we have a Simple Linear Regression Model for the Dataset ""Salary_Data"" which is the dataset of Salary based on years of Experience of the Employee. So, this model helps to predict the expected salary for the employee based on No of years of experience.\n'], 'url_profile': 'https://github.com/gauravshrma22', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Jamunapur Chauraha Post- Inswardaspur Dist- Raebareli', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MrBeast-Anirban', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pragyakapoor', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashdrogba', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/smyyl1591', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/FitriaNurAida', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Germany ', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/phschaefer', 'info_list': ['R', 'Updated May 1, 2020', 'R', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Polynomial-regression-model\nThis Model is for predicting the Expected Salary to be offered to a person. The dataset contain the different posts and the salary level at that post. Now, let the person is at a post from last 2 years, so the salary of the employee is between the salary of current post and the next post. So, if we predict based on simple linear regression model, then the prediction might be too wrong as shown in the first part of the model. Then we will see how accurate the Polynomial model. We will increase the degree starting from 2 to get the accurate model.\n'], 'url_profile': 'https://github.com/gauravshrma22', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Simple-linear-regression-\n'], 'url_profile': 'https://github.com/gayatri1990', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-Model\nThis model is a multiple linear regression model which helps you to predict the Profit of a company based on the data of 50 startup company on how much they spend on R&D, Administration, Marketing and the fourth feature is a categorical feature i.e. State in which the startup works.\n'], 'url_profile': 'https://github.com/gauravshrma22', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '271 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xiaomiaoright', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Matlabs\nPolynomial regression\nPCA and LDA\n'], 'url_profile': 'https://github.com/ahmetkrgztr', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['BrainAgeRegression\nThe project for Brain Age Regression offered at Imperial College London\n'], 'url_profile': 'https://github.com/jeremyxie712', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'hyderabad', 'stats_list': [], 'contributions': '182 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shivamjadhav2000', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Random_Forest_Regression_R\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['marks-pred-regression\nPrediction of students’ exam marks for a subject using other (multiple) subjects with Machine Learning algorithm- Linear Regression.\nCodes will be uploaded SOON.\n'], 'url_profile': 'https://github.com/rajatvisitme', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashdrogba', 'info_list': ['Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'MATLAB', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 30, 2020', 'Apache-2.0 license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pragyakapoor', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/someshkr', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'Philadelphia, PA', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/oshostak', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'Hamirpur, H.P.', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['ANN_Regression_fake_0\nThis is an illustration to show the use of Artificial neural networks using Tensorflow 2.0 and keras. The dataset is fake and is for iluustration purpose only.\n'], 'url_profile': 'https://github.com/iamchetansharma8', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Regression using Tensorflow.js\nUsed tfjs and tfjs-vis library to train a model to predict values based on previous data.\n1. Load the data and prepare it for training.\n2. Defined the architecture of the model \n\n3. Train the model and monitor its performance as it trains.\n\n4. Evaluate the trained model by making some predictions.\n\n5. Monitor in browser using tf-vis library.\n\n'], 'url_profile': 'https://github.com/iampa1', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Regression-predicting-house-prices\nI will be exploring the housing sale prices in King County, USA between the time period May 2014 - May 2015.\nFirstly, I will go through a thorough data exploration to identify most important features and to explore the intercorrelation between features. After that I apply data normalization between varialbes and conduct feature engineering.\n'], 'url_profile': 'https://github.com/Azimtheviper', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rohilverma', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Fabireyees', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'Los Angeles, California', 'stats_list': [], 'contributions': '365 contributions\n        in the last year', 'description': ['Housing Price Regression Model\nHousing Price Regression Model\nFor Kaggle Competition: House Prices: Advanced Regression Techniques\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\nTech/framework used\nBuilt with\n\nPython, TensorFlow\n\nTo run this project, make sure all libraries are installed properly.\nXGBoost is difficult to install on Macs, use guide here:\nhttps://xgboost.readthedocs.io/en/latest/build.html\nhttps://machinelearningmastery.com/install-xgboost-python-macos/\nCollaborate work by Jonathan Chen, Haoshen Hong, Ziyao Zhou, Ruoning Guan, and Yiyang Cheng\nUpdated by Jonathan Chen in 2020\n'], 'url_profile': 'https://github.com/jonathanthec', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saivineeth181', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'JavaScript', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Python', 'Updated Sep 3, 2020', 'Updated Apr 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['MachineLearnind-LinearRegression\nMy first ML model. Predicting the companies profit over its factor like R & D spend, marketing, etc.\n'], 'url_profile': 'https://github.com/dhairyakataria', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/FitriaNurAida', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'vadodara, gujarat', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Multiple linear regression\n'], 'url_profile': 'https://github.com/Rupal106', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saivineeth181', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neeraj2296', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['House-Price-Regression\n'], 'url_profile': 'https://github.com/lakshyauttrani', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gtmray', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Saarbrücken', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Regression-in-Matlab\ncombinations of L1, L2 Loss functions and Regularizations\n'], 'url_profile': 'https://github.com/supreethmv', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '122 contributions\n        in the last year', 'description': ['Linear-Regression-single-variable-\nA very simple Linear Regression model to get started with Machine Learning\n'], 'url_profile': 'https://github.com/Naresh6017', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Helsinki, Finland', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Ridge-and-Lasso-Regression\nEGULARIZATION\nThere is the case that when you predict the model, performance in the training set is good but the model performs on the test set very badly, maybe it is because of overfitting. To avoid overfitting to improve the performance on test set to help with the regularization, you use the Ridge or Lasso Regression to do that.¶\nAnother case that you need to do regularization is that when you have so many features while there is not so many data points in your dataset. In this case, Linear Regression will fail to come up with the model. So Ridge or Lasso Regression will be applied.\nTo know exactly when you have to choose Ridge or Lasso, lets do some comparision between two models and then we can understand when we you them.\n'], 'url_profile': 'https://github.com/Susanhuynh', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated May 1, 2020', 'Updated Apr 28, 2020', '1', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'MATLAB', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}"
"{'location': 'Rohtak, Haryana, India', 'stats_list': [], 'contributions': '619 contributions\n        in the last year', 'description': ['\nMultiple Linear Regression Model\nUsing Multiple Linear Regression model to predict the consumption of fuel by a car.\nThe data set was downloaded from this website :- https://vincentarelbundock.github.io/Rdatasets/datasets.html\nThis repository was done to implement my learnings about the machine learning algorithms.\nAny kind of insights and changes are highly appreciated.\n'], 'url_profile': 'https://github.com/m0-k1', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Logistic-Regression-in-R\n'], 'url_profile': 'https://github.com/rabhadiaavinash', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['multi-linear-regression\nBA class project 3\n'], 'url_profile': 'https://github.com/joezhou12', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Polynomial_Regression_R\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Support_Vector_Regression_R\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Salt Lake City, UT', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rachitamehta', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['miniProjectLinearRegression\n'], 'url_profile': 'https://github.com/wiroger9595', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Sylhet,Bangladesh', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': [""Logistic-Regression-based-OCR\nIt's a machine learning classification problem. I've used data from sklearn datasets. First, I've load data , split it, checked if the splitting working or not. Then, train and predict the data. The accuracy of this model is 95%. This project is on Logistic Regression based Optical Character Recognition.\n""], 'url_profile': 'https://github.com/bolaram', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Patiala,India', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['Time_Series_data-Regression-\n\nPredicting the sale price of bulldozers using Machine Learning\nMain goal is to visualize the given time series data and predict the sale price of bulldozers. It is one of the challaenges on kaggle\n\n\n\nProblem Definition\nPredict the auction sale price for a piece of heavy equipment to create a ""blue book"" for bulldozers.\n\n\nData\nhttps://www.kaggle.com/c/bluebook-for-bulldozers/data There are 3 main datasets:\nTrain.csv is the training set, which contains data through the end of 2011.\nValid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your        score on this set is used to create the public leaderboard.\nTest.csv is the test set, which won\'t be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set            determines your final rank for the competition.\n\n\nEvaluation\nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n\n\n'], 'url_profile': 'https://github.com/himanshu530', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['linear rolling window regression\n'], 'url_profile': 'https://github.com/zhenkanglau', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'R', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'R', 'Updated Apr 29, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'MATLAB', 'Updated Apr 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['miniProjectLinearRegression\n'], 'url_profile': 'https://github.com/wiroger9595', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Sylhet,Bangladesh', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': [""Logistic-Regression-based-OCR\nIt's a machine learning classification problem. I've used data from sklearn datasets. First, I've load data , split it, checked if the splitting working or not. Then, train and predict the data. The accuracy of this model is 95%. This project is on Logistic Regression based Optical Character Recognition.\n""], 'url_profile': 'https://github.com/bolaram', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Patiala,India', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['Time_Series_data-Regression-\n\nPredicting the sale price of bulldozers using Machine Learning\nMain goal is to visualize the given time series data and predict the sale price of bulldozers. It is one of the challaenges on kaggle\n\n\n\nProblem Definition\nPredict the auction sale price for a piece of heavy equipment to create a ""blue book"" for bulldozers.\n\n\nData\nhttps://www.kaggle.com/c/bluebook-for-bulldozers/data There are 3 main datasets:\nTrain.csv is the training set, which contains data through the end of 2011.\nValid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your        score on this set is used to create the public leaderboard.\nTest.csv is the test set, which won\'t be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set            determines your final rank for the competition.\n\n\nEvaluation\nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n\n\n'], 'url_profile': 'https://github.com/himanshu530', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['Linear-Regression-Practice\n'], 'url_profile': 'https://github.com/Ernest-Anderson-Hutasoit', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['Multivariate_linear_regression\n'], 'url_profile': 'https://github.com/KhomotsoT', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pragyakapoor', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Logistic_Regression_R\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ElectroNath', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['Linear-Regression-with-sklearn\n'], 'url_profile': 'https://github.com/migot01', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Windsor, Ontario', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Harshal131', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated May 2, 2020', 'R', 'Updated Apr 30, 2020', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sagitta999', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Multiple_Linear_Regression_R\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['NaiveBayes_LogisticRegression\nNaive Bayes and Logistic Regression for classifying spam and ham emails.\nThe dataset used for training the model labels good emails as ham and the other ones as spam.\nFor developing the Naive Bayes model we have used the Bag of Words model\nFor developing the Logistic Regression model we have used the Bernoulli model.\n'], 'url_profile': 'https://github.com/AbhiGit95', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChuckDevCC', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Multi-Task-Quantile-Regression\nFinal project for the ""Advanced Machine learning techniques"" course thought by Stephan Clémençon for the 3rd year of the engeenering cycle at ENSAE Paris.\nAuthors\nDimitri Meunier and Pierre Delanoue\nReferences\nJoint quantile regression in vector-valued RKHSs (2016), M. Sangnier, O. Fercoq, F. d\'Alché-Buc. Neural Information Processing Systems (NIPS).\nVector Quantile Regression: An Optimal Transport Approach (2016), Guillaume Carlier, Victor Chernozhukov and Alfred Galichon. Annals of Statistics.\n'], 'url_profile': 'https://github.com/DimSum2k20', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Linear-Regression-with-Python\nCourse by Rhyme\n'], 'url_profile': 'https://github.com/AlizaRK', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['Breast Cancer Logistic Regression\nData Description\nThis data set contains information of 285 patients with 33 features describing their diagnosis.\nBackground\nAs someone who is driven in understanding what the chances of a disease having serious effect on a person would be, I wanted to be able to dive into this dataset to understand the probabilities of breast cancer being benign or malignant based on characteristics that will be focused on.\nFindings\nFor each respective mean in the four charts above, we see that Area, Compactness, Concavity, and Concave all seem to have a probability of being benign at the lower values of their x-axes.\n\nFor each respective mean in the four charts above, we see that Area, Compactness, Concavity, and Concave all seem to have a probability of being benign at the lower values of their x-axes.\n\nHere, the probabilities vary a little differently along the mean axes. While most cases are still benign, we see that for Fractal Dimension and Smoothness, it is a little less obvious to say that if a value falls under a certain range, then it’ll be considered benign or malignant. This is not the case for the Perimeter and Radius means however. We can clearly see that it is much easier to interpret the probability of an existing case of breast cancer or not.\n\nSymmetry and Texture means behave more like Smoothness and Fractal Dimension. There is a less linear pattern of a mean value being able to show the probability of breast cancer being malignant or benign.\n'], 'url_profile': 'https://github.com/andrew-alarcon17', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kush1781', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'Bengaluru,Karnataka,India ', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['linear-regression-using-keras-1\nBuilt a neural network using keras to estimate house pricing\n'], 'url_profile': 'https://github.com/imkundang', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Simple_Linear_Regression_R\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Updated Apr 27, 2020', 'R', 'Updated Apr 29, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Apr 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Python Regression Project - Predicting Real Estate Prices in Ames, Iowa\nThe data for this project is taken from kaggle.com, as part of a data analysis competition.\nContained in this repository is a link to the data, a jupyter notebook with exploratory anaysis of the data, and a link to a YouTube video of me summarizing the project.\nYouTube Link: https://youtu.be/8dLh-fULRyA\nkaggle Link: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n'], 'url_profile': 'https://github.com/ZZanetti', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['Multiple--Linear--Regression\nsklearn vs scratch\n'], 'url_profile': 'https://github.com/marmu123', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/noya19', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Support-Vector-Regression\nA Python based Support Vector Regression Model for prediction of Covid19 cases in India\nThe proposed work utilizes Support Vector Regression model to predict the number of total number of deaths, recovered cases, cumulative number of confirmed cases and number of daily cases. The data is collected for the time period of 1st March,2020 to 30th April,2020 (61 Days).  The total number of cases as on 30th April is found to be 35043 confirmed cases with 1147 total deaths and 8889 recovered patients. The model has been developed in Python 3.6.3 to obtain the predicted values of aforementioned cases till 30th June,2020. The proposed methodology is based on prediction of values using support vector regression model with Radial Basis Function as the kernel and 10% confidence interval for the curve fitting. The data has been split into train and test set with test size 40% and training 60%. The model performance parameters are calculated as mean square error, root mean square error, regression score and percentage accuracy. The model has above 97% accuracy in predicting deaths, recovered, cumulative number of confirmed cases and 87% accuracy in predicting daily new cases. The results suggest a gaussian decrease of the number of cases and could take another 3 to 4 months to come down the minimum level with no new cases being reported. The method is very efficient and has higher accuracy than linear or polynomial regression.\n'], 'url_profile': 'https://github.com/DebanjanParbat', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': ['Multi-Class-Logistic-Regression\nLaboratory #9 for Uni\n'], 'url_profile': 'https://github.com/george200150', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Linear_Regression-Kenny\n'], 'url_profile': 'https://github.com/bisector1', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'FUTA, Akure Nigera', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['ML-linear-Regression-project\n'], 'url_profile': 'https://github.com/Adegitetaiwo', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': ' Iran,Tabriz', 'stats_list': [], 'contributions': '239 contributions\n        in the last year', 'description': ['Titanic-Logistic-Regression\nYou can download the Dataset From Kaggle : https://www.kaggle.com/c/titanic\nLink of NoteBook: https://www.kaggle.com/thelonecoder/titanic-logistic-regression\n'], 'url_profile': 'https://github.com/AliNajafi1998', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '295 contributions\n        in the last year', 'description': ['Seaborn_Grid_Regression\nSeaborn is a visualisation library.\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/iamsamuelhere', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Multiple-linear-regression\n'], 'url_profile': 'https://github.com/g204', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 3, 2020', '1', 'Python', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'United States of America', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': ['DL_pytorch_regression\n'], 'url_profile': 'https://github.com/amrish1222', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '122 contributions\n        in the last year', 'description': ['Linear-Regression-Multi-Variable-\nMulti-variable Linear Regression model (Nothing so ground breaking here XD)\n'], 'url_profile': 'https://github.com/Naresh6017', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '271 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xiaomiaoright', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Simple-linear-regression\nPerforming Simple linear regression on a Salary_data dataset using R and python.\n'], 'url_profile': 'https://github.com/devranjandas1006', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '694 contributions\n        in the last year', 'description': ['regression-king-county-housing-prices\n'], 'url_profile': 'https://github.com/EricB10', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'Utrecht, The Netherlands', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['metaRegressionRoB\nMeta-regression of randomised clinical trial data with risk-of-bias scores\n'], 'url_profile': 'https://github.com/wmotte', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['logistic-regression-model-\nThis is the one project on the dataset in which i have applied logistic regression\n'], 'url_profile': 'https://github.com/parool-karwat', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['ML_LinearRegression_Project\nHow to use LinearRegression Model\n'], 'url_profile': 'https://github.com/ire-mide1', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'FUTA, Akure Nigera', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['M-linear-Regression-Project_\n'], 'url_profile': 'https://github.com/Adegitetaiwo', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anshitamakode', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'Apache-2.0 license', 'Updated May 1, 2020', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020']}"
"{'location': 'New York, NY', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kristybell', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'Brazil - São Paulo', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['multiple_linear_regression\nIn this notebook we will use multiple linear regression to calculate the occupancy rate of the houses, we will use the dataframe boston of the sklearn\n'], 'url_profile': 'https://github.com/Weilton', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nadaAlqahtani', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bisector1', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'Gwalior', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tarunkumar111', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'Bogota', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Logistic-Regression-Classifier\n'], 'url_profile': 'https://github.com/gbalza', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'Ankara, Turkey', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['random-forest-regression\nRandom forest regression on a kaggle dataset. python scikit learn implementation.\nUsing Machine Learning Algorithms to predict delays in a kaggle dataset. Sci-kit Learn Library of Python is used.\ndata url = https://www.kaggle.com/giovamata/airlinedelaycauses/data#DelayedFlights.csv\nEach cell is explained right after its execution.\n'], 'url_profile': 'https://github.com/CihanErsoy', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['gas-data-regressions\nAnalyzing potential models for virial coefficient of experimental gas data\n'], 'url_profile': 'https://github.com/rohini-omnilytiqs', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rsparks93', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}","{'location': 'Utrecht, The Netherlands', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['metaRegressionRoB\nMeta-regression of randomised clinical trial data with risk-of-bias scores\n'], 'url_profile': 'https://github.com/wmotte', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Apache-2.0 license', 'Updated May 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['logistic-regression-model-\nThis is the one project on the dataset in which i have applied logistic regression\n'], 'url_profile': 'https://github.com/parool-karwat', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['ML_LinearRegression_Project\nHow to use LinearRegression Model\n'], 'url_profile': 'https://github.com/ire-mide1', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'FUTA, Akure Nigera', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['M-linear-Regression-Project_\n'], 'url_profile': 'https://github.com/Adegitetaiwo', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anshitamakode', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['regression-neural-networks\nWe will use video game sales data to train a neural network that will predict how much we can expect future video games to earn based on our historical data\n'], 'url_profile': 'https://github.com/spandanmohanty07', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['ML_HW4\n'], 'url_profile': 'https://github.com/tongbs', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/builder2000', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""NILM: classification VS regression\nNon-Intrusive Load Monitoring (NILM)  aims to predict the status\nor consumption of  domestic appliances in a household only by knowing\nthe aggregated power load. NILM can be formulated as regression problem\nor most often as a classification problem. Most datasets gathered\nby smart meters allow to  define naturally a regression problem,\nbut the corresponding classification problem  is a derived one,\nsince it requires a conversion from the power signal to the status of each\ndevice by a thresholding method. We treat three different thresholding\nmethods to perform this task, discussing their differences on various\ndevices from the UK-DALE dataset. We analyze the performance of\ndeep learning state-of-the-art architectures on both the regression and\nclassification problems, introducing criteria to select the most convenient\nthresholding method.\nSource: see publications\nSet up\nCreate the environment using Conda\n\n\nInstall miniconda\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh | bash\n\nSay yes to everything and accept default locations. Refresh bash shell with bash -l\n\n\nUpdate conda\nconda update -n base -c defaults conda\n\n\n\nClone this repository and cd into the folder\n\n\nCreate and activate conda environment (removing previously existing env of the same name)\nconda remove --name better-nilm --all\nconda env create -f environment.yml --force\nconda activate better-nilm\n\n\n\nDownload UK-DALE\nUK-DALE dataset is hosted on the following link:\nhttps://data.ukedc.rl.ac.uk/browse/edc/efficiency/residential\n/EnergyConsumption/Domestic/UK-DALE-2017/UK-DALE-FULL-disaggregated\nThe files needed by this module are ukdale.zip and ukdale.h5.zip.\nDownload both and unzip them in a folder named data inside the root.\nOnce you are done, your local directory should look like this:\nbetter_nilm\n|_ better_nilm\n   |_ [python scripts and subfolders]\n|_ data\n   |_ ukdale\n      |_ [house_1 to house_5]\n   |_ ukdale.h5\n\nCredit: Jack Kelly\nScripts\nThe folder better_nilm contains an executable script to train the\nmodels a plot their results. Run the following line on the root folder\n(make sure to have the enviroment active and the data downloaded):\npython better_nilm/train_test_model.py\n\nThis will train and score the CONV model using the default parameters.\nThe script stores several outputs in the outputs folder,\nincluding:\n\n.txt files with the model scores over the test data\n.png files showing samples of the model's prediction.\n.png files with the scores against the classification weight.\n\nThe list with all the available parameters and their default values is stored in the\nconfiguration file.\nIf you want to use your own set of parameters, duplicate the aforementioned\nconfiguration file and modify the paremeters you want to change (without deleting any\nparameter). You can then use that config file with the following command:\npython better_nilm/train_test_model.py  --path_config <path to your config file>\n\nFor more information about the script, run:\npython better_nilm/train_test_model.py  --help\n\nThresholding methods\nThere are three threshold methods available. Read our paper\nto understand how each threshold works.\n\n'mp', Middle-Point\n'vs', Variance-Sensitive\n'at', Activation Time\n\nPublications\nNILM as a regression versus classification problem:\nthe importance of thresholding\nContact information\nAuthor: Daniel Precioso, PhD student at Universidad de Cádiz\n\nEmail: daniel.precioso@uca.es\nGithub\nLinkedIn\nResearchGate\n\n""], 'url_profile': 'https://github.com/UCA-Datalab', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['CLV-prediction-using-R\nBy using Machine Learning algorithm of Linear Regression the Customer Life Value(CLV) has been predicted.\n'], 'url_profile': 'https://github.com/arpanmanna8', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '220 contributions\n        in the last year', 'description': ['Boston-Housing---Linear-Model-Ridge-Lasso-ElasticNet-Regression\nApplication of Regularization techniques i.e. Ridge, Lasso and Elastic Net Regression using Boston Housing dataset.\n'], 'url_profile': 'https://github.com/Bhrugu-scientist', 'info_list': ['Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 29, 2020', 'Python', 'Updated May 3, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', '7', 'Python', 'Updated Feb 10, 2021', 'Updated Apr 27, 2020', 'R', 'Updated Apr 28, 2020']}"
"{'location': 'Taipei, Taiwan', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yuju1998', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Trichy', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': [""ML-Model-Flask-Deployment\nThis is a demo project to elaborate how Machine Learn Models are deployed on production using Flask API\nPrerequisites\nYou must have Scikit Learn, Pandas (for Machine Leraning Model) and Flask (for API) installed.\nProject Structure\nThis project has four major parts :\n\nmodel.py - This contains code fot our Machine Learning model to predict employee salaries absed on trainign data in 'hiring.csv' file.\napp.py - This contains Flask APIs that receives employee details through GUI or API calls, computes the precited value based on our model and returns it.\nrequest.py - This uses requests module to call APIs already defined in app.py and dispalys the returned value.\ntemplates - This folder contains the HTML template to allow user to enter employee detail and displays the predicted employee salary.\n\nRunning the project\n\nEnsure that you are in the project home directory. Create the machine learning model by running below command -\n\npython model.py\n\nThis would create a serialized version of our model into a file model.pkl\n\nRun app.py using below command to start Flask API\n\npython app.py\n\nBy default, flask will run on port 5000.\n\nNavigate to URL http://localhost:5000\n\nYou should be able to view the homepage as below :\n\nEnter valid numerical values in all 3 input boxes and hit Predict.\nIf everything goes well, you should  be able to see the predcited salary vaule on the HTML page!\n\n\nYou can also send direct POST requests to FLask API using Python's inbuilt request module\nRun the beow command to send the request with some pre-popuated values -\n\npython request.py\n\nAcknowledgment : https://github.com/krishnaik06/Deployment-flask\n""], 'url_profile': 'https://github.com/ishdutt', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '215 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ewcross', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Tartu, Estonia', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': [""Housing_Analysis\nDisclaimer: Your neighbor is a real estate agent and wants some help predicting housing prices for regions in the USA. It would be great if you could somehow create a model for her that allows her to put in a few features of a house and returns back an estimate of what the house would sell for.\nShe has asked you if you could help her out with your new data science skills. You say yes, and decide that Linear Regression might be a good path to solve this problem!\nYour neighbor then gives you some information about a bunch of houses in regions of the United States,it is all in the data set: USA_Housing.csv.\nThe data contains the following columns:\n'Avg. Area Income': Avg. Income of residents of the city house is located in. 'Avg. Area House Age': Avg Age of Houses in same city 'Avg. Area Number of Rooms': Avg Number of Rooms for Houses in same city 'Avg. Area Number of Bedrooms': Avg Number of Bedrooms for Houses in same city 'Area Population': Population of city house is located in 'Price': Price that the house sold at 'Address': Address for the house.\n""], 'url_profile': 'https://github.com/Leyla-Hasanova', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Denver, CO', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/toppytim', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['DoesMoneyMakePeopleHappy\n'], 'url_profile': 'https://github.com/datadime', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Hyperparameter-tuning-and-Cross-validation-in-Nueral-network\nCook book codes for Optimizing and Validation of Classification or Regression Keras models\nIn this Repo i have uploaded three codes containing simple Nueral Network using Keras\nIt contains 3 files in which two files contains codes for K fold cross validation using scikit learn keras wrapper and third file\nincludes Grid search cv using keras wrapper\nAll 3 files are properly commented explaining most of the steps of the code\n'], 'url_profile': 'https://github.com/mayankAgarwa', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Trivandrum,Kerala', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Bike-Sharing-Demand\nBike Sharing Demand is a regression project based on hour dataset\n'], 'url_profile': 'https://github.com/lekmeera', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['LinReg-Python-Library\nEDITED\nThis Repository contains LinReg library to perform simple linear regression with single dependant and independant variables.\nYou can go through the LinReg.py file to understand its working.\nA guide on using the same is in file ""Manual.pdf"".\nFollow the below steps to get it insatlled:\nStep 1: Download the LinReg.py file.\nStep 2: Navigate to \'site-packages\' folder in your \'python37\' folder.\n""C:\\python37\\Lib\\site-packages"" - I have installed my python in C: Drive.\nStep 3: Navigate to the path above and paste the LinReg.py file here.\nStep 4: Now open the Python IDLE and import it with name ""import LinReg"" ans start using the functions in it.\n**Kindly Note for editors other than Python IDLE find the location where its packages are installed and paste the LinReg.py file in the same location.\nSmall sample code is provided below to get the regression coefficients as a list:\n\nimport LinReg\nx = LinReg.train(\'/Users/Vikram/Desktop/data.txt\',stype=\'tab\')\nprint(x)\n\nHere \'train\' function returns a list containing the slope and intercept values of the given data. The first argument is the data file name with its location, the second argument is the \'stype\'-seperation type, which can currently hold two values, i.e. \'tab\' and \'csv\'. The default stype is set to \'csv\'.\n'], 'url_profile': 'https://github.com/vikramvangara', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Atlanta, Georgia, USA ', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/omkaroak333', 'info_list': ['SAS', 'Updated Apr 29, 2020', 'CSS', 'Updated Apr 28, 2020', 'Python', 'Updated May 12, 2020', '1', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rodewan', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'San Diego', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': [""Vehicle_price_prediction\nImplementing a Linear Regression model to predict the vehicle prices using it's different attributes.\n""], 'url_profile': 'https://github.com/ruddysimon', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Predicting-Salaries-with-Simple-Linear-Regression-in-R\nThe hands on project on Simple Linear Regression in R: Predicting Salaries is divided into following tasks:\nTask 1: Introduction – Defining the goal of our project.\nTask 2: Importing the dataset.\nTask 3: Performing train/test splits.\nTask 4: Fitting the model and making Predictions.\nTask 5: Visualizing training set results.\nTask 6: Visualizing testing set results.\n'], 'url_profile': 'https://github.com/Akash-Ravichandran', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'SF Bay Area', 'stats_list': [], 'contributions': '804 contributions\n        in the last year', 'description': ['house_prices\nHouse Prices Kaggle competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\n'], 'url_profile': 'https://github.com/chelseanbr', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'Alexandria, Egypt', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/waseem-medhat', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': [""My model was rated 7 out of 25. : )\nFish Market\nThe goal of this homework is to training a regression model to predict the weight of a fish.\nFirst, I looked at how fish measurements were made and then I researched how is the best way to predict the weight of a fish based on its measurements.\nThe research showed that we should never use a cross lenght to weight a fish. So I didn't use the Length 3 in my model.\nAlso, the research showed that the best way to predict the weight is using the formula (length x girth x girth)/800.\nSince I don't have the girth measurement in the dataset, I added a column Girth with an approximate number (Length2 * Width).\nI tested Multilinear Regression, Polynomial Regression, SVR, Decision Tree and Random Florest. Multilinear Regression is the one that showed to be best model.\nTechnology\n\nPython (Pandas, Numpy, Scikit-learn)\n\nAdditional Research\nhttps://www.almanac.com/content/how-estimate-weight-fish\nhttps://www.koaw.org/measuring-fishes\n""], 'url_profile': 'https://github.com/daniela-matos', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'Illmitz, Austria', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Predicting House Sale Prices With Linear Regression & Cross Validation\nIn this project we predict house sale prices using housing data for the city of Ames, Iowa, United States from 2006 to 2010.\n\nDownload Data\nColumns Explanation\n\ntransform_features() - Within this function we remove the non-numerical and other features that are not meaningful for our model. Besides that we remove all features that have more than 25% missing values. Finally we either delete the rows (for non continuous features) or replace the missing values for the mean()\nselect_features() - After identifying the features which are highly correlated, we remove those of the pairs, which have less impact on the model. Within this funciton we also re-scale the features and remove those with very low variance.\ntrain_and_test() - Here we train and test our model with the remaining features. We perform the process by splitting the set in 50/50 (train/test) and also try different cross-validation combinations.\nResults show that, for this dataset, increasing the number of datasets in the k-fold cross validation leads to always smaller RMSEs\n'], 'url_profile': 'https://github.com/burnier', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': '08824, NJ, USA', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['For this project, I have used Google play Store apps data. This data can be used for app-making businesses to succeed. Insightful Data Analysis can help developers to capture the app market better. The dataset is chosen from Kaggle. It is the web scraped data of 10k Play Store apps for analyzing the Android market. It has total 10841 rows and 13 columns. Some key observations include how the performance of the App can be improved from the Reviews/Ratings obtained to get more business values out of it. I have used two different machine learning libraries to test the accuracy of prediction from the dataset.\nAfter downloading the dataset. I got rid of unnecessary data fields and converted the necessary data fields from object to float or integers. Because, to train our model, I will need numeric values. So, I went through intense data cleaning.\nAfter cleaning the data, I have used Postgres SQL database to upload the data.\nWith the clean data, I have done some exploratory analysis and created few visualizations in Tableau to get an idea of the data pattern.\nMachine learning is a program or system that builds (trains) a predictive model from input data. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\nI have used two different ML libraries: Scikit Learn, which uses statistical models\nAnd Pytorch which uses neural network for its models.\nI have used SVC Model of scikit learn library\nHere, ‘Content Rating’ is used as label for classifying based on various features.\nFirst I have created a train test split. My label is Content Rating and the features are rating, reviews, price, size and numeric content rating values.\nMultiple Linear Regression and error test\nI have Divided the data into “attributes” and “labels”.\nX variable contains all the attributes/features and y variable contains labels.\nIn our dataset, Average Rating falls between 4 and 5.\nWe have performed prediction on the test data, and we can see that our model has returned pretty good predicted results. Actual and predicted scores are pretty close.\n'], 'url_profile': 'https://github.com/tasnuvaairen', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'Peterborough', 'stats_list': [], 'contributions': '288 contributions\n        in the last year', 'description': ['libgd-nginx-resize-demo\nShowing a regression in the Nginx Image Filter resizing palette transparent PNGs\n'], 'url_profile': 'https://github.com/markdingram', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['AB-Test\nPerforming AB test and Logistic Regression for an e-commerce website\n'], 'url_profile': 'https://github.com/mehulzawar92', 'info_list': ['Scilab', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Dockerfile', 'Apache-2.0 license', 'Updated Apr 28, 2020', 'HTML', 'Updated May 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JosephParrott2', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['Pattern-Recognition-Project\nSentimental Analysis for online reviews using Naive Bayes and Logistic Regression Classifier\nPython 3.6.7\nPackages needed: pandas, nltk, sklearn,gensim, scippy\nYoutube video links:part1: https://youtu.be/Z2Hbho5TVEM\npart2: https://youtu.be/aCj3qCAouc0\n'], 'url_profile': 'https://github.com/PrajwalaNagaraj', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['survival_analysis_2\nThis is a project to practice hypothesis testing and regression problems in survival data.\n'], 'url_profile': 'https://github.com/Akbarnejadhn', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'ISLAMABAD', 'stats_list': [], 'contributions': '370 contributions\n        in the last year', 'description': ['LogisticRegressionPythonCODE-using-numpy\nA simple core logistic regression mathematical code to calculate cost function and gradient\n'], 'url_profile': 'https://github.com/abdullahbilalawan', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Youtube-Views-Prediction\nYoutube Views Prediction is done by using linear regression and random forest\nDataset:\nhttps://www.kaggle.com/datasnaek/youtube-new#INvideos.csv\n'], 'url_profile': 'https://github.com/AdeshRamgude', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Machine-Learning | Apprentice-Chef\nThe objective of this analysis was to build a regression based analysis and a classification-based predictive model using Apprentice Chef database.\nApprentice Chef is an innovative company with a unique spin on cooking at home. Their targeted customer is the busy professional with little to no skills in the kitchen, but appreciate a good meal.\nThe first analysis (A1) is destined to better understand how much revenue to expect from each customer within their first year of orders.\nThe second analysis (A2) explores which variables impact most for the success of their cross-sell, Halfway There, a promotion where subscribers receive a half bottle with the meal every Wednesday. The objective here is to understand which customer will be more succeptible to purchase this promotion.\n A1: Regression based analysis \nFiles:\nA1_Analysis.ipynb\nA1_Model.py\nA1_Write_Up\n A2: Classification-based predictive model \nFiles:\nA2_Analysis.ipynb\nA2_Model.py\nA2_Write_Up\nIn both assignments, we have three different files.  Analysis  comprehend the exploratory process of handling the data to find the build the model and find the business insights.  Model  contains only the code needed to run the final model, giving the respective score. Lastly,  Write_Up  is where we summarized the analysis with the most impactful business insights that we were able to gather from the analysis, followed by an actionable recommendation.\n'], 'url_profile': 'https://github.com/tdealmeidamarcondes2019', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'Boston ', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yusubovkamil', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'HCMC', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['adaptive-wing-loss\nKeras Implementation of Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression\n\nPaper\nOfficial Implementation in Pytorch\n\nInstallation\nInstall system requirements\n\nPython 3.7\n\nInstall python dependencies\npip install -r requirements.txt\nDataset preparation\nDowload WFLW dataset and annotaion, extract and place them under directory dataset as below:\nadaptive-wing-loss\n├── dataset\n\xa0\xa0 ├── WFLW_annotations\n\xa0\xa0 └── WFLW_images\n'], 'url_profile': 'https://github.com/smookie2', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Ames Iowa Housing Prices Analysis\nby: Elton Yeo\nContext and Problem Statement\nTo predict housing prices in Ames, Iowa, by understanding the relationships among various variables such as lot area, neighbourhood etc., and using linear regression models to predict the prices.\nThis will allow prospective home-owners to better manage their cashflow when preparing to buy a house, current home-owners to know how to best add value to their houses, and policy-makers to better monitor and regulate housing prices.\nExecutive Summary\nIn this analysis, we considered 80 different variables and their effects on housing prices in Ames, Iowa. We broke down the variables into categorical, numerical, and ordinal. We analysed and cleaned the data to impute missing values and change wrong data types. We conducted feature engineering by combining variables and log-transforming sale prices. We also dropped categorical variables with mode >80%.\nWe used linear regression, Ridge and Lasso models on our cleaned data, and chose Ridge based on the highest r-squared score and lowest root mean squared error. Our predicted housing prices based on the test set generated a Kaggle score of 35118, which is within the top 60 on the leaderboard.\nData Analysis, Cleaning and Feature Engineering\nWe considered whether any of the data was of the wrong type, and also the number of missing values in each variable.\n\nData of the wrong type was amended.\nFor missing values, we assumed in the case of cateogrical variables that none of those features were found in the houses i.e. missing values were filled with ""none"". In the case of numerical variables, we assumed that none of those features were found in the house i.e missing values were filled with ""0"".\n\nFor variables which were similar, we combined them e.g. there was no need to differentiate among the different types of porches, what was important was whether or not the house had a porch.\nLooking at the distribution of saleprice (which is only found in the train set), we saw that it was a unimodal distribution with a right skew. Since this was our target variable, we log-transformed it to fit it into a normal distribution. This corrected the skewness of the distribution and allowed it to be modelled more easily later.\nBased on visualisations of the data, we saw that most numerical variables had multimodal distributions. Scatterplots of the numerical variable also allowed us to identify an impossible outlier i.e. year the garage was built was 2207 - this was amended to 2007. Boxplots of the categorical data against sale price showed that most of these variables were found within the lower end of sale prices i.e. less than $300,000.\nWe also dropped categorical variables with mode >80%. This is because there is much less variance and hence such categorical variables will not have significant impact on the model.\nModelling\nWe used linear regression, Ridge and Lasso models on our cleaned data and compared r-squared and root mean squared error figures to determine the best model to predict housing prices from our test set. Our chosen model was Ridge. Our predicted housing prices generated a Kaggle score of 35118, which is within the top 60 on the leaderboard.\nConclusion and Recommendations\nThe features which added the most value to housing prices were:\n\nOverall quality (rates the overall material and finish of the house)\nGround living area (Above ground living area square feet)\nTotal square feet (combination of first floor, second floor and basement area square feet)\n\nThe features which hurt the value of housing prices the most were:\n\nMisc val (the value of miscellaneous features)\nBeing in the Edwards neighborhood\nBeing in a commercial zone\n\nIn order to increase the value of their houses, owners could consider improving the overall quality of the house by commencing renovation and upgrading works. This could include waterproofing, new coats of paint, stronger materials etc. Owners could also consider building additional floors if permitted, to increase the above total sqaure feet of the home across all levels and thereby increase space to live, work and play in.\nBuying houses in the Northridge Heights neighbourhood or Stone Brook neighbourhood would be good investments as being in those neighbourhoods added value to the houses.\nThis model would likely generalise to other cities as overall quality, and total sqaure feet of the house across all room types, would be important to most homebuyers even in different cities. In order to make the model even more universal, we could consider removing features which are specific to Ames, Iowa such as the specific neighbourhoods. In order to make a comparable model for another city, we could collect more data on race, income, age, occupation, political affiliation, neighbourhood etc. to consider how these various factors might affect the prices of houses across neighbourhoods in the other city.\n'], 'url_profile': 'https://github.com/yeoelton', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Spam-classifier\nSpam e-mail classifier based on logistic regression and multi-layer perceptron.\n'], 'url_profile': 'https://github.com/Rostux', 'info_list': ['Python', 'Updated May 2, 2020', 'Python', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Apr 29, 2020']}"
"{'location': 'Denver, CO', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['MicrosoftML\nUsing Microsoft Machine Learning to practice classifications and linear regression analysis.\n'], 'url_profile': 'https://github.com/toppytim', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': [""Logistic Regression implementation in Python\nPython implementation of logistic regression algorithm. Used in a data mining project for phishing website prediction. Project for CS 235 at UC Riverside, instructed by Professor Evangelos Papalexakis. Implementation and notebook files, code, and comments are the exclusive work of Nathan Gootee and can be found at https://github.com/njgootee/logisticregression . Collaborative and other group member's work for project is NOT located in this repository.\nData set\nData set sourced from https://www.kaggle.com/akashkr/phishing-website-dataset .  Feature descriptions can also be found at this webpage.\nRun instructions\nTo use implementation of LR, simply include in current directory and import file. For example implementation process see phishing_lr.ipynb. To follow implementation as applied to phishing website data set, download files and place in same directory, then open and execute code cells in phishing_lr.ipynb notebook file.\n""], 'url_profile': 'https://github.com/njgootee', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""Logistic regression method for passenger survival prediction attempt.\nThis notebook is an attempt to predict survival of titanic ship passenger given a person's regular information such as age, sex, and others using python scikit-learn's logistic regression method.\nThis notebook uses a dataset called 'titanic' from seaborn library. The dataset is also available on Kaggle as it is one of the topics that Kaggle use for their competition.\n""], 'url_profile': 'https://github.com/billSianipar', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sujay-Hazra', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['cps_data\nA difference-in-differences regression to test CA versus AZ working women and labor attachment\n'], 'url_profile': 'https://github.com/JSBourkland', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'Goa, India', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Transfer_Value_Predictor\nUsing random forest regression to predict the values of football players according to FIFA19 data set.\n'], 'url_profile': 'https://github.com/ayushkr3301', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jimbolit', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'Athens, GA', 'stats_list': [], 'contributions': '225 contributions\n        in the last year', 'description': ['telco-customer-churn-logistic-regression\n'], 'url_profile': 'https://github.com/shivaniarbat', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['LinerRegression-GoogleColab-Jupyter\nMachine learning project using sample house data to calculate average house prices through linear regression.\n'], 'url_profile': 'https://github.com/sanketghanmare', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}","{'location': 'Vienna', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Python package DescribeLinearModel\nPython package with DescribeLinearModel Class for calculating correlation and regression line of data points\nConfiguration\n\nPython 3 with libraries numpy, matplotlib, sys, os\n\nInstallation\nUse the package manager pip to install DescribeLinearModel.\npip install DescribeLinearModel\nUsage\nThis pure python package is a Class for calculating correlation and regression line of data points and can be used as follows:\n\nImport library DescribeLinearModel:\n\nfrom DescribeLinearModel import DescribeLinearModel\n\nNew object object_name with the DescribeLinearModel class can be initiated with following command:\n\nobject_name = DescribeLinearModel(x_data, y_data)\nx_data and y_data are lists of floats\n\nNew object object_name with the DescribeLinearModel class has following methods:\n\nobject_name.read_file_x_data(""x_data.txt"", True)\nreads in x coordinates of data points from a txt file\nobject_name.read_file_y_data(""y_data.txt"", True)\nreads in y coordinates of data points from a txt file\nobject_name.set_data_point(new_x, new_y)\nappends new data point to existing data attributes (x_data, y_data)\nobject_name.change_data_point(index_data_point, new_x, new_y)\noverwrites existing data point from existing data attributes (x_data, y_data)\nobject_name.calculate_slope()\ncalculates the slope of the regression line from all data points\nobject_name.calculate_intercept()\ncalculates the y-intercept c of the regression line from all data points\nobject_name.calculate_r()\ncalculates pearson\'s correlation coefficient r from all data points\nobject_name.calculate_r_squared()\ncalculates the coefficient of determination r_squared from all data points\nobject_name.plot_scatterplot()\noutputs a scatterplot including a regression line of the instance variable data object_name\nDescribeLinearModel python package is uploaded on PyPi\nFile Manifest\n\nfolder DescribeLinearModel containing:\n\nDescribeLinearModel.py - python file with code of class DescribeLinearModel\nDescribeLinearModel_unittest.py - python file with unittests of DescribeLinearModel class\ninit.py - python file to execute DescribeLinearModel class when loading the module\nsetup.cfg - Python package setup configuration file\nLICENSE.md - markdown file with license.md for this software package\nREADME.md - markdown file with instructions how to install and use this python package\n\n\nsetup.py - python setup file with metadata about the package, necessary for pip installing\n\nContributing \nCopyright and Licencing\nThis project is licensed under the terms of the MIT license\nContact\nAuthor: Eugen Iftimoaie\nFor questions feel free to contact me on my e-mail adress: eugen.iftimoaie@gmx.de\n'], 'url_profile': 'https://github.com/eugeniftimoaie', 'info_list': ['Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'HTML', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Python', 'MIT license', 'Updated Jun 16, 2020']}"
"{'location': 'Athens, GA', 'stats_list': [], 'contributions': '225 contributions\n        in the last year', 'description': ['telco-customer-churn-logistic-regression\n'], 'url_profile': 'https://github.com/shivaniarbat', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['LinerRegression-GoogleColab-Jupyter\nMachine learning project using sample house data to calculate average house prices through linear regression.\n'], 'url_profile': 'https://github.com/sanketghanmare', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': [""Predicting the number of active e-scooter users\nProblem statement: Predict the total number of active e-scooter users with the given attributes\nApproach:\n\nData extraction\nExploratory data analysis\nEnd-to-end machine learning pipeline\n\nRunning the files\n\nChange directory to this folder\nIn the command prompt, type\n\nsh run.sh\n\nThis would run the run.sh file, which loads the installs the necessary packages under the requirements.txt file and runs the main.py Python script.\n(1) Data Extraction\nThe dataExtraction.py file uses SQL query to extract the dataset from the given connection details. The extracted data is then saved as rental_data.csv.\nWe need these variables from rental_data:\ndate:                   YYYY-MM-DD format\nhr:                     0 to 23\nweather:                condition for the hr\ntemperature:            average for the hr (Fahrenheit)\nfeels_like_temperature: average feeling for the hr (Fahrenheit)\nrelative_humidity:      average for the hr\nwindspeed:              average for the hr\npsi:                    pollutant standard index (0 to 400)\nguest_scooter:          no. of guest users for the hr\nregistered_scooter:     no. of registered users for the hr\n\n(2) Exploratory Data Analysis\nThe eda.ipynb file shows the data visualization of the rental dataset. This file can be split into two parts:\n\nHypothesis generation and testing\nTime series analysis\n\nBoth of these would help in identifying the variables that need to be pre-processed.\nNotably, there were many outliers in the guest_scooter and registered_scooter dependent variables. We will be using a logarithmic transformation to deal with these outliers.\n(3) End-to-End Machine Learning Pipeline\nThe main.py file contains the entire machine learning pipeline from data extraction to feature engineering. This file will call the functions from other files:\n\ndataExtraction.py - Mentioned above in (1) Data Extraction\nfeatureEngineering.py - Cleans the data and returns a clean_data.csv file\nregression.py - Fits and predicts the clean dataset to the various supervised learning regression models\n\nModel choices\n3 regression models were used\n\nRidge regression\nLasso regression\nRandom forest regression\n\nAn important point to note is that the guest users and registered users were trained on separate models. From the EDA, we find out that these two groups have different seasonality trend although they exhibit the same overall trend.\nIn addition, a new scoring method was created using make_scorer from sklearn.metrics. We used the Root Mean Squared Logarithmic Error to calculate the error since we are took the logarithmic transformation for the guest_scooter and registered_scooter variables.\nModel evaluation\nThe error metrics used is the Root Mean Squared Logarithmic Error (RMSLE). RMSLE incurs a larger penalty for the underestimation of the actual variable than its overestimation. This is especially useful for when the underestimation of the target variable is not acceptable but overestimation can be tolerated.\nAfter using GridSearchCV to find the best parameters, the RandomForestRegressor gave the lowest RMSLE values. The scores are as follows:\n(1) Random forest regressor\nRandom forest regression for guest_scooter\nBest parameters: {'max_depth': 7, 'n_estimators': 300}\nRMSLE score: 0.23545610486361496\n\nRandom forest regression for registered_scooter\nBest parameters: {'max_depth': 8, 'n_estimators': 500}\nRMSLE score: 0.12729247135692237\n\n(2) Ridge regression\nRidge regression for guest_scooter\nBest parameters: {'alpha': 0.01, 'max_iter': 3000}\nRMSLE score: 0.27159622182418625\n\nRidge regression for registered_scooter\nBest parameters: {'alpha': 0.01, 'max_iter': 3000}\nRMSLE score: 0.1675662194215094\n\n(3) Lasso regression\nLasso regression for guest_scooter\nBest parameters: {'alpha': 0.01, 'max_iter': 3000}\nRMSLE score: 0.27203342609624465\n\nLasso regression for registered_scooter\nBest parameters: {'alpha': 0.01, 'max_iter': 3000}\nRMSLE score: 0.1675955071161706\n\nGiven a test dataset, we can use the Random Forest Regression to predict the future demand of scooters. To get the total demand, we simply add the demand from guest and registered users.\nMoving forward\nTime series modelling can be conducted to predict the total demand for scooters.\n""], 'url_profile': 'https://github.com/agrilive', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '183 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/savanismit', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['Linear-regression-for-bigcity-data\na linear regression model for population of bigcity in 1920 vs population in 1930\n'], 'url_profile': 'https://github.com/Keerthanab14', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple benchmark to demo performance regression seen in elasticsearch index creation.\nAssumes ruby 2.6.6 and bundler are installed either directly or via a ruby version manager like rbenv or rvm.\nAlso assumes docker for running elasticsearch\nSetup\nInstallation\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:5.6.16\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:6.8.8\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:7.6.2\nbundle install\n\nRunning\nRun each command in a separate terminal window\nES 5.6.16\ndocker run --rm -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" -e ""bootstrap.memory_lock=true"" --ulimit ""memlock=-1:-1""  -e ""action.auto_create_index=.watches,.triggered_watches,.watcher-history-*"" -e ES_JAVA_OPTS=""-Xms4g -Xmx4g"" -e ""xpack.security.enabled=false"" -e ""logger.org.elasticsearch.cluster.service=TRACE"" docker.elastic.co/elasticsearch/elasticsearch:5.6.16\n\nbundle exec ruby bench_es_index_mgmt.rb && bundle exec ruby bench_es_index_mgmt.rb && bundle exec ruby bench_es_index_mgmt.rb\nES 6.8.8\ndocker run --rm -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" -e ""bootstrap.memory_lock=true"" --ulimit ""memlock=-1:-1"" -e ""action.auto_create_index=.watches,.triggered_watches,.watcher-history-*"" -e ES_JAVA_OPTS=""-Xms4g -Xmx4g -Xlog:disable"" -e ""logger.org.elasticsearch.cluster.service=TRACE"" docker.elastic.co/elasticsearch/elasticsearch:6.8.8\n\nbundle exec ruby bench_es_index_mgmt.rb && bundle exec ruby bench_es_index_mgmt.rb && bundle exec ruby bench_es_index_mgmt.rb\nES 7.6.2\ndocker run --rm -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" -e ""bootstrap.memory_lock=true"" --ulimit ""memlock=-1:-1"" -e ""action.auto_create_index=.watches,.triggered_watches,.watcher-history-*"" -e ES_JAVA_OPTS=""-Xms4g -Xmx4g -Xlog:disable"" -e ""logger.org.elasticsearch.cluster.service=TRACE"" docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n\nbundle exec ruby bench_es_index_mgmt.rb && bundle exec ruby bench_es_index_mgmt.rb && bundle exec ruby bench_es_index_mgmt.rb\nHow to read the results\neach run of the script will output something like\nWarming up --------------------------------------\n   create 20 indices     1.000  i/100ms\n   20 indices wait:0     1.000  i/100ms\nCalculating -------------------------------------\n   create 20 indices      1.018  (± 0.0%) i/s -     31.000  in  30.506013s\n   20 indices wait:0      1.489  (± 0.0%) i/s -     45.000  in  30.441973s\n\nComparison:\n   20 indices wait:0:        1.5 i/s\n   create 20 indices:        1.0 i/s - 1.46x  slower\n\nAs more indices are created and deleted, the creation of new indices (in 6.8 and 7.2) will gradually slow down and the number of iterations per second (i/s) will gradually decrease on subsequent runs. Version 5.6 does not exhibit this behavior.\nShell Scripts\nper https://discuss.elastic.co/t/index-creation-slows-down-over-time/230775/11\nThere are two versions: one for linux and the other for macOS due to macOS not having gnu date installed (and therefore not supporting the %N format token out of the box).\nThe macOS one requires that you have gnu core-utils installed. Easiest way is to use brew install core-utils.\n'], 'url_profile': 'https://github.com/APMG', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'Colorado, USA', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': [""Project Summary\nPredict the price of homes at sale for the Ames Iowa Housing dataset.\nBased on House Prices: Advanced Regression Techniques Kaggle Competition\nContents:\n\nProblem Statement\nExecutive Summary\nData Summary\nModels and Techniques\nConclusions\n\nProblem Statement\nCreate a regression model based on the Ames Housing Dataset that predicts the price of a house at sale.\nExecutive Summary\nIn this project we were tasked with creating a regression model based on the Ames Housing Dataset that predicts the price of a house at sale. The approach, as represented in the respective notebooks, was split into two phases: Exploratory Data Analysis (EDA) and Modeling. Our models were then tested on unseen data, scored, and ranked relative to the models of other members of our cohort.\nI submitted a total of 13 models, 4 of which are included in my Models workbook. Each model improves sequentially, illustrating my process.\nAt a high level, there are two clear themes when it comes to fitting a good multiple linear regression model, based on my models. First, taking the log of y, SalePrice, increases the model’s performance by all measures used. We saw this between Model 1 and Model 2. Second, increasing the number of dependent variables is generally a good thing. We observed this between Model 0 to Model 1 as well as Model 2 to Model 3. In the case of the latter, the increase was substantial (from 16 features to 303 features). Certainly, this leaves plenty of room for further investigation and fine tuning.\nData Summary\nData Source:\n\nThe data for this project came from General Assembly’s DSI-US-11 Project 2 Regression Challenge, which is an adapted version of the data in Kaggle's Ames Housing Data competition.\nIn all datasets, each row represents a home and each column a various feature used in computing assessed home values.\n\nDatasets:\n\ntrain.csv\n\nOriginal training data set (2501 rows, 81 columns)\n\n\ntest.csv\n\nOriginal testing data set, same columns as train.csv excluding target variable  SalePrice (878 rows, 80 columns)\n\n\ntrain_processed.csv\n\nReflects changes to train.csv, most notably dropping outliers and adding dummy columns for categorical features (2049 rows, 304 columns)\n\n\ntest_processed.csv\n\nIncludes same changes to features as train_processed.csv (878 rows, 286 columns)\n\n\n\nNo outside datasets were used.\nData Dictionary\nFeatures listed in the data dictionary below represent those features included in the model I submitted to Kaggle. A full list of feature descriptions can be found here.\n\n\n\nFeature\nType\nDataset\nDescription\n\n\n\n\nOverall Qual\nordinal\ntrain\nRates the overall material and finish of the house (1-10 with 1 being Very Poor and 10 Very Excellent)\n\n\nGarage Area\nfloat\ntrain\nSize of garage in square feet\n\n\n1st Flr SF\nfloat\ntrain\nFirst Floor square feet\n\n\nMS SubClass\nnominal\ntrain\nIdentifies the type of dwelling involved in the sale (16 categories)\n\n\nLot Frontage\nfloat\ntrain\nLinear feet of street connected to property\n\n\nYear Built\nint\ntrain\nOriginal construction date\n\n\nYear Remod/Add\nint\ntrain\nRemodel date (same as construction date if no remodeling or additions)\n\n\nFull Bath\nint\ntrain\nNumber of full bathrooms above ground\n\n\nTotRms AbvGrd\nint\ntrain\nTotal rooms above ground (does not include bathrooms)\n\n\nFireplaces\nint\ntrain\nNumber of fireplaces\n\n\nHeating QC_Ex\nordinal\ntrain\nHeating quality and condition (5-point scale from Poor to Excellent)\n\n\nNeighborhood_NridgHt\nint\ntrain_processed\ndummy of 'Neighborhood' for Northridge Heights neighborhood\n\n\nExter Qual_TA\nint\ntrain_processed\ndummy of 'Exter Qual', which evaluates the quality of the material on the exterior; TA for Average/Typical\n\n\nOpen Porch SF\nfloat\ntrain\nOpen porch area in square feet\n\n\nWood Deck SF\nfloat\ntrain\nWood deck area in square feet\n\n\nCentral Air_Y\nint\ntrain_processed\nhas central air conditioning\n\n\n\nModels and Techniques\n\nOur predictions are based on a multiple linear regression (MLR) model. The dependent variables used in the final model are a combination of features included in the original dataset as well as dummies from some of the categorical features.\nThe 02_Models notebook contains 3 models, excluding the model used to calculate the baseline score, with each progressive model improving.\nModel 0 took in the top 2 correlated variables and was used to calculate the baseline score. Model 1 is composed of 16 prediction features, taken from the top 15 positively and negatively correlated features. Model 2, the model used in the Kaggle submission, is the same as Model 1, except it is fit using log y. Finally, Model 3 takes in all numeric variables (including dummies).\nEvaluation metrics were root mean squared error (RMSE), mean absolute error (MAE), as well as R2 scores for each train, test, and cross validation.\n\nConclusions\n\nAt a high level, there are two clear themes when it comes to fitting a good MLR model, based on my models. First, taking the log of y, SalePrice, increases the model’s performance by all measures used. We saw this between Model 1 and Model 2. Second, increasing the number of dependent variables is generally a good thing. We observed this between Model 0 to Model 1 as well as Model 2 to Model 3. In the case of the latter, the increase was substantial (from 16 features to 303 features). Certainly, this leaves plenty of room for further investigation and fine tuning.\nThe features included in Models 2&3 illustrate that it can be important to include both positively and negatively correlated variables. This is can be counterintuitive, in that people generally associate positively correlated features to predicting a target y. However, when we get our coefficients on well-chosen negatively correlated features, this can help to decrease residuals.\n\n""], 'url_profile': 'https://github.com/chillahwhale', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dsradecki', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['modelling-VO2--data\nThis repo shows practice of multiple linear regression models using R.\n'], 'url_profile': 'https://github.com/shaun-cameron', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['bmi_predictive_analysis\nUsing linear regression model and support vector machine to predict the body mass index of patients.\n'], 'url_profile': 'https://github.com/benarems', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Ruby', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', 'HTML', 'Updated May 3, 2020', 'Updated May 1, 2020']}"
"{'location': 'Waterloo, ON', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Replication files for ""Asymptotic Inference for the Constrained Quantile Regression Process""\nFirst written 2017-10-27, cleaned up 2020-04-30.\nThis repository contains files to replicate the figures in the paper.\nTwo directories contain the files used to run each simulation experiment, and\nthey depend on two helper files located in the top-level directory.\nThe experiment directories\nEach directory contains a sim.R file, the script for the experiments, well as\na shell script that was used on the SLURM system to execute the command.  The\nfile sim.R should produce the data file normal_sim.rda or\noneregressor_treatment_sim.rda.  The files plot_normal.R and\nplot_treatment.R each create two p-value plots corresponding to supremum-norm\nor L2-norm statistics.\nCommon files\nBoth simulation experiments depend on the file const_inf_utils.cpp, which\ncontains C++ code that is used to speed up the numerical work.  They also share\nthe common file plotting_utils.R to create plots.\n'], 'url_profile': 'https://github.com/tmparker', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Vienna, Austria', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Computational Compound Design Assignment\nThis repository will deal as a basic layout for the machine learning\nassignments for the computational compound design lecture of Prof. von Lilienfeld (Uni Basel).\nLayout\nassignment\nWill contain the solution to the respective exercise.\nkrr\nContains the Python functionalities for training etc.\nqm7\n.xyz files and PBE0/def2-TZVP atomization energies for all molecules\nin QM7.\nInstallation\nInstall the requirements via:\npip install -r requirements.txt\nand add the repository to your $PYTHONPATH\nexport PYTHONPATH=""your_path/ccd_krr""\n'], 'url_profile': 'https://github.com/Dom1L', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Boston, Massachusetts', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Spotify-Chart-Trend-Analysis\nARIMA, Random Forest, Linear Support Vector Classifier, K-Nearest Neighbours, Logistic Regression, Decision Tree Classifier\n'], 'url_profile': 'https://github.com/ojasphansekar', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Avrl-agrwl', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['Linear Regression\nWith python and without an incremental algorithm this time\n\nFits a best fit line to a dataset of fish height vs weight from Aung Pyae\nThis is computed by solving a system of linear equations given by\n\n\nset to zero, so as to find the minimum of the cost function J,\n\nwhere xi and yi denote the ith fish height and weight, respectively, not exponentiation.\nresulting in the following system\n\nThe solution of this system results a vector of thetas that fits a linear hypothesis function to the dataset of fish heights and weights, corresponding to the function\n\nwhich, in the context of this dataset, suggests an increase in weight of about 100 grams for every centimeter increase in height of Bream\n'], 'url_profile': 'https://github.com/SAXTEN2011', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['Identification of Complementarity-determining region3(CDRH3) on the heavy chain using a logistic regression model\nMethods\nI used a logistic regression model to create a machine learning model to return a vector of predicted class labels for the predictor data in the matrixes we created. Logistic Regression is a machine learning algorithm which can be used for classification or regression problems.\nData was divided for training and testing purposes for the entire dataset. I created a subset file of 500 sequences from the entire dataset.\n'], 'url_profile': 'https://github.com/Carol-P18', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Göttingen', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['\ngp-responses\nThis repository contains the code for my paper on Gaussian process\nresponses in distributional regression, i.e.\xa0the scripts for the\nsimulation study, the application section, and the figures. These\nscripts build on the following R packages, which I develop in separate\nrepositories:\n\nbamlssAPI\nbamlssGP\nggnuplot\n\nHere is the full list of R packages that are required to run the code in\nthis repository:\n\nbamlss\nbamlssGP\ncoda\ndevtools\ndplyr\nforcats\nfuzzyjoin\ngeosphere\nggnuplot\nggplot2\nhere\njanitor\nknitr\nmvtnorm\nparallel\npurrr\nreadr\nrequirements\nrmarkdown\nscales\nsf\nstringr\ntidyr\n\n'], 'url_profile': 'https://github.com/hriebl', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Predicting product quality for Tennesee Eastman process\nThe TEP was created by the Eastman Chemical Company to provide a realistic industrial process for evaluating process control and monitoring methods. The test process is based on a simulation of an actual industrial process. The process consists of five major units: a reactor, condenser, compressor, separator, and stripper. The components A,C,D,E,F,B are the input for the process. G and H are the final product while F is the by-product.\nThe use of  intelligent data analysis tools to estimate key variables in the complex chemical processes is an important activity. It is sometimes essential to accurately predict the difficult-to-measure variables, especially quality as it has a direct impact on the economic operation of the plant.\n\nProcess Variables\nThere are 41 measured variables and 12 manipulated variables in the process. Among those 41 measures variables 22 are sampled every 3 minutes, XMEAS(l) through XMEAS (22). The rest 19 are composition variables. These composition measurements, XMEAS (23) through XMEAS (41), are taken from Streams 6, 9, and 11. The sampling interval and time delay for Streams 6 and 9 are both equal to 6 minutes, and for Stream 11 are equal to 15 minutes. All the process measurements include Gaussian noise.\nOrganization of Dataset\nIn the dataset there are 44 files. The files with name ""*_te"" are for testing. Files named as “d00” and “d00_te” are for normal operating conditions. Remaining are the data sets for faulty conditions for training and testing. Each training data file contains 480 rows and 52 columns. Each testing set contains 960 rows and 52 columns. The files with faults also have some data points for normal operation.\nThe dataset can be download from https://ieee-dataport.org/documents/tennessee-eastman-simulation-dataset#files\nAlgorithm\nSupport Vector Regression (TE_SVR.ipynb) and Artificial Neural Networks (TE_ANN.ipynb) is used to estimate the quality of products G and H.\n'], 'url_profile': 'https://github.com/UtkarshPanara', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '405 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/baoanh1310', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Montreal, QC, Canada', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Linear-Regression-for-dummies\nIn this repository, there is a sample code that lets you build and train a simple linear regression model. The dataset used  is open-source and can be found here: https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv.\n'], 'url_profile': 'https://github.com/richmugwa1', 'info_list': ['R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 28, 2020', 'MATLAB', 'MIT license', 'Updated Apr 28, 2020', 'R', 'Updated Nov 18, 2020', 'Jupyter Notebook', 'Updated Oct 11, 2020', 'CSS', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}"
"{'location': 'Pilibhit, Uttar Pradesh, India', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""House_predictions_TF_2.0\nNote: The original code was written in Jupyter notebook but Github isn't supporting the notebooks temporarily, so had to convert it into .py format\nRegression task which helps to find prices of houses depending on various other variables.\nThe regression is done on a dataset(Kaggle) containing information about houses and has following feature variables:\nid - Unique ID for each home sold\ndate - Date of the home sale\nprice - Price of each home sold\nbedrooms - Number of bedrooms\nbathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower\nsqft_living - Square footage of the apartments interior living space\nsqft_lot - Square footage of the land space\nfloors - Number of floors\nwaterfront - A dummy variable for whether the apartment was overlooking the waterfront or not\nview - An index from 0 to 4 of how good the view of the property was\ncondition - An index from 1 to 5 on the condition of the apartment,\ngrade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\nsqft_above - The square footage of the interior housing space that is above ground level\nsqft_basement - The square footage of the interior housing space that is below ground level\nyr_built - The year the house was initially built\nyr_renovated - The year of the house’s last renovation\nzipcode - What zipcode area the house is in\nlat - Lattitude\nlong - Longitude\nsqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors\nsqft_lot15 - The square footage of the land lots of the nearest 15 neighbors\n""], 'url_profile': 'https://github.com/Saquibkhan456', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Mumbai, india', 'stats_list': [], 'contributions': '326 contributions\n        in the last year', 'description': ['GUI-Implementation-of-Linear-Regression\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression. If we plot the independent variable (x) on the x-axis and dependent variable (y) on the y-axis, linear regression gives us a straight line that best fits the data points,\nLinear Regression is a machine learning algorithm based on supervised learning. ... Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output).\nIn our project, we have implemented Linear Regression Algorithm in Python and created a GUI using Tkinter.\nIt basically consists of 2 parts :\nPlotting a straight line on linear regression graph\nUpdation in dataset given\nSo on clicking the ‘regression’ button, we get the graph corresponding to all the sentiment subjectivity (Y) and sentiment polarity (X).\nFrom the graph, for our given value of X we get the value of Y\nThis is analysis of the dataset present in the shared folder.\nThe second feature is that we can update our dataset using this GUI.\nOn clicking the ‘update’ button, we need to enter ‘app name’ and ‘review’ as text inputs, choose our sentiment from positive, negative or neutral and input polarity and subjectivity sentiment values.\nConstraints :\n-Sentiment values should range between -1 to 1\n-Decimals are allowed\n-If chosen ‘neutral’, enter sentiment values as 0\n'], 'url_profile': 'https://github.com/jugal-chauhan', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Rohtak, Haryana, India', 'stats_list': [], 'contributions': '619 contributions\n        in the last year', 'description': ['Predicting-Breast-Cancer\nLogistic Regression Model has been used to predict the chances of Breast Cancer.\nAbout Dataset\nAttribute Information:-\n\n\nid\ndiagnosis: M = malignant, B = benign\n\n\n\nColumns 3 to 32\nTen real-valued features are computed for each cell nucleus:\n\n\n\nradius: distances from center to points on the perimeter\ntexture: standard deviation of gray-scale values\nperimeter\narea\nsmoothness: local variation in radius lengths\ncompactness: perimeter^2 / area - 1.0\nconcavity: severity of concave portions of the contour\nconcave points: number of concave portions of the contour\nsymmetry\nfractal dimension: ""coastline approximation"" - 1 The mean, standard error, and ""worst"" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n\n'], 'url_profile': 'https://github.com/m0-k1', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Guangzhou, China', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Stata 新命令：wmtreg——回归结果的输出\n\n作者：王美庭\nEmail: wangmeiting92@gmail.com\n\n摘要\n本文主要介绍了个人编写的可将回归结果输出至 Stata 界面、Word 以及 LaTeX 的wmtreg命令。\n目录\n\n摘要\n一、引言\n二、命令的安装\n三、语法与选项\n四、实例\n五、输出效果展示\n\n一、引言\n本文介绍的wmtreg的命令，可以将回归结果输出至 Stata 界面、Word 的 .rtf 文件和 LaTeX 的.tex 文件。基于esttab内核，wmtreg不仅具有了esttab的优点，同时也简化了书写语法。\n本文阐述的wmtreg命令，和已经或即将推出wmtsum、wmttest、wmtcorr和wmtmat命令，都可以通过append选项成为一个整体，将输出结果集中于一个 Word 或 LaTeX 文件中。关于以上系列命令更多的优点，可参见「Stata 新命令：wmtsum——描述性统计表格的输出」。\n二、命令的安装\nwmtreg命令以及本人其他命令的代码都将托管于 GitHub 上，以使得同学们可以随时下载安装这些命令。\n首先你需要有github命令，如果没有，可参见「Stata 新命令：wmtsum——描述性统计表格的输出」进行安装。\n然后你就可以运行以下命令安装最新的wmtreg命令及其帮助文件了：\ngithub install Meiting-Wang/wmtreg\n当然，你也可以github search一下，也能找到wmtreg命令安装的入口：\ngithub search wmtreg\n或许，你还想一下子找到wmtsum、wmttest、wmtcorr、wmtreg以及wmtmat所有命令在 GitHub 的安装入口，那么你可以：\ngithub search wmt\n三、语法与选项\n命令语法：\nwmtreg [est_store_names] [using filename] [, options]\n\n\nest_store_names: 输入要报告的回归模型，默认导入所有已经储存好的回归模型\nusing: 可以将结果输出至 Word（ .rtf 文件）和 LaTeX（ .tex 文件）\n\n\n选项（options）：\n\n一般选项\n\ndrop(varlist)：不报告指定变量的回归系数\nkeep(varlist)：只报告指定变量的回归系数\norder(varlist)：设定处于表格最顶端的变量\nvarlabels(matchlist)：更改所报告表格中变量的名称\nb(fmt)：报告普通的回归系数\nbeta(fmt)：报告标准化的回归系数\nse(fmt): 在回归系数下方报告标准误\nt(fmt): 在回归系数下方报告 t 值\np(fmt): 在回归系数下方报告 p 值\nonlyb: 只报告回归系数，而不报告 se、t、p 值\nstaraux: 将显著性星号标注下 se、t 或 p 值上(* p<0.1, ** p<0.05, *** p<0.01)\nnostar: 不报告星号\nindicates(string): 不报告指定变量的回归系数，换之以 Yes 或 No 表示变量存在或不存在\nscalars(string): 可填入r2、ar2、pr2、aic、F、ll和N，默认为r2和N。同时我们也可以为每一个写入的变量设置数值格式，如r2(%9.2f)\nnonumbers: 不报告每个回归模型的序列号\nnomtitles: 不报告每个回归模型的名称\nmtitles(string): 设置每一个回归模型的名称。另外，关键字depvars表示每个回归模型的名称将用因变量代替，esn表示每个回归模型的名称将用其在 Stata 内存中的模型储存名代替。\ntitle(string): 设置表格的标题，Regression result为默认值。\nmgroups(string): 为回归模型设置分组，并附之以指定的组名，如mgroups(2 2 A B)表示前两个回归为组别 A，后两个回归为组别 B\nreplace：将结果输出至 Word 或 LaTeX 时，替换已有的文件\nappend：将结果输出至 Word 或 LaTeX 时，可附加在已经存在的文件中\n\n\nLaTeX 专有选项\n\nalignment()：设置 LaTeX 表格的列对齐格式，可输入math或dot，math设置列格式为居中对齐的数学格式（自动添加宏包booktabs和array），dot表示小数点对齐的数学格式（自动添加宏包booktabs、array和dcolumn）。默认为math\npage()：可添加用户额外需要的宏包\n\n\n\n\n\n以上其中的一些选项可以缩写，详情可以在安装完命令后help wmtreg\n\n\n四、实例\n* 回归结果输出实例\nclear all\nsysuse auto.dta, clear\ngen lprice = ln(price)\ntab rep78, gen(rep78_num)\ndrop rep78_num1\n\nreg lprice mpg headroom\nest store m1\nreg lprice mpg trunk headroom\nest store m2\nreg lprice mpg trunk headroom rep78_num*\nest store m3\nreg lprice mpg trunk headroom foreign rep78_num*\nest store m4\n\nwmtreg //报告所有已经储存的回归结果\nwmtreg m1 m2 m3 m4 //报告指定的回归结果\nwmtreg m?, drop(_cons) //不报告常数项\nwmtreg m?, keep(mpg headroom trunk) //只报告指定变量的回归系数\nwmtreg m?, order(foreign mpg) //将变量foreign和mpg置于报告变量的最上方\nwmtreg m?, varl(mpg mpgtest trunk trunktest) //将变量mpg和trunk的展示名称改为mpgtest和trunktest\nwmtreg m?, b(%9.2f) se(%9.3f) //设定回归系数的格式，且默认下在系数的下方报告系数的标准误\nwmtreg m?, ind(""rep78=rep78_num*"" ""foreign=foreign"") //不报告foreign与rep78_num*系列变量的回归系数，换之以Yes或No的形式表示其否在回归中出现\nwmtreg m?, s(r2(%9.3f) F N(%9.0fc)) //报告指定的scalars，并设定其数值格式\nwmtreg m?, mg(Group1 Group2 2 2) //设置前两个回归为组别group1，后两个回归为组别group2\nwmtreg m? using Myfile.rtf, replace //将回归结果导出至Word\nwmtreg m? using Myfile.tex, replace //将回归结果导出至LaTeX\nwmtreg m? using Myfile.tex, replace a(dot) //将回归结果导出至LaTeX，并将其列表格设置为小数点对齐\n\n以上所有实例都可以在help wmtreg中直接运行。\n\n\n五、输出效果展示\n\nStata\n\nwmtreg m1 m2 m3 m4\nRegression result\n--------------------------------------------------------------\n                 (1)          (2)          (3)          (4)\n              lprice       lprice       lprice       lprice\n--------------------------------------------------------------\nmpg           -0.036***    -0.030***    -0.036***    -0.038***\n             (0.008)      (0.008)      (0.009)      (0.009)\nheadroom      -0.052       -0.116*      -0.105       -0.096\n             (0.052)      (0.063)      (0.064)      (0.064)\ntrunk                       0.025*       0.024*       0.028*\n                          (0.014)      (0.014)      (0.014)\nrep78_num2                               0.122        0.079\n                                       (0.275)      (0.272)\nrep78_num3                               0.164        0.099\n                                       (0.255)      (0.254)\nrep78_num4                               0.287        0.152\n                                       (0.257)      (0.264)\nrep78_num5                               0.437        0.256\n                                       (0.269)      (0.283)\nforeign                                               0.207*\n                                                    (0.117)\n_cons          9.573***     9.278***     9.172***     9.148***\n             (0.271)      (0.314)      (0.355)      (0.349)\n--------------------------------------------------------------\nR-sq           0.252        0.284        0.349        0.381\nN                 74           74           69           69\n--------------------------------------------------------------\nStandard errors in parentheses\n* p<0.1, ** p<0.05, *** p<0.01\nwmtreg m?, ind(""rep78=rep78_num*"" ""foreign=foreign"")\nRegression result\n--------------------------------------------------------------\n                 (1)          (2)          (3)          (4)\n              lprice       lprice       lprice       lprice\n--------------------------------------------------------------\nmpg           -0.036***    -0.030***    -0.036***    -0.038***\n             (0.008)      (0.008)      (0.009)      (0.009)\nheadroom      -0.052       -0.116*      -0.105       -0.096\n             (0.052)      (0.063)      (0.064)      (0.064)\ntrunk                       0.025*       0.024*       0.028*\n                          (0.014)      (0.014)      (0.014)\n_cons          9.573***     9.278***     9.172***     9.148***\n             (0.271)      (0.314)      (0.355)      (0.349)\nrep78             No           No          Yes          Yes\nforeign           No           No           No          Yes\n--------------------------------------------------------------\nR-sq           0.252        0.284        0.349        0.381\nN                 74           74           69           69\n--------------------------------------------------------------\nStandard errors in parentheses\n* p<0.1, ** p<0.05, *** p<0.01\n\nWord\n\nwmtreg m? using Myfile.rtf, replace\n\n\nLaTeX\n\nwmtreg m? using Myfile.tex, replace\n\nwmtreg m? using Myfile.tex, replace mg(Group1 Group2 2 2)\n\nwmtreg m? using Myfile.tex, replace a(dot)\n\n\n在将结果输出至 Word 或 LaTeX 时，Stata 界面上也会呈现对应的结果，以方便查看。\n\n'], 'url_profile': 'https://github.com/Meiting-Wang', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YigitDeniz', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['House-_Price_prediction\nTo apply advance regression techniques to find out the house price\n'], 'url_profile': 'https://github.com/mallik005', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MeetBonny', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['IBM_ML_COURSE_Final_Project\nLoan Payoff prediction and Finding Best Classifiers among KNN , SVM, Decision Tree, Logistic Regression.\n'], 'url_profile': 'https://github.com/abdulrehman03365', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'São Paulo - SP - Brazil', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nelsonPires5', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': [""Diamond-Price-Predictor\nA Linear Regression model that predicts the price of the diamond.\nThe model is trained on a dataset containing the attributes of almost 54,000 diamonds.\nMatrix\n\nloss: 364475\nmae: 328\nmse: 364475\n\nList of features\n\nprice: price in US dollars ($326--$18,823)\ncarat: weight of the diamond (0.2--5.01)\\n\ncut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\ncolor: diamond colour, from J (worst) to D (best)\nclarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\nx: length in mm (0--10.74)\ny: width in mm (0--58.9)\nz: depth in mm (0--31.8)\ndepth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\ntable: width of top of diamond relative to widest point (43--95)\n\nThe Diamonds.ipynb has a detailed explaination and code for exploratory data analysis and model creation\nusing Tensorflow.\nGetting Started\nPrerequisites\nYou should first install all the dependency libraries by running the following command\npip install -r requirements.txt\n\nHow to Predict\nfrom predict import predictPrice\n\n# Example\npredictPrice(carat=2.29, cut='Premium', color='I', clarity='VS2', x=8.5, y=8.47, z=5.16)\n\n# returns float value of price\n\n""], 'url_profile': 'https://github.com/chetansb77', 'info_list': ['Python', 'Updated May 3, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', '1', 'Stata', 'MIT license', 'Updated Jul 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['heroku-deployment-demo\nPredict insurance amount based on human features using regression and deploy model to Heroku.\n'], 'url_profile': 'https://github.com/samip-thakkar', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Adelaide, Australia', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Rainfall prediction using Logistic Regression model.\n'], 'url_profile': 'https://github.com/toukirnaim08', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['\nPredicting real estate prices in France using XGBoost\nThis project is a Machine Learning model to predict real estate prices of properties across France. It is a supervised regression task. The model used for this project is XGBoost.\nDataset\nThe dataset contains the data of 350+ real-estate properties across France. The features are:\n\n\nthe city where the property is located (Aix-En-Provence, Vincennes, Toulouse, Paris, Marseille, Manosque, Lyon, Issy-Les-Moulineaux, Gif Sur Yvette, Enghien Les Bains or Bourg La Reine)\n\n\nthe region (Paca, Ile de France or Sud Ouest)\n\n\nthe type (F1, F2, F3, F4, F5, F6, F7)\n\n\nthe area in square metres\n\n\nthe rent per year in €\n\n\nthe price of the property in €\n\n\nFeatures importance\n\nFor this dataset, the two most dominant features are clearly the annual rent and the area of the property, which is expected for a regression model on real estate prices. The other features help to fine-tune the accuracy of the model.\nHow to download a copy of the project\nTo download a copy of the project, just go on the main page of the project on GitHub, click on ""Clone or download"" and then ""Download ZIP"".\nAfter this, you should be able to run the project using Jupyter Notebook.\nLibraries to install\n\nJupyter Notebook\npandas\nnumPy\nSeaborn\nMatplotlib\nscikit-learn\nXGBoost\ngraphviz\n\nAuthor\n\nThomas Le Menestrel\n\nLicense\nThis project is licensed under the MIT License - see the LICENSE file for details\n'], 'url_profile': 'https://github.com/tlemenestrel', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kristybell', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jigsaw17', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '204 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/asbaravkar', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'San Jose', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['All models are wrong, but some are useful\nContains projects which work on different data analysis tasks including Classification, Regression and Clustering etc.\n'], 'url_profile': 'https://github.com/jimrong79', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-using-R\n'], 'url_profile': 'https://github.com/jainil24', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sidupadhyayula', 'info_list': ['CSS', 'Updated May 3, 2020', 'R', 'Updated Jun 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Updated May 3, 2020', 'Python', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 15, 2020', 'R', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}"
"{'location': 'Pune , INDIA', 'stats_list': [], 'contributions': '850 contributions\n        in the last year', 'description': [""HackerEarth-STD-Challenge\nHackerEarth Machine Learning challenge: How effective is the STD drug?\nA new pharmaceutical startup is recently acquired by one of the world's largest MNCs. For the acquisition process, the startup is required to tabulate all drugs that they have sold and account for each drug's effectiveness. A dedicated team has been assigned the task to analyze all the data. This data has been collected over the years and it contains data points such as the drug's name, reviews by customers, popularity and use cases of the drug, and so on. Members of this team are by the noise present in the data. Your task is to make a sophisticated NLP-based Machine Learning model that has the mentioned features as the input. Also, use the input to predict the base score of a certain drug in a provided case.\nI used XGBOOST in the end and fineTuned it using HyperOpt, did NLP , made new features, but few worked few didn't, the notebook is self-explaining, so let's directly jump into it.\n\n""], 'url_profile': 'https://github.com/bhrt-sharma', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Linear-Regression-Return-and-Speed\nThis repo studies the relationship between stocks return and their ascent and descent rate, and the numbers of up and downs during a bull market.\nThe universe of stocks are S&P500, from 2010-2019.\nThere are 3 notebooks in this repo:\nMain.ipynb                 - The regression analysis\nhttps://github.com/lauwo711/Linear-Regression-Return-and-Speed/blob/master/Main.ipynb\nScrap Data.ipynb           - Crawl S&P500 data\nhttps://github.com/lauwo711/Linear-Regression-Return-and-Speed/blob/master/Scrap%20Data.ipynb\nRegression Functions.ipynb - Functions for tests and regression, included notes of OLS, WLS and Robust Regression.\nhttps://github.com/lauwo711/Linear-Regression-Return-and-Speed/blob/master/Regression%20Functions.ipynb\n'], 'url_profile': 'https://github.com/lauwo711', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'Finland', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Logistic-Regression-without-any-library\nMNIST dataset, no scikit. Regularization and Cross validation included. enjoy.\n'], 'url_profile': 'https://github.com/Erenhub', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': [""Video-Game-Sales-Regression\nFor this project I pulled a data set from Kaggle, https://www.kaggle.com/gregorut/videogamesales. This data set contains 16,599 video game titles sold from 1980 to 2017 and contains the following information for each title:\n\n\nRank - Ranking of overall sales\n\n\nName - The games name\n\n\nPlatform - Platform of the games release (i.e. PC,PS4, etc.)\n\n\nYear - Year of the game's release\n\n\nGenre - Genre of the game\n\n\nPublisher - Publisher of the game\n\n\nNA_Sales - Sales in North America (in millions)\n\n\nEU_Sales - Sales in Europe (in millions)\n\n\nJP_Sales - Sales in Japan (in millions)\n\n\nOther_Sales - Sales in the rest of the world (in millions)\n\n\nGlobal_Sales - Total worldwide sales.\n\n\nFor my linear regression model I used the global sales and year variables to see if there is a linear relationship between the year a video game was released and global sales. One question I want to answer with my model is whether there is a relationship between the year a video game was released and the global sales for the video game. The independent variable for my model is the year of release while the dependent variable is the global sales. There were some severe linearity, homoscedasticity, and normality violations that I attempted to correct by subsetting the data to only include titles that sold less than one million copies.\nFor my regression analysis I used an alpha of 0.05. Each increase in year corresponds to a 0.0008 average decrease in games sold globally, (p = 0.045). The R2 value is very small at 0.0005, so hardly any of the global sales can be explained by year. My findings were just barely statistically significant with a p value of 0.045, so I cannot say that there is a linear relationship between year and global sales with such a small coefficient.\nOne very big weakness for this analysis is the extreme violation of normality, most of my data was pulling off to the sides in my normal QQ plot. I think this is due to the low and high ends of the amount of game copies sold globally for each title, there are a lot of titles with under 200,000 copies sold. Subsetting the data even further might correct this issue.\n""], 'url_profile': 'https://github.com/rsparks93', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': [""-House-Prices-Advanced-Regression-Techniques\nKaggle Competition for house price prediction\nMy Score is : 0.14785\nRank :  2622\nKaggle Link : https://www.kaggle.com/khanakbar1234\nGoal\nIt is a job to predict the sales price for each house. For each Id in the test set, we are predict the value of the SalePrice variable.\nMetric\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\nCompetition Description:\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nPractice Skills\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting\n""], 'url_profile': 'https://github.com/khanakbar145', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saurav-8908128021', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'Korea', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ponshee', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sapmmadhu', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Logistic-Regression-on-Titanic-Datasets\nWe have vizulize the data sets\ncleaning it\nconvert categorical features\nand build logistic regression model\n\nHere we trying to predict the classification- survival or deceased\\\n'], 'url_profile': 'https://github.com/shubham171019', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}","{'location': 'Kolkata,West Bengal', 'stats_list': [], 'contributions': '482 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\ndraft 1\n'], 'url_profile': 'https://github.com/soumya997', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'MIT license', 'Updated Nov 30, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['REGRESSION MODEL TO PREDICT HOME SALE PRICES IN AMES, IA\nproblem statement\nIn this project, we are going to create a regression model based on the Ames Housing Dataset.This model will predict the price of a house at sale and can be used to give advice to people to increase their houses price by improving some features in their houses. like all the data science projects at first, data cleaning and exploratory analysis has been done. second, by data visuallization an insight about the data is driven. third, by feature engineering new features are produced. fourth, a linear regression model is fitted. finally, the model has been evaluated and improved to be used as a house price predictor.\n\nDataset\nThe Ames Housing Dataset is an exceptionally detailed and robust dataset with over 70 columns of different features relating to houses.\nData Dictionary\nsince the dataset is big a description of the dataset is provided here: http://jse.amstat.org/v19n3/decock/DataDocumentation.txt.\n\nData Cleaning and Exploratory Data Analysis\nat first data is cleaned. the columns with high number of missing values are droped and columns with low number of missing values are imputed.\nSome trends have been identified by data analysis and data visuallization.best features that have the most relationship to the house price are selected for modeling purpose based on the result from data visuallization.\n\nFeature Engineering\nthe sklearn feature engineering tool (polynomial features) is used to produce new features. categorical variables are dummified using one-hot-encoder. then some feature selection methods like f-test and variance threshold are used to select the best features for modeling purpose and improving the exsiting models.\n\nModeling process\nThe regression model is generated using the training data and evaluated using test data.this process is thru these steps:\n\ntrain-test split\ncross-validation / grid searching for hyperparameters\nmaking prediction for test data and plot the results\nmodel evaluation using metrics like R2 and RMSE\ngetting the model coefficients for interpretation\nthe predictions are submitted to Kaggle to see how the model deos against unknown data.\n\n\nConclusion/Recommendation\nThe model had the R2 score of %9 and RMSE of $19,000. from the model coefficients we found that the features that could add most value to a home are large living area, high overall quality and the lot areas. and the features that could hurt the value of a home the most are low overall quality, low kitchen quality and low exterior material quality. one recommendation for the homeowners to increase their house price is improving their house quality by remodeling the house. Future work could be make the model more universal by decreasing the number of features used in the model, in other word selecting the best features to predict a house price.\n'], 'url_profile': 'https://github.com/raminvafadary', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vineeth356', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NayanShelke', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Logistic-Regression-with-Scikit-Learn\nCoursera(Developing AI application in Azure)\n'], 'url_profile': 'https://github.com/surajsah2053', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Baton Rouge, LA, USA', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prasoonsamir', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['State-Data-OPTIONAL-\nMITx: 15.071x The Analytics Edge\nIMPORTANT NOTE: This problem is optional, and will not count towards your grade. We have created this problem to give you extra practice with the topics covered in this unit.\nState Data (OPTIONAL)\nWe often take data for granted. However, one of the hardest parts about analyzing a problem you\'re interested in can be to find good data to answer the questions you want to ask. As you\'re learning R, though, there are many datasets that R has built in that you can take advantage of.\nIn this problem, we will be examining the ""state"" dataset, which has data from the 1970s on all fifty US states. For each state, the dataset includes the population, per capita income, illiteracy rate, murder rate, high school graduation rate, average number of frost days, area, latitude and longitude, division the state belongs to,  region the state belongs to, and two-letter abbreviation.\nLoad the dataset and convert it to a data frame by running the following two commands in R:\ndata(state)\nstatedata = cbind(data.frame(state.x77), state.abb, state.area, state.center,  state.division, state.name, state.region)\nIf you can\'t access the state dataset in R, here is a CSV file with the same data that you can load into R using the read.csv function: statedata.csv\nAfter you have loaded the data into R, inspect the data set using the command: str(statedata)\nThis dataset has 50 observations (one for each US state) and the following 15 variables:\nPopulation - the population estimate of the state in 1975\nIncome - per capita income in 1974\nIlliteracy - illiteracy rates in 1970, as a percent of the population\nLife.Exp - the life expectancy in years of residents of the state in 1970\nMurder - the murder and non-negligent manslaughter rate per 100,000 population in 1976\nHS.Grad - percent of high-school graduates in 1970\nFrost - the mean number of days with minimum temperature below freezing from 1931–1960 in the capital or a large city of the state\nArea - the land area (in square miles) of the state\nstate.abb - a 2-letter abreviation for each state\nstate.area - the area of each state, in square miles\nx - the longitude of the center of the state\ny - the latitude of the center of the state\nstate.division - the division each state belongs to (New England, Middle Atlantic, South Atlantic, East South Central, West South Central, East North Central, West North Central, Mountain, or Pacific)\nstate.name - the full names of each state\nstate.region - the region each state belong to (Northeast, South, North Central, or West)\n'], 'url_profile': 'https://github.com/martyraturi', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Predicting-House-Prices-with-Regression\n'], 'url_profile': 'https://github.com/Nayan7', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Logistic-regression-w-o-sklearn\n'], 'url_profile': 'https://github.com/Shreyas670', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Mendoza, Argentina', 'stats_list': [], 'contributions': '327 contributions\n        in the last year', 'description': ['Walmart Sales - EDA and regression\nThis is my one of my first data science projects and it was made with python. The dataset came from a Kaggle competition and contains 3 files, sales by Store and Dept, some characteristics of stores and features for all the dates. Task--> create a model to make future sales predictions for each dept on each store.\nINDEX\n\nEDA and preprocessing\n\nJoining tables\nTypes\nCategorical features\n\nStore, type and size\nDept\nDate and IsHoliday\n\n\nNumeric features\n\nTemperature\nFuel price\nCPI and Unemployment\nMarkDowns\nCorrelations and conclusion\n\n\nPreprocessing\n\nMissing values\nCategorical features\n\n\n\n\nBaseline model\nFeature creation\nModel with new features\nHyper-parameter tunning\nSubmission\n\n'], 'url_profile': 'https://github.com/leogcalderon', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['In this project, we will implement a Logistic Regression model to classify MNIST digits.\nThe dataset is imported from the TensorFlow Module.\n'], 'url_profile': 'https://github.com/gowtham-krishnaswamy', 'info_list': ['Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}"
"{'location': 'Sao Paulo, Brazil', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Project title\nLinear regression and model fitting\nOverview\nOn this project 2 experiments are used:\n\n\nBattery characteristic equation and power curve\n\n\nSolar cell characteristic equation and power curve\n\n\nMeasurements are performed on the lab. Then, the constants of theoretical equations are estimated.\nCheck the figures to see the graphs and estimated values.\nP.S.: The contents in the figures are written in portuguese\nMore details\nThis is originally an experiment for Experimental Physics distipline at University of Sao Paulo\nFor the battery experiment:\nWe measure the tension and current across the battery. We are able to estimate the the characteristic constants related to the battery such as the e.m.f and the internal resistance. With the same measurements, we can plot the power characteristic curve\nFor the solar cell experiment:\nWe measure the tension and current across the solar cell. Given the theoretical model, we can estimate the constants related to that specific solar cell.\nAll information needed to understand the experiment are contained on the figures, including the equation for the models as well as the estimated constants and their uncertainty.\nHow to see the output from the code\nTwo figures are created, one for each experiment. Glance at the figures to see the measurements, how the model fits the measurements. Below the figure there are absolute residual graphs to have a better understanding about the measurements and fitted model deviations. Also on the figures, you can see the model equations for the experiment, the estimated constants along with their respective uncertainties, and the chi squared and number of degrees of freedom (GL).\nLicense\nYou may use this code for your projects. Just be a good guy Greg and remember to acknowledge me whenever you use it!\nAuthors\n\nHelano Oliveira\n\n'], 'url_profile': 'https://github.com/helano263', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kavyanshpandey', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dhruval-p', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'Ithaca, NY', 'stats_list': [], 'contributions': '563 contributions\n        in the last year', 'description': [""The Keys to Your New House\nA multivariate linear regression analysis of the King County Housing Dataset\nAuthor: Michael Erb\nThe contents of this repository detail an analysis of the King County Housing Dataset and a Multivariate Linear Regression Analysis of that dataset to predict the sale price.\nBusiness problem:\nTheKeys will be a web based service that helps individuals or families moving to a new area under the premise of being recruited to work for a mid to large size corporation, as well as help those same corporations stay well informed of the housing market of their perspective and current employees to help them with offering adequate compensation to those employees given the housing options available.\nThe first beta for this service will be for King County, WA, which is home to many large corporations including Amazon, Starbucks, and Microsoft, plus many mid sized corporations that can be reviewed here..\nData\nThe King County Housing Dataset, available here.\nIt includes details about each house including sqft, bedrooms and bathrooms, age of the house, location of the house by zip code and latitude/longitude and the sale price and date.  The dataset has a date range of May 2nd, 2014 through May 27th, 2015.\nMethods\n\nExploratory Data Analysis of the dataset to understand each data feature and their relationship among each other and to the target, the housing sale price.\nIterative improvements to linear regression models, by adding additional features, interactions, polynomials and choosing which features to keep in the models using stepwise selection and checking for multicollinearity.\n\nResults\nHeat Map of Housing Activity by Gross Sales\n\nThe housing sales activity is concentrated in the western side of the county, and the greatest concentration of high value housing is in Seattle.  The first and third most active zip codes in the county are parts of Seattle and are indicated by the red markers in the upper left hand corner. The most active zip code outside of Seattle is Maple Valley which generally has lower priced housing than Seattle.\n\nThe positive correlation  between the sqft of living space above the basement of a house and the house's selling price.  The two most active zip codes in terms of house sales are indicated in orange and green, with orange being part of Seattle and green being Maple Valley, a less expensive area of the county.\nIn certain situations the addition of a bedroom actually lowers the price of a house, while, for realistic houses, adding a bathroom always increases the price.\nA multivariate regression model for predicting housing prices was created with an R-Squared value of 0.83.\nRecommendations\nUse the current model to predict price ranges by zip code so that home buyers can see what their macro options are for housing location and price.  They could then combine this knowledge with commute time when looking for a housing location.  The model can also provide general price modifications to a house such as knowing how much an additional bedroom or bathroom would cost, and how the price changes depending on the addition or subtraction of square footage.\nLimitations & Next Steps\nThe accuracy of the linear regression models is not sufficient for launching the business, but they are a step in the right direction.  An expert could do a better job making predictions, but of course not at the scale and speed of our models.\nTo make improvements to our model, we need additional data.  To start, better location information, specifically neighborhood level data, historical price information for each house and real time localized information about the housing market.\nFor Further Information\nPlease review the narrative of the analysis in the Jupyter notebooks, review the presentation, or read the related blog article.\n""], 'url_profile': 'https://github.com/merb92', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Load-Forecasting-Using-Regression-Models\n'], 'url_profile': 'https://github.com/vvrgit', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Linear-Regression-with-Scikit-Learn\ncoursera(Developing AI application in Azure)\n'], 'url_profile': 'https://github.com/surajsah2053', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nikhileswar-Reddy', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '289 contributions\n        in the last year', 'description': ['HoneyBeeProduction-LinearRegression\nIn this project, I implememted a linear regression algorithm to analyze the decline of the honeybee population and predict its future in the coming years.\n'], 'url_profile': 'https://github.com/cierrajohnsoncarter', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '252 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SohelRaja', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': [""Climate-Change-Using-Linear-Regression\nMITx: 15.071x The Analytics Edge - Assignment Two\nClimate Change\nThere have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.\nIn this problem, we will attempt to study the relationship between average global temperature and several other factors.\nThe file climate_change.csv contains climate data from May 1983 to December 2008. The available variables include:\nYear: the observation year.\nMonth: the observation month.\nTemp: the difference in degrees Celsius between the average global temperature in that period and a reference value. This data comes from the Climatic Research Unit at the University of East Anglia.\nCO2, N2O, CH4, CFC.11, CFC.12: atmospheric concentrations of carbon dioxide (CO2), nitrous oxide (N2O), methane  (CH4), trichlorofluoromethane (CCl3F; commonly referred to as CFC-11) and dichlorodifluoromethane (CCl2F2; commonly referred to as CFC-12), respectively. This data comes from the ESRL/NOAA Global Monitoring Division.\nCO2, N2O and CH4 are expressed in ppmv (parts per million by volume  -- i.e., 397 ppmv of CO2 means that CO2 constitutes 397 millionths of the total volume of the atmosphere)\nCFC.11 and CFC.12 are expressed in ppbv (parts per billion by volume).\nAerosols: the mean stratospheric aerosol optical depth at 550 nm. This variable is linked to volcanoes, as volcanic eruptions result in new particles being added to the atmosphere, which affect how much of the sun's energy is reflected back into space. This data is from the Godard Institute for Space Studies at NASA.\nTSI: the total solar irradiance (TSI) in W/m2 (the rate at which the sun's energy is deposited per unit area). Due to sunspots and other solar phenomena, the amount of energy that is given off by the sun varies substantially with time. This data is from the SOLARIS-HEPPA project website.\nMEI: multivariate El Nino Southern Oscillation index (MEI), a measure of the strength of the El Nino/La Nina-Southern Oscillation (a weather effect in the Pacific Ocean that affects global temperatures). This data comes from the ESRL/NOAA Physical Sciences Division.\n""], 'url_profile': 'https://github.com/martyraturi', 'info_list': ['Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Apr 27, 2020', 'Updated May 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Linear-Regression-using-gradient-descent\n'], 'url_profile': 'https://github.com/Shreyas670', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Linear_Regression-Predicting_Car_Prices\nThe objective is to predict the car prices using number of features available. We will be using sklearn OLS model to predict the prices of Cars.\nData is processed and saved as CarsProcessedData.csv\nOLS model is built by manually selecting required features by checking their p-value\n'], 'url_profile': 'https://github.com/anmolarora01', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['In this project, we will implement a Logistic Regression model to classify MNIST digits.\nThe dataset is imported from the TensorFlow Module.\n'], 'url_profile': 'https://github.com/GokulKrishnaswamy', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Regression_Model_Chance_of_Admission_Prediction\nRegression_Model_Chance_of_Admission_Prediction\n'], 'url_profile': 'https://github.com/EPRADDH', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'London ON Canada', 'stats_list': [], 'contributions': '333 contributions\n        in the last year', 'description': ['Docker_flask_linear_regression_example_on_VSC\nDocker_flask_linear_regression_example_on_VSC\nReference\n\n\nLinuxTips video by Natalia Raythz\n\nabout Natalia\n\n\n\nAnother Linear Regression example\n\n\nVSC - Developing inside a Container\n\n\nPython flask.request.args() Examples\n\n\npickle - Python object serialization\n\n\nsklearn.datasets.load_diabetes\n\n\nDocker commands after coding (assuming docker desktop is running)\ndocker build -t  stayhome .\ndocker images\ndocker run -p 1000:80 stayhome\nLocal link to access\n\n\nhttp://localhost:1000/isAlive\n\n\nAnswer must be:\n\nTRUE\n\n\n\n\n\nhttp://localhost:1000/prediction/?f=0.04\n\n\nAnswer must be:\n\n[ 190.11089458]\n\n\n\n\n\n'], 'url_profile': 'https://github.com/ademarrohregger', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'São Paulo, Brazil', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['LinearRegressor\nImplementation of a linear regressor with PyTorch\n\n'], 'url_profile': 'https://github.com/paulosestini', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""Data-Analysis-in-Health-Sciences\nThese are problems involving a master's level course I took involving data analysis in health sciences. Univariate and multivariate statistical techniques for continuous and dichotomous outcomes were studied. Focus on multiple linear and logistic regression models and additional topics such as Poisson and polytomous regression, conditional logistic regression, introduction to survival analysis models and introduction to correlated data analysis. R software was used.\n""], 'url_profile': 'https://github.com/AnirudhBakshi', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/siddharthswaroop13', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Tensorflow2-Linear-Regression-From-Scratch\n'], 'url_profile': 'https://github.com/Nuhru1', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['Linear-regression-on-wine-quality-dataset\nI had taken this project from https://machinelearningmastery.com/implement-linear-regression-stochastic-gradient-descent-scratch-python/\nIn this we will train a linear regression model using stochastic gradient descent on the wine quality dataset.\nA k value of 5 was used for cross-validation, giving each fold 4,898/5 = 979.6 or just under 1000 records to be evaluated upon each iteration. A learning rate of 0.01 and 50 training epochs were chosen with a little experimentation.\nyou can try your own configuration and see the score.\nWe can see that the RMSE (on the normalized dataset) is 0.126, lower than the baseline value of 0.148 if we just predicted the mean (using the Zero Rule Algorithm).\n'], 'url_profile': 'https://github.com/vivekgohel56', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 1, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 28, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['SoftwareTestingProject\nNavigate into /data/mahout/ and use java -jar javacg-0.1-SNAPSHOT-static.jar filename.jar to populate mahout-coverage.txt file with Method-level coverage data.\nRun tcp_total.py -path mahout-coverage.txt to populate mahout-total-result.txt with static code coverage based TCP algorithm with total strategy.\n'], 'url_profile': 'https://github.com/kishanpatel98', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Vitznau, Switzerland', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['Project-WU-Linear_Regression_Houses_Detailed\nThis repo contains several mini projects to study a factors which influence houses price\n'], 'url_profile': 'https://github.com/PankoAliaksandr', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, we have looked mainly at R-squared values along with some visualization techniques to confirm if the data and residuals fit the the given set of assumptions. In this lesson, we shall look at some statistical procedures to further understand our model and results it produces.We shall be looking at the results we obtained in the regression analysis outcomes for Advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts may be new to some of us. We shall cover some of these in detail in later sections. Here, we shall focus more on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nUse Q-Q plots for check for the normality in residual errors\nUse the Jarque-Bera test for normal distribution of residuals\nCheck for heteroscedasticity using Goldfeld-Quandt test to check whether variance is the same in 2 subsamples\n\nLet\'s get started\n\nRegression diagnostic is a set of procedures available for regression analysis that seek to assess the validity of a model in any of a number of different ways. This assessment may be an exploration of the model\'s underlying statistical assumptions, an examination of the structure of the model by considering formulations that have fewer, more or different explanatory variables, or a study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions.Wiki\n\nHere we shall revisit some of the methods we have already seen, along with some new tests and how to interpret them .\nNormality Check (Q-Q plots)\nWe have already seen Q-Q plots as a measure to check for the normality (or any other distribution). These are also referred to as normal density plots when used with a standard normal quantiles. These plots are a good way to inspect the distribution of model errors. We saw this earlier with a toy dataset, let\'s plot a Q-Q for the residuals in sales~TV model.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'fivethirtyeight\')\n\ndata = pd.read_csv(\'Advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True,   )\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\nfig.show()\n/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n  ""matplotlib is currently using a non-GUI backend, ""\n\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn terms of Q-Q plots above, we can see that residuals are better normally distributed in the case of TV than that of radio. We can also spot an outlier in the left tail of radio residuals, dealing with this might help inprove the fitness of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. This migt need a bit of practice before you can truly start to intertrep them. Following image shows you how to relate a histogram to Q-Q plots.\n\nTo make it easier to read QQ-plots, it is nice to start with just considering histograms and/or scatter plots of the residuals as given by statsmodels.\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q plots are not reliable when n is large.\n\nJarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n\nJB = n [(√b1)2 / 6 + (b2 – 3)2 / 24]\n\nHere n is the sample size, √b1 is the sample skewness coefficient and b2 is the kurtosis coefficient.\nFollowing explains how to run this test in statsmodels. A J-B value indicates that errors are not normally distributed. A result of 1 and above would means that the normality null hypothesis has been rejected at the 5% significance level. In other words, the data does not come from a normal distribution. A value of 0 indicates the data is normally distributed. We have already seen JB test using model.summary(). Following code shows how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615612),\n (\'Prob\', 0.7157646605518617),\n (\'Skew\', -0.08863202396577192),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have the JB value = 0.66, which is obviously not ideal. The kurtosis is high as well as the Skew values show that underlying data is moderately skewed. Let\'s see what it gives us for the radio residuals. The p-value is also quite high to reject the null hypothesis for normality\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.909695462802713),\n (\'Prob\', 1.7473104737075628e-05),\n (\'Skew\', -0.763695254048004),\n (\'Kurtosis\', 3.5442808937621675)]\n\nSo this looks quite even worse in terms of JB results and skew, kurtosis in the data.\nThis shows us that JB test could be a much better option for testing normality than Q-Q plots we saw earlier.\nChecking Hetereoscadasticity (GOLDFELD-QUANDT test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. GQ test is performed by checking if we can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So its general practice to perform normality tests before GQ test.\nFor keen students, here is an in-depth visual explanation on how this test is performed.\nThe test statistic for this test is the ratio of mean square residual errors for the regressions on the two subsets of data. Sample observations are divided into two groups, and evidence of heteroskedasticity is based on hypothesis testing on residual terms as shown below\n.\n\nHere is a brief description of involved steps:\n\nOrder the data in ascending order.\nDivide your data into three parts and drop values in the middle part.\nRun separate regression analysis on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nLarge F values typically indicate that the variances are different.If the error term is homoscedastic, there should be no systematic difference between residuals. However, if the standard deviation of the distribution of the error term is proportional to the X variable, one part will generate a higher sum of square values than the other .\nHere is how you would run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid, model.model.exog)\nlist(zip(name, test))\n[(\'F statistic\', 1.207121297471317), (\'p-value\', 0.17652851936962863)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid, model2.model.exog)\nlist(zip(name, test))\n[(\'F statistic\', 1.1409213847001904), (\'p-value\', 0.2576335266276604)]\n\nThe null hypothesis for the GQ test is homoskedasticity. The larger the F-statistic, the more evidence you’ll have against the homoskedasticity assumption and the more likely you have heteroskedasticity (different variance for the two groups). The given P-value above tells us how far we are from the alpha level of 0.05. So the heteroscedasticity which we observed visually can also be confirmed using this test. In such cases, it is also possible to use a different alpha values to suit the needs of experimenter.\nSummary\nIn this lesson we looked at a few methods o check for regression assumptions in addition to the visual methods we saw earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last section, you saw how you can account for interactions between two variables by including interaction effects in your model. In this section you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nUnderstand how to account for non-linear relationships between predictors and target variable using polynomial terms\n\nAn example with one predictor\nThe data sety ""yields.csv"", with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport pandas as pd\nyld = pd.read_csv(""yield.csv"", sep=\'\\s+\', index_col = 0)\nimport matplotlib.pyplot as plt\nyld.head()\ny = yld[""Yield""]\nplt.scatter(yld[""Temp""],y, color = ""green"")\nplt.xlabel(""Temperature"")\nplt.ylabel(""Yield"");\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how our model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(yld[[""Temp""]], y)\nplt.scatter(yld[""Temp""], y, color = ""green"")\nplt.plot(yld[""Temp""], reg.predict(yld[[""Temp""]]))\nplt.xlabel(""Temperature"")\nplt.ylabel(""Yield"");\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(yld[[""Temp""]]))\n\nr2_score(y, reg.predict(yld[[""Temp""]]))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of a curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nfrom sklearn.linear_model import LinearRegression\nX = yld[[""Temp""]]\nX[""Temp_sq""] = yld[""Temp""]**2\n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[""Temp""],y, color = ""green"")\nplt.plot(X[""Temp""],reg_q.predict(X))\nplt.xlabel(""Temperature"")\nplt.ylabel(""Yield"");\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nfrom sklearn.metrics import mean_squared_error, r2_score\nmean_squared_error(y, reg_q.predict(X))\nr2_score(y, reg_q.predict(X))\n0.6948165884110552\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[""Temp""],y, color = ""green"")\n\nX_pred = pd.DataFrame(np.linspace(50,100,50), columns = [""temp""])\nX_pred[""sq""] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[""temp""],y_pred)\nplt.xlabel(""Temperature"")\nplt.ylabel(""Yield"");\n\nHigher order relationships\nThe use of polynomials is not restricted to quadratic relationships, you can explore cubic relationships,... as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in Polynomial option in the preprocessing library! Let\'s call it with a polynomial of 5!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[""Yield""]\nX = yld[[""Temp""]]\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\n\nreg_poly = LinearRegression().fit(X_fin, y)\npredict_X = reg_poly.predict(X_fin)\nX_linspace = pd.DataFrame(np.linspace(50,100,50), columns = [""Temp""])\nX_linspace_fin = poly.fit_transform(X_linspace)\n\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[""Temp""],y, color = ""green"")\nplt.plot(X_linspace,y_poly_pred)\nplt.xlabel(""Temperature"")\nplt.ylabel(""Yield"");\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nmean_squared_error(y, reg_poly.predict(X_fin))\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$). Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear model. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Boston, Massachusetts', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Linear-Regression-and-K-Nearest-Neighbor\nLinear Regression on sample Insurance data/\nK-Nearest Neighbor on sample Fishmarket data\nThe following respository is a sample coding analysis on Insurance and Fishmarket data. A simple linear regression was used to the test the insurance policy data and the K-Nearest Neighbor analysis was used on the Fishmarket data.\nA copy of the R code, RMD file with questions, and a finalized pdf print file are provided for your convenience. Also provided were the two csv files which can be used for the R and RMD file. Please ensure you save your all files (R, RMD, and CSV) in your working directory before running the files.\n'], 'url_profile': 'https://github.com/jmart368', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SAURAVBORAH22', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Giza - Egypt', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Skishta', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Jogeshwari, Mumbai, Maharashtra 400102', 'stats_list': [], 'contributions': '258 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-50-Startups-Dataset-\nIn this Dataset we are going to analysis the Profit on the 50 Startups with there Independent variables !\n'], 'url_profile': 'https://github.com/ganesh10-india', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NanJing, China', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YanLiu96', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['This is a linear regression model to predict insurance charges. The dataset is attached in this repository.\nLinear Regression is implemented to predict the insurance charges based on various factors and the accuracy of model is tested.\n'], 'url_profile': 'https://github.com/GokulKrishnaswamy', 'info_list': ['Python', 'Updated May 4, 2020', 'R', 'Updated Apr 29, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'R', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}"
"{'location': 'NanJing, China', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YanLiu96', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'Greater Philadelphia Area, USA', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Predicting-House-Price-with-Linear-Regression\n'], 'url_profile': 'https://github.com/jessiehangle', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sidupadhyayula', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Simple Linear Regression\nIntroduction\nFor aspiring data scientists, like yourselves, regression analysis the first real ""learning"" algorithm that they come across with. It is one of the simplest algorithms to master but still requires some statistical understanding of the underlying. This lesson will introduce you to regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nDescribe statistical modeling with simple regression\nExplain simple linear regression as solving for equation y=mx+c\nCalculate the slope and y-intercept using the slope value\nDraw a regression line based on calculated slope and intercept\nPredict the target of a previously unseen input feature\n\nLinear regression\nSo far, we have covered topics like basic hypothesis testing, variable relationships, statistical learning. We shall now build on these ideas to explain the regression process. Regression is a parametric technique used to predict the value of a target variable Y based on one or more input feature variables X. Regression is proven to give credible results if the data follows parametric assumptions which will be covered in upcoming lessons. Regression Analysis helps us with analytics in following ways:\n\nFinding an association, relationship between variables.\nIdentifying which variables contribute more towards the outcomes.\n\nand most importantly ..\n\nPrediction of future observations.\n\nWhy is it called ""linear"" regression?\nAs we saw in pevious lesson, linear implies that the model functions along a straight or nearly straight line. Linear suggests that the relationship between dependent and independent variable can be expressed in a straight line. A Simple Linear Regression  uses a single feature (independent variable) to predict a target (dependent variable) by fitting a best linear relationship, whereas Multiple Linear Regression uses more than one features to predict a target variable by fitting a best linear relationship. In this section, we shall mainly focus on simple regression to build a sound understanding.\n\nSo let\'s move on and see how to calculate such a line.\nCalculating Slope and Intercepts\nWe all know from elementary geometry that equation of a stright line can be written as:\ny = mx + c\nFollowing what we have covered so far, we can say from the equation that:\n\ny is the dependent variable i.e. the variable that needs to be estimated and predicted.\nx is the independent variable i.e. the variable that is controllable. It is the input.\nm is the slope. It determines what will be the angle of the line. It is the parameter denoted as β.\nc is the intercept. A constant that determines the value of y when x is 0.\n\n\nLinear regression is nothing but a manifestation of this simple equation. The formula for the best-fit line (or regression line) is still ""a line"".\n\nA model really cant get any simpler than this. This equation here is the same one used to find a line in algebra, but in statistics the points don’t lie perfectly on a line as shown above\n\nThe line is a model around which the data lie if a strong linear pattern exists.\n\nFollowing image that we saw in previous lesson explains this further.\n\nThe slope (m) of a line is the change in Y over the change in X (Δy/Δx shown above). For example, a slope of 4/3 means as the x-value increases by 4 units, the y-value moves up by 3 units on average.\nThe y-intercept (b) is the value on the y-axis where the line crosses the axis. For example, in the equation y= 2x +2, the line crosses the y-axis at the value b = 2. The coordinates of this point would be (0, 2).\n\nWhen a line crosses the y-axis, the x-value is always 0.\n\nYou may be thinking that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem . Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated following measures for variables x and y:\n\n\nThe mean of the x (x_bar)\n\n\nThe mean of the y (y_bar)\n\n\nThe standard deviation of the x values (denoted Sx)\n\n\nThe standard deviation of the y values (denoted Sy)\n\n\nThe correlation between X and Y (denoted r - following Pearson Correlation)\n\n\nCalculating Slope\nThe formula for the slope (shown as b below), of the best-fit line is\n\nSo You simply divide sy by sx and multiply the result by r.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line is negative in this case.\n\nSlope is not same as correlation.Slope takes the untiless correlation and attaches units to it. Think of Sy divided by Sx as the variation (resembling change) in Y over the variation in X, in units of X and Y.\n\nCalculating Intercept\nSo now that we have slope value (b), we can put it back into our formula to calculate intercept (shown as a below).\n\nx_bar and y_bar are the mean values for variables x and y. So to calculate the y-intercept of the best-fit line, you start by finding the slope of the best-fit line using the above steps. Then to find the y-intercept, you multiply slope value mean of x and subtract the result from mean of y.\nPredict from the model\nOnce we have a regression line with defined parameters - slope and intercept as shown above, We can easily predict the y_hat (target) value for a new x (feature) value using the parameter values:\n\nRemember that the different between y and y_hat is that y_hat is the y value predicted by the fitted model, whereas y carries actual values of variable (called the truth values) that were used to calculated the best fit.\nNext we shall move on and try to code these equations in to draw regression line to a simple dataset to see all of this in action.\nSummary\nIn this lesson, we learnt the basics of simple linear regression between two variables as a problem of fitting a straight line to best describe the data associations on a 2-dimensional plane. Remember this is only half the process. Once we have coded these equations as functions, we shall move on to calculating the loss in our model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is the most common regression technique for linear models. As long as our model satisfies the least squares regression assumptions , we can always get the best possible estimates.\nObjectives\nYou will be able to:\n\nUnderstand and discuss the assumptions that must be held for least squares regression\nUnderstand linearity, normality and heteroscedasticity assumptions\nIdentify approaches to check for regression assumptions\n\nAbout regression\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if you don’t satisfy some assumptions, you might not be able to trust the results.\nIn this lesson, we shall look at ordinary least squares regression assumptions, their importance and look at some techniques to help us determine whether our model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique i.e. it uses parameters learned from the data, so it makes certain assumptions assumptions. These assumptions define the complete scope of regression analysis and it is mandatory that underlying data fulfills these assumptions. If these assumptions are violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions which will be mentioned below.\nLinearity\n\nLinearity assumptions requires that there should be a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS would fail to capture the trend mathematically, thus resulting in an inaccurate relationship. Also, this will result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, we can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. These will be covered soon in the course.\nNote: As an extra measure, you must also check for outliers as presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibit the model to estimate the true relationship between variables by introducing bias.\nNormality\n\nSatisfying the normality assumption allows you to perform statistical hypothesis testing and generate reliable confidence intervals and prediction intervals.\n\nLinearity assumption requires variables to be normally distributed for concluding some sort of statistical significance. It also require that the calculated error values also follow a normal distribution to produce unbiased estimates with the minimum variance.\nHowever, keep in mind that this assumption is not mandatory for regression if you dont want to do any hypothesis testing. OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for this assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them for data variables while plotting the density function through parametric and non parametric means. On the same lines, the errors generated by the model can also be checked for normality. As the error term follows a normal distribution, we can develop a better confidence on the results and thus calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a dataset comes from a known distribution such as a Normal or exponential etc. For regression, when checking if the data is Normally distributed, we can use a Normal Q-Q plot to check that assumption. Remember It is just a visual check, not quantitative, so its interpretation remains subjective. However, it is a good first check to see the overall shape of data against required distribution. If you can reject normality through QQ plot, you have saved yourself from lot of statistical testing. You must, however, be very careful when deciding that data is totally normal, just by looking at a QQ plot. Here is an example comparing histograms and corresponding qqplots. We can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected in the same way.\n\nSo we see that a qq plot helps us validate the assumption of normal distribution. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of qqplots in detail.\nNormality can be checked with a goodness of fit tests, e.g., the Kolmogorov-Smirnov test.  When the data is not normally distributed a non-linear transformation (e.g., log-transformation) also fixes this issue.\nHeteroscedasticity\n\nheteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable that predicts it.\n\nA scatterplot of these variables will often create a cone-like shape, as the scatter (or variability) of the dependent variable  widens or narrows as the value of the independent variable increases. The inverse of heteroscedasticity is homoscedasticity, which indicates that a DV\'s variability is equal across values of an IV.\n\nSo a scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The following scatter plots show examples of data that are not homoscedastic (i.e., heteroscedastic). You can also use significance tests like Breusch-Pagan / Cook – Weisberg test or White general test to detect this phenomenon. If we find p < 0.05, we reject the null hypothesis and infer that heteroscedasticity is present.\nThere are other assumptions for linear regression that applies to more complicated cases and will be covered as we go through advanced techniques.\nAs a first check, always looks at plots for the residuals (we shall make a few in following lessons) and if you see anything similar to shown below, you are violating one or more assumptions and hence the results are not reliable and will lack confidence.\n\nSummary\nIn this lesson, we looked at some assumptions for simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, we can run our regression model and further our experiments based on our observations. Next we shall see all of this with examples.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project - Regression Modeling with the Ames Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Ames Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'ames.csv\' as a pandas dataframe\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\'seaborn\')\names = pd.read_csv(\'ames.csv\')\n\nsubset = [\'YrSold\', \'MoSold\', \'Fireplaces\', \'TotRmsAbvGrd\', \'GrLivArea\',\n          \'FullBath\', \'YearRemodAdd\', \'YearBuilt\', \'OverallCond\', \'OverallQual\', \'LotArea\', \'SalePrice\']\n\ndata = ames.loc[:, subset]\nThe columns in the Ames housing data represent the dependent and independent variables. We have taken a subset of all columns available to focus on feature interpretation rather than preprocessing steps. The dependent variable here is the sale price of a house SalePrice. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n# You observations here \nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nClearly, the results are not very reliable. The best R-Squared is witnessed with OverallQual, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Ames dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nUnderstand and discuss the assumptions that must be held for least squares regression\nUnderstand linearity, normality and heteroscedasticity assumptions\nIdentify approaches to check for regression assumptions\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper on the topic of ordinary least squares regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions are made. These assumptions define the complete scope of regression analysis and it is mandatory that underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions. We provide an overview in this lesson!\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, Ordinary Least Squares (OLS) will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. You will learn about p-values later, but for now, you can remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots for the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Machine-Learning-Single-Multivariate-Linear-Regression\nThis is a simple Linear Regression Implementation based on hte cost function and gradient descent Formulae\n'], 'url_profile': 'https://github.com/Harshubh-Meherishi', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '486 contributions\n        in the last year', 'description': ['Kaggle-Compedetion-House-price-Advance-Regression-Techniques\nThis includes a basic regression model of a Housing price dataset with numerous features, dealt this dataset using Basic Feature Engineering Techniques, and predicted using a Decision Tree Regressor respectively.\n'], 'url_profile': 'https://github.com/GauravSahani1417', 'info_list': ['Updated Apr 28, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Linear_Lasso_Ridge_Regression-with-CV\nLinear_Lasso_Ridge_Regression with Cross Validation in one\nThis project has a detailed EDA on famous Boston Housing dataset along with the application of Linear Regression and some regularization techniques such as Lasso(L2 Regularization) and Ridge(L1 Regularization) Regressions\n'], 'url_profile': 'https://github.com/abhiramkadali', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': [""Pisa\nMITx: 15.071x The Analytics Edge - PISA\nReading Test Scores\nThe Programme for International Student Assessment (PISA) is a test given every three years to 15-year-old students from around the world to evaluate their performance in mathematics, reading, and science. This test provides a quantitative way to compare the performance of students from different parts of the world. In this homework assignment, we will predict the reading scores of students from the United States of America on the 2009 PISA exam.\nThe datasets pisa2009train.csv and pisa2009test.csv contain information about the demographics and schools for American students taking the exam, derived from 2009 PISA Public-Use Data Files distributed by the United States National Center for Education Statistics (NCES). While the datasets are not supposed to contain identifying information about students taking the test, by using the data you are bound by the NCES data use agreement, which prohibits any attempt to determine the identity of any student in the datasets.\nEach row in the datasets pisa2009train.csv and pisa2009test.csv represents one student taking the exam. The datasets have the following variables:\ngrade: The grade in school of the student (most 15-year-olds in America are in 10th grade)\nmale: Whether the student is male (1/0)\nraceeth: The race/ethnicity composite of the student\npreschool: Whether the student attended preschool (1/0)\nexpectBachelors: Whether the student expects to obtain a bachelor's degree (1/0)\nmotherHS: Whether the student's mother completed high school (1/0)\nmotherBachelors: Whether the student's mother obtained a bachelor's degree (1/0)\nmotherWork: Whether the student's mother has part-time or full-time work (1/0)\nfatherHS: Whether the student's father completed high school (1/0)\nfatherBachelors: Whether the student's father obtained a bachelor's degree (1/0)\nfatherWork: Whether the student's father has part-time or full-time work (1/0)\nselfBornUS: Whether the student was born in the United States of America (1/0)\nmotherBornUS: Whether the student's mother was born in the United States of America (1/0)\nfatherBornUS: Whether the student's father was born in the United States of America (1/0)\nenglishAtHome: Whether the student speaks English at home (1/0)\ncomputerForSchoolwork: Whether the student has access to a computer for schoolwork (1/0)\nread30MinsADay: Whether the student reads for pleasure for 30 minutes/day (1/0)\nminutesPerWeekEnglish: The number of minutes per week the student spend in English class\nstudentsInEnglish: The number of students in this student's English class at school\nschoolHasLibrary: Whether this student's school has a library (1/0)\npublicSchool: Whether this student attends a public school (1/0)\nurban: Whether this student's school is in an urban area (1/0)\nschoolSize: The number of students in this student's school\nreadingScore: The student's reading score, on a 1000-point scale\n""], 'url_profile': 'https://github.com/martyraturi', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '193 contributions\n        in the last year', 'description': ['Logistic-Regression-with-a-Neural-Network-mindset\nneural networks and deep learning (week 2 cousera cours)\nIn this assignment you will:\n\n\nWork with logistic regression in a way that builds intuition relevant to neural networks.\n\n\nLearn how to minimize the cost function.\n\n\nUnderstand how derivatives of the cost are used to update parameters.\n\n\n'], 'url_profile': 'https://github.com/DerbelSahar', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['This is a linear regression model to predict insurance charges. The dataset is attached in this repository.\nLinear Regression is implemented to predict the insurance charges based on various factors and the accuracy of model is tested.\n'], 'url_profile': 'https://github.com/gowtham-krishnaswamy', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '289 contributions\n        in the last year', 'description': ['BostonHousePricePrediction-LinearRegression\nIn this project, I implemented a linear regression model to predict the price of homes in Boston\n'], 'url_profile': 'https://github.com/cierrajohnsoncarter', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '703 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sbekrin', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Lab\nNow have all the necessary functions to calculate the slope, intercept, best-fit line, prediction and visualizations. In this lab you will put them all together to run a regression experiment and calculate model loss.\nSWBAT\n\nRun a complete regression analysis through code only.\n\nHere are all the formulas to put everything in perspective\nSlope\n\nIntercept\n\nR-squared\n\nPrediction\n\nUse the functions created earlier to implement these formulas to run a regression analysis using X and Y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.float64)\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of x = 4.5.\n# Make prediction using given value and visualize on the scatter plot\n\n\nLevel up - Optional\nLoad the ""heightWeight.csv"" dataset. Use the height as an independant and weight as a dependant variable and draw a regression line to data using your code above. Calculate your R-square for the model and try to predict new values of Y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next We shall see how we can use python\'s built in modules to perform such analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/madhugadi', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['VectorAR, ARIMA, HoltWinters, HarmonicRegression\nThis report will explore these 4 techniques in time series modeling and forecasting: 1. Vector Autoregressive Models (Vector AR), 2. ARIMA models, 3. Holt-Winters, 4. Harmonic Regression applied on\nvarious datasets. Each data set used will be accompanied with an exploratory data analysis. Each fitted models will be accompanied with a brief theoretical formulation, and an extensive expl\noration of modeling approaches and decisions, along with forecasting performance (pseudo-out-of-sample testing) and residual analysis.\nVector AR models leverage the linear dependencies between multiple time series in order to conduct forecasts on the joint outcome of 3 related series. Harmonic regression is a technique that \ncaptures seasonality with a single Fourier series, and advantageous to seasonal dummy variables for data sets with high seasonality (such as weekly data that we will use). We will also explor\ne the relative advantages of ARIMA versus Holt-Winters, two popular methods for modeling behaviors of time series data.\n'], 'url_profile': 'https://github.com/siduojiang', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 2, 2020']}"
"{'location': 'Giza - Egypt', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Skishta', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Gurgaon', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['House-sales-in-King-Country-Regression\n'], 'url_profile': 'https://github.com/SainikhileshVuggumudi', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm if the data and residuals fit the regression assumptions. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nUse Q-Q plots to check for the normality in residual errors\nUse the Jarque-Bera test for normal distribution of residuals\nCheck for heteroscedasticity using the Goldfeld-Quandt test to check whether the variance is the same in 2 samples\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly plot a Q-Q for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'fivethirtyeight\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\nfig.show()\n/Users/matthew.mitchell/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:459: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n  ""matplotlib is currently using a non-GUI backend, ""\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroskedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoskedasticity. The larger the F-statistic, the more evidence we will have against the homoskedasticity assumption and the more likely we have heteroskedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3,192 contributions\n        in the last year', 'description': ['Quarkus 1.4.1 Maven regression\nThis repo demonstrates a regression in what I believe to be io.quarkus.bootstrap.resolver.maven.\nmaster branch: Project generated from code.quarkus.io, default everything, with the only change being to this README and the addition of setup/settings.xml\nq-1.4.1-maven-revision branch: Only change from master is replacing  with ${release} and adding a Maven property defining that version\nq-1.3.2 branch: Only change from master is reverting back to Quarkus 1.3.2.Final\nq-1.3.2-maven-revision branch: Only change from q-1.3.2 is replacing  with ${release} and adding a Maven property defining that version\nBackground\n\n\nMaven ${revision}\nSince 3.5.0-beta-1, maven has supported using a ${revision} placeholder for the version in pom files.\nhttps://maven.apache.org/maven-ci-friendly.html\nThis is often used in CI systems to be able to inject version at build time via mvn -Drevision=1.2.3 package\n\n\nCompanies frequently require all dependencies to be pulled through an internal Maven repository that also proxies upstream. One easy way to accomplish this is by providing a settings.xml file that specifies that repository. For the purpose of this demonstration, we just override the name but leave the repository as Maven Central.\n\n\nSetup\nCopy setup/settings.xml to ~/.m2/settings.xml\nThis is necessary to demonstrate that settings.xml repository configuration is ignored when Quarkus downloads test dependencies. It configures a repository named ""get-all-from-here"" which simply points at maven central. In a real scenario, this would point at a company\'s private Maven repository.\nExpectation\nAll dependencies should be retrieved from ""get-all-from-here"" repository. If at any point we see dependencies pulled from ""central"", that proves our Maven configuration is not being honored.\nTest Steps\n\nPurge local Maven repo to ensure all dependencies will be downloaded\nExecute mvn test and search for lines that contain ""central""\n\nmvn clean\nrm -Rf ~/.m2/repository\nmvn test | grep central\n\nExpectation is to have no output, meaning all dependencies were downloaded from our demonstration repository named ""get-all-from-here""\nResults\n\n\nmaster\ngit checkout master\nrm -Rf ~/.m2/repository\nmvn test | grep central\n\nOutput:\nmvn test  49.28s user 5.27s system 48% cpu 1:51.89 total\ngrep central  0.40s user 1.29s system 1% cpu 1:51.89 total\n\nDetermination:\n\nAll dependencies pulled from ""get-all-from-here"" defined repository, as expected\n\n\n\nq-1.4.1-maven-revision\ngit checkout q-1.4.1-maven-revision\nrm -Rf ~/.m2/repository\nmvn test | grep central\n\nOutput:\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy/1.4.1.Final/quarkus-resteasy-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy/1.4.1.Final/quarkus-resteasy-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-parent/1.4.1.Final/quarkus-resteasy-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-parent/1.4.1.Final/quarkus-resteasy-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-build-parent/1.4.1.Final/quarkus-build-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-build-parent/1.4.1.Final/quarkus-build-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-parent/1.4.1.Final/quarkus-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-parent/1.4.1.Final/quarkus-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/36/jboss-parent-36.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/36/jboss-parent-36.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bom-deployment/1.4.1.Final/quarkus-bom-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bom-deployment/1.4.1.Final/quarkus-bom-deployment-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bom/1.4.1.Final/quarkus-bom-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bom/1.4.1.Final/quarkus-bom-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-bom/4.1.45.Final/netty-bom-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-bom/4.1.45.Final/netty-bom-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/jackson-bom/2.10.3/jackson-bom-2.10.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/jackson-bom/2.10.3/jackson-bom-2.10.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/jackson-parent/2.10/jackson-parent-2.10.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/jackson-parent/2.10/jackson-parent-2.10.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/oss-parent/38/oss-parent-38.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/fasterxml/oss-parent/38/oss-parent-38.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-bom/4.5.3.Final/resteasy-bom-4.5.3.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-bom/4.5.3.Final/resteasy-bom-4.5.3.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/35/jboss-parent-35.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/35/jboss-parent-35.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/junit-bom/5.6.2/junit-bom-5.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/junit-bom/5.6.2/junit-bom-5.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/fabric8/kubernetes-client-bom/4.9.0/kubernetes-client-bom-4.9.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/fabric8/kubernetes-client-bom/4.9.0/kubernetes-client-bom-4.9.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-stack-depchain/3.8.5/vertx-stack-depchain-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-stack-depchain/3.8.5/vertx-stack-depchain-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-stack/3.8.5/vertx-stack-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-stack/3.8.5/vertx-stack-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-parent/14/vertx-parent-14.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-parent/14/vertx-parent-14.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-dependencies/3.8.5/vertx-dependencies-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-dependencies/3.8.5/vertx-dependencies-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/optaplanner/optaplanner-bom/7.36.1.Final/optaplanner-bom-7.36.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/optaplanner/optaplanner-bom/7.36.1.Final/optaplanner-bom-7.36.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/kie/kie-user-bom-parent/7.36.1.Final/kie-user-bom-parent-7.36.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/kie/kie-user-bom-parent/7.36.1.Final/kie-user-bom-parent-7.36.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/31/jboss-parent-31.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/31/jboss-parent-31.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http/1.4.1.Final/quarkus-vertx-http-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http/1.4.1.Final/quarkus-vertx-http-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http-parent/1.4.1.Final/quarkus-vertx-http-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http-parent/1.4.1.Final/quarkus-vertx-http-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core/1.4.1.Final/quarkus-core-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core/1.4.1.Final/quarkus-core-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core-parent/1.4.1.Final/quarkus-core-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core-parent/1.4.1.Final/quarkus-core-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/annotation/ca-parent/1.3.5/ca-parent-1.3.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/annotation/ca-parent/1.3.5/ca-parent-1.3.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/ee4j/project/1.0.5/project-1.0.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/ee4j/project/1.0.5/project-1.0.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/enterprise/jakarta.enterprise.cdi-api/2.0.2/jakarta.enterprise.cdi-api-2.0.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/enterprise/jakarta.enterprise.cdi-api/2.0.2/jakarta.enterprise.cdi-api-2.0.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/el/jakarta.el-api/3.0.3/jakarta.el-api-3.0.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/el/jakarta.el-api/3.0.3/jakarta.el-api-3.0.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/interceptor/jakarta.interceptor-api/1.2.5/jakarta.interceptor-api-1.2.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/interceptor/jakarta.interceptor-api/1.2.5/jakarta.interceptor-api-1.2.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/inject/jakarta.inject-api/1.0/jakarta.inject-api-1.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/inject/jakarta.inject-api/1.0/jakarta.inject-api-1.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-ide-launcher/1.4.1.Final/quarkus-ide-launcher-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-ide-launcher/1.4.1.Final/quarkus-ide-launcher-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-development-mode-spi/1.4.1.Final/quarkus-development-mode-spi-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-development-mode-spi/1.4.1.Final/quarkus-development-mode-spi-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config/1.7.0/smallrye-config-1.7.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config/1.7.0/smallrye-config-1.7.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config-parent/1.7.0/smallrye-config-parent-1.7.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config-parent/1.7.0/smallrye-config-parent-1.7.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/smallrye-parent/16/smallrye-parent-16.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/smallrye-parent/16/smallrye-parent-16.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/arquillian/arquillian-bom/1.6.0.Final/arquillian-bom-1.6.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/arquillian/arquillian-bom/1.6.0.Final/arquillian-bom-1.6.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/shrinkwrap-bom/1.2.6/shrinkwrap-bom-1.2.6.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/shrinkwrap-bom/1.2.6/shrinkwrap-bom-1.2.6.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/resolver/shrinkwrap-resolver-bom/3.1.4/shrinkwrap-resolver-bom-3.1.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/resolver/shrinkwrap-resolver-bom/3.1.4/shrinkwrap-resolver-bom-3.1.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven/3.6.3/maven-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven/3.6.3/maven-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-parent/33/maven-parent-33.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-parent/33/maven-parent-33.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/apache/21/apache-21.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/apache/21/apache-21.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/descriptors/shrinkwrap-descriptors-bom/2.0.0/shrinkwrap-descriptors-bom-2.0.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/descriptors/shrinkwrap-descriptors-bom/2.0.0/shrinkwrap-descriptors-bom-2.0.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config-common/1.7.0/smallrye-config-common-1.7.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config-common/1.7.0/smallrye-config-common-1.7.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/config/microprofile-config-api/1.4/microprofile-config-api-1.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/config/microprofile-config-api/1.4/microprofile-config-api-1.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/config/microprofile-config-parent/1.4/microprofile-config-parent-1.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/config/microprofile-config-parent/1.4/microprofile-config-parent-1.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/arquillian/arquillian-bom/1.1.13.Final/arquillian-bom-1.1.13.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/arquillian/arquillian-bom/1.1.13.Final/arquillian-bom-1.1.13.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/resolver/shrinkwrap-resolver-bom/2.2.4/shrinkwrap-resolver-bom-2.2.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/resolver/shrinkwrap-resolver-bom/2.2.4/shrinkwrap-resolver-bom-2.2.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/descriptors/shrinkwrap-descriptors-bom/2.0.0-alpha-10/shrinkwrap-descriptors-bom-2.0.0-alpha-10.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/descriptors/shrinkwrap-descriptors-bom/2.0.0-alpha-10/shrinkwrap-descriptors-bom-2.0.0-alpha-10.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging/3.3.2.Final/jboss-logging-3.3.2.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging/3.3.2.Final/jboss-logging-3.3.2.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/15/jboss-parent-15.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/15/jboss-parent-15.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logmanager/jboss-logmanager-embedded/1.0.4/jboss-logmanager-embedded-1.0.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logmanager/jboss-logmanager-embedded/1.0.4/jboss-logmanager-embedded-1.0.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent-mr-jar/32/jboss-parent-mr-jar-32.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent-mr-jar/32/jboss-parent-mr-jar-32.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/32/jboss-parent-32.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/32/jboss-parent-32.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/wildfly/common/wildfly-common/1.5.4.Final-format-001/wildfly-common-1.5.4.Final-format-001.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/wildfly/common/wildfly-common/1.5.4.Final-format-001/wildfly-common-1.5.4.Final-format-001.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/34/jboss-parent-34.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/34/jboss-parent-34.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging-annotations/2.1.0.Final/jboss-logging-annotations-2.1.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging-annotations/2.1.0.Final/jboss-logging-annotations-2.1.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging-tools-parent/2.1.0.Final/jboss-logging-tools-parent-2.1.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging-tools-parent/2.1.0.Final/jboss-logging-tools-parent-2.1.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/20/jboss-parent-20.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/20/jboss-parent-20.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/threads/jboss-threads/3.1.1.Final/jboss-threads-3.1.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/threads/jboss-threads/3.1.1.Final/jboss-threads-3.1.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/slf4j/slf4j-parent/1.7.30/slf4j-parent-1.7.30.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/slf4j/slf4j-parent/1.7.30/slf4j-parent-1.7.30.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/slf4j/slf4j-jboss-logging/1.2.0.Final/slf4j-jboss-logging-1.2.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/slf4j/slf4j-jboss-logging/1.2.0.Final/slf4j-jboss-logging-1.2.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/graalvm/sdk/graal-sdk/19.3.1/graal-sdk-19.3.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/graalvm/sdk/graal-sdk/19.3.1/graal-sdk-19.3.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/security/quarkus-security/1.1.0.Final/quarkus-security-1.1.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/security/quarkus-security/1.1.0.Final/quarkus-security-1.1.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/reactive/mutiny/0.4.4/mutiny-0.4.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/reactive/mutiny/0.4.4/mutiny-0.4.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/reactive/mutiny-project/0.4.4/mutiny-project-0.4.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/reactive/mutiny-project/0.4.4/mutiny-project-0.4.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/smallrye-parent/15/smallrye-parent-15.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/smallrye-parent/15/smallrye-parent-15.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/arquillian/arquillian-bom/1.5.0.Final/arquillian-bom-1.5.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/arquillian/arquillian-bom/1.5.0.Final/arquillian-bom-1.5.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/resolver/shrinkwrap-resolver-bom/2.2.6/shrinkwrap-resolver-bom-2.2.6.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/shrinkwrap/resolver/shrinkwrap-resolver-bom/2.2.6/shrinkwrap-resolver-bom-2.2.6.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core/1.4.1.Final/quarkus-vertx-core-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core/1.4.1.Final/quarkus-vertx-core-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core-parent/1.4.1.Final/quarkus-vertx-core-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core-parent/1.4.1.Final/quarkus-vertx-core-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc/1.4.1.Final/quarkus-arc-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc/1.4.1.Final/quarkus-arc-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc-parent/1.4.1.Final/quarkus-arc-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc-parent/1.4.1.Final/quarkus-arc-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc/1.4.1.Final/arc-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc/1.4.1.Final/arc-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc-parent/1.4.1.Final/arc-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc-parent/1.4.1.Final/arc-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/transaction/jakarta.transaction-api/1.3.3/jakarta.transaction-api-1.3.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/transaction/jakarta.transaction-api/1.3.3/jakarta.transaction-api-1.3.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/context-propagation/microprofile-context-propagation-api/1.0.1/microprofile-context-propagation-api-1.0.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/context-propagation/microprofile-context-propagation-api/1.0.1/microprofile-context-propagation-api-1.0.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/context-propagation/microprofile-context-propagation-parent/1.0.1/microprofile-context-propagation-parent-1.0.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/context-propagation/microprofile-context-propagation-parent/1.0.1/microprofile-context-propagation-parent-1.0.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty/1.4.1.Final/quarkus-netty-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty/1.4.1.Final/quarkus-netty-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty-parent/1.4.1.Final/quarkus-netty-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty-parent/1.4.1.Final/quarkus-netty-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec/4.1.45.Final/netty-codec-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec/4.1.45.Final/netty-codec-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-parent/4.1.45.Final/netty-parent-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-parent/4.1.45.Final/netty-parent-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/9/oss-parent-9.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/9/oss-parent-9.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-common/4.1.45.Final/netty-common-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-common/4.1.45.Final/netty-common-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-buffer/4.1.45.Final/netty-buffer-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-buffer/4.1.45.Final/netty-buffer-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-transport/4.1.45.Final/netty-transport-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-transport/4.1.45.Final/netty-transport-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-resolver/4.1.45.Final/netty-resolver-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-resolver/4.1.45.Final/netty-resolver-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-handler/4.1.45.Final/netty-handler-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-handler/4.1.45.Final/netty-handler-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-core/3.8.5/vertx-core-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-core/3.8.5/vertx-core-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-handler-proxy/4.1.45.Final/netty-handler-proxy-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-handler-proxy/4.1.45.Final/netty-handler-proxy-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-socks/4.1.45.Final/netty-codec-socks-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-socks/4.1.45.Final/netty-codec-socks-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-http/4.1.45.Final/netty-codec-http-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-http/4.1.45.Final/netty-codec-http-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-http2/4.1.45.Final/netty-codec-http2-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-http2/4.1.45.Final/netty-codec-http2-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-resolver-dns/4.1.45.Final/netty-resolver-dns-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-resolver-dns/4.1.45.Final/netty-resolver-dns-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-dns/4.1.45.Final/netty-codec-dns-4.1.45.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-dns/4.1.45.Final/netty-codec-dns-4.1.45.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.3/jackson-core-2.10.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.3/jackson-core-2.10.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/jackson-base/2.10.3/jackson-base-2.10.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/jackson-base/2.10.3/jackson-base-2.10.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web/3.8.5/vertx-web-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web/3.8.5/vertx-web-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web-parent/3.8.5/vertx-web-parent-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web-parent/3.8.5/vertx-web-parent-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-ext-parent/34/vertx-ext-parent-34.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-ext-parent/34/vertx-ext-parent-34.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-ext/34/vertx-ext-34.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-ext/34/vertx-ext-34.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web-common/3.8.5/vertx-web-common-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web-common/3.8.5/vertx-web-common-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-auth-common/3.8.5/vertx-auth-common-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-auth-common/3.8.5/vertx-auth-common-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-auth/3.8.5/vertx-auth-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-auth/3.8.5/vertx-auth-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-bridge-common/3.8.5/vertx-bridge-common-3.8.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-bridge-common/3.8.5/vertx-bridge-common-3.8.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common/1.4.1.Final/quarkus-resteasy-server-common-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common/1.4.1.Final/quarkus-resteasy-server-common-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-parent/1.4.1.Final/quarkus-resteasy-server-common-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-parent/1.4.1.Final/quarkus-resteasy-server-common-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common/1.4.1.Final/quarkus-resteasy-common-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common/1.4.1.Final/quarkus-resteasy-common-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-parent/1.4.1.Final/quarkus-resteasy-common-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-parent/1.4.1.Final/quarkus-resteasy-common-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-core/4.5.3.Final/resteasy-core-4.5.3.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-core/4.5.3.Final/resteasy-core-4.5.3.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-jaxrs-all/4.5.3.Final/resteasy-jaxrs-all-4.5.3.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-jaxrs-all/4.5.3.Final/resteasy-jaxrs-all-4.5.3.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-dependencies/4.5.3.Final/resteasy-dependencies-4.5.3.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-dependencies/4.5.3.Final/resteasy-dependencies-4.5.3.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/arquillian/arquillian-bom/1.4.1.Final/arquillian-bom-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/arquillian/arquillian-bom/1.4.1.Final/arquillian-bom-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/projectreactor/reactor-bom/Dysprosium-SR2/reactor-bom-Dysprosium-SR2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/projectreactor/reactor-bom/Dysprosium-SR2/reactor-bom-Dysprosium-SR2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/ws/rs/jboss-jaxrs-api_2.1_spec/2.0.1.Final/jboss-jaxrs-api_2.1_spec-2.0.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/ws/rs/jboss-jaxrs-api_2.1_spec/2.0.1.Final/jboss-jaxrs-api_2.1_spec-2.0.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/xml/bind/jboss-jaxb-api_2.3_spec/2.0.0.Final/jboss-jaxb-api_2.3_spec-2.0.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/xml/bind/jboss-jaxb-api_2.3_spec/2.0.0.Final/jboss-jaxb-api_2.3_spec-2.0.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/xml/bind/jboss-jaxb-api_2.3_spec-parent/2.0.0.Final/jboss-jaxb-api_2.3_spec-parent-2.0.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/xml/bind/jboss-jaxb-api_2.3_spec-parent/2.0.0.Final/jboss-jaxb-api_2.3_spec-parent-2.0.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-core-spi/4.5.3.Final/resteasy-core-spi-4.5.3.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-core-spi/4.5.3.Final/resteasy-core-spi-4.5.3.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/sun/activation/jakarta.activation/1.2.1/jakarta.activation-1.2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/sun/activation/jakarta.activation/1.2.1/jakarta.activation-1.2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/sun/activation/all/1.2.1/all-1.2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/sun/activation/all/1.2.1/all-1.2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/ee4j/project/1.0.2/project-1.0.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/ee4j/project/1.0.2/project-1.0.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-junit5/1.4.1.Final/quarkus-junit5-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-junit5/1.4.1.Final/quarkus-junit5-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-test-framework/1.4.1.Final/quarkus-test-framework-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-test-framework/1.4.1.Final/quarkus-test-framework-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bootstrap-core/1.4.1.Final/quarkus-bootstrap-core-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bootstrap-core/1.4.1.Final/quarkus-bootstrap-core-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bootstrap-parent/1.4.1.Final/quarkus-bootstrap-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bootstrap-parent/1.4.1.Final/quarkus-bootstrap-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm/7.3.1/asm-7.3.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm/7.3.1/asm-7.3.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/ow2/1.5/ow2-1.5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/ow2/1.5/ow2-1.5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-embedder/3.6.3/maven-embedder-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-embedder/3.6.3/maven-embedder-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings/3.6.3/maven-settings-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings/3.6.3/maven-settings-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-utils/3.2.1/plexus-utils-3.2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-utils/3.2.1/plexus-utils-3.2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus/5.1/plexus-5.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus/5.1/plexus-5.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings-builder/3.6.3/maven-settings-builder-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings-builder/3.6.3/maven-settings-builder-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-builder-support/3.6.3/maven-builder-support-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-builder-support/3.6.3/maven-builder-support-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-interpolation/1.25/plexus-interpolation-1.25.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-interpolation/1.25/plexus-interpolation-1.25.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-core/3.6.3/maven-core-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-core/3.6.3/maven-core-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model/3.6.3/maven-model-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model/3.6.3/maven-model-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-repository-metadata/3.6.3/maven-repository-metadata-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-repository-metadata/3.6.3/maven-repository-metadata-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-artifact/3.6.2/maven-artifact-3.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-artifact/3.6.2/maven-artifact-3.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven/3.6.2/maven-3.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven/3.6.2/maven-3.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/commons/commons-lang3/3.9/commons-lang3-3.9.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/commons/commons-lang3/3.9/commons-lang3-3.9.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/commons/commons-parent/48/commons-parent-48.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/commons/commons-parent/48/commons-parent-48.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-plugin-api/3.6.3/maven-plugin-api-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-plugin-api/3.6.3/maven-plugin-api-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/sisu/org.eclipse.sisu.plexus/0.3.4/org.eclipse.sisu.plexus-0.3.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/sisu/org.eclipse.sisu.plexus/0.3.4/org.eclipse.sisu.plexus-0.3.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/sisu/sisu-plexus/0.3.4/sisu-plexus-0.3.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/sisu/sisu-plexus/0.3.4/sisu-plexus-0.3.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-component-annotations/2.1.0/plexus-component-annotations-2.1.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-component-annotations/2.1.0/plexus-component-annotations-2.1.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-containers/2.1.0/plexus-containers-2.1.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-containers/2.1.0/plexus-containers-2.1.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model-builder/3.6.3/maven-model-builder-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model-builder/3.6.3/maven-model-builder-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-resolver-provider/3.6.3/maven-resolver-provider-3.6.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-resolver-provider/3.6.3/maven-resolver-provider-3.6.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-api/1.4.1/maven-resolver-api-1.4.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-api/1.4.1/maven-resolver-api-1.4.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver/1.4.1/maven-resolver-1.4.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver/1.4.1/maven-resolver-1.4.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-spi/1.4.1/maven-resolver-spi-1.4.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-spi/1.4.1/maven-resolver-spi-1.4.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-util/1.4.1/maven-resolver-util-1.4.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-util/1.4.1/maven-resolver-util-1.4.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-impl/1.4.1/maven-resolver-impl-1.4.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-impl/1.4.1/maven-resolver-impl-1.4.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/3.2.1/maven-shared-utils-3.2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/3.2.1/maven-shared-utils-3.2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-components/30/maven-shared-components-30.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-components/30/maven-shared-components-30.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-parent/30/maven-parent-30.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-parent/30/maven-parent-30.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/apache/18/apache-18.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/apache/18/apache-18.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/commons-io/commons-io/2.6/commons-io-2.6.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/commons-io/commons-io/2.6/commons-io-2.6.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/commons/commons-parent/42/commons-parent-42.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/commons/commons-parent/42/commons-parent-42.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/inject/guice/4.2.1/guice-4.2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/inject/guice/4.2.1/guice-4.2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/inject/guice-parent/4.2.1/guice-parent-4.2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/inject/guice-parent/4.2.1/guice-parent-4.2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/google/5/google-5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/google/5/google-5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/aopalliance/aopalliance/1.0/aopalliance-1.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/aopalliance/aopalliance/1.0/aopalliance-1.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/guava/guava/27.0.1-jre/guava-27.0.1-jre.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/guava/guava/27.0.1-jre/guava-27.0.1-jre.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/guava/guava-parent/27.0.1-jre/guava-parent-27.0.1-jre.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/guava/guava-parent/27.0.1-jre/guava-parent-27.0.1-jre.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/guava/guava-parent/26.0-android/guava-parent-26.0-android.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/guava/guava-parent/26.0-android/guava-parent-26.0-android.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/errorprone/error_prone_parent/2.2.0/error_prone_parent-2.2.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/errorprone/error_prone_parent/2.2.0/error_prone_parent-2.2.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/mojo/animal-sniffer-parent/1.17/animal-sniffer-parent-1.17.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/mojo/animal-sniffer-parent/1.17/animal-sniffer-parent-1.17.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/mojo/mojo-parent/40/mojo-parent-40.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/mojo/mojo-parent/40/mojo-parent-40.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/commons-cli/commons-cli/1.4/commons-cli-1.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/commons-cli/commons-cli/1.4/commons-cli-1.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/plexus/plexus-sec-dispatcher/1.4/plexus-sec-dispatcher-1.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/plexus/plexus-sec-dispatcher/1.4/plexus-sec-dispatcher-1.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/spice/spice-parent/12/spice-parent-12.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/spice/spice-parent/12/spice-parent-12.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/forge/forge-parent/4/forge-parent-4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/forge/forge-parent/4/forge-parent-4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/plexus/plexus-cipher/1.4/plexus-cipher-1.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/plexus/plexus-cipher/1.4/plexus-cipher-1.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-connector-basic/1.4.1/maven-resolver-connector-basic-1.4.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-connector-basic/1.4.1/maven-resolver-connector-basic-1.4.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-transport-wagon/1.4.1/maven-resolver-transport-wagon-1.4.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-transport-wagon/1.4.1/maven-resolver-transport-wagon-1.4.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-http/3.3.4/wagon-http-3.3.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-http/3.3.4/wagon-http-3.3.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-providers/3.3.4/wagon-providers-3.3.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-providers/3.3.4/wagon-providers-3.3.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon/3.3.4/wagon-3.3.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon/3.3.4/wagon-3.3.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-http-shared/3.3.4/wagon-http-shared-3.3.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-http-shared/3.3.4/wagon-http-shared-3.3.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jsoup/jsoup/1.12.1/jsoup-1.12.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jsoup/jsoup/1.12.1/jsoup-1.12.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpclient/4.5.12/httpclient-4.5.12.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpclient/4.5.12/httpclient-4.5.12.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcomponents-client/4.5.12/httpcomponents-client-4.5.12.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcomponents-client/4.5.12/httpcomponents-client-4.5.12.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcomponents-parent/11/httpcomponents-parent-11.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcomponents-parent/11/httpcomponents-parent-11.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.4.13/httpcomponents-core-4.4.13.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.4.13/httpcomponents-core-4.4.13.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/commons-codec/commons-codec/1.13/commons-codec-1.13.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/commons-codec/commons-codec/1.13/commons-codec-1.13.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-provider-api/3.3.4/wagon-provider-api-3.3.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-provider-api/3.3.4/wagon-provider-api-3.3.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-file/3.3.4/wagon-file-3.3.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-file/3.3.4/wagon-file-3.3.4.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logging/commons-logging-jboss-logging/1.0.0.Final/commons-logging-jboss-logging-1.0.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logging/commons-logging-jboss-logging/1.0.0.Final/commons-logging-jboss-logging-1.0.0.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/18/jboss-parent-18.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/18/jboss-parent-18.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-test-common/1.4.1.Final/quarkus-test-common-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-test-common/1.4.1.Final/quarkus-test-common-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core-deployment/1.4.1.Final/quarkus-core-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core-deployment/1.4.1.Final/quarkus-core-deployment-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/gizmo/gizmo/1.0.2.Final/gizmo-1.0.2.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/gizmo/gizmo/1.0.2.Final/gizmo-1.0.2.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-util/7.3.1/asm-util-7.3.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-util/7.3.1/asm-util-7.3.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-tree/7.3.1/asm-tree-7.3.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-tree/7.3.1/asm-tree-7.3.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-analysis/7.3.1/asm-analysis-7.3.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-analysis/7.3.1/asm-analysis-7.3.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jandex/2.1.3.Final/jandex-2.1.3.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jandex/2.1.3.Final/jandex-2.1.3.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/12/jboss-parent-12.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/12/jboss-parent-12.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-builder/1.4.1.Final/quarkus-builder-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-builder/1.4.1.Final/quarkus-builder-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp-deployment/1.4.1.Final/quarkus-jsonp-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp-deployment/1.4.1.Final/quarkus-jsonp-deployment-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp-parent/1.4.1.Final/quarkus-jsonp-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp-parent/1.4.1.Final/quarkus-jsonp-parent-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp/1.4.1.Final/quarkus-jsonp-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp/1.4.1.Final/quarkus-jsonp-1.4.1.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/glassfish/jakarta.json/1.1.6/jakarta.json-1.1.6.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/glassfish/jakarta.json/1.1.6/jakarta.json-1.1.6.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/glassfish/json/1.1.6/json-1.1.6.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/glassfish/json/1.1.6/json-1.1.6.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter/5.6.2/junit-jupiter-5.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter/5.6.2/junit-jupiter-5.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-api/5.6.2/junit-jupiter-api-5.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-api/5.6.2/junit-jupiter-api-5.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/platform/junit-platform-commons/1.6.2/junit-platform-commons-1.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/platform/junit-platform-commons/1.6.2/junit-platform-commons-1.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-params/5.6.2/junit-jupiter-params-5.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-params/5.6.2/junit-jupiter-params-5.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-engine/5.6.2/junit-jupiter-engine-5.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-engine/5.6.2/junit-jupiter-engine-5.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/platform/junit-platform-engine/1.6.2/junit-platform-engine-1.6.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/platform/junit-platform-engine/1.6.2/junit-platform-engine-1.6.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured/4.3.0/rest-assured-4.3.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured/4.3.0/rest-assured-4.3.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured-parent/4.3.0/rest-assured-parent-4.3.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured-parent/4.3.0/rest-assured-parent-4.3.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/5/oss-parent-5.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/5/oss-parent-5.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy/3.0.2/groovy-3.0.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy/3.0.2/groovy-3.0.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy-xml/3.0.2/groovy-xml-3.0.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy-xml/3.0.2/groovy-xml-3.0.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpmime/4.5.3/httpmime-4.5.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpmime/4.5.3/httpmime-4.5.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcomponents-client/4.5.3/httpcomponents-client-4.5.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcomponents-client/4.5.3/httpcomponents-client-4.5.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/project/7/project-7.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/project/7/project-7.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/apache/13/apache-13.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/apache/13/apache-13.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/hamcrest/hamcrest/2.1/hamcrest-2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/hamcrest/hamcrest/2.1/hamcrest-2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ccil/cowan/tagsoup/tagsoup/1.2.1/tagsoup-1.2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ccil/cowan/tagsoup/tagsoup/1.2.1/tagsoup-1.2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/json-path/4.3.0/json-path-4.3.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/json-path/4.3.0/json-path-4.3.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy-json/3.0.2/groovy-json-3.0.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy-json/3.0.2/groovy-json-3.0.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured-common/4.3.0/rest-assured-common-4.3.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured-common/4.3.0/rest-assured-common-4.3.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/xml-path/4.3.0/xml-path-4.3.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/xml-path/4.3.0/xml-path-4.3.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/xml/bind/jakarta.xml.bind-api-parent/2.3.2/jakarta.xml.bind-api-parent-2.3.2.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/xml/bind/jakarta.xml.bind-api-parent/2.3.2/jakarta.xml.bind-api-parent-2.3.2.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/sling/org.apache.sling.javax.activation/0.1.0/org.apache.sling.javax.activation-0.1.0.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/sling/org.apache.sling.javax.activation/0.1.0/org.apache.sling.javax.activation-0.1.0.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/sling/sling/15/sling-15.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/sling/sling/15/sling-15.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/apache/12/apache-12.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/apache/12/apache-12.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy/1.4.1.Final/quarkus-resteasy-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http/1.4.1.Final/quarkus-vertx-http-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-development-mode-spi/1.4.1.Final/quarkus-development-mode-spi-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/security/quarkus-security/1.1.0.Final/quarkus-security-1.1.0.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/reactive/mutiny/0.4.4/mutiny-0.4.4.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy/1.4.1.Final/quarkus-resteasy-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/enterprise/jakarta.enterprise.cdi-api/2.0.2/jakarta.enterprise.cdi-api-2.0.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/security/quarkus-security/1.1.0.Final/quarkus-security-1.1.0.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/el/jakarta.el-api/3.0.3/jakarta.el-api-3.0.3.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/enterprise/jakarta.enterprise.cdi-api/2.0.2/jakarta.enterprise.cdi-api-2.0.2.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http/1.4.1.Final/quarkus-vertx-http-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/interceptor/jakarta.interceptor-api/1.2.5/jakarta.interceptor-api-1.2.5.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core/1.4.1.Final/quarkus-vertx-core-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/reactive/mutiny/0.4.4/mutiny-0.4.4.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-development-mode-spi/1.4.1.Final/quarkus-development-mode-spi-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty/1.4.1.Final/quarkus-netty-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec/4.1.45.Final/netty-codec-4.1.45.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core/1.4.1.Final/quarkus-vertx-core-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-handler/4.1.45.Final/netty-handler-4.1.45.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/el/jakarta.el-api/3.0.3/jakarta.el-api-3.0.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/interceptor/jakarta.interceptor-api/1.2.5/jakarta.interceptor-api-1.2.5.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec/4.1.45.Final/netty-codec-4.1.45.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-core/3.8.5/vertx-core-3.8.5.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty/1.4.1.Final/quarkus-netty-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-buffer/4.1.45.Final/netty-buffer-4.1.45.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-transport/4.1.45.Final/netty-transport-4.1.45.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-common/4.1.45.Final/netty-common-4.1.45.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-buffer/4.1.45.Final/netty-buffer-4.1.45.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-handler-proxy/4.1.45.Final/netty-handler-proxy-4.1.45.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-transport/4.1.45.Final/netty-transport-4.1.45.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-common/4.1.45.Final/netty-common-4.1.45.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-socks/4.1.45.Final/netty-codec-socks-4.1.45.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-handler/4.1.45.Final/netty-handler-4.1.45.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-http/4.1.45.Final/netty-codec-http-4.1.45.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-http2/4.1.45.Final/netty-codec-http2-4.1.45.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-core/3.8.5/vertx-core-3.8.5.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-resolver/4.1.45.Final/netty-resolver-4.1.45.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-http/4.1.45.Final/netty-codec-http-4.1.45.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-handler-proxy/4.1.45.Final/netty-handler-proxy-4.1.45.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-resolver/4.1.45.Final/netty-resolver-4.1.45.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-socks/4.1.45.Final/netty-codec-socks-4.1.45.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.3/jackson-core-2.10.3.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-dns/4.1.45.Final/netty-codec-dns-4.1.45.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/netty/netty-resolver-dns/4.1.45.Final/netty-resolver-dns-4.1.45.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-http2/4.1.45.Final/netty-codec-http2-4.1.45.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web/3.8.5/vertx-web-3.8.5.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web-common/3.8.5/vertx-web-common-3.8.5.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-codec-dns/4.1.45.Final/netty-codec-dns-4.1.45.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-auth-common/3.8.5/vertx-auth-common-3.8.5.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/netty/netty-resolver-dns/4.1.45.Final/netty-resolver-dns-4.1.45.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.3/jackson-core-2.10.3.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-bridge-common/3.8.5/vertx-bridge-common-3.8.5.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common/1.4.1.Final/quarkus-resteasy-server-common-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web/3.8.5/vertx-web-3.8.5.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc/1.4.1.Final/quarkus-arc-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-web-common/3.8.5/vertx-web-common-3.8.5.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc/1.4.1.Final/arc-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common/1.4.1.Final/quarkus-resteasy-server-common-1.4.1.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc/1.4.1.Final/arc-1.4.1.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc/1.4.1.Final/quarkus-arc-1.4.1.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-bridge-common/3.8.5/vertx-bridge-common-3.8.5.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/vertx/vertx-auth-common/3.8.5/vertx-auth-common-3.8.5.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-core/4.5.3.Final/resteasy-core-4.5.3.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common/1.4.1.Final/quarkus-resteasy-common-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/context-propagation/microprofile-context-propagation-api/1.0.1/microprofile-context-propagation-api-1.0.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/transaction/jakarta.transaction-api/1.3.3/jakarta.transaction-api-1.3.3.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/ws/rs/jboss-jaxrs-api_2.1_spec/2.0.1.Final/jboss-jaxrs-api_2.1_spec-2.0.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/transaction/jakarta.transaction-api/1.3.3/jakarta.transaction-api-1.3.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common/1.4.1.Final/quarkus-resteasy-common-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/xml/bind/jboss-jaxb-api_2.3_spec/2.0.0.Final/jboss-jaxb-api_2.3_spec-2.0.0.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-core-spi/4.5.3.Final/resteasy-core-spi-4.5.3.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/context-propagation/microprofile-context-propagation-api/1.0.1/microprofile-context-propagation-api-1.0.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-core/4.5.3.Final/resteasy-core-4.5.3.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/config/microprofile-config-api/1.4/microprofile-config-api-1.4.jar\nDownloading from central: https://repo.maven.apache.org/maven2/com/sun/activation/jakarta.activation/1.2.1/jakarta.activation-1.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/ws/rs/jboss-jaxrs-api_2.1_spec/2.0.1.Final/jboss-jaxrs-api_2.1_spec-2.0.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/resteasy/resteasy-core-spi/4.5.3.Final/resteasy-core-spi-4.5.3.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/microprofile/config/microprofile-config-api/1.4/microprofile-config-api-1.4.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm/7.3.1/asm-7.3.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/com/sun/activation/jakarta.activation/1.2.1/jakarta.activation-1.2.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bootstrap-core/1.4.1.Final/quarkus-bootstrap-core-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/spec/javax/xml/bind/jboss-jaxb-api_2.3_spec/2.0.0.Final/jboss-jaxb-api_2.3_spec-2.0.0.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-junit5/1.4.1.Final/quarkus-junit5-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings/3.6.3/maven-settings-3.6.3.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-embedder/3.6.3/maven-embedder-3.6.3.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings/3.6.3/maven-settings-3.6.3.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-core/3.6.3/maven-core-3.6.3.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-embedder/3.6.3/maven-embedder-3.6.3.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-artifact/3.6.2/maven-artifact-3.6.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-junit5/1.4.1.Final/quarkus-junit5-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-component-annotations/2.1.0/plexus-component-annotations-2.1.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm/7.3.1/asm-7.3.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-bootstrap-core/1.4.1.Final/quarkus-bootstrap-core-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-plugin-api/3.6.3/maven-plugin-api-3.6.3.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model/3.6.3/maven-model-3.6.3.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model/3.6.3/maven-model-3.6.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-plugin-api/3.6.3/maven-plugin-api-3.6.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-artifact/3.6.2/maven-artifact-3.6.2.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-core/3.6.3/maven-core-3.6.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-component-annotations/2.1.0/plexus-component-annotations-2.1.0.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-util/1.4.1/maven-resolver-util-1.4.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-api/1.4.1/maven-resolver-api-1.4.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-builder-support/3.6.3/maven-builder-support-3.6.3.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model-builder/3.6.3/maven-model-builder-3.6.3.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/3.2.1/maven-shared-utils-3.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-builder-support/3.6.3/maven-builder-support-3.6.3.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/commons-io/commons-io/2.6/commons-io-2.6.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-util/1.4.1/maven-resolver-util-1.4.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-api/1.4.1/maven-resolver-api-1.4.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/inject/guice/4.2.1/guice-4.2.1-no_aop.jar\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/guava/guava/27.0.1-jre/guava-27.0.1-jre.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model-builder/3.6.3/maven-model-builder-3.6.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/3.2.1/maven-shared-utils-3.2.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/inject/guice/4.2.1/guice-4.2.1-no_aop.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/com/google/guava/guava/27.0.1-jre/guava-27.0.1-jre.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/commons-cli/commons-cli/1.4/commons-cli-1.4.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings-builder/3.6.3/maven-settings-builder-3.6.3.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/eclipse/sisu/org.eclipse.sisu.plexus/0.3.4/org.eclipse.sisu.plexus-0.3.4.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-utils/3.2.1/plexus-utils-3.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/commons-io/commons-io/2.6/commons-io-2.6.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-interpolation/1.25/plexus-interpolation-1.25.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/commons-cli/commons-cli/1.4/commons-cli-1.4.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/plexus/plexus-sec-dispatcher/1.4/plexus-sec-dispatcher-1.4.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-interpolation/1.25/plexus-interpolation-1.25.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/sonatype/plexus/plexus-cipher/1.4/plexus-cipher-1.4.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-utils/3.2.1/plexus-utils-3.2.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-resolver-provider/3.6.3/maven-resolver-provider-3.6.3.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings-builder/3.6.3/maven-settings-builder-3.6.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/sisu/org.eclipse.sisu.plexus/0.3.4/org.eclipse.sisu.plexus-0.3.4.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-repository-metadata/3.6.3/maven-repository-metadata-3.6.3.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-spi/1.4.1/maven-resolver-spi-1.4.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-resolver-provider/3.6.3/maven-resolver-provider-3.6.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-spi/1.4.1/maven-resolver-spi-1.4.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/plexus/plexus-cipher/1.4/plexus-cipher-1.4.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-repository-metadata/3.6.3/maven-repository-metadata-3.6.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/sonatype/plexus/plexus-sec-dispatcher/1.4/plexus-sec-dispatcher-1.4.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-http/3.3.4/wagon-http-3.3.4.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-transport-wagon/1.4.1/maven-resolver-transport-wagon-1.4.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-connector-basic/1.4.1/maven-resolver-connector-basic-1.4.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-impl/1.4.1/maven-resolver-impl-1.4.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-http-shared/3.3.4/wagon-http-shared-3.3.4.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-http-shared/3.3.4/wagon-http-shared-3.3.4.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-transport-wagon/1.4.1/maven-resolver-transport-wagon-1.4.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-impl/1.4.1/maven-resolver-impl-1.4.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/resolver/maven-resolver-connector-basic/1.4.1/maven-resolver-connector-basic-1.4.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-http/3.3.4/wagon-http-3.3.4.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging/3.3.2.Final/jboss-logging-3.3.2.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-file/3.3.4/wagon-file-3.3.4.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-provider-api/3.3.4/wagon-provider-api-3.3.4.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/jsoup/jsoup/1.12.1/jsoup-1.12.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logging/commons-logging-jboss-logging/1.0.0.Final/commons-logging-jboss-logging-1.0.0.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging/3.3.2.Final/jboss-logging-3.3.2.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-test-common/1.4.1.Final/quarkus-test-common-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jsoup/jsoup/1.12.1/jsoup-1.12.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-file/3.3.4/wagon-file-3.3.4.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-provider-api/3.3.4/wagon-provider-api-3.3.4.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/gizmo/gizmo/1.0.2.Final/gizmo-1.0.2.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core-deployment/1.4.1.Final/quarkus-core-deployment-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-util/7.3.1/asm-util-7.3.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logging/commons-logging-jboss-logging/1.0.0.Final/commons-logging-jboss-logging-1.0.0.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-tree/7.3.1/asm-tree-7.3.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/gizmo/gizmo/1.0.2.Final/gizmo-1.0.2.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-analysis/7.3.1/asm-analysis-7.3.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-test-common/1.4.1.Final/quarkus-test-common-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-builder/1.4.1.Final/quarkus-builder-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core-deployment/1.4.1.Final/quarkus-core-deployment-1.4.1.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-util/7.3.1/asm-util-7.3.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp-deployment/1.4.1.Final/quarkus-jsonp-deployment-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp/1.4.1.Final/quarkus-jsonp-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-tree/7.3.1/asm-tree-7.3.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/glassfish/jakarta.json/1.1.6/jakarta.json-1.1.6.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ow2/asm/asm-analysis/7.3.1/asm-analysis-7.3.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jandex/2.1.3.Final/jandex-2.1.3.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-builder/1.4.1.Final/quarkus-builder-1.4.1.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/glassfish/jakarta.json/1.1.6/jakarta.json-1.1.6.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-api/5.6.2/junit-jupiter-api-5.6.2.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter/5.6.2/junit-jupiter-5.6.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp-deployment/1.4.1.Final/quarkus-jsonp-deployment-1.4.1.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-jsonp/1.4.1.Final/quarkus-jsonp-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jandex/2.1.3.Final/jandex-2.1.3.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/platform/junit-platform-commons/1.6.2/junit-platform-commons-1.6.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter/5.6.2/junit-jupiter-5.6.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-params/5.6.2/junit-jupiter-params-5.6.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-engine/5.6.2/junit-jupiter-engine-5.6.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-api/5.6.2/junit-jupiter-api-5.6.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/junit/platform/junit-platform-engine/1.6.2/junit-platform-engine-1.6.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core/1.4.1.Final/quarkus-core-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/platform/junit-platform-commons/1.6.2/junit-platform-commons-1.6.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-engine/5.6.2/junit-jupiter-engine-5.6.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/inject/jakarta.inject-api/1.0/jakarta.inject-api-1.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-params/5.6.2/junit-jupiter-params-5.6.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-ide-launcher/1.4.1.Final/quarkus-ide-launcher-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-core/1.4.1.Final/quarkus-core-1.4.1.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/junit/platform/junit-platform-engine/1.6.2/junit-platform-engine-1.6.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config/1.7.0/smallrye-config-1.7.0.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config-common/1.7.0/smallrye-config-common-1.7.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logmanager/jboss-logmanager-embedded/1.0.4/jboss-logmanager-embedded-1.0.4.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/inject/jakarta.inject-api/1.0/jakarta.inject-api-1.0.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging-annotations/2.1.0.Final/jboss-logging-annotations-2.1.0.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logmanager/jboss-logmanager-embedded/1.0.4/jboss-logmanager-embedded-1.0.4.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/threads/jboss-threads/3.1.1.Final/jboss-threads-3.1.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config/1.7.0/smallrye-config-1.7.0.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/smallrye/config/smallrye-config-common/1.7.0/smallrye-config-common-1.7.0.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/slf4j/slf4j-jboss-logging/1.2.0.Final/slf4j-jboss-logging-1.2.0.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-ide-launcher/1.4.1.Final/quarkus-ide-launcher-1.4.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/graalvm/sdk/graal-sdk/19.3.1/graal-sdk-19.3.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/logging/jboss-logging-annotations/2.1.0.Final/jboss-logging-annotations-2.1.0.Final.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/threads/jboss-threads/3.1.1.Final/jboss-threads-3.1.1.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/wildfly/common/wildfly-common/1.5.4.Final-format-001/wildfly-common-1.5.4.Final-format-001.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured/4.3.0/rest-assured-4.3.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy/3.0.2/groovy-3.0.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/slf4j/slf4j-jboss-logging/1.2.0.Final/slf4j-jboss-logging-1.2.0.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy-xml/3.0.2/groovy-xml-3.0.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured/4.3.0/rest-assured-4.3.0.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/graalvm/sdk/graal-sdk/19.3.1/graal-sdk-19.3.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpclient/4.5.12/httpclient-4.5.12.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/wildfly/common/wildfly-common/1.5.4.Final-format-001/wildfly-common-1.5.4.Final-format-001.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/commons-codec/commons-codec/1.13/commons-codec-1.13.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy/3.0.2/groovy-3.0.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpmime/4.5.3/httpmime-4.5.3.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy-xml/3.0.2/groovy-xml-3.0.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/hamcrest/hamcrest/2.1/hamcrest-2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/commons-codec/commons-codec/1.13/commons-codec-1.13.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/ccil/cowan/tagsoup/tagsoup/1.2.1/tagsoup-1.2.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/json-path/4.3.0/json-path-4.3.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpclient/4.5.12/httpclient-4.5.12.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy-json/3.0.2/groovy-json-3.0.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpmime/4.5.3/httpmime-4.5.3.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured-common/4.3.0/rest-assured-common-4.3.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/hamcrest/hamcrest/2.1/hamcrest-2.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/rest-assured/xml-path/4.3.0/xml-path-4.3.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/json-path/4.3.0/json-path-4.3.0.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/ccil/cowan/tagsoup/tagsoup/1.2.1/tagsoup-1.2.1.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/commons/commons-lang3/3.9/commons-lang3-3.9.jar\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/groovy/groovy-json/3.0.2/groovy-json-3.0.2.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/rest-assured-common/4.3.0/rest-assured-common-4.3.0.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/sling/org.apache.sling.javax.activation/0.1.0/org.apache.sling.javax.activation-0.1.0.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/commons/commons-lang3/3.9/commons-lang3-3.9.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/rest-assured/xml-path/4.3.0/xml-path-4.3.0.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/sling/org.apache.sling.javax.activation/0.1.0/org.apache.sling.javax.activation-0.1.0.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-deployment/1.4.1.Final/quarkus-resteasy-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-deployment/1.4.1.Final/quarkus-resteasy-deployment-1.4.1.Final.pom (3.8 kB at 84 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-deployment/1.4.1.Final/quarkus-resteasy-server-common-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-deployment/1.4.1.Final/quarkus-resteasy-server-common-deployment-1.4.1.Final.pom (2.2 kB at 41 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jsoup/jsoup/1.11.3/jsoup-1.11.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jsoup/jsoup/1.11.3/jsoup-1.11.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-provider-api/3.3.3/wagon-provider-api-3.3.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon-provider-api/3.3.3/wagon-provider-api-3.3.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon/3.3.3/wagon-3.3.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/wagon/wagon/3.3.3/wagon-3.3.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-deployment/1.4.1.Final/quarkus-resteasy-common-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-deployment/1.4.1.Final/quarkus-resteasy-common-deployment-1.4.1.Final.pom (2.0 kB at 36 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-spi/1.4.1.Final/quarkus-resteasy-common-spi-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-spi/1.4.1.Final/quarkus-resteasy-common-spi-1.4.1.Final.pom (817 B at 17 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc-deployment/1.4.1.Final/quarkus-arc-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc-deployment/1.4.1.Final/quarkus-arc-deployment-1.4.1.Final.pom (1.6 kB at 38 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc-processor/1.4.1.Final/arc-processor-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc-processor/1.4.1.Final/arc-processor-1.4.1.Final.pom (1.7 kB at 40 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-spi/1.4.1.Final/quarkus-resteasy-server-common-spi-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-spi/1.4.1.Final/quarkus-resteasy-server-common-spi-1.4.1.Final.pom (889 B at 20 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-undertow-spi/1.4.1.Final/quarkus-undertow-spi-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-undertow-spi/1.4.1.Final/quarkus-undertow-spi-1.4.1.Final.pom (2.0 kB at 32 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-http-parent/1.4.1.Final/quarkus-http-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-http-parent/1.4.1.Final/quarkus-http-parent-1.4.1.Final.pom (780 B at 17 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-servlet/3.0.6.Final/quarkus-http-servlet-3.0.6.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-servlet/3.0.6.Final/quarkus-http-servlet-3.0.6.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-parent/3.0.6.Final/quarkus-http-parent-3.0.6.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-parent/3.0.6.Final/quarkus-http-parent-3.0.6.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/28/jboss-parent-28.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/28/jboss-parent-28.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-core/3.0.6.Final/quarkus-http-core-3.0.6.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-core/3.0.6.Final/quarkus-http-core-3.0.6.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-http-core/3.0.6.Final/quarkus-http-http-core-3.0.6.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-http-core/3.0.6.Final/quarkus-http-http-core-3.0.6.Final.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/servlet/jakarta.servlet-api/4.0.3/jakarta.servlet-api-4.0.3.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/servlet/jakarta.servlet-api/4.0.3/jakarta.servlet-api-4.0.3.pom (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-metadata-web/11.0.0.Final/jboss-metadata-web-11.0.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-metadata-web/11.0.0.Final/jboss-metadata-web-11.0.0.Final.pom (4.7 kB at 108 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-as-parent-metadata/11.0.0.Final/jboss-as-parent-metadata-11.0.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-as-parent-metadata/11.0.0.Final/jboss-as-parent-metadata-11.0.0.Final.pom (17 kB at 173 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/16/jboss-parent-16.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/jboss-parent/16/jboss-parent-16.pom (32 kB at 358 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-metadata-common/11.0.0.Final/jboss-metadata-common-11.0.0.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-metadata-common/11.0.0.Final/jboss-metadata-common-11.0.0.Final.pom (4.0 kB at 119 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http-deployment/1.4.1.Final/quarkus-vertx-http-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http-deployment/1.4.1.Final/quarkus-vertx-http-deployment-1.4.1.Final.pom (3.0 kB at 35 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core-deployment/1.4.1.Final/quarkus-vertx-core-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core-deployment/1.4.1.Final/quarkus-vertx-core-deployment-1.4.1.Final.pom (2.2 kB at 50 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty-deployment/1.4.1.Final/quarkus-netty-deployment-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty-deployment/1.4.1.Final/quarkus-netty-deployment-1.4.1.Final.pom (1.6 kB at 29 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-kubernetes-spi/1.4.1.Final/quarkus-kubernetes-spi-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-kubernetes-spi/1.4.1.Final/quarkus-kubernetes-spi-1.4.1.Final.pom (862 B at 12 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-kubernetes-parent/1.4.1.Final/quarkus-kubernetes-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-kubernetes-parent/1.4.1.Final/quarkus-kubernetes-parent-1.4.1.Final.pom (786 B at 16 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-security-spi/1.4.1.Final/quarkus-security-spi-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-security-spi/1.4.1.Final/quarkus-security-spi-1.4.1.Final.pom (1.4 kB at 13 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-security-parent/1.4.1.Final/quarkus-security-parent-1.4.1.Final.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-security-parent/1.4.1.Final/quarkus-security-parent-1.4.1.Final.pom (819 B at 6.3 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/graalvm/nativeimage/svm/19.3.1/svm-19.3.1.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/graalvm/nativeimage/svm/19.3.1/svm-19.3.1.pom (2.7 kB at 31 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-spi/1.4.1.Final/quarkus-resteasy-common-spi-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-deployment/1.4.1.Final/quarkus-resteasy-common-deployment-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-spi/1.4.1.Final/quarkus-resteasy-server-common-spi-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-deployment/1.4.1.Final/quarkus-resteasy-server-common-deployment-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-http-core/3.0.6.Final/quarkus-http-http-core-3.0.6.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-http-core/3.0.6.Final/quarkus-http-http-core-3.0.6.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-core/3.0.6.Final/quarkus-http-core-3.0.6.Final.jar\nProgress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (8.2/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (11/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (2.7/5.2 kB) | quarkus-resteasy-common-spProgress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (8.2/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (11/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (8.2/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (11/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (11/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (11/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (14/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (11/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (16/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (11/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (16/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (14/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (16/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (16/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (19/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (16/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (19/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (19/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (22/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (19/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (22/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (22/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (25/26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (22/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (22/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4.1.Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (25/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4.1.Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (27/29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4.1.Progress (4): quarkus-resteasy-common-deployment-1.4.1.Final.jar (26 kB) | quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (29 kB) | quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB) | quarkus-resteasy-common-spi-1.4.1.Fin                                                                                                                                                                                                                                                  Downloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-spi/1.4.1.Final/quarkus-resteasy-server-common-spi-1.4.1.Final.jar (5.2 kB at 92 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-spi/1.4.1.Final/quarkus-resteasy-common-spi-1.4.1.Final.jar (3.4 kB at 60 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-servlet/3.0.6.Final/quarkus-http-servlet-3.0.6.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/jakarta/servlet/jakarta.servlet-api/4.0.3/jakarta.servlet-api-4.0.3.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-core/3.0.6.Final/quarkus-http-core-3.0.6.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-metadata-common/11.0.0.Final/jboss-metadata-common-11.0.0.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-common-deployment/1.4.1.Final/quarkus-resteasy-common-deployment-1.4.1.Final.jar (26 kB at 265 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-metadata-web/11.0.0.Final/jboss-metadata-web-11.0.0.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/jakarta/servlet/jakarta.servlet-api/4.0.3/jakarta.servlet-api-4.0.3.jar (0 B at 0 B/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/http/quarkus-http-servlet/3.0.6.Final/quarkus-http-servlet-3.0.6.Final.jar (0 B at 0 B/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-undertow-spi/1.4.1.Final/quarkus-undertow-spi-1.4.1.Final.jar\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty-deployment/1.4.1.Final/quarkus-netty-deployment-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-server-common-deployment/1.4.1.Final/quarkus-resteasy-server-common-deployment-1.4.1.Final.jar (29 kB at 277 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core-deployment/1.4.1.Final/quarkus-vertx-core-deployment-1.4.1.Final.jar\nProgress (5): jboss-metadata-common-11.0.0.Final.jar (66/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (2.7/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (2.7/8.1 kB) | quarkus-uProgress (5): jboss-metadata-common-11.0.0.Final.jar (69/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (2.7/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (2.7/8.1 kB) | quarkus-uProgress (5): jboss-metadata-common-11.0.0.Final.jar (69/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (5.5/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (2.7/8.1 kB) | quarkus-uProgress (5): jboss-metadata-common-11.0.0.Final.jar (69/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (5.5/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (5.5/8.1 kB) | quarkus-uProgress (5): jboss-metadata-common-11.0.0.Final.jar (69/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (5.5/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (5.5/8.1 kB) | quarkus-uProgress (5): jboss-metadata-common-11.0.0.Final.jar (71/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (5.5/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (5.5/8.1 kB) | quarkus-uProgress (5): jboss-metadata-common-11.0.0.Final.jar (71/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (8.2/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (5.5/8.1 kB) | quarkus-uProgress (5): jboss-metadata-common-11.0.0.Final.jar (71/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (8.2/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-underProgress (5): jboss-metadata-common-11.0.0.Final.jar (71/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (8.2/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-underProgress (5): jboss-metadata-common-11.0.0.Final.jar (74/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (8.2/10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-underProgress (5): jboss-metadata-common-11.0.0.Final.jar (74/475 kB) | jboss-metadata-web-11.0.0.Final.jar (52/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (74/475 kB) | jboss-metadata-web-11.0.0.Final.jar (55/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (74/475 kB) | jboss-metadata-web-11.0.0.Final.jar (55/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (77/475 kB) | jboss-metadata-web-11.0.0.Final.jar (55/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (77/475 kB) | jboss-metadata-web-11.0.0.Final.jar (58/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (77/475 kB) | jboss-metadata-web-11.0.0.Final.jar (58/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (77/475 kB) | jboss-metadata-web-11.0.0.Final.jar (60/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (80/475 kB) | jboss-metadata-web-11.0.0.Final.jar (60/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (80/475 kB) | jboss-metadata-web-11.0.0.Final.jar (60/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (82/475 kB) | jboss-metadata-web-11.0.0.Final.jar (60/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (82/475 kB) | jboss-metadata-web-11.0.0.Final.jar (63/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (82/475 kB) | jboss-metadata-web-11.0.0.Final.jar (63/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (85/475 kB) | jboss-metadata-web-11.0.0.Final.jar (63/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (85/475 kB) | jboss-metadata-web-11.0.0.Final.jar (66/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (85/475 kB) | jboss-metadata-web-11.0.0.Final.jar (66/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (88/475 kB) | jboss-metadata-web-11.0.0.Final.jar (66/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (88/475 kB) | jboss-metadata-web-11.0.0.Final.jar (69/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (88/475 kB) | jboss-metadata-web-11.0.0.Final.jar (69/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (90/475 kB) | jboss-metadata-web-11.0.0.Final.jar (69/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (90/475 kB) | jboss-metadata-web-11.0.0.Final.jar (71/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (93/475 kB) | jboss-metadata-web-11.0.0.Final.jar (71/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (93/475 kB) | jboss-metadata-web-11.0.0.Final.jar (74/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (96/475 kB) | jboss-metadata-web-11.0.0.Final.jar (74/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (96/475 kB) | jboss-metadata-web-11.0.0.Final.jar (77/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (99/475 kB) | jboss-metadata-web-11.0.0.Final.jar (77/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (99/475 kB) | jboss-metadata-web-11.0.0.Final.jar (80/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow-Progress (5): jboss-metadata-common-11.0.0.Final.jar (101/475 kB) | jboss-metadata-web-11.0.0.Final.jar (80/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (101/475 kB) | jboss-metadata-web-11.0.0.Final.jar (82/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (104/475 kB) | jboss-metadata-web-11.0.0.Final.jar (82/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (104/475 kB) | jboss-metadata-web-11.0.0.Final.jar (85/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (107/475 kB) | jboss-metadata-web-11.0.0.Final.jar (85/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (107/475 kB) | jboss-metadata-web-11.0.0.Final.jar (88/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (110/475 kB) | jboss-metadata-web-11.0.0.Final.jar (88/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (110/475 kB) | jboss-metadata-web-11.0.0.Final.jar (90/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (112/475 kB) | jboss-metadata-web-11.0.0.Final.jar (90/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (112/475 kB) | jboss-metadata-web-11.0.0.Final.jar (93/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (115/475 kB) | jboss-metadata-web-11.0.0.Final.jar (93/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (118/475 kB) | jboss-metadata-web-11.0.0.Final.jar (93/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (121/475 kB) | jboss-metadata-web-11.0.0.Final.jar (93/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (123/475 kB) | jboss-metadata-web-11.0.0.Final.jar (93/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (126/475 kB) | jboss-metadata-web-11.0.0.Final.jar (93/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (129/475 kB) | jboss-metadata-web-11.0.0.Final.jar (93/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertowProgress (5): jboss-metadata-common-11.0.0.Final.jar (132/475 kB) | jboss-metadata-web-11.0.0.Final.jar (93/468 kB) | quarkus-netty-deployment-1.4.1.Final.jar (10 kB) | quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB) | quarkus-undertow                                                                                                                                                                                                                                                  Downloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-netty-deployment/1.4.1.Final/quarkus-netty-deployment-1.4.1.Final.jar (10 kB at 36 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-kubernetes-spi/1.4.1.Final/quarkus-kubernetes-spi-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-core-deployment/1.4.1.Final/quarkus-vertx-core-deployment-1.4.1.Final.jar (8.1 kB at 28 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http-deployment/1.4.1.Final/quarkus-vertx-http-deployment-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-undertow-spi/1.4.1.Final/quarkus-undertow-spi-1.4.1.Final.jar (24 kB at 74 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc-processor/1.4.1.Final/arc-processor-1.4.1.Final.jar\nProgress (5): jboss-metadata-common-11.0.0.Final.jar (225/475 kB) | jboss-metadata-web-11.0.0.Final.jar (175/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | quarkus-kubernetes-spi-1.4.1.Final.jar (8.1 kB) | arc-processor-1.4Progress (5): jboss-metadata-common-11.0.0.Final.jar (228/475 kB) | jboss-metadata-web-11.0.0.Final.jar (175/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | quarkus-kubernetes-spi-1.4.1.Final.jar (8.1 kB) | arc-processor-1.4Progress (5): jboss-metadata-common-11.0.0.Final.jar (228/475 kB) | jboss-metadata-web-11.0.0.Final.jar (178/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | quarkus-kubernetes-spi-1.4.1.Final.jar (8.1 kB) | arc-processor-1.4Progress (5): jboss-metadata-common-11.0.0.Final.jar (228/475 kB) | jboss-metadata-web-11.0.0.Final.jar (178/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | quarkus-kubernetes-spi-1.4.1.Final.jar (8.1 kB) | arc-processor-1.4Progress (5): jboss-metadata-common-11.0.0.Final.jar (230/475 kB) | jboss-metadata-web-11.0.0.Final.jar (178/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | quarkus-kubernetes-spi-1.4.1.Final.jar (8.1 kB) | arc-processor-1.4Progress (5): jboss-metadata-common-11.0.0.Final.jar (230/475 kB) | jboss-metadata-web-11.0.0.Final.jar (181/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | quarkus-kubernetes-spi-1.4.1.Final.jar (8.1 kB) | arc-processor-1.4Progress (5): jboss-metadata-common-11.0.0.Final.jar (230/475 kB) | jboss-metadata-web-11.0.0.Final.jar (181/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | quarkus-kubernetes-spi-1.4.1.Final.jar (8.1 kB) | arc-processor-1.4                                                                                                                                                                                                                                                  Downloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-kubernetes-spi/1.4.1.Final/quarkus-kubernetes-spi-1.4.1.Final.jar (8.1 kB at 18 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc-deployment/1.4.1.Final/quarkus-arc-deployment-1.4.1.Final.jar\nProgress (5): jboss-metadata-common-11.0.0.Final.jar (266/475 kB) | jboss-metadata-web-11.0.0.Final.jar (211/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | arc-processor-1.4.1.Final.jar (44/310 kB) | quarkus-arc-deployment-Progress (5): jboss-metadata-common-11.0.0.Final.jar (266/475 kB) | jboss-metadata-web-11.0.0.Final.jar (211/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | arc-processor-1.4.1.Final.jar (44/310 kB) | quarkus-arc-deployment-Progress (5): jboss-metadata-common-11.0.0.Final.jar (269/475 kB) | jboss-metadata-web-11.0.0.Final.jar (211/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | arc-processor-1.4.1.Final.jar (44/310 kB) | quarkus-arc-deployment-Progress (5): jboss-metadata-common-11.0.0.Final.jar (269/475 kB) | jboss-metadata-web-11.0.0.Final.jar (214/468 kB) | quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB) | arc-processor-1.4.1.Final.jar (44/310 kB) | quarkus-arc-deployment-                                                                                                                                                                                                                                                  Downloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-vertx-http-deployment/1.4.1.Final/quarkus-vertx-http-deployment-1.4.1.Final.jar (22 kB at 43 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-security-spi/1.4.1.Final/quarkus-security-spi-1.4.1.Final.jar\nProgress (5): jboss-metadata-common-11.0.0.Final.jar (335/475 kB) | jboss-metadata-web-11.0.0.Final.jar (235/468 kB) | arc-processor-1.4.1.Final.jar (60/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (27/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (337/475 kB) | jboss-metadata-web-11.0.0.Final.jar (235/468 kB) | arc-processor-1.4.1.Final.jar (60/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (27/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (337/475 kB) | jboss-metadata-web-11.0.0.Final.jar (239/468 kB) | arc-processor-1.4.1.Final.jar (60/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (27/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (337/475 kB) | jboss-metadata-web-11.0.0.Final.jar (239/468 kB) | arc-processor-1.4.1.Final.jar (60/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (27/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (337/475 kB) | jboss-metadata-web-11.0.0.Final.jar (239/468 kB) | arc-processor-1.4.1.Final.jar (63/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (27/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (337/475 kB) | jboss-metadata-web-11.0.0.Final.jar (243/468 kB) | arc-processor-1.4.1.Final.jar (63/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (27/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (337/475 kB) | jboss-metadata-web-11.0.0.Final.jar (247/468 kB) | arc-processor-1.4.1.Final.jar (63/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (27/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (337/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (63/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (27/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (337/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (63/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (30/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (340/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (63/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (30/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (340/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (66/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (30/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (340/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (66/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (33/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (343/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (66/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (33/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (343/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (69/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (33/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (343/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (69/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (36/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (345/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (69/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (36/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (345/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (71/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (36/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (348/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (71/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (36/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (348/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (74/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (36/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (348/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (74/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (38/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (351/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (74/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (38/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (351/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (74/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (351/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (77/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (354/475 kB) | jboss-metadata-web-11.0.0.Final.jar (251/468 kB) | arc-processor-1.4.1.Final.jar (77/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (354/475 kB) | jboss-metadata-web-11.0.0.Final.jar (256/468 kB) | arc-processor-1.4.1.Final.jar (77/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (354/475 kB) | jboss-metadata-web-11.0.0.Final.jar (260/468 kB) | arc-processor-1.4.1.Final.jar (77/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (354/475 kB) | jboss-metadata-web-11.0.0.Final.jar (264/468 kB) | arc-processor-1.4.1.Final.jar (77/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (354/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (77/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (354/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (80/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (356/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (80/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (356/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (82/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (359/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (82/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (359/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (85/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (362/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (85/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (362/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (88/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (365/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (88/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (365/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (90/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (367/475 kB) | jboss-metadata-web-11.0.0.Final.jar (268/468 kB) | arc-processor-1.4.1.Final.jar (90/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (367/475 kB) | jboss-metadata-web-11.0.0.Final.jar (272/468 kB) | arc-processor-1.4.1.Final.jar (90/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (367/475 kB) | jboss-metadata-web-11.0.0.Final.jar (276/468 kB) | arc-processor-1.4.1.Final.jar (90/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (367/475 kB) | jboss-metadata-web-11.0.0.Final.jar (276/468 kB) | arc-processor-1.4.1.Final.jar (93/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (367/475 kB) | jboss-metadata-web-11.0.0.Final.jar (280/468 kB) | arc-processor-1.4.1.Final.jar (93/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (367/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (93/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (370/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (93/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (370/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (96/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (373/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (96/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (41/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (373/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (96/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (373/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (99/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (376/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (99/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.1Progress (5): jboss-metadata-common-11.0.0.Final.jar (376/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (378/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (381/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (384/475 kB) | jboss-metadata-web-11.0.0.Final.jar (284/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (384/475 kB) | jboss-metadata-web-11.0.0.Final.jar (288/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (387/475 kB) | jboss-metadata-web-11.0.0.Final.jar (288/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (387/475 kB) | jboss-metadata-web-11.0.0.Final.jar (292/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (387/475 kB) | jboss-metadata-web-11.0.0.Final.jar (297/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (387/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (389/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (392/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (395/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (398/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (400/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (403/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (406/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (409/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (411/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (414/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (101/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.Progress (5): jboss-metadata-common-11.0.0.Final.jar (414/475 kB) | jboss-metadata-web-11.0.0.Final.jar (301/468 kB) | arc-processor-1.4.1.Final.jar (104/310 kB) | quarkus-arc-deployment-1.4.1.Final.jar (44/142 kB) | quarkus-security-spi-1.4.                                                                                                                                                                                                                                                  Downloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-security-spi/1.4.1.Final/quarkus-security-spi-1.4.1.Final.jar (3.3 kB at 4.8 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/graalvm/nativeimage/svm/19.3.1/svm-19.3.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-metadata-common/11.0.0.Final/jboss-metadata-common-11.0.0.Final.jar (475 kB at 530 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-deployment/1.4.1.Final/quarkus-resteasy-deployment-1.4.1.Final.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/jboss/metadata/jboss-metadata-web/11.0.0.Final/jboss-metadata-web-11.0.0.Final.jar (468 kB at 427 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-resteasy-deployment/1.4.1.Final/quarkus-resteasy-deployment-1.4.1.Final.jar (22 kB at 20 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus-arc-deployment/1.4.1.Final/quarkus-arc-deployment-1.4.1.Final.jar (142 kB at 121 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/arc/arc-processor/1.4.1.Final/arc-processor-1.4.1.Final.jar (310 kB at 242 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/graalvm/nativeimage/svm/19.3.1/svm-19.3.1.jar (10 MB at 773 kB/s)\nmvn test  52.19s user 5.37s system 58% cpu 1:39.18 total\ngrep central  0.44s user 1.30s system 1% cpu 1:39.18 total\n\nDetermination:\n\nQuarkus test dependencies pulled from ""central"", ignoring repository configuration defined in ~/.m2/settings.xml. If you run the above mvn test command without the | grep central you can see that when Quarkus runs its tests during the maven-surefire-plugin lifecycle, it downloads from the default ""central"" repository.\n\n\n\n`q-1.3.2\ngit checkout q-1.3.2\nrm -Rf ~/.m2/repository\nmvn test | grep central\n\nOutput:\nmvn test  43.33s user 5.12s system 61% cpu 1:19.07 total\ngrep central  0.35s user 1.09s system 1% cpu 1:19.07 total\n\nDetermination:\n\nAll dependencies pulled from ""get-all-from-here"" defined repository, as expected\n\n\n\nq-1.3.2-maven-revision\ngit checkout q-1.3.2-maven-revision\nrm -Rf ~/.m2/repository\nmvn test | grep central\n\nOutput:\nmvn test  46.44s user 5.01s system 68% cpu 1:15.10 total\ngrep central  0.34s user 1.06s system 1% cpu 1:15.10 total\n\nDetermination:\n\nAll dependencies pulled from ""get-all-from-here"" defined repository, as expected\n\n\n\n'], 'url_profile': 'https://github.com/jeffhubLR', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Shanghai China', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Text-Classification-with-Logis--tic-Regression\n提取 NGram,分别用 tf 和 tfidf 向量化本文， 然后开始训练、验证和测试数据集。而后用逻辑回归进行二分类和多分类，二分类的核 心是 sigmoid 函数，多分类的核心是 softmax 函数。最后使用随机梯度下降调试获取模 型的最优参数，得到最优模型\n'], 'url_profile': 'https://github.com/Richardsh', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'Venezuela', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/franmarq', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Poyraz-Ozmen', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'HTML', 'Updated Jul 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated May 2, 2020']}"
"{'location': 'Dehradun/Gurgaon', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['AutoRegression-SimpleRNN-on-Sinwave\nRecurrent Neural Network\n'], 'url_profile': 'https://github.com/saileshraturi', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'São Paulo', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': ['Predict wages by years of experience (Simple Linear Regression)\nPredicting wages by years of experience in a small enterprise. In this case it was applied simple linear regression because there is just one independent variable.\nResults:\n\n'], 'url_profile': 'https://github.com/ricardomotoyama', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Regression Modeling with Boston Housing dataset - Lab\nIn this final lab, we shall apply the regression analysis and diagnostics techniques covered in this section to a familiar ""Boston Housing"" dataset. We performed a detailed EDA for this dataset in earlier section and hence carry a good understanding of how this dataset is composed. This this lab we shall try to identify the predictive ability of some of features found in this dataset towards identifying house price.\nObjectives:\nYou will be able to:\n\nBuild many linear models with boston housing data set using OLS\nFor each model, analyze OLS diagnostics for model validity\nVisually explain the results and interpret the diagnostics from Statsmodels\nComment on the goodness of fit for a simple regression model\n\nLet\'s get started.\nImport necessary libraries and load \'BostonHousing.csv\' as pandas dataframe.\n# Your code here\nThe data features and target are present as columns in boston data. Boston data gives a set of independent as independent variables in data and the housing rate as MEDV in target property. Also feature names are listed in feature_names. The desription is available at KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not ?)\n# Your code here \n\n# You observations here \nBased on this , we shall choose a selection of features which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck for linearity assumption for all chosen features with target variable using scatter plots and comment on the results\n# Your code here \n\n\n\n\n\n# Your observations here \nOkie so obviously our data needs a lot of pre-procesing to improve the results. This key behind such kaggle competitions is to process the data in such a way that we can identify the relationships and make predictions in the best possible way. For now, we shall leave the dataset untouched and just move on with regression. So far, our assumptions, although not too strong, but still hold to a level that we can move on.\nLet\'s do Regression\nRight here is the real deal. Let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). We shall do this is a loop and in every iteration, we shall pick one of the independent variables  perform following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog().\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your obervations here \nSo clearly the results are not highly reliable. the best good of fit i.e. r-squared is witnessed with rm. So clearly in this analysis this is our best predictor.\n\nSo how can we improve upon these results .\n\nPre-Processing\n\nThis is where pre-processing of data comes in. Dealing with outliers, normalizing data, scaling values etc can help regression analysis get more meaningful results from the given set of data\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis techniques and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. In the next section we shall look at multiple regression where we can use multiple features AT ONCE to define a relationship with outcome. We shall also look at some pre-processing and data simplification techniques and re-visit the boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the the goodness of fit.\n\nSummary\nIn this lab, we attempted to bring in all the skills learnt so far to a slighlt detailed dataset. We looked at the outcome of our analysis and realized that the data might need some pre-processing to see a clear improvement in results. We shall pick it up in the next section from this point and bring in data pre-processing techniques along with some assumptions that are needed for multiple regression .\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': '@kenya', 'stats_list': [], 'contributions': '200 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/atembamanu', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Houston, Tx', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['Predicting circulating temperatures in a horizontal wellbore while drilling.\nThis is a model that uses regression analysis to predict and model circulating temperatures in a horizontal wellborn while drilling.\nIt uses real world data where certain variables were held constant and others were changed.  Through the use of regression analysis\nthis model can be used as a prototype to predict circulating temperatures in drilling operations where the basin temperatures\nare elevated.\n'], 'url_profile': 'https://github.com/ikeshare192', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '208 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dharma610', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Adaptive-Wing-Loss-Face-Alignment-Heatmap-Regression\nThis is a partial implementation of the paper ""Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression"" in Tensorflow.\nSome components have yet to be added, including boundary prediction and coordinate aggregation.\n'], 'url_profile': 'https://github.com/andrewhou1', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': [""GRAVEE: Good Risk Assessment Values for Environmental Exposures\nEstimating Point of Departure (POD) from dose-response data using spline meta-regression\nGRAVEE is a method for estimating a chemical's Point of Departure (POD) based on\nquantitative dose-response data.\nInput Data\nThe input data should be a data frame with two columns, ordered as dose\nand response. Doses should not be transformed prior to upload. The\nvalues must be numeric.\nExample:\n\n\n\nDose\nResponse\n\n\n\n\n1\n1.4\n\n\n1\n0.28\n\n\n1\n0.9\n\n\n5\n8.32\n\n\n5\n8.6\n\n\n5\n7\n\n\n10\n10.2\n\n\n10\n9.45\n\n\n...\n...\n\n\n\nAnalysis\nAll analyses are performed using log10-transformed doses.Results\nare reported on the original dose scale.\nThe core function of this package is calculate_pod_quantiles(). GRAVEE\nsimulates new samples of dose-response data using bootstrap resampling. Each\nsimulated sample is fit to a dose-response curve using spline meta-regression.\nMenger Curvature is calculated along the interpolated doses of the fitted\ndose-response curve and the POD is reported as the dose with the maximal\ncurvature. GRAVEE reports a 95% confidence interval of PODs using the\ncalculated PODs from each simulated sample.\nLicense\nThis work was created by US government employees working in their government capacity. As a result, this work is in the public domain within the United States. The US Government may exercise copyright in other jurisdictions.\nContact\nKim T. To - kimberly.t.to@erdc.dren.mil\nLyle Burgoon - lyle.d.burgoon@erdc.dren.mil\n""], 'url_profile': 'https://github.com/k-t-to', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,853 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Updated Apr 29, 2020', 'R', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7,853 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,853 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'Islamabad, Pakistan', 'stats_list': [], 'contributions': '172 contributions\n        in the last year', 'description': ['Predicting_House_Sale_Prices\nIn this project, we explored the linear regression model, the approach to model fitting and some techniques for cleaning, transforming and selecting features. Moreover, we explored ways to improve the model we built. However, we worked with housing data for the city of Ames, Iowa, United States from 2006 to 2010. You can read more about why the data was collected here. You can also read about the different columns in the data here.\n'], 'url_profile': 'https://github.com/syed0019', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'Berkeley, California', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ethan-ding', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Adaptive-Wing-Loss-for-Face-Alignment-Heatmap-Regression\nThis is a partial implementation of the paper ""Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression"" in Tensorflow.\nSome components have yet to be added, including boundary prediction and coordinate aggregation.\n'], 'url_profile': 'https://github.com/andrewhou1', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Build Classification Models to Predict the Onset of Diabetes Based on Diagnostic Measures\nData Source: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n\nVisualize the distribution and correlation of diagnostic measures and diabetes test results using Matplotlib and Seaborn in Python.\nDevelop logistic regression, SVM and random forest models to predict the onset of diabetes.\nApply GridSearchCV to find the best hyperparameters for models. \nAssess model performance (confusion matrix, precision, recall, ROC AUC) and interpret performance from business impact viewpoint (false negatives vs false positives)\n\n'], 'url_profile': 'https://github.com/boyasun', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['London Bikes\nAim of the Project\nThe aim of this project was threefold:\n\nTo identify patterns in the spread of the number of bike shares in London.\nTo evaluate the relationship between weather, time, and the number of bike shares.\nTo assess whether a statistical model can predict the number of bike shares.\n\nTable of Contents\n\nHardware Used\nFile Descriptions\nMethods Used\nTechnologies Used\nExecutive Summary\n\nThe Data\nData Cleaning\nExploratory Data Analysis\nFeature Engineering\nModelling\nConclusion & Further Steps\n\n\n\nHardware Used\nMacBook Pro (Retina, 13-inch, Early 2015)\nProcessor: 2.9 GHz Dual-Core Intel Core i5\nMemory: 8 GB 1867 MHz DDR3\nOS: macOS Catalina (version 10.15.3)\n\nFile Descriptions\n\npreprocessing.ipynb: notebook for initial data cleaning, exploration, and analysis.\nmodelling.ipynb: notebook for feature engineering, machine learning, and predictions.\nrequirements.txt: includes a list of python libraries used in the project.\ndata:\n\nlondon_merged.csv: raw data file\nlondon_merged_processed.csv: cleaned data\nlondon_merged_modelling.csv: cleaned data, after feature engineering\n\n\n\nMethods Used\n\nData cleaning\nData exploration and visualisation\nFeature engineering\nMachine Learning\n\nTechnologies Used\n\nPython\nNumpy\nPandas\nMatplotlib\nSeaborn\nStatsmodels\nScikit-learn\nYellowbrick\n\nExecutive Summary\nAs stated earlier, there were three aims of this project:\n\nTo identify patterns in the spread of the number of bike shares in London.\nTo evaluate the relationship between weather, time, and the number of bike shares.\nTo assess whether a statistical model can predict the number of bike shares.\n\nThe Data\nThe data, acquired from Kaggle, is an hourly data of bike sharing counts, temperature, humidity, wind speed, season, holidays, and so on. The data (17,414 rows) spans over 2 years, from 2015 to 2017.\nData Cleaning\nThe numerical weather and season codes in the data were replaced with appropriate labels, and some new columns were created (time, day_of_the_week, week_number, month) out of the original timestamp column. The holiday and weekend columns were combined into a single holiday column, due to the original column not having a significant amount of entries.\nExploratory Data Analysis\nI evaluated the representation of various columns (such as season, weather, holidays) within the data, through multivariate (non-graphical) analysis, and was happy with the representation.\nNext, the data was plotted in order to assess any interesting patterns.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\nFigure 1 represents the overall pattern of how the number of bike shares is spread throughout the day. It can be observed that the number of bike shares first peaks at around 8 a.m. (which is probably due to the morning work commute) and then peaks again at around 5 and 6 p.m. (which is probably due to the evening work commute).\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\nFig. 2 represents the same data as in Fig. 1 but differentiated between weekdays and weekends (and holidays). An interesting observation is that whereas the spread on weekdays mimics the spread in Fig. 1, the spread on weekends and holidays has a different pattern. It spreads smoothly like a bell-curve, gently peaking at 2 and 3 p.m.\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\nFigure 3 represents how the number of bike shares are spread throughout the year. It can be observed that the spread follows a smooth bell-curve, peaking during summertime in July.\nNext, correlations between the number of bike shares and other variables were assessed.\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\nFigure 4 represents the correlation between bikes and temperature, and suggests an overall positive correlation with more variance during extreme temperatures.\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\nFigure 5 represents the correlation between bikes and humidity, and suggests an overall negative correlation.\nFeature Engineering\nIn order to transform their qualitative nature, dummy variables were created for the weather, season, time, day, and month columns. The data types, and some abnormalities within the count column were corrected.\nModelling\nThe data was firstly split into two sub-datasets, train (14,914 rows) and test (2,500) rows. Within the train sub-dataset, 5 KFold splits were created to train and validate the models. R-squared (coefficient of determination) was chosen to be the scoring metric to assess the accuracy of the models.\nThe models developed were as follows:\n\nLinear Regression\nLasso Regression\nRidge Regression\nElasticNet Regression\n\nIn order to determine the best alpha value for the Lasso, Ridge, and ElasticNet models, the models were looped through a list of alpha values and their R-squared values were recorded, along with the number of variables that the models found useful.\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\nFigure 6 represents the four best performing models.\nThe Lasso model was selected due to its simplicity (lesser number of features), and was tested against the test sub-dataset. The residuals resulting from the predictions made through the model suffered from heteroscedasticity (a condition where variance of the residuals increases as the response value increases).\nAs a solution, the count data was log-transformed (for both the train and test sub-datasets) and the modelling process was reimplemented.\n\n\n\n\n\n\n\n\nFigure 7\n\n\n\nFigure 7 shows the four best performing models after the log-transformation. It must be noted that the R-squared scores were improved due to the transformation. Again, the Lasso model was selected due to its simplicity, and was tested against the test sub-dataset.\n\n\n\n\n\n\n\n\nFigure 8\n\n\n\nFigure 8 represents the final residual plot. The residuals were greatly improved in comparison to the previous model, but there is still room for improvement for higher response values.\nConclusion & Further Steps\n\nThere are daily and yearly patterns to how the number of bike rides are spread. The spread is also different for weekdays and weekends.\nThere are linear relationships between the target and the features; and linear models can reach an accuracy score of around 83%\nThe models have trouble predicting higher values.\n\nFurther Steps:\n\nDue to its nature, the data has high multicollinearity, which needs to be dealt with in order the improve the models.\nA polynomial regression model must also be built, which would require further feature engineering in order to reduce the number of features fed into the model.\nARIMA, SARIMA, and VAR forecasting on the data would also be interesting and useful step.\n\n'], 'url_profile': 'https://github.com/meehadjawwad', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['Sigmoid_Neurons\nCan we achieve better accuracy that SKLearn built-in functions?\nAnswer is Yes. In this notebook, I will demonstrate how Sigmoid Neurons implemented from scratch can provide better accuracy than SKLearn built-in Logistc Regression funtion.\nSKLearn Logistic Regression implements Sigmoid curve for classification. However, it is more generalised. A Sigmoid Neuron implemented from scratch gives better accuracy(5-7% better accuracy) over SKLearn LogisticRegresstion funtion.\n Accuracy Achieved:\n\nSKLearn LogisticRegression: 89.47368421052632 %\nSigmoid Neurons: 94.73684210526315 %\n\nImprovement in accuracy of Sigmoid Neurons over SKLearn LogisticRegression = 5.263157894736835 %\n\n'], 'url_profile': 'https://github.com/YashRajRobotics', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'ON, Canada', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['TensorFlow2.0-notes\nTensorFlow Core Learning Algorithms The algorithms we will focus on include:  Linear Regression Classification Clustering Hidden Markov Models\n\nLinear Regression\nClassification\nClustering\nHidden Markov Models\n\n'], 'url_profile': 'https://github.com/Abigail6on', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '422 contributions\n        in the last year', 'description': ['King County Residential Real Estate Bargain Hunting\nTable of Contents\n\nIntroduction\nTechnologies\nTo get started\nInsights and Business Recommendations\nFurther Studies\n\nIntroduction\nThis project was assigned to me in Flatiron School\'s Immersive Data Science Program. The goal of the project is to analyze King County home sale data over a two year period (2015 and 2016) and find out how a residential real estate investor can gain a foothold in this market.\nHousing prices in King County, WA have been exploding over the past decade due to many factors including the availability of great jobs, a rich culture, and an outdoor recreation opportunities in the surrounding areas. It is now harder than ever for house hunters to find a bargain in what seems like is a total sellers\' market.\nMy goal in this project was to understand where, when, and what type of houses are available at decent prices and develop a multiple linear regression model capable of giving an indication whether or not investor should bargain. Meeting this goal required careful data cleaning, exploratory data analysis, thoughtful feature engineering, and finally an iterative approach to mutliple regression modelling.\nThree questions explored:\n\nQuestion1: location\nQuestion2: timing\nQuestion3: other attributes which have impact on price.\n\nIn addition, Multiple Linear Regression analysys peformed to provide accurate predictions for house prices.\nTechnologies\nThis project was created using the following languages and libraries. An environment with the correct versions of the following libraries will allow re-production and improvement on this project.\n\nPython version: 3.6.9\nMatplotlib version: 3.0.3\nSeaborn version: 0.9.0\nPandas version: 0.24.2\nNumpy version: 1.16.2\nStatsmodels version: 0.9.0\nScipy version: 1.2.1\nSklearn version: 0.20.3\n\nTo get started\n\nClone this repository.\nDataset can be found in the file ""kc_house_data.csv"".\nCheck requirements in Technologies section above and download libraries if necessary.\n\nInsights and Recommendations\nQuestion1: How Does Location Affect House Prices?\nExploration (EDA)\nThis question will explore which part of the King county is most expensive, how location can affect house prices and the correlation between house prices and location. If there is high correlation between house price and location data, location data can be used as a predictor for linear regression analysis.\nAverage price calculated for each zip code and after grouping the prices by zip codes, the mean () function is used instead of sum () to reach better results.\n\nDirection column created to search how is average price distributed by directions. A scatter plot created with avaliable latitude and longitude data. To specify the four directions the scatter plot used as a base.\n\nFour directions(NW, NE, SE, SE) calculated for each latitude and longitude combination and added to the dataframe. To analyze location\'s effect on house prices \'zipcode_king_county.geojson\' is loaded to work with folium. It helped to create a heatmap with Choropleth() function showing average price for each zip code.\n\nQ1: Findings/Insights/Recommendations\nFindings\n\nThe top 5 zip codes by average price are 98039, 98004, 98040, 98112 and 98102.\nLocation is one of the most important features in predicting house prices.\nAs expected the northwest of the county has the highest price and highest variance.\nIt can be concluded that the Bellevue, Seattle and Mercer Island have the highest average house prices.\nNortheast is the second in terms of highest price and variance.\nSouthwest and Southeast parts of the county has lowest prices.\nThe average price for northeast and northwest are almost the same.\nThe average price of southeast is higher than southwest part of the county.\n\nRecommendations\n\nThe results showed that location has hight impact on house prices therefore location data can be used as a proxy to predict house prices.\n\nNext steps\n\nMore concrete insights can be reached with broader datasets including features such as crime rates, distance to transportation and schools etc.\n\nQ2: When Is The Best Time To Buy A House?\nThis question aims to answer which season and month is more affordable in terms of buying a house, as well as months and seasons role in predicting house prices. In order to answer our second question the \'month\' and \'season\' columns which are previously created and added to the dataframe are used. Furthermore, groupby() function is used to group data in months and seasons. During grouping the price by months and seasons, the mean () function is used to reach unbiased results.\n\nQ2: Findings/Insights/Recommendations\nFindings\n\nAs predicted the best season to buy a house is winter.\nFall has the highest price range followed by summer and spring.\nSpring has the highest average price followed by summer and fall.\nThe average prices and median for each season are very close. Season is not genuine as expected to be used as a predictor in the regression model.\nWhile October has the highest variance, February has the lowest variance.\nFebruary has the lowest, April has the highest price average.\nLike seasons there isn\'t a the big difference between the median values and average price for months.\nMonth is also not an effective predictor for the regression model.\n\nRecommendations\n\nThe results showed that seaons and months has considerable low impact on house prices therefore season and month data cannot be used as an effective proxy to predict house prices.\n\nNext steps\n\nMore concrete insights can be reached with broader datasets including a longer period of purchases.\n\nModel Features\nBaseline model uses \'is_renovated\', \'waterfront\',\'with_basement\', \'view\', \'condition\', \'condition_4\', spring, summer,\'spring\',\'summer\', grade, \'sqft_living\',\'sqft_lot\',\'sqft_above\',\'sqft_living15\',\'sqft_lot15\',\'bathrooms\' and \'yr_built\' to predict the house prices. Some of them are added as dummy variables to the dataframe. The baseline model explains variance for 61 %.\nFor refining the model zip codes and directions are added to the second model as dummy variables. With the location feature the model\'s R-squared value reached to 87 %. To avoid multicollinearity \'sqft_above\' value dropped and the R-squared value decreased by 0.001.\nWith Recursive Feature Elimination 70 features is chosen and the final model accounts for 85 % of the house prices.\nThe final model\'s mean squared values for test and train sets indicates that the final model is not ovetfitting or underfitting.\n'], 'url_profile': 'https://github.com/kristinepetrosyan', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated May 19, 2020']}"
"{'location': 'San Diego, California', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['Default_Dataset(Logistic Regression)\nThis dataset is available in the ISLR package in R-Studios.\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).\nLogistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\nTo have a better understanding of logistic regression, I have have used the ""Default"" dataset from the ISLR package. And experimented with it here.\n'], 'url_profile': 'https://github.com/vbihare', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'New Zealand', 'stats_list': [], 'contributions': '485 contributions\n        in the last year', 'description': [""StreamingRandomPatchesRegressor\nThis is the implementation of the Streaming Random Patches regressor for evolving data streams\nas described in the forthcoming paper:\n\nOn Ensemble Techniques for Data Stream Regression.\nHeitor Gomes, Jacob Montiel, Saulo Martiello Mastelini,\nBernhard Pfahringer, and Albert Bifet.\nIJCNN'20. International Joint Conference on Neural Networks. 2020.\n\n""], 'url_profile': 'https://github.com/jacobmontiel', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'Austin, TX/Cambridge, MA', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Overview:\nThe goal of this project is to predict the optimal type of uncertainty set for a given dataset, to be used for linear regression. The optimal type of uncertainty set is defined as one which minimizes the out-of-sample mean squared error. Candidates for uncertainty set types include the Frobenius Norm, Frobenius Two, Frobenius Infinity, Induced One-Two, Induced Two-One, Induced Two-Infinity, and Induced Infinity-Two.\nProcedure:\nRead in R datasets and create a metadataset showing features of each individual dataset (such as number of observations, number of features, domain, etc). For each dataset in the metadataset, determine which type of uncertainty set minimizes out-of-sample mean squared error (this will be the y-variable to predict). Finally, build machine learning models to predict which type of uncertainty set is best for a given dataset.\nContributors:\nThis project was the final course project for Machine Learning Under a Modern Optimization Lens at MIT. I worked with Charlie Chimento, my project partner. I contributed the code to read in R Datasets, optimize over uncertainty sets and find the optimal uncertainty set for a given dataset, and to build Optimal Classification Tree and multinomial logistic regression models. Charlie contributed the code to engineer features, researched the statistical theory for the report, and wrote the final project paper.\n'], 'url_profile': 'https://github.com/desireewaugh', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Coronavirus-Prediction-Using-ML\nPrediction of Coronavirus cases using SVM and Linear Regression Models and plotting various graphs to highlight the ongoing Pandemic.\n'], 'url_profile': 'https://github.com/prathmeshs044', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '548 contributions\n        in the last year', 'description': ['Flipping Houses\nAn analysis of best predictors of home sale prices using the King County Housing dataset\nAuthor: Jessica Forrest-Baldini\nThe contents of this repository detail a multiple linear regression analysis using a range of features in the King County Housing dataset to best predict home sale prices.\nBusiness Problem\nA group of real estate investors are new to the area; King County, Washington, home to Seattle and many other towns, cities and neighborhoods. They want to know the best strategy for increasing the value of a home to maximize profits when flipping houses in King County.\nData\nkc_house_data.csv\nSource: https://www.kaggle.com/harlfoxem/housesalesprediction\nMethods\n\nMultiple Linear Regression Analysis\nAnalysis of the Coefficients\n\nSince many of the data columns/features were skewed, and the model residuals were not normally distributed, I decided to log-transform, normalize and min-max scale the data set.\nI also removed extreme outliers that were skewing the normality of the residuals by removing the top and bottom percentile of homes by price and square feet of the lot size. This removed about 2% of the data.\nTransforming the data made it so I could not predict exact impact on price, however I did an analysis of the coefficients using a coefficient plot (which I created a function for since seaborn depreciated their coefplot function previously), with 95% confidence intervals. This made it very apparent which features have the greatest impact on home sale prices, so I could effectively gain insights and make valuable strategy suggestions to the real estate investors.\nI searched for interactions and added the top interaction into the dataset, which became the best predictor by a landslide and contributed to insights and business strategy recommendations.\nResults\nThe most influential feature on home sale price was grade\nWhen adding in the top interaction, the most influential feature on home sale price was grade interacting with square feet of living space, which makes total sense and contributes to our business recommendations\nThere was a clear neighborhood effect at play as well, as we could see with latitude and average square feet of the 15 nearest neighbors’ homes\nRecommendations\nBased on the results and findings, I recommend:\n\nInvest in larger lower grade homes (relative to the neighborhood) and increase the homes grade according to the King County grading scale\nInvest in larger homes on the fringes of relatively more affluent neighborhoods that are bordering more average neighborhoods and increase the grade\nInvest in homes in neighborhoods that are being gentrified and increase the grade\n\nGrading system used in King County: https://info.kingcounty.gov/assessor/esales/Glossary.aspx?type=r\nIncreasing the grade is done by upgrading materials, fixtures and style (both interior and exterior) of the home.\nFurther Research\nFor further research I would recommend doing a neighborhood analysis with a map to gain an understanding of the different neighborhoods in King County; and their respective home price ranges, sizes, grades and conditions. This will allow the strategy outlined above to be applied most effectively.\nAlso, discover which neighborhoods are undergoing gentrification, or which neighborhoods border more affluent neighborhoods where homes on the fringes could be purchased and upgraded.\nhttps://www.governing.com/gov-data/seattle-gentrification-maps-demographic-data.html\nData for this can be found at:\nThank you!\nFor any additional questions, please feel free to connect with me at jlforrestbaldini@gmail.com or on LinkedIn at https://www.linkedin.com/in/jessica-forrest-baldini.\nSocial Preview Photo\nPhoto by Luca Micheli on Unsplash\n'], 'url_profile': 'https://github.com/JessicaFB', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Adaptive-Wing-Loss-for-Face-Alignment\nThis is a partial implementation of the paper ""Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression"" in Tensorflow.\nSome components have yet to be added, including boundary prediction and coordinate aggregation.\nThe current trained model is not included due to file size limitations.\n'], 'url_profile': 'https://github.com/andrewhou1', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['mytwitterproject_analyticsvidhya\n'], 'url_profile': 'https://github.com/durgarao98', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'Gurugram, India', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yashpixels', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Qingwen99', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Adaptive-Wing-Loss\nThis is a partial implementation of the paper ""Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression"" in Tensorflow.\nSome components have yet to be added, including boundary prediction and coordinate aggregation.\n'], 'url_profile': 'https://github.com/andrewhou1', 'info_list': ['R', 'Updated Jul 13, 2020', '2', 'Python', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', '1', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'MATLAB', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iamvibinvijay', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Machine-Learning-Project\nApplication of K-nn and Logical Regression Algorithms on ""First 10 Minute of Diamond Matches in League of Legends"" Dataset\n'], 'url_profile': 'https://github.com/emreutkusolak', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '181 contributions\n        in the last year', 'description': ['\nData analysis - Campus recruitment\nPerforming EDA and creating logistic regression to predict chances of securing job placement during a campus recruitment\nWhat do we see?\nIn this dataset, we explored the criteria to secure a job placement. Does student who excel academically have a higher chance to secure a job placement? Not really. My analysis have shown that specialisation place a much more significant role instead. It seems like students with specialisation related to finance have a higher chance of getting a job placement.\nDataset\nYou can download the dataset here(19 KB).\nContributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\nLicense\nMIT\n'], 'url_profile': 'https://github.com/kianweelee', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Gold-prices-prediction\nPrediction of Closing Prices of Gold using Machine Learning. Datasets from yahoo.\n'], 'url_profile': 'https://github.com/SurajSarangi', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in SciKit Learn\nIntroduction\nIn this lecture, we'll briefly introduce logistic regression with the SciKit Learn package.\nObjectives\nYou will be able to:\n\nUnderstand and implement logistic regression\nCompare testing and training errors\n\nGenerally, the process for implementing logistic regression via SciKit Learn is very similar to that which we previously saw. There are a couple exceptions to this. First, rather than using patsy, we simply define y and X by specifying the columns from the dataframe, with no special syntax necessary. That said, if you need to create dummy variables for categorical variables, you must do this in a previous step manually. (See below.) Secondly, SciKit Learn will not display statistical measures such as the P-values associated with the various features. This is a shortcoming of SciKit Learn, although SciKit Learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we failed to discuss previously is train test split. As we saw in linear regression, train test split is an essential part of model building in order to help determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing Logistic Regression in SciKit Learn using dummy variables and a proper train-test split.\nStep 1: Import the Data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nStep 2: Define X and y\nNote that we first have to create our dummy variables, and then can use these to define X and y.\ndf = pd.get_dummies(df)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbing, Mr. Anthony', 'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)',\n       ...\n       'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38', 'Cabin_F4',\n       'Cabin_G6', 'Cabin_T', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1731)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbing, Mr. Anthony\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\n...\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1731 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats])\ny = df.Survived\nX.head() #Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_female\nSex_male\nCabin_A10\nCabin_A14\nCabin_A16\nCabin_A19\n...\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 156 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\nX = X.fillna(value=0) #Fill null values\nfor col in X.columns:\n    X[col] = (X[col]-min(X[col]))/ (max(X[col]) - min(X[col])) #We subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_female\nSex_male\nCabin_A10\nCabin_A14\nCabin_A16\nCabin_A19\n...\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 156 columns\n\nTrain-Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an intial model to the training set. In SciKit Learn you do this by first creating an instance of the regression class. From there, then use the fit method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept = False, C = 1e12)\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n          fit_intercept=False, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs on our test set.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n#We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros.\nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    568\n1    100\nName: Survived, dtype: int64\n0    0.850299\n1    0.149701\nName: Survived, dtype: float64\n\nNot bad; our classifier was 85% correct for our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    180\n1     43\nName: Survived, dtype: int64\n0    0.807175\n1    0.192825\nName: Survived, dtype: float64\n\nAnd still 80% accurate for our test data!\nSummary\nIn this lesson, we took a more complete look at a data science pipeline for logistic regression, splitting the data into train and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before then having a more detailed discussion of more nuanced methods for evaluating our classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': [""GLMs: Multinomial and Poisson Regression, and the Proportional Odds Model\nThis report will explore 3 generalized linear models (GLM) for the analysis of categorical data: multinomial regression for multiclass classification, Poisson regression for analyzing count d\nata, and the proportional odds model for modeling cumulative probability distribution of a multiclass outcome. We will perform exploratory data analysis, extensive model exploration and selec\ntion, and inference. We will study cereal placement and its success as a marketing strategy, and predictors for alcohol consumption.\nData is taken from Chris Bilder's Textbook:\nBilder, Christopher R., and Thomas M. Loughin. Analysis of Categorical Data with R. CRC Press, 2015.\nData is provided on his website:\nhttp://www.chrisbilder.com/categorical/programs_and_data.html\n""], 'url_profile': 'https://github.com/siduojiang', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real ""learning algorithm"" that aspiring data scientists will come across. It is one of the simplest algorithms to master, but it still requires some mathematical and statistical understanding of the underlying regression process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nDescribe statistical modeling with simple regression\nExplain simple linear regression analysis as solving for straight line equation: $y=mx+c$\nCalculate the slope and y-intercept given a set of data points\nCalculate a regression line based on calculated slope and intercept\nPredict a target value for a previously unseen input feature, based on model coefficients\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more variables). Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows standard parametric assumptions like normality, linearity etc. These will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomenons or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nAs you learned in previous lesson, the term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variable can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one features to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best fit line"" in a simple linear regression context\nCalculating Regression Coefficients: Slope and Intercepts\nAs seen in the previous lesson, you remember that a straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll to keep in mind:\n\nA quick recap:\n\na dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t actually lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes that data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the blue diamonds there are our observations with associated x- and y-values.\nNow, when we draw our regression line based on these few blew diamonds, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the blue diamonds in the plot below. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least squares method.\nYou can visit this Wikipedia link to get take a look into the maths behind derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply slope value by mean of x and subtract the result from mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations in to draw a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, You learned the basics of simple linear regression between two variables as a problem of fitting a straight line to best describe the data associations on a 2-dimensional plane.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Analysis using Linear Algebra and Numpy - Code Along\nIntroduction\nIn the previous sections, we have seen that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among data entities (variables). Linear regression is a highly important predictive analytical tool that most data scientists use in their routine analyses. Here we shall try to develop a basic intuition for regression from a linear algebra perspective using vectors and matrices operations, quite similar to what we saw in previous lessons and labs. This lesson covers least squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou are advised to follow this link for a deeper mathematical and geometric understanding of the topic. Here we shall try to keep things in a more data oriented domain.\nObjectives\nYou will be able to:\n\nUnderstand the role of linear algebra towards regression modeling\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nDevelop simple linear algebraic models for simple and multivariate regression\n\nRegression Analysis\nWe know that the purpose of regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house etc.\nLet's use a very simple toy example to understand how this works with linear algebra. Say we are collecting data on total number sales per day for some business. Imagine we have got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, 'o')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2 dimensional space) to describe the relationship between data variables as shown in the example below:\n\nFollowing this, if we were to identify a relationship between the day and total number of sales, our goal would be to seek a function that describes this line, and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable . For this, we first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n\n$$y = mx+c$$\n\nwhere `c` is the intercept of the line and `m` denotes the slope, as shown below: \n\nWe can write the fitting function based on above as sales being a function of days.\n\nsales = f (days)\n\nOr in more general terms\n\ny = C + DX (from y = mx + c)\n\n\n(where y is the number of sales per day, x represents the day,  C (intercept) and D (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales)\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\nC + 1(D) = 1\n\n\nC + 2(D) = 2\n\n\nC + 3(D) = 2\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When x = 1, y = 1; and when x = 2, y = 2 i.e. we can draw a straight line passing through these points. When x = 3, b = 2, we  know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere would be some error between our line and the REAL relationship between these parameters.\n\nThis behavior is can be simulated by using numpy's polyfit() function (similar to statsmodels ols) to draw a regression line to our data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get b(intercept) and m(slope) . the degree parameter = 1 to show astraight line\nb, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, 'o')\nplt.plot(x, b + (m * x), '-')\nplt.xticks(x)\n\nplt.show()\nprint (b,m)\n#\xa0Code here \nThe numbers we see reflect our slope (0.5) and intercept values (0.66).\nThe line drawn above using this built in regression model clearly does not touch all the data points. Hence this would be considered an APPROXIMATION of the function we are trying to find. Now let's see how to achieve the same functionality with matrix algebra instead of polyfit function.\nCreate matrices and vectors\nA linear system like above tells us that we are now in a linear algebra domain. And we should only have to deal with a few vectors and matrices in order to solve such problems.\nRecalling a linear systems from previous lessons, we have:\n\nBias and Error terms\nThe extra column of ones in $A$ refers to the bias (or the intercept (c) from $mx+c$ shown above). If we don't include this constant value, then our function will have to go through the origin (0,0), which would seriously limit the types of relationships the model could describe. In machine learning, the size, or weight, of bias will be inferred by the learning algorithm. The 1s is just an arbitrary number that forms a basis for learning what the bias is going to be. (Visit the link at the bottom to learn more on this)\nIn above , we are hoping that there is some linear combination of the columns of A that gives us our vector of observed b values.\nUnfortunately, we already know $b$ does not fit our model perfectly. That means it is outside the column space of A and we can't solve that equation for the vector $x$ directly. Every line we draw will have some value of error e associated with it.\n\nThe goal is to choose the vector x for unknown variables to make e as small as possible.\n\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept, a and independent variable(s),along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written:\n\n**y = X b + e **\n\nThis says to get y (sales), multiply each X by the appropriate vector b (unknowns),then add error term. We create a matrix X, which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector b.\nLet's assume that error will equal zero on average and forget it to sketch a proof:\n\ny = X b\n\nNow we want to solve for b, so we need to get rid of X. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by XT:\n\nXT y = XT X b\n\nAnd now we have a square matrix that with any luck has an inverse, which we will call (XT X)-1 . Multiply both sides by this inverse, and we have\n\n(XT X)-1 XTy = (XTX)-1 (XT X b)\n\nIt turns out that a matrix multiplied by its inverse is the identity matrix (A-1A=I):\n\n(XT X)-1 XT y = I b\n\nSo if we solve for the b (called weights - or unknown variables), we find that\nb = (XT X)-1 XT y\nHere, we shall focus here on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{x}$ (x-hat), we need to solve the above equation\nRemember all above variables represent vectors. The elements of the vector x-hat are the estimated regression coefficients C and D we are looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve Xb = y. But if any of the observed points deviate from the model, we cant find a direct solution. So we multiply both sides by the transpose of X. The transpose of X times X will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet's use above formula to calculate a solution for our toy problem\n# Calculate the solution\n\nX = np.array([[1,1],[1,2],[1,3]])\ny = np.array([1,2,2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nSo our solution gives us an intercept of 0.6 and slope value 0.5. Let's see what we get if we draw a line with these values with given data.\n# Define data points\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, 'o')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), '-')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nSuccess.. There we have it, an approximated line function. Just like the one we saw with polyfit, by using simple matrix algebra.\nRegression with multiple variables\nSo above we saw how with simple regression we can draw a line on a 2d space to describe data relationships based on distribution of elements. If we perform similar function with multiple variables, we would have a parameter space that is not 2D. With 3 paramaters i.e. two input and one output feature, our fitting function would look like a plane as shown below:\n\nWhen we have more than one input variables, each data point can be seen as is a feature vector xi, composed of x1 , x2 , …, xm , where m is the total number of features (columns). For multiple regression, each data point should contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nWe can write this in general terms, as we saw earlier:\n\nX β = y\n\nWhere X are the input feature values, $\\beta$ represents the co-efficients and y is the output (value to be predicted). In a simple least-squares linear regression model we seek a vector $\\beta$ such that the product X$\\beta$ most closely approximates the outcome vector y.\nFor each value of input features xi, we can compute a predicted outcome value  as:\n\nThe general formula to compute the beta vector, following the intuition from linear regression shown above, looks like:\nb = (XT X)-1 XT y\nSo we see that the general solution involves taking matrix transpose, inverse, dot multiplications  on the lines of solving a linear system of equations.\nIn the next lab, we shall use a simple dataset and with above formulation for multivariate regression, we would try to fit a model to the data and see how well it performs.\nFurther Reading\nYou are strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with visual , as well as an indepth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, we had a gentle introduction to regression analysis and how we can use linear algebra to solve regression problems. We saw a toy example in the case of simple linear regression , relating days to number of sales and calculated a function that approximates the linear mapping. We also introduced the case of multiple input variables for the case of multiple regression and described it as matrix algebra. In th next lab, we shall use these equations to solve a real world problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lecture, you\'ll be introduced to the logistic regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, we\'ll go into more formal notation of logistic regression models. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nUnderstand and implement logistic regression\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationshio between two variables. An example is given below. In this example, we want to find a relationship between age and monthly income. It is definitely reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income"", fontsize=14)\nplt.show()\n<matplotlib.figure.Figure at 0x1058d4e80>\n\nIn linear regression, we\'d try to find a relationship between age and monthly income. Conceptually, we\'d try to fit a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c = ""black"")\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income"", fontsize=14)\nplt.show()\n\nThe idea is that we could use this line to make predictions in the future. In this case, we modeled the general relationship as follows: the extected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this gives us an indication of what we could expect.\nSo how is this related to logistic regression?\nNow, imagine you get a data set where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not thet earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin =income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nLet\'s have a look at what happens when we plot this.\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income (> or < 4000)"", fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not be exactly desired here, but let\'s still have a look at what happens when we formallty build a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# create linear regression object\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income"", fontsize=14)\nplt.plot(age, lin_income, c = ""black"")\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, let\'s look at a logistic regression model and fit that to our data.\n# Create logistic regression object\nregr = LogisticRegression(C=1e5)\n# Train the model using the training sets\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class=\'ovr\', n_jobs=1, penalty=\'l2\', random_state=None,\n          solver=\'liblinear\', tol=0.0001, verbose=0, warm_start=False)\n\n# store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n# create the linear predictor\nlin_pred= (age * coef + interc)\n# perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n#sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income"", fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c = ""black"")\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. In the next section we\'ll dive a little deeper into the mathematics of logistic regression models.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression class, a formally, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2, x_2 +\\ldots + \\beta_n x_n $$\nwhen we have $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$. $ \\hat y $ is a estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our data set) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to make a guess of what the probability is of belonging to one group versus another. And that exactly is what logistic regression models can do!\nessentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, The outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real data example\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\nsalaries = pd.read_csv(""salaries_final.csv"", index_col = 0)\nfrom patsy import dmatrices\ny, X = dmatrices(\'Target ~ Age  + C(Race) + C(Sex)\',\n                  salaries, return_type = ""dataframe"")\nimport statsmodels.api as sm\nlogit_model = sm.Logit(y.iloc[:,1], X)\nresult = logit_model.fit()\n/Users/lore.dirick/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools\n\n\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: Target[>50K]   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 21 Nov 2018   Pseudo R-squ.:      0.09666\n\n\nTime: 09:04:19   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\n      LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nIntercept    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nC(Race)[T.Asian-Pac-Islander]     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nC(Race)[T.Black]     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nC(Race)[T.Other]    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nC(Race)[T.White]     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nC(Sex)[T.Male]     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nnp.exp(result.params)\nIntercept                        0.011977\nC(Race)[T.Asian-Pac-Islander]    2.715861\nC(Race)[T.Black]                 1.198638\nC(Race)[T.Other]                 0.891987\nC(Race)[T.White]                 2.396965\nC(Sex)[T.Male]                   3.343142\nAge                              1.039480\ndtype: float64\n\nYou can also use scikit learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15)\nmodel_log = logreg.fit(X, y.iloc[:,1])\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n          fit_intercept=False, intercept_scaling=1, max_iter=100,\n          multi_class=\'ovr\', n_jobs=1, penalty=\'l2\', random_state=None,\n          solver=\'liblinear\', tol=0.0001, verbose=0, warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706343,  0.96178903,  0.14397984, -0.14384057,  0.83689458,\n         1.2067121 ,  0.03871011]])\n\nSummary\nIn this lab we built upon our previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression and briefly analyzed their output. In the upcoming lessons we will continue to investigate logistic regression from various viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into python functions. This will deepen and solidify your understanding of the logistic regression!\nObjectives\nYou will be able to:\n\nUnderstand and implement logistic regression\n\nOverview\nRecall that the logistic regression algorithm take our previous intuition from logistic regression. In logistic regression, we start by taking our input data, X and multiplying it by a vector of weights for each of the individual features, which produces our output y. Afterwards we\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear Regression Setup\nWrite a simple function predict_y that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$ \\hat{y}i = X{i1} \\bullet w_1 + X_{i2} \\bullet w_2 + X_{i3} \\bullet w_3 + ... + X_{in} \\bullet w_n$\nHint: think about which mathematical operation we previously discussed that will take a matrix (X) and multiply it by a vector of weights (w) to succinctly do this in a single operation.\n#Your code here\nThe Sigmoid Function\nRecall that the sigmoid function is used to map our previous linear regression model to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$\\frac{1}{1+e^(-x)}$\nWrite this as a python function where x is the input and the function outputs the result of the sigmoid function.\n#Your code here\nGraphing the Sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Graph the output of your sigmoid function using 10,000 X values evenly spaced from -20 to 20.\n#Your code here\nGradient Descent with the Sigmoid Function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, we are looking to minimize the error between our model\'s predictions and the actual data labels. To do this, we first calculate an error vector based on the current model\'s feature weights. We then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, we take the gradient, multiply it by our step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    #Create a for loop of iterations\n        #Generate predictions using the current feature weights\n        #Calculate an error vector based on these initial predictions and the correct labels\n        #Calculate the gradient \n        #As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        #Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        #For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(),error_vector) \n        #Update the weight vector take a step of alpha in direction of gradient \n    #Return finalized Weights\nRunning Your Algorithm\nNow that we\'ve coded everything from the ground up, we can further investigate the convergence behavior of our gradient descent algorithm. Remember that gradient descent does not gaurantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nLet\'s begin by running our algorithm and plotting the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to find train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence on stable weights.\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\nX = df[df.columns[:-1]]\ny = df.target\nprint(y.value_counts())\nX.head()\nsci-kit learn\nFor comparison, import sci-kit learn\'s standard LogisticRegression function. Initialize a regression object with no intercept and with C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and we have not performed any regularization such as Lasso or Ridge (sci-kit learn uses l2 by default). The high value of C will essentially negate this.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by sci-kit learn.\n# Your code here\nLevel - Up\nUpdate the gradient descent algorithm to also return the prediction error after each iteration. Then rerun the algorithm and create a graph displaying the prediction errors versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, we\'ll continue to explore this from a few more angles, plotting our data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression - Lab\nIntroduction\nRegression analysis forms the basis of machine learning experiments. Understanding regression will help you to get the foundations of most machine learing algorithms. Ever wondered what\'s at the heart of an artificial neural network processing unstructured data like music and graphics? It can be linear regression!\nObjectives\nYou will be able to:\n\nCalculate the slope of a line using standard slope formula\nCalculate the y-intercept using the slope value\nDraw a regression line based on calculated slope and intercept\nPredict the label of a previously unseen data element\n\nLet\'s get started\nA first step towards understanding regression is getting a clear idea about ""linear"" regression and basic linear algebra.\nThe calculation for the best-fit line\'s slope, m is calculated as :\n\nAs in our previous lesson, let\'s break down the formula into its parts. First we shall import the required libraries and define some data points to work with. We shall first create some toy data as numpy arrays. Let\'s do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use(\'ggplot\')\n\n# Initialize vectors X and Y with given values and create a scatter plot\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nShow a scatter plot between X and Y and comment on the output\n# Scatter plot\n\n# Your observations about relationship in X and Y \n\n\n\n#\nIn a data analysis context, we can think of these points as two vectors:\n\nvector X: the features of our model\nvector Y: the labels for given features\n\nWrite a function calc_slope()\nWrite a function calc_clope() that takes in x and y vectors and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line\'s y-intercept is:\n\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope using above above  and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slop and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, b = best_fit(X,Y)\n# m,b\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and b as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that y=mx+b. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using Y= mX+b for each point in X.\ndef reg_line (m, b, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,b,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\n\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in words\n# Your answer here\n\n\nPredicting label for new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of y according to our model. Recall the formula\n\nLet\'s try to find a y prediction for a new value of x = 7 and unknown y, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with actual data and regression line\n# Plot as above and show the predicted value\n\nWe now know how to create our own models, which is great, but we\'re stilling missing something integral: how accurate is our model? This is the topic for discussion in the next lab.\nSummary\nIn this lesson, we learnt how we can draw a best fit line for given data labels and features, by first calculating the slope and intercept. The calculated regression line was then used to predict the label (y-value) of a previously unseen feature (x-value). The lesson uses a simple set of data points for demonstration. Students should be able to plug in other datasets and practice with predictions for accuracy.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dhruval-p', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'New Dehli, Delhi', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['Analyzing-peoples-behavior-on-Ads-through-Logistics-Regression\n'], 'url_profile': 'https://github.com/yashugarg', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ordinary Least Squares in Statsmodels (OLS) - Lab\nIntroduction\nIn the previous code along, we looked all the requirements for running an ols simple regression using statsmodels. We worked with a toy example to understand the process and all the necessary steps that must be performed. In this lab , we shall look at a slightly more complex example to study the impact of spendings in different advertising channels of total sales.\nObjectives\nYou will be able to:\n\nSet up an analytical question to be answered by regression analysis\nStudy regression assumptions for real world datasets\nVisualize the results of regression analysis\n\nLet\'s get started\nIn this lab, we will work with the ""Advertising Dataset"" which is a very popular dataset for studying simple regression. The dataset is available at Kaggle, but we have already downloaded for you. It is available as ""Advertising.csv"". We shall use this dataset to ask ourselves a simple analytical question:\nThe Question\nWhich advertising channel has a strong relationship with sales volume, and can be used to model and predict the sales.\nStep 1: Read the dataset and inspect its columns and 5-point statistics\n# Load necessary libraries and import the data\n# Check the columns and first few rows\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTV\nradio\nnewspaper\nsales\n\n\n\n\n1\n230.1\n37.8\n69.2\n22.1\n\n\n2\n44.5\n39.3\n45.1\n10.4\n\n\n3\n17.2\n45.9\n69.3\n9.3\n\n\n4\n151.5\n41.3\n58.5\n18.5\n\n\n5\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n# Get the 5-point statistics for data \n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTV\nradio\nnewspaper\nsales\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n147.042500\n23.264000\n30.554000\n14.022500\n\n\nstd\n85.854236\n14.846809\n21.778621\n5.217457\n\n\nmin\n0.700000\n0.000000\n0.300000\n1.600000\n\n\n25%\n74.375000\n9.975000\n12.750000\n10.375000\n\n\n50%\n149.750000\n22.900000\n25.750000\n12.900000\n\n\n75%\n218.825000\n36.525000\n45.100000\n17.400000\n\n\nmax\n296.400000\n49.600000\n114.000000\n27.000000\n\n\n\n\n# Describe the contents of this dataset\nStep 2: Plot histograms with kde overlay to check for the normality of the predictors\n# For all the variables, check if they hold normality assumption\n\n\n\n\n# Record your observations on normality here \nRemember . Nothing is perfect . So be positive\n\nStep 3: Test for the linearity assumption.\nUse scatterplots to plot each predictor against the target variable\n# visualize the relationship between the preditors and the target using scatterplots\n\n# Record yor observations on linearity here \nConclusion so far !\nBased on above initial checks, we can confidently say that TV and radio appear to be good predictors for our regression analysis. Newspaper is very heavily skewed and also doesnt show any clear linear relationship with the target.\n\nWe shall move ahead with our analysis using TV and radio , and count out the newspaper due to the fact that data violates ols assumptions\n\nNote: Kurtosis can be dealt with using techniques like log normalization to ""push"" the peak towards the center of distribution. We shall talk about this in the next section.\nStep 4: Run a simple regression in statsmodels with TV as a predictor\n# import libraries\n\n# build the formula \n\n# create a fitted model in one line\nStep 5: Get regression diagnostics summary\n\n\nOLS Regression Results\n\nDep. Variable: sales   R-squared:             0.612\n\n\nModel: OLS   Adj. R-squared:        0.610\n\n\nMethod: Least Squares   F-statistic:           312.1\n\n\nDate: Fri, 12 Oct 2018   Prob (F-statistic): 1.47e-42\n\n\nTime: 21:04:59   Log-Likelihood:      -519.05\n\n\nNo. Observations:    200   AIC:                   1042.\n\n\nDf Residuals:    198   BIC:                   1049.\n\n\nDf Model:      1    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept     7.0326     0.458    15.360  0.000     6.130     7.935\n\n\nTV     0.0475     0.003    17.668  0.000     0.042     0.053\n\n\n\n\nOmnibus:  0.531   Durbin-Watson:         1.935\n\n\nProb(Omnibus):  0.767   Jarque-Bera (JB):      0.669\n\n\nSkew: -0.089   Prob(JB):              0.716\n\n\nKurtosis:  2.779   Cond. No.               338.\n\n\nRecord your observations on ""Goodness of fit""\nNote here that the coefficients represent associations, not causations\nStep 6:  Draw a prediction line with data points omn a scatter plot for X (TV) and Y (Sales)\nHint: We can use model.predict() functions to predict the start and end point of of regression line for the minimum and maximum values in the \'TV\' variable.\n# create a DataFrame with the minimum and maximum values of TV\n\n# make predictions for those x values and store them\n\n\n# first, plot the observed data and the least squares line\n      TV\n0    0.7\n1  296.4\n0     7.065869\n1    21.122454\ndtype: float64\n\n\nStep 7: Visualize the error term for variance and heteroscedasticity\n\n\n# Record Your observations on residuals\nNext, repeat above for radio and go through the same process, recording your observations\n\nR-Squared: 0.33203245544529525\nIntercept    9.311638\nradio        0.202496\ndtype: float64\n\n\n\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: sales   R-squared:             0.332\n\n\nModel: OLS   Adj. R-squared:        0.329\n\n\nMethod: Least Squares   F-statistic:           98.42\n\n\nDate: Fri, 12 Oct 2018   Prob (F-statistic): 4.35e-19\n\n\nTime: 20:52:55   Log-Likelihood:      -573.34\n\n\nNo. Observations:    200   AIC:                   1151.\n\n\nDf Residuals:    198   BIC:                   1157.\n\n\nDf Model:      1    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept     9.3116     0.563    16.542  0.000     8.202    10.422\n\n\nradio     0.2025     0.020     9.921  0.000     0.162     0.243\n\n\n\n\nOmnibus: 19.358   Durbin-Watson:         1.946\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     21.910\n\n\nSkew: -0.764   Prob(JB):           1.75e-05\n\n\nKurtosis:  3.544   Cond. No.               51.4\n\n\n# Record your observations here for goodnes of fit \nThe Answer\nBased on above analysis, we can conclude that none of the two chosen predictors is ideal for modeling a relationship with the sales volumes. Newspaper clearly violated normality and linearity assumptions. TV and radio did not provide a high value for co-efficient of determination - TV performed slightly better than the radio. There is obvious heteroscdasticity in the residuals for both variables.\n\nWe can either look for further data, perform extra pre-processing or use more advanced techniques.\n\nRemember there are lot of technqiues we can employ to FIX this data.\nWhether we should call TV the ""best predictor"" or label all of them ""equally useless"", is a domain specific question and a marketing manager would have a better opinion on how to move forward with this situation.\nIn the following lesson, we shall look at the more details on interpreting the regression diagnostics and confidence in the model.\nSummary\nIn this lesson, we ran a complete regression analysis with a simple dataset. We looked for the regression assumptions pre and post the analysis phase. We also created some visualizations to develop a confidence on the model and check for its goodness of fit.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Polynomial Regression - Lab\nIntroduction\nIn this lab you'll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUnderstand how to account for non-linear relationships between predictors and target variable using polynomial terms\n\nCreate the best plot using polynomials!\nBelow, we created a plot with a clearly non-linear shape.\n\nplot a polynomial function using PolynomialFeatures for polynomials up until the second, third and fourth degree.\nprint out the $R^2$ value for each of the three results. Draw conclusions with respect to which degree is best.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n% matplotlib inline\n\ndef pol(x):\n    return x * np.cos(x)\n\nx = np.linspace(0, 12, 100)\nrng = np.random.RandomState(1234)\nrng.shuffle(x)\nx = np.sort(x[:25])\ny = pol(x) + np.random.randn(25)*2\n\nplt.scatter(x, y, color='green', s=50, marker='.')\n\nplt.show();\n\nSolution\n## your code here\nSummary\nGreat! You now know how to include polynomials in your linear model!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nAs we saw with KNN, we need alternative evaluation metrics to determine the effectiveness of classification algorithms. In regression, we were predicting values so it made sense to discuss error as a distance of how far off our estimates were. In classifying a binary variable however, we are either correct or incorrect. As a result, we tend to deconstruct this as how many false positives versus false negatives we come across.\nIn particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this review lab, we\'ll review precision, recall and accuracy in order to evaluate our logistic regression models.\nObjectives\nYou will be able to:\n\nUnderstand and assess precision recall and accuracy of classifiers\nEvaluate classification models using various metrics\n\nTerminology Review\nLet\'s take a moment and review some classification evaluation metrics:\n$Precision = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}}$\n$Recall = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}}$\n$Accuracy = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}}$\n\nAt times, we may wish to tune a classification algorithm to optimize against precison or recall rather then overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is much preferable to optimize for precision, the number of cancer positive cases, then it is to optimize recall, the percentage of our predicted cancer-risk patients who are indeed positive.\n1. Split the data into train and test sets\nimport pandas as pd\ndf = pd.read_csv()\n#Your code here\n2. Create a standard logistic regression model\n#Your code here\n3. Write a function to calculate the precision\ndef precision(y_hat, y):\n    #Your code here\n4. Write a function to calculate the recall\ndef recall(y_hat, y):\n    #Your code here\n5. Write a function to calculate the accuracy\ndef accuracy(y_hat, y):\n    #Your code here\n6. Calculate the precision, recall and accuracy of your classifier\nDo this for both the train and the test set.\n#Your code here\n7. Comparing Precision Recall and Accuracy of Test vs Train Sets\nPlot the precision, recall and accuracy for test and train splits using different train set sizes. What do you notice?\nimportimport  matplotlib.pyplotmatplot  as plt\n%matplotlib inline\ntraining_Precision = []\ntesting_Precision = []\ntraining_Recall = []\ntesting_Recall = []\ntraining_Accuracy = []\ntesting_Accuracy = []\n\nfor i in range(10,95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) #replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None\n\n# 6 lines of code here\nCreate 3 scatter plots looking at the test and train precision in the first one, test and train recall in the second one, and testing and training accuracy in the third one.\n# code for test and train precision\n# code for test and train recall\n# code for test and train accuracy\nSummary\nNice! In this lab, you gained some extra practice with evaluation metrics for classification algorithms. You also got some further python practice by manually coding these functions yourself, giving you a deeper understanding of how they work. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhishek285', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'New Dehli, Delhi', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['Applying-Linear-Regression-on-a-Ecommerce-customer-dataset\n'], 'url_profile': 'https://github.com/yashugarg', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'MOHALI, PUNJAB', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sudhirsolanki', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this final lab, we shall apply the regression analysis and diagnostics techniques covered in this section to a familiar ""Boston Housing"" dataset. We performed a detailed EDA for this dataset in earlier section and hence carry a good understanding of how this dataset is composed. This this lab we shall try to identify the predictive ability of some of features found in this dataset towards identifying house price.\nObjectives\nYou will be able to:\n\nBuild many linear models with boston housing data set using OLS\nFor each model, analyze OLS diagnostics for model validity\nVisually explain the results and interpret the diagnostics from Statsmodels\nComment on the goodness of fit for a simple regression model\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as pandas dataframe.\n# Your code here\nThis dataset contains a number of features that can be used to explain the medv target variable. A full description of the various features is available at KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not ?)\n# Your code here \n\n# You observations here \nBased on this , we shall choose a selection of features which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck for linearity assumption for all chosen features with target variable using scatter plots and comment on the results\n# Your code here \n\n\n\n\n\n# Your observations here \nOkie so obviously our data needs a lot of pre-procesing to improve the results. This key behind such kaggle competitions is to process the data in such a way that we can identify the relationships and make predictions in the best possible way. For now, we shall leave the dataset untouched and just move on with regression. So far, our assumptions, although not too strong, but still hold to a level that we can move on.\nLet\'s do Regression\nRight here is the real deal. Let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). We shall do this is a loop and in every iteration, we shall pick one of the independent variables  perform following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog().\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your obervations here \nSo clearly the results are not highly reliable. the best good of fit i.e. r-squared is witnessed with rm. So clearly in this analysis this is our best predictor.\n\nSo how can we improve upon these results\n\nPre-Processing\n\nThis is where pre-processing of data comes in. Dealing with outliers, normalizing data, scaling values etc can help regression analysis get more meaningful results from the given set of data\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis techniques and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. In the next section we shall look at multiple regression where we can use multiple features AT ONCE to define a relationship with outcome. We shall also look at some pre-processing and data simplification techniques and re-visit the boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the the goodness of fit.\n\nSummary\nIn this lab, we attempted to bring in all the skills learnt so far to a slighlt detailed dataset. We looked at the outcome of our analysis and realized that the data might need some pre-processing to see a clear improvement in results. We shall pick it up in the next section from this point and bring in data pre-processing techniques along with some assumptions that are needed for multiple regression .\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, we shall apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. We shall follow the approach highlighted in previous lesson where we used numpy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. We shall also evaluate how good our model fit was.\nIn order to make this experiment interesting. We shall use NumPy at every single stage of this experiment i.e. loading data, creating matrices, performing test train split, model fitting and evaluations.\nObjectives\nYou will be able to:\n\nUse linear algebra to apply simple regression modeling in Python and NumPy only\nApply train/test split using permutations in NumPy\nUse matrix algebra with inverses and dot products to calculate the beta\nMake predictions from the fitted model using previously unseen input features\nEvaluate the fitted model by calculating the error between real and predicted values\n\nFirst let\'s import necessary libraries\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset we will use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage) etc. and an output (dependent) variable, price. We shall formulate a linear algebra problem to find linear mappings from input to out features using the equation provided in the previous lesson.\nThis will allow us to find a relationship between house features and house price for the given data, allowing us to find unknown prices for houses, given the input features.\nA description of dataset and included features is available at THIS LINK\nIn your repo, the dataset is available as windsor_housing.csv containing following variables:\nthere are 11 input features (first 11 columns):\nlotsize\tbedrooms\tbathrms\tstories\tdriveway\trecroom\tfullbase\tgashw\tairco\tgaragepl\tprefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how we can perform a regression experiment, similar to one we performed in statsmodels, using mathematical manipulations. So we we wont be using any Pandas or statsmodels goodness here. The key objectives here are to a) understand regression with matrix algebra, and b) Mastery in NumPy scientific computation.\nStage 1: Prepare Data for Modeling\nLet\'s give you a head start by importing the dataset. We shall perform following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time.\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values).\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a numpy array and inspect first few rows\n\n\nNOTE: read.csv() would read the csv as a text file, so we must convert the contents to float at some stage.\n# Your Code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 test train Split\nExplore NumPy\'s official documentation to manually split a dataset using numpy.random.shuffle(),  numpy.random.permutations() or using simple resampling method.\n\nPerform a RANDOM 80/20 split on data using a method of your choice , in NumPy using one of the methods above.\nCreate x_test, y_test, x_train and y_train arrays from the split data.\nInspect the contents to see if the split performed as expected.\n\n# Your code here \n\n\n\n# Split results\n\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith our X and y in place, We can now compute our beta values with x_train and y_train as:\n$\\beta$ = (x_trainT . x_train)-1 . x_trainT . y_train\n\nUsing numpy operations (transpose, inverse) that we saw earlier, compute the above equation in steps.\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n# Calculated beta values\n\n# [-3.07118956e+03  2.13543921e+00  4.04283395e+03  1.33559881e+04\n#   5.75279185e+03  7.82810082e+03  3.73584043e+03  6.51098935e+03\n#   1.28802060e+04  1.09853850e+04  6.14947126e+03  1.05813305e+04]\n[7.13503312e+03 3.14678249e+00 3.28225971e+02 1.28839251e+04\n 6.68332755e+03 3.44682885e+03 3.19440776e+03 5.07824499e+03\n 1.32822228e+04 1.10098716e+04 2.88253206e+03 9.16916600e+03]\n\nStep 4: Make Predictions\nGreat , we now have a set of coefficients that describe the linear mappings between X and y. We can now use the calculated beta values  with the test datasets that we left out to calculate y predictions.\nFor this we need to perform the following tasks:\nNow we shall all features in each row in turn and multiply it with the beta computed above. The result will give a prediction for each row which we can append to a new array of predictions.\n$\\hat{y}$ = x.$\\beta$ = $\\beta$0 + $\\beta$1 . x1 + $\\beta$2 . x2 + .. + $\\beta$m . xm\n\nCreate new empty list (y_pred) for saving predictions.\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row.\nAppend the predictions to y_pred.\nPrint the new set of predictions.\n\n#\xa0Your code here \nStep 5: Evaluate Model\nVisualize Actual vs. Predicted\nThis is exciting, so now our model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\n\nThis doesn\'t look so bad, does it ? Our model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help us calculate the RMSE value (Root Mean Squared Error) for our model.\nRoot Mean Squared Error\nHere is the formula for this again.\n\n\nInitialize an empty array err.\nfor each row in y_test and y_pred, take the squared difference and append error for each row in err array.\nCalculate RMSE from err using the formula shown above.\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n\n# RMSE = 16401.913562758735\nNormalized Root Mean Squared Error\nThe above error is clearly in terms of the dependant variable i.e. the final house price. We can also use a normlized mean squared error in case of multiple regression which can be calculated from RMSE using following formula:\n\nCalculate normalized Root Mean Squared Error\n\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n\n# 0.09940553674399233\n0.09940553674399233\n\nSO there it is. A complete multiple regression analysis using nothing but numpy. Having good programming skills in numpy would allow to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques we saw here, we can easily build a whole neural network from scratch.\nLevel up - Optional\n\nCalculated the R_squared and adjusted R_squared for above experiment.\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedascticity.\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost.\n\nSummary\nSo there we have it. A predictive model for predicting house prices in a given dataset. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. We still have a number of shortcomings in our modeling approach and we can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kristybell', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,853 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '1,824 contributions\n        in the last year', 'description': ['Pycaret\nreference  :   https://github.com/pycaret\nCREDITS TO MOEZ ALI\n'], 'url_profile': 'https://github.com/DheerajKumar97', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['bitcoincovid\nThis project was completed with classmates for our final project in DS3000. My main contributions to this project were scraping and cleaning COVID19 data, splitting our data into various training and testing sets, running RFE feature selection to extract important financial features, testing various regression algorithms on our training and testing sets, interpreting the results of each regression and supporting the optimization of the regression algorithms.\nPlease click on the JupyNB link to view the code. Thank you and enjoy!\n'], 'url_profile': 'https://github.com/kristenflaherty', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Port Harcourt', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/meoclark', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': [""AQ-Optimization\nCode (without data) used to create regression models used to predict audience quality (AQ), a very important KPI for healthcare marketers' pharma clients\nAQ Predictions\n\nUsed to predict AQ data in advance so that HU has a better idea of how close they are to meeting their target goals\n\nContent Optimization\n\nUsing factors in the AQ prediction model, this code gives quality scores to specific articles. Scores are used to optimize promoted content\n\n""], 'url_profile': 'https://github.com/jarmand-caw', 'info_list': ['Updated Apr 27, 2020', 'Updated Apr 27, 2020', 'HTML', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': [""AQ-Optimization\nCode (without data) used to create regression models used to predict audience quality (AQ), a very important KPI for healthcare marketers' pharma clients\nAQ Predictions\n\nUsed to predict AQ data in advance so that HU has a better idea of how close they are to meeting their target goals\n\nContent Optimization\n\nUsing factors in the AQ prediction model, this code gives quality scores to specific articles. Scores are used to optimize promoted content\n\n""], 'url_profile': 'https://github.com/jarmand-caw', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Context\nThe two datasets are related to red and white variants of the Portuguese ""Vinho Verde"" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\nThese datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\nThis dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality.\nContent\nFor more information, read [Cortez et al., 2009].\nInput variables (based on physicochemical tests):\n1 - fixed acidity\n2 - volatile acidity\n3 - citric acid\n4 - residual sugar\n5 - chlorides\n6 - free sulfur dioxide\n7 - total sulfur dioxide\n8 - density\n9 - pH\n10 - sulphates\n11 - alcohol\nOutput variable (based on sensory data):\n12 - quality (score between 0 and 10)\nWhat\'s interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as \'good/1\' and the remainder as \'not good/0\'.\nThis allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value.\nWithout doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm)\nKNIME is a great tool (GUI) that can be used for this.\n1 - File Reader (for csv) to linear correlation node and to interactive histogram for basic EDA.\n2- File Reader to \'Rule Engine Node\' to turn the 10 point scale to dichtome variable (good wine and rest), the code to put in the rule engine is something like this:\n$quality$ > 6.5 => ""good""\nTRUE => ""bad""\n3- Rule Engine Node output to input of Column Filter node to filter out your original 10point feature (this prevent leaking)\n4- Column Filter Node output to input of Partitioning Node (your standard train/tes split, e.g. 75%/25%, choose \'random\' or \'stratified\')\n5- Partitioning Node train data split output to input of Train data split to input Decision Tree Learner node and\n6- Partitioning Node test data split output to input Decision Tree predictor Node\n7- Decision Tree learner Node output to input Decision Tree Node input\n8- Decision Tree output to input ROC Node.. (here you can evaluate your model base on AUC value)\nInspiration\nUse machine learning to determine which physiochemical properties make a wine \'good\'!\n'], 'url_profile': 'https://github.com/siddharthswaroop13', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Machine learning models on various 21 datasets (from UCI and Kaggle)\nThe project is based on datasets from various sectors namely finance, health, industrial, crime, education, social media, biology, product and multimedia from the UCI repository and Kaggle.\nPreprocessed the data, split dataset into training and test set, if required and finally transformed it.\n\n\nTrained and evaluated 8 classification methods across 10 classification datasets, 7 regression methods across 10 regression datasets and 2 classification methods (Convolutional Neural Network and Decision Tree Classifier) on an image classification dataset (CIFAR10).\n\n\nUsed selected hyperparameters to search over and find the best combination of them using Grid Search and Randomized Search as required to improve model’s accuracy.\n\n\nIDE used: jupyter notebook.\n/classification-models/\n\t 10 jupyter notebooks per classification dataset each named as model.ipynb\n\n/regression-models/\n\t10 jupyter notebooks per regression dataset each named as model.ipynb\n\n/Classifier interpretability/\n\tmodel.ipynb - CNN and Decision Tree Implemented on CIFAR10 dataset\n\tdataset/ - please put the CIFAR10 dataset inside this folder\n\t\n\n1. To run the entire project run following python script:\ntype this command in terminal: python main.py\n\n-It will start executing each jupyter notebook sequentially, a \n message regarding which current dataset model is being executed\n\n2. To see the results, please check each jupyter notebook after it completes its execution.\nTech Stack\n\nPython 3\nJupyter Notebook\nscikit-learn\nNumPy\npandas\nMatplotlib\nSeaborn\npytorch\n\n'], 'url_profile': 'https://github.com/ArjumanMansuri', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['Sarcasm-Discord-Bot\nA basic Discord bot written in Python that detects whether or not a given statement is sarcastic. (~83% accuracy, model based on Logistic Regression)\nDeveloped by acridexception#5064. (Feel free to add me on Discord :))\nUsage\nInstall dependencies from requirements.txt.\nThen, go to Discord Developers and create a new application. After filling out the details, click on ""Create a Bot User"" and confirm.\nGet the bot\'s token and paste in a file called token.txt stored in the same location as bot.py.\nThen go to OAuth2 and select ""bot"" and copy the URL generated, then paste in your browser. Select the server you want to add it to and click ""authorize"". You should be good to go!\nOnce the bot is in your server, you can call it using:\n!detect : predicts whether the latest message in the channel is sarcastic.\n!detect <string> : predicts whether the given string is sarcastic.\n\nSample Usage\n\n'], 'url_profile': 'https://github.com/ananda-sreenidhi', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'Marseille, France', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sellamiakrem', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'Ankara, Turkey', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['classification-models\nimplementatin of knn, decision tree, support vector machine and logistic regression on the same dataset to see which one produces the best result.\nThis work is the final assignment of an online course called Machine Learning with Python, which is provided by IBM. I used different classification methods to\ncheck different evaluation accuracies.\nI worked on a loan dataset to predict a binary classification. If a loan taken out by a customer is paidoff or goes to collection.\nOnehot encoding, data standardization, seaborn visualization, data cleansing are implemented thorughout the project.\nDecision tree model among all seems to work best on this specific case with the accuracy scores: jaccard score 0.77 and f1 score:0.72\n'], 'url_profile': 'https://github.com/CihanErsoy', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'Cape Coast', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maxwellsarpong', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '448 contributions\n        in the last year', 'description': [""Stock Price Prediction Using LSTM Network\nResearch in the field of Stock Prediction isn't a new topic of interest. Stock prediction has been focused on since a\nlong period of time.\nSo what was the means of prediction of the stock prices when we didn't have sequential models that\nunderstand the dependency of the current data on the previous data over the time sequence?\nWell before the use of Deep Learning in the field of Stock Prediction, the process of building robust models for Stock Price Prediction had started. Several methods had been developed like,\n\nLast Value Method\nLinear Regression\nMoving Average\nAutoRegressive Integrated Moving Average (ARIMA)\nProphet\n\nBut Sequence Models changed everything and led to a massive improvement in state-of-the-art results in the finance industry.\nstock_prediction_using_sequence_models.ipynb includes Stock Prediction using an LSTM network\nStock_Prediction_Using_Different_Methods.ipynb includes Stock Price Prediction using methods :\n\nLast Value Method\nMoving Average\nLinear Regression\nExponentially Moving Average\n\n""], 'url_profile': 'https://github.com/shreyanshchordia', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/avads1', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['ADMISSION\nThis dataset is created for prediction of Graduate Admissions from an Indian perspective.\nThe dataset contains several parameters which are considered important during the application for Masters Programs.\nThe parameters included are :\nGRE Scores ( out of 340 )\nTOEFL Scores ( out of 120 )\nUniversity Rating ( out of 5 )\nStatement of Purpose and Letter of Recommendation Strength ( out of 5 )\nUndergraduate GPA ( out of 10 )\nResearch Experience ( either 0 or 1 )\nChance of Admit ( ranging from 0 to 1 ).\nThis dataset is inspired by the UCLA Graduate Dataset.\nThe test scores and GPA are in the older format. The dataset is owned by Mohan S Acharya and was downloaded from Kraggle.\nThe model here focusses on the chance of ADMISSION based on the other factors as stated above , inform of a Regression Model by comparing the adjustedR-Squared of all the models.\nThis also includes the entilation of Decision Tree and Support Vector Machine (of Linear,Polynomial & Radial type Kernel).\n'], 'url_profile': 'https://github.com/Jogesh-Mishra', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', '1', 'Python', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Python', 'Updated Sep 30, 2020', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'R', 'Updated Apr 27, 2020']}"
"{'location': 'Hamirpur, H.P.', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': [""House-Price-in-King-County_ANN\nIt is a regression model trained by me to predict house prices in King County(Seattle), with the help of ANN's using Tensorflow 2.0 and keras.\nThe dataset used is taken from kaggle. Link-https://www.kaggle.com/harlfoxem/housesalesprediction\nFeature Columns\nid - Unique ID for each home sold\ndate - Date of the home sale\nprice - Price of each home sold\nbedrooms - Number of bedrooms\nbathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower\nsqft_living - Square footage of the apartments interior living space\nsqft_lot - Square footage of the land space\nfloors - Number of floors\nwaterfront - A dummy variable for whether the apartment was overlooking the waterfront or not\nview - An index from 0 to 4 of how good the view of the property was\ncondition - An index from 1 to 5 on the condition of the apartment,\ngrade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\nsqft_above - The square footage of the interior housing space that is above ground level\nsqft_basement - The square footage of the interior housing space that is below ground level\nyr_built - The year the house was initially built\nyr_renovated - The year of the house’s last renovation\nzipcode - What zipcode area the house is in\nlat - Lattitude\nlong - Longitude\nsqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors\nsqft_lot15 - The square footage of the land lots of the nearest 15 neighbors\n""], 'url_profile': 'https://github.com/iamchetansharma8', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['IBM_MachineLearning_Capstone_project\nThis is a  Machine Learning project in which I used  machine learning Algorithms like KNN, SVM, DecisionTrees, and Logistic Regression with Python libraries like scikit-learn.\n'], 'url_profile': 'https://github.com/nishikantgurav', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Causal-Inference---Effect-of-premium-tech-support-on-customer-churn\nInferred causal effect of premium tech support on customer churn rate for a telecom company using Propensity Score Matching, Power Test and regression model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/anunay1992', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'Algiers, Algeria', 'stats_list': [], 'contributions': '227 contributions\n        in the last year', 'description': ['Sentiment Analysis with scikit-learn\nIn this notebook I built a logistic regression model to classify movie reviews as either positive or negative. I used the popular IMDB data set. The goal is to use a simple logistic regression estimator from scikit-learn for document classification.\nKey steps:\n\nClean and pre-process text data.\nPerform feature extraction with The Natural Language Toolkit (NLTK).\nBuild and employ a logistic regression classifier using scikit-learn.\nTune model hyperparameters and evaluate model accuracy.\n\nData Owner: https://ai.stanford.edu/~amaas/data/sentiment/\n\n'], 'url_profile': 'https://github.com/KhaledMEB', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['LOAN_PREDICTION\nUsed 3 models logistic regression , decision tree and random forest to predict whether to provide loan to a customer or not\n'], 'url_profile': 'https://github.com/Mitesh-C', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Tutorials\n“Any fool can know. The point is to understand.” ― Albert Einstein\n'], 'url_profile': 'https://github.com/SophMarch', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""Classifier for Seattle Airbnb's\nTable of Contents\n\nInstallation\nProject Motivation\nFile Descriptions\nResults\nLicensing, Authors, and Acknowledgements\n\nInstallation \nThere should be no necessary libraries to run the code here beyond the Anaconda distribution of Python.  The code should run with no issues using Python versions 3.*.\nProject Motivation\nIn this project I explore the Seattle's Airbnb dataset, which is available in Kaggle, you can download it from https://www.kaggle.com/airbnb/seattle.\nThe main purpose of this project is try to get some insights from the Airbnb rental market, which could be useful for people who already use this platform or is thinking about using for renting a property.\nIn order to do so, I asked my self the following questions, and tried subsequently to answer them using the dataset. The questions are:\n\nThe pricier the property listed, the better are the reviews?\nDo owners of top-performance properties use different words from the rest in their descriptions?\nCan we fit a classification model to the data ,with the purpose of predicting whether a rental is going to have a perfect rating?\n\nFile Descriptions \nThere is only one Jupyter notebook for this project, and it contains all the code necessary.\nResults\nThe main findings of the code can be found at the post available here.\nLicensing, Authors, Acknowledgements\nI am the only author.\nFeel free to use this project in the best way you can :)\n""], 'url_profile': 'https://github.com/cherencia', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['HAI-Infections\nLinear Regression Analysis on Nosocomial Infections using a dataset from an extract from the Study on the Efficacy of Nosocomial Infection Control (SENIC)\nThis was an assignment in my Stastical Computer Packages course at the George Washington University, and I learned a lot about feature selection using wrapper methods and machine learning techniques in Regression because of this project.\nCredits to Vikashraj Luhaniwal from TowardDataScience.com for his article on Feature Selection Using Wrapper Methods for the feature selection code I use in this  notebook. Also credits to Dr. Tirthajyoti Sarkar from TowardDataScience.com for  his article on How to Check the Quality of a Regression Model with Python and the code I use from his github to do the residual analysis in this notebook. Links to these articles are posted below:\nCode and Resources Used\nPython Version: 3.6\nPackages: pandas, numpy, matplotlib, seaborn, statsmodels\nFeature Selection Using Wrapper Methods by Vikashraj Luhaniwal: https://towardsdatascience.com/feature-selection-using-wrapper-methods-in-python-f0d352b346f\nHow to Check the Quality of a Regression Model with Python by Dr. Tirthajyoti Sarkar: https://towardsdatascience.com/how-do-you-check-the-quality-of-your-regression-model-in-python-fa61759ff685\nData\nThe is from an extract from the Study on the Efficacy of Nosocomial Infection Control (SENIC). The variables are the following:\n\nlength of stay\nage\ninfection risk\nroutine culturing\nroutine chest x-ray\nnum of beds\nmed school affiliation\navg daily census\nnum of nurses\navailable facilities & services\n\nData Cleaning\nWith our test data being patient ID\'s 1-5, these rows are dropped from the original dataset.\nEDA\nWith Infection Risk as our Target Variable, I develop an understanding of the data with the following methods:\n\nView descriptive statistics of data (means, standard deviations, etc.)\nVisualize distributions and linearity via pairplot.\nVisualize correlations between variables.\nFurther analyze variables with high correlations to target variable.\n\nDescriptive Statistics\n\nCorrelations\n\nSeeing that length of stay, routine culturing, routine chest x-ray, and available facilities & services have the highest correlation with infection risk, I then plotted their respective scatter plots against infection risk.\nSample Scatter Plots\n\n\nModel Building\nTo build a Linear Regression Model with the test data, I used these three feature selection methods:\n\nForward Selection\nBackward Elimination\nStepwise Selection\n\nFor the most part, all three methods selected ""length of stay"", ""routine culturing"", and ""available facilities & services"" as the top three features.\nSample Feature Selection Output\n\nModel Analysis: Residual Analysis & Verifying Assumptions\nTo ensure we can trust our model, I had to verify the following assumptions about Linear Models:\n\nIndependence of predictors\nLinearity with Target Variable\nHomoscedasticity\nNormally Distributed\nNo Multicollinearity\n\nMore of this is explored in the Jupyter Notebook, but overall, with the exception of No Multicollinearity (there is multicollinearity), all assumptions are satisfied.\nLinearity & Independence\n\nHomoscedasticity\n\nMulticollinearity\n\n\nNote that there is Multicollinearity since length of stay and available facilities & services have VIFs > 10.\n\nPrediction Results\nOur model, without further optmization, predicts the following:\nTest Data\n\nPredictions of Test Data\n\nConfidence and Prediction Intervals\n\nOptimization Ideas\nSince the model has an adjusted R-squared of .471, it is obvious that the model does need more optimazation to become more accurate and useful. I recommend the following:\n\nSampling data which includes a factor that scores for the quality of sanitation a healthcare facility has, using criteria such as hand washing, presence of rodents, preventive measures against germ spread, use of gloves, etc.\nRecord or engineer with existing data the average of how many patients per room in a healthcare facility.\nDelete outliers, of which there are few, from dataset.\n\n'], 'url_profile': 'https://github.com/MarcelinoV', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/graemeashley12', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Absenteeism-Data\nLogistic regression model that predicts if a company employee will be excessively absent based on their characteristics, using Pandas, Numpy and sklearn.\n'], 'url_profile': 'https://github.com/sujaysdesai', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', '1', 'Jupyter Notebook', 'Updated Oct 7, 2020', 'Updated Apr 29, 2020', 'HTML', 'Updated May 1, 2020']}"
"{'location': 'Vellore', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Used_Car_Price_prediction\nA Polynomial regression model to predict the price of a used car, the R2 score is 92% for now, working on increasing the score.\n'], 'url_profile': 'https://github.com/Muralidharan-ds', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'los angeles, ca', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': [""salary_yearsofexperience\nThe first step in machine learning is linear regression.  Here's how to build one with scikit-learn on jupyter notebooks.\nYou can find the dataset here:\nhttps://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset#Salary_Data.csv\nYou can find the 45 min tutorial here:\nhttps://www.youtube.com/watch?v=LXk1tTMtkVo&t=1044s\nAnd the full course on Udemy here:\nhttps://www.udemy.com/course/machinelearning/?couponCode=MLYUYT\n""], 'url_profile': 'https://github.com/jerodme', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Virality_Prediction\nUsing PRAW wrapper scraped the reddit site and extracted the top posts and predicted the possibility for a post to go viral.\n'], 'url_profile': 'https://github.com/Jughead19', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vik-sin', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lecture, you were given a broad overview of logistic regression. This included two seperate packages for creating logistic regression models. We\'ll first investigate building logistic regression models with\nObjectives\nYou will be able to:\n\nUnderstand and implement logistic regression\n\nReview\nThe stats model example we covered had four essential parts:\n* Importing the data\n* Defining X and y\n* Fitting the model\n* Analyzing model results\nThe corresponding code to these four steps was:\nimport pandas as pd\nfrom patsy import dmatrices\nimport statsmodels.api as sm\n\n#Step 1: Importing the data\nsalaries = pd.read_csv(""salaries_final.csv"", index_col = 0)\n\n#Step 2: Defining X and y\ny, X = dmatrices(\'Target ~ Age  + C(Race) + C(Sex)\',\n                  salaries, return_type = ""dataframe"")\n\n#Step 3: Fitting the model\nlogit_model = sm.Logit(y.iloc[:,1], X)\nresult = logit_model.fit()\n\n#Step 4: Analyzing model results\nresult.summary()\n\nMost of this should be fairly familiar to you; importing data with Pandas, initializing a regression object, and calling the fit method of that object. However, step 2 warrants a slightly more in depth explanation.\nThe dmatrices() method above mirrors the R languages syntax. The first parameter is a string representing the conceptual formula for our model. Afterwards, we pass the dataframe where the data is stored, as well as an optional parameter for the formate in which we would like the data returned. The general pattern for defining the formula string is: y_feature_name ~ x_feature1_name + x_feature2_name + ... + x_featuren_name. You should also notice that two of the x features, Race and Sex, are wrapped in C(). This indicates that these variables are categorical and that dummy variables need to be created in order to convert them to numerical quantities. Finally, note that y itself returns a Pandas DataFrame with two columns as y itself was originally a categorical variable. With that, it\'s time to try and define a logistic regression model on your own!\nYour Turn - Step 1: Import the Data\nImport the data stored in the file titanic.\n#Your code here\nStep 2: Define X and Y\nFor our first foray into logistic regression, we are going to attempt to build a model that classifies whether an indivdual survived the Titanic shiwrech or not (yes its a bit morbid). Follow the programming patterns described above to define X and y.\n#Your code here\nStep 3: Fit the model\nNow with everything in place, initialize a regression object and fit your model!\nWarning: If you receive an error of the form ""LinAlgError: Singular matrix""\nStats models was unable to fit the model due to some Linear Algebra problems. Specifically, the matrix was not invertable due to not being full rank. In layman\'s terms, there was a lot of redundant superfulous data. Try removing some features from the model and running it again.\n# Your code here\nStep 4: Analyzing results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n#Your code here\nYour analysis here\nLevel - up\nCreate a new model, this time only using those features you determined were influential based on your analysis in step 4.\n#your code here\nSummary\nWell done. In this lab we practiced using stats models to build a logistic regression model. We then reviewed interpreting the results, building upon our previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Sci-kit learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in SciKit Learn - Lab\nIntroduction\nIn this lab, we are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the final column labelled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nYou will be able to:\n\nUnderstand and implement logistic regression\nCompare testing and training errors\n\nLet's get started!\n#Starter Code\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n#Starter Code\ndf = pd.read_csv('heart.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\nDefine appropriate X and y\nRecall the dataset is whether or not a patient has heart disease and is indicated in the final column labelled 'target'. With that, define appropriate X and y in order to model whether or not a patient has heart disease.\n#Your code here \nX = \ny = \nNormalize the Data\nNormalize the data prior to fitting the model.\n#Your code here\nTrain Test Split\nSplit the data into train and test sets.\n#Your code here\nFit a model\nFit an intial model to the training set. In SciKit Learn you do this by first creating an instance of the regression class. From there, then use the fit method from your class instance to fit a model to the training data.\nlogreg = LogisticRegression(fit_intercept = False, C = 1e12) #Starter code\n#Your code here\nPredict\nGenerate predictions for the train and test sets. Use the predict method from the logreg object.\n#Your code here\nInitial Evaluation\nHow many times was the classifier correct for the training set?\n#Your code here\nHow many times was the classifier correct for the test set?\n#Your code here\nAnalysis\nDescribe how well you think this initial model is based on the train and test performance. Within your description, make note of how you evaluated perforamnce as compared to our previous work with regression.\n#Your answer here\nSummary\nIn this lab, you practiced a standard data science pipeline, importing data, splitting into train and test sets and fitting a logistic regression model. In the upcoming labs and lessons, we'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,853 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['E-commerce-Conversion-Rate-AB-Test-Analysis\nOverview\nThis project is to analyze A/B Testing data for an e-commerce website.\n\nEvaluation metric: conversion rate\nUnit of diversion: user id\n\nLibraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport statsmodels.api as sm\nfrom statsmodels.stats.proportion import proportions_ztest\n\nData\nDatasets are downloaded from website. They are provided here.\n\nab_data.csv\n\n\ncountries.csv\n\nMethodology\n\nSimilate binomial distributions to construct distributions for p difference\nRun logistic regression to analyze coefficients of page variale\n\nResults\nShown in the end of notebook.\n'], 'url_profile': 'https://github.com/Monica1104', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'LUMS, Lahore', 'stats_list': [], 'contributions': '371 contributions\n        in the last year', 'description': ['Sentiment Analyzer (Multinomial Logistic Regressor & Naive Bayes Classifier)\nTo analyze and predict sentiments of travellers from their tweets about 6 US Airline.\nTwitter US Airline Sentiment Dataset - Source: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n'], 'url_profile': 'https://github.com/saadullah01', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '243 contributions\n        in the last year', 'description': ['poissonFits\nThis package provides convenient functions for fitting Poisson distributions, regressions, and processes to count data of some phenomena or events. It also contains data-sets for examples of approximately Poisson distributed phenomena. These include data-sets named in R as ""stormReport"", ""ENSO"", and ""atlanticStormsENSO"" and ""shootings"". See the internal R documentation for more info and sources.\nInstallation\nYou can install the latest version from GitHub with:\ndevtools::install_github(""shill1729/poissonFits"")\nFit a Poisson distribution on the number of hurricanes per year in the Atlantic basin\nWe can load hurricane data from NHC NOAA, since they conveniently provide an XML file of all storm reports since 1958. See documentation on ""stormReport"" for more details. The data-set is provided with the package, however, a scraper is also available to get the latest file of storm-reports (which I imagine does not change frequently until after new storm seasons are over). NOTE: Early data contains named storms without designating them either a hurricane, a tropical storm, or a subtropical storm. The parameter for the Poisson distribution is chosen via MLE. A Chi-square test is then performed on the goodness of fit of the distribution to the empirical data.\nlibrary(poissonFits)\n# Significance level for Chi-square test\nalpha <- 0.05\n\n# Default loads no tropical storms;\ncountData <- loadStormData()\n\n# Pick either the atlantic, pacific, or world\ncountsYear <- countData$atlantic\n# Get just the counts\ncountsData <- countsYear$freq\n\n# Pass just the count data to poissonFit\npoissonFit(countsData)\n'], 'url_profile': 'https://github.com/shill1729', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': [""PUBG Data Analysis with XGBoost\nThis repository holds my submission for PUBG Finish Placement Prediction (Kernels Only) Kaggle competition.\nThis goal of this competition is to predict the finishing place of a person playing PUBG using features such as the players kills, assists and distance travelled. For my submission I have used a Random Forest Regressor and achieved a Public Score of 0.04768 placing me around the middle of the field of entrants.\nLink to the kaggle notebook (https://www.kaggle.com/mattburt07/pubg-data-analysis-using-random-forest-approach)\nSkills Demonstrated\nThis project involved heavy use of Pandas, a Python library for data analysis and manipulation\nFastai's libraries were used for some utility functions\nSK Learn was used for the Random Forest models\nSciPy and Matplotlib were used for visualisation of feature correlations\n""], 'url_profile': 'https://github.com/mattburtnz07', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'New Haven, CT', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/idjoannachen', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Stockholm, Sweden', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['CalifornianHousingDatasetAnalysis\n'], 'url_profile': 'https://github.com/VeereshElango', 'info_list': ['Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'MATLAB', 'Updated Oct 27, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['HousePricesPrediction\n'], 'url_profile': 'https://github.com/DawidSzczerba', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dhruval-p', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '239 contributions\n        in the last year', 'description': ['Flight-Price-Prediction\nIt is a regression problem where we are predicting the flight prices. This is hackathon problem statement which involves lot of EDA and Feature Engineering as most of the predictors are categorical variables.\n'], 'url_profile': 'https://github.com/Vinay-Kalmoodkar', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['bayescopulareg\nBayesian analysis of multivariate generalized linear models via copulas\nThis package contains two main functions.\n\nbayescopulaglm samples from the posterior of a multivariate generalized linear model (currently supported families are poisson, gaussian, binomial, and gamma.\npredict.bayescopulaglm which samples from the predictive posterior density of a bayescopulaglm object.\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Giza - Egypt', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Skishta', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '320 contributions\n        in the last year', 'description': [""Testing_ML_Algorithms\nThis repository contains machine learning projects, each of which will test various machine learning algorithms. Data has been collected from numerous online sources to tackle a wide range of classification and regression problems\nhttps://archive.ics.uci.edu/ml/index.php\nhttps://www.kaggle.com/c/dog-breed-identification/overview\nAdditional Useful URL's\nhttps://paperswithcode.com/\n""], 'url_profile': 'https://github.com/mhaythornthwaite', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/syedkashif9786', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Cincinnati', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': [""Synopsis\nCredit card fraud is an inclusive term for fraud committed using a payment card, such as a credit card or debit card. This means a fraud when someone uses your credit card or credit account to make a purchase you didn't authorize. This activity can happen in different ways: If you lose your credit card or have it stolen, it can be used to make purchases or other transactions, either in person or online.\nIt is important that credit card companies be able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. Thus we build a machine learning model to predict whether an anonymized credit card transactions is labeled as fraudulent or genuine.\nWorked on R on a highly unbalanced Kaggle dataset. Modeled the balanced data (SMOTE) to recognize fraudulent credit card transactions with 99% accuracy using KNN, Logistic Regression, Decision Tree, Random Forest and Neural Network techniques.\n""], 'url_profile': 'https://github.com/varmavarun98', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'Pittsburgh, Pennsylvania ', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Creditcard-fraud-detection\nSolving one of the common problem in Fintech industry using Machine learning and AI. Performed Multinomial Logistic Regression | LightGBM | Feed Forward Neural Nets and compared all the three models using different scenarios.\n'], 'url_profile': 'https://github.com/vamshibussa', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,854 contributions\n        in the last year', 'description': ['House-price-analysis-and-prediction\nDescription :\nGoal of the project is to use regression techniques in order to estimate the sale price of a house given the features and pricing data for around 3,000 houses sold in past.\nTkinter library is used for GUI interface of the project.\n\nEDA\nBuild a simple model\nTry various features on a simple model\nBuild a complex model\nTrain with promising features\n\nWe have analyzed the effect of various parameters on sale price of a house and selected those parameters which have maximum effect. Using these parameters, we applied machine learning regression algorithms to predict the price of a house.\n•\tThere were 80 total attributes in the dataset, some of which we derived from current columns. I used 23 attributes in our models.\n'], 'url_profile': 'https://github.com/manpreet-kau-r', 'info_list': ['Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'C++', 'Updated Dec 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', '1', 'Python', 'MIT license', 'Updated Mar 2, 2021', 'Jupyter Notebook', 'Updated May 2, 2020', 'R', 'Updated May 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vidisha2211', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['SLR\nSimple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables: One variable, denoted x, is regarded as the predictor, explanatory, or independent variable.\n'], 'url_profile': 'https://github.com/chinmayejain', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sagar3019', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['PwC-JP-Analytics-Hackathon-2018\nCode made for PwC Japan Analytics Hackathon in 2018. We were tasked to analyze the effects of price markdowns on retail sales. I used panel data regression with interactive fixed effects in R.\n'], 'url_profile': 'https://github.com/rivaldophilip', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'Austin, Tx', 'stats_list': [], 'contributions': '167 contributions\n        in the last year', 'description': [""Hello!\nThis project is submitted as a part of Machine learning homework of Data Analysis and Visualization homework.\nWelcome to the famous Tsukiji fish market of Tokyo, Japan! We came here to collect data on some of the fish they have here\nbut we didn't wake up at 5am for the tuna auction and by the time we showed up they were only left with a few species of fish.\nWe got to work and gathered measurements from a few different species of fish and want to train a regression model to predict\nthe weight of a fish using some of the features we were able to measure. We have no idea which features will be good predictors.\n30% of the data is with held by the instructor before he handed it to us. The withheld data will be used for scoring.\nHere's what is needed:\n\nA function that accepts a csv path and returns the predictions of the regression model using csv.\nUse a pipenv and scikit learn to submit the final hw.\n\nThe solution is in dt.joblib and the function to be used is predict_from_csv(path_to_csv)\n""], 'url_profile': 'https://github.com/PGGokhale', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mittalrr', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Machine Learning CART Regression Tree\nUse code\nI have included the readfile.py code which encapsulates the method, I am currently working on a more general method to adapt this code to any similar format csv file.\nBelow is an outline of the steps followed in the code (readfile.py)\nInitialise\n\nset file name\nset plotting styles\n\nWrangling with Read data file with readfile format\n\ndetermines the type of data file (csv,xls etc.) and uses the appropriate pd.read functions\nreturns the data frame of data\n\nData Cleaning\n\nEnsure index of df is the datetime object with D/M/Y H/M/S format\nRead in the temperature data file for the prediction and ensure same index format\nMerge the two dataframe so that temp becomes a column of the common df\ndrop any na values (not useful for the learning methods to produce synthetic data)\nCreate a weekday column (useful for this model and it is a dependant variable)\nDo the same for the hours and minutes in the day - using a lambda function make the hm into a continuous numeric scale so the ML can interpret this data more easily\ninput if necessary the True False column for a base heating period (factories often have in the winter a higher minimum energy consumption to keep all pipes unfrozen in the night etc.)\nFrom a list of bank holidays - change the day of the week to sunday regardless of the day as no one will be in the office\n\nVisualisation\nI prefer to make a series of plots before the training of the data to ensure I have correctly understood the data. This can give you some idea of predictor variables and if they have been misdefined. Sometimes it is not this simple!\nPLOTS:\n\nEnergy Consumption over known period through time\nMean Daily Consumption\nMean Weekly\nscatter distribution of temp and power rating\n\nMachine Learning\n\nNew dataframe with the appropriate predictor variables (not target values)\nAccount for all bank holidays in the same way\nClean dataframe to be correctly ordered and match previous\nimport decision tree regressor and adaboost methods (adaboost optional)\nCreate X and Y dfs\nUse sklearn test _ train split for X_train test Y etc.\nUse elbow method running through iteration of depths of regressor tree.\nRun model boosted with selected depths\n\nFinal Plots and Validation\n\nplot full year\ncoherence plots - do low temps correspond to high power in all cases - if not why not?\nValidation calculations of the total energy consumption (area under curve) compared with the billing accounted for\n\nCode files to come in general form:\n\nformat_data.py\nmachine_learning.py\n\n'], 'url_profile': 'https://github.com/ruckapark', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'Egypt, Cairo', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': [""Numerical Methods' Algorithms\nSome Implemented methods for numerical analysis  including\n\nLagrange interpolation technique\nEigenValues and EigenVectors\nLeast Squares Regression\nNumerical Differentiation\nNumerical Integration\n\nRequirements\n\nPython 3.7 or later\nmatplotlib\nnumpy\n\nNote that: these methods have been implemented without the knwoledge of their originaly made algorithms\n""], 'url_profile': 'https://github.com/ahmedsalahacc', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'Barcelona, Spain', 'stats_list': [], 'contributions': '974 contributions\n        in the last year', 'description': ['lassoloops\n\n\n\n\n\n\nR package to perform a bootstrap validation in lasso regression models\nInstallation\n# install.packages(""devtools"")\ndevtools::install_github(""pcastellanoescuder/lassoloops"")\nCode of Conduct\nPlease note that the lassoloops project is released with a Contributor\nCode of\nConduct.\nBy contributing to this project, you agree to abide by its terms.\n'], 'url_profile': 'https://github.com/pcastellanoescuder', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['Stock_price_prediction\nThe repository contains all the field scripts for predicting the price of a stock.\nContents\nStock_Price_prediction.ipynb\nImplements all necessary python libraries to predict the price of a stock\nRequirements\nPython 3\nPandas\nNumpy\nMatplotlib\nsklearn\nRunning platform information\nThe current source code has been tested on the following system configuration:\n1.CPU Intel i5 10th Generation with 64 GB physical memory\n2.Windows 10 operating system\n3.Python 3.8.2\n3.1 Numpy 1.18.3\n\n3.2 Pandas 1.0.3\n\n3.3 Matplotlib 3.2.1\n\n3.4 sklearn 0.22.2\n\nDemo Result\n\nLicense\nThe current repository is freely available under MIT.\n'], 'url_profile': 'https://github.com/Swarupa567', 'info_list': ['Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'Updated Apr 27, 2020', 'PHP', 'Updated May 2, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Sep 24, 2020', 'R', 'GPL-3.0 license', 'Updated May 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 5, 2020']}"
"{'location': 'India', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['Stock_price_prediction\nThe repository contains all the field scripts for predicting the price of a stock.\nContents\nStock_Price_prediction.ipynb\nImplements all necessary python libraries to predict the price of a stock\nRequirements\nPython 3\nPandas\nNumpy\nMatplotlib\nsklearn\nRunning platform information\nThe current source code has been tested on the following system configuration:\n1.CPU Intel i5 10th Generation with 64 GB physical memory\n2.Windows 10 operating system\n3.Python 3.8.2\n3.1 Numpy 1.18.3\n\n3.2 Pandas 1.0.3\n\n3.3 Matplotlib 3.2.1\n\n3.4 sklearn 0.22.2\n\nDemo Result\n\nLicense\nThe current repository is freely available under MIT.\n'], 'url_profile': 'https://github.com/Swarupa567', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '384 contributions\n        in the last year', 'description': [""Energy Prediction Webapp\nThis is a energy consumption prediction webapp which uses Facebook's Prophet Algorithm along with some additional regressors like temperature and holiday to predict the future energy consumption. You can predict energy consumption values, get graphical analysis of energy prediction values, download the predicted data, see 5 day weather forecast and email customer care for your issues using the webapp.\nTechnologies Used\n\nWebapp using MERN stack\nMachine Learning Algorithm Used : Facebook's Prophet Algorithm\nAPI for getting temperature prediction: Open Weather API\nAPI for fetching live data for dashboard: Google Spreadsheet API\n\nFile Structure\n\nsrc/model: contains code to train machine learning models.\nsrc/webapp: contains code of MERN based webapp.\ndata: contains training data.\ndiagrams: contains UML diagrams.\ndocuments: contains SRS, Project Plan, Design document, Test plan document and various other documents related to our system.\n\nWebapp\n\n\nDashboard showing live energy consumption statistics of a complex\n\n\n\nPrediction page showing the values of energy consumption prediciton.\n\n\n\nShowing prediction values in graphical form\n\n\n\nShowing 5-day weather prediction.\n\n\n\n""], 'url_profile': 'https://github.com/KanishAnand', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['news-popularity-model\nContents of the Project:-\n1)Integrated scraped data and ML model to predict virality(Main Project):\nHere using Regression model that uses a dataset OnlineNewsPopularityClassification.csv to train itself and then check the virality of the scraped data.The virality is checked on the basis of various information that has been scraped from Times of India website.\nThe file includes these data for evaluation:\n\nLater on after using sentiment analysis and weighing the relevant words with the ones in popular news a model is created.\nFor this the data like number of tokens, number of shares etc are used from the respected website.\n\nLater on the virality or popularity score is given:\n\nThe score lies between 0 and 1(0 corresponding to not popular news and 1 corresponds for popular news)\n2)Classification Model:\nThis model has various algos like Logistics Regression,Random Forest Classifier,SVM but the one actively used is RandomForestClassifier due to its best results.\nThe Output shows Essentials of the model using RandomForestClassifier after being trained and tested.\n\nThe Output shows the labels before and after standardization.\nIt also shows the accuracy of this model.\n\n3)Regression Model:\nThis model uses Bayesian Linear Regression to solve the problem and give us the required accuracy of the model.\n4)News Aggregator(made using Django by scraping popular websites like Times of India etc):\nI also made a website using Django that can be later used to project the popular news on a single site only.\n\nWeb Scraping of Times Of India:\n\nWeb Scraping Of Hindustan TImes:\n\nWeb Scraping of The Economist:\n\nFuture of the Project:\nA much proper integration of model and NewsAggregator which could predict the virality of the news and display the link to the site on my website asap.\nLicense\nThe project is available as open source under the terms of the MIT License.\n'], 'url_profile': 'https://github.com/pratham0203', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'University of Delaware', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['STAT151A\nA coordinated treatment of linear and generalized linear models and their application. Linear regression, analysis of variance and covariance, random effects, design and analysis of experiments, quality improvement, log-linear models for discrete multivariate data, model selection, robustness, graphical techniques, productive use of computers, in-depth case studies.\n'], 'url_profile': 'https://github.com/dayuyang1999', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '510 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/austinteshuba', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'Cincinnati', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['TextMiningTweetsForDisasterManagement\nTwitter is a free social networking service that allows people to broadcast short posts called tweets. Tweets can serve as a data source to detect real time disaster events that can be utilized to dig significant information for crisis reaction and relief operation.  It is easy for a human to identify whether a tweet is related to disaster or not as some words can be used both in disaster as well as non disaster tweets but very difficult for a machine to identify.\nCreated a machine learning model in python on a Kaggle dataset to build machine learning models in Python for classifying tweets to disastrous or non-disastrous. Used the GloVe technique for word embeddings. Modeled the data using Logistic Regression, KNN, XG Boost, Classification Decision Tree and Voting Classification Method.\n'], 'url_profile': 'https://github.com/varmavarun98', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'Greater Seattle Area', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['taxi-ride-predictor\nPredicts the travel time of a taxi ride in New York. Cleans, filters, and visualizes data. Implements various iterations of linear regression and neural network models. Reaches an RMSE of 193 seconds using a sequential neural network model with two hidden layers of 128 neurons each. Python, Keras, TensorFlow.\n'], 'url_profile': 'https://github.com/iiguelmamene', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Predictive-Analytics\n• Analyzed and manipulated 26 million ratings from 270,000 users for 45,000 movies dataset on Kaggle • Utilized R to develop Logistic and linear regression models on movie rating and revenue • Assessed and compared the fit of different models to generate model reports and rating & revenue forecast\n'], 'url_profile': 'https://github.com/SerenaWang0815', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'Giza - Egypt', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Skishta', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Data-Science-Project-Detect-Credit-Card-Fraud-with-Machine-Learning-in-R\nThis project intends to model fraud detection of credit cards using the Card Transaction dataset that contains a mix of fraud and non-fraudulent transactions applying a multiple of algorithms including Decision Trees, Logistic Regression, Artificial Neural Networks and Gradient Boosting Classifier.\n'], 'url_profile': 'https://github.com/mau955', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Python', 'Updated May 2, 2020', 'Python', 'Updated May 16, 2020', 'Updated May 2, 2020', '1', 'Python', 'Updated Apr 27, 2020', 'Updated Apr 30, 2020']}"
"{'location': 'Washington, D.C.', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': [""Predicting Financial Well-Being: A Machine Learning Approach\nFinal Project for Course PPOL 565: Data Science II - Applied Statistical Learning, Spring 2020\nThis repository contains the code, source data, and written report for an analysis exploring Americans' financial well-being. In this analysis, I use a combination of linear regression and a k-nearest neighbors algorithm to predict individuals' financial well-being, and explore some of the most important factors that help in predicting financial well-being.\nYou can find a brief summary of the files available in this repository below:\n\nFinancial_Well-Being.ipynb: Jupyter Notebook containing the code used to conduct this analysis\nFinancial_Well-Being.pdf: Final written report summarizing the key findings from the analysis\ndata.csv: Source data for the analysis\n\nSource: Consumer Financial Protection Bureau's Financial Well Being Survey\n\n\n\n""], 'url_profile': 'https://github.com/andygreen-1', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kristybell', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'Vitznau, Switzerland', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['MiniProject-WQU-OLS_DisneySP500\nIn this assignment, we want to evaluate how Disney performed as an invest- ment between October 2008 and September 2013 and how risky it is. To do that we need to regresse monthly raw returns on Disney against returns on the S&P 500 over that period. Analyze parameters of the resulting regression model.\n'], 'url_profile': 'https://github.com/PankoAliaksandr', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'Sylhet,Bangladesh', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Cross-Sell-Prediction\nHere, we are predicting sell growth of a product by spending money in advertising by TV, Radio, Newspaper. Here in the dataset we have 200 rows and 4 columns. We split the data into 70-30%, fit the data, predict the data. And finally calculate the error. In this linear regression model our target is minimize the error as much as possible.\n'], 'url_profile': 'https://github.com/bolaram', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Real-estate-price-prediction\nThis is a data science project where we will first\nbuild a model using sklearn and linear regression using home\nprices dataset from kaggle.com. Second step would be to write a python\nflask server that uses the saved model to serve http requests. Third\ncomponent is the website built in html, css and javascript that allows\nuser to enter home square ft area, bedrooms etc and it will call python\nflask server to retrieve the predicted price. During model building we\nwill cover almost all data science concepts such as data load and\ncleaning, outlier detection and removal, feature engineering,\ndimensionality reduction, gridsearchcv for hyperparameter tunning, k\nfold cross validation etc. Technology and tools wise this project\ncovers:\nPython\nNumpy and Pandas for data cleaning\nMatplotlib for data visualization\nSklearn for model building\nJupyter notebook, visual studio code and pycharm as IDE\nPython flask for http server\nHTML/CSS/Javascript for UI\n'], 'url_profile': 'https://github.com/harshsax2811', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': [""Baseball_hit_prediction-\nUsing Logistic regression to predict whether a pitch will be a hit or not based on a big data with more than 70 dimensions  Using the Trackman pitch-level data building a model that predicts the probability of a batted ball being a hit based on batted ball characteristics and relevant contextual data. Please regard fielding errors as non-out hits for the purposes of this analysis.\nTo better understand the data-set please refer to the glossary below which explains what each feature means, especially one who doesn't follow baseball can really get lost going through all the features of this massive data-set.\n_ _ _ _ _ _ _ _ _ _ _ _ GLOSSARY _ _ _ _ _ _ _ _ _ _ _ _\nGameId- The unique key identifier per game\nBallparkId- The unique key identifier for each unique ball park the specific game is being played at\nGameEventId- The unique key identifier for each unique game event\nGameTime- The time the game began at\nSeason- The season the game was played\nAtBatId- The unique key identifier for the at bat\nTrackmanUid- The unique key identifier of the pitch\nBatterId- The unique key identifier of the batter hitting\nBatterHitting- Which side of the plate the batter is hitting from\nBatterPositionId- The position on the field the batter plays\nbatterTimesFaced- The amount of times the pitcher has faced the batted in the game\ncumulativeBattersFaced- The amount of batters the pitcher has faced in the game\nPinchHit- Was the batter pinch hitting?\nPitcherId- The unique key identifier of the pitcher pitching\nPitcherThrowing- Which arm does the pitcher throw with\nCatcherId- The unique key identifier of the catcher\nHomeTeamId- The unique key identifier of the home team\nAwayTeamId- The unique key identifier of the away team\nPitchType- The classified pitch type of the pitch\nBalls- How many balls in the count when the pitch was thrown\nStrikes- How many strikes in the count when the pitch was thrown\nOuts- How many outs in the inning when the pitch was thrown\nInning- The inning the pitch was thrown\nInningTop- Was it the top or the bottom of the inning\nPlateAppearance- Plate appearance of the inning\nPitchOfPA- How many pitches in the plate appearance before the pitch\nRunners_on- Were there runners on\ncalledStrike- Was the pitch called a strike?\nSwing- Did the pitch generate a swing?\nswingStrike- Did the pitch generate a swing and a miss?\nFoul- Did the pitch generate a foul?\ninPlay- Was the pitch hit into play?\nH1b- Did the pitch result in a single?\nH2b- Did the pitch result in a double?\nH3b- Did the pitch result in a triple?\nhr- Did the pitch result in a homerun?\nfieldingError- Did the pitch result in a fielding error?\ninPlayOut- Was the play an out?\nHorzBreak- How much horizontal movement did the pitch generate?\nVertBreak- How much horizontal movement did the pitch generate?\nStartSpeed- How hard was the pitch thrown?\nPx- The terminal horizontal location of the pitch\nPz- The terminal vertical location of the pitch\nRelHeight- How high the pitch was released by the pitcher?\nRelSide- How far from the pitcher’s body was the pitch released?\nExtension- How far from the pitcher’s body (towards the plate) was the pitch released?\nVertRelAngle- The vertical release angle of the pitch\nHorzRelAngle- The horizontal release angle of the pitch\nSpinRate- The spin rate of the pitch\nSpinAxis- The axis orientation of the pitch (0-360 degrees)\nBatterHeight- How tall the batter is\nPitcherHeight- How tall the pitcher is\nExitSpeed- How hard the batted ball was hit off the bat\nAngle- The angle off the bat in which the ball was hit\nBearing- The angle from the landing position of the ball and home plate\nDirection- The angle off the bat on contact and home plate\n""], 'url_profile': 'https://github.com/anirudh217sharma', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': [""The Best Classifier\nIn this project, I have built a classifier to predict whether a loan case will be paid off or not. I've loaded a historical dataset from previous loan applications, clean the data, and applied different classification algorithm on the data. Following algorithms are used to build the models:  k-Nearest Neighbour, Decision Tree, Support Vector Machine, Logistic Regression. The result is reported as the accuracy of each classifier, using the following metrics when these are applicable:  Jaccard index, F1-score and LogLoss.\n""], 'url_profile': 'https://github.com/abhisheksuraj21', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'Newark, DE', 'stats_list': [], 'contributions': '524 contributions\n        in the last year', 'description': [""Purpose\nThis is the final project for the course Introduction to Machine Learning (CISC 684) at the University of Delaware. The course instructor is Dr. Amo Tong. The team members associated with this project are Kleio Baxevani, Prem Chand, and Desiderio Pilla, who are all students in the Master of Science in Data Science (MSDS) program at UD.\nProblem Statement\nThe goal of this project is to create an algorithm that increases the profit realized by investors. This goal will be acheived by using two models, a Logistic Regression and Neural Network. The data was scraped from the internet for individual stocks. This includes daily price and trading data, as well as earnings data for every quarter since 1995. The scraped data was manipulated to calculate a variety of value- and momentum-based metrics.\nPremise\nBackground\nThe stock market is a platform through with securities (also referred to as shares, stocks, equity, etc.) are traded daily. When an individual or corporation buys a share of a stock, they are purchasing a fraction of ownership of the publicly traded company the stock belongs to.\nThe price of a single share changes throughout the business hours of the stock exchange it is traded on, normally 9 am to 5 pm, Monday to Friday in the United States. During these hours, the market is open, and securities can be bought and sold freely. When the market closes, no securities can be traded. The price of a share can fluctuate between the close of the market and the next time it opens; often, news or earnings reports about a company can have a major impact on the value of a stock, and take place after market hours. Investors are unable to trade shares of any company until the market opens, by which the opening share price will already reflect the new information revealed since the previous day's close. Some institutions are given the ability to trade pre- and post-hours, but not the typical investor.\nThere are many economic metrics used to measure the value of a security. There are momentum indicators, which measure if the price of a share is rising, falling, how quickly, and for how long. There are value metrics, which compare the actual price of a security with a mathematical calculation of what the price should be. This calculation varies for every analyst, though some common metrics take into account a companiy's revenue, profit, cash flow, liabilities, etc.\nGoal\nThe goal of this project is to create an algorithm that increases the profit realized by investors.\nMethods\nThis goal will be acheived by using two models, a Logistic Regression and Neural Network.\nThe data was scraped from the internet for individual stocks. This includes daily price and trading data, as well as earnings data for every quarter since 1995.\nThe scraped data was manipulated to calculate a variety of value- and momentum-based metrics.\nFor each trial, a testing date is given. The training data includes a 1 or 2 year period (chosen as a hyperparamter of the model) occurring at least n days before the testing date. The test data is comprised of the 6 month period following the testing date.\nN corresponds the the classifcation of an instance. This data has a binary classification defined by whether or not the price of the security increased n business days after the current date. It is for this reason that there must be a gap of n days between the training and test data. This value is also selected as a hyperparameter of the model. Predicting the price of a stock 1 day into the future (which would practically yield the highest returns), is nearly impossible. It is unrealistic to do so given the feature spaced used in this project, as no geopolitical or media information was gleaned.\n*Note: This model does not take dividend payouts into account, and all stock prices are split-adjusted.\nPerformance\nEconomic climates can vary by month, quarter, and year. To gain a more accurate understanding of a model's performance, every 6 month period was tested from the period beginning on January 1st, 2003 to the period beginning on January 1st, 2016 (which ends on July 1st, 2016). The total returns of the model are then aggregated and compared with the total returns of the unengaged investor.\nThe unengaged investor is a control group to compare the model against. Many analysts try to predict the market, and end up making less money than they would have if they didn't do anything. The unengaged investor mimics the returns that would be realized if a stock was bought on the first day of a period and kept untili the final day of the period (also referred to at the static return). The model buys and sells a security each day depending on the predicted classification of the test data.\nWhen an instance is classified as True (meaning the model expects the share price to increase in the future), the model buys a share of the stock. It then holds the stock until it classifies an instance as False (meaning the model expects the share price to decrease in the future). The maximum returns would be realized if the model were to buy a security the day before it increases, and sell it the day before it decreases, for every day in the test period. The inverse is also possible, which would be the worst case.\nThe performance of this model will not be measured by the accuracy of its predictions. Rather, the most practical measurement of this model are to compare its returns to the unengaged investor. If the model produces higher returns than an investor who did not use the model and merely held a security for the duration of the period, than utilizing the model to guide investment decisions will have been worthwhile.\nNote: Other measures of success could have also been selected. For example, one could have optimized the model to reduce volatility or minimize losses. This project, however, optimizes the model by maximizing returns without regard for volaility or short-term losses.\nEvaluation\nResults\nBelow are the annualized returns of each model for the tested companies:\n\n\n\nStock\nStatic Return\nLogistic Regression\nNeural Network\n\n\n\n\nAmazon.com, Inc.\n30.55%\n31.51%\n21.95%\n\n\nDuPont de Nemours, Inc.\n3.69%\n5.66%\n7.93%\n\n\nExxon Mobil Corporation\n7.46%\n11.74%\n13.47%\n\n\nIBM Corporation\n4.80%\n7.66%\n8.31%\n\n\nJohnson & Johnson\n5.90%\n3.70%\n7.25%\n\n\nJ.P. Morgan Chase & Co.\n6.84%\n1.95%\n8.92%\n\n\nMicrosoft Corporation\n4.89%\n7.79%\n6.55%\n\n\nWalt Disney Co\n13.83%\n13.68%\n16.11%\n\n\n\nBolded returns reflect annualized returns that were greater than the unengaged investor\nConclusions\nThe above chart shows that the model was able to successfully outperform the unengaged investor for all 8 securities tested. Companies were chosen from a broad range of sectors to test the robustness of the model.\nThe logistic regression proved to be the more lucrative investing technique for 5/8 holdings, with an average return of 0.72% /yr greater than the static return.\nThe neural network proved to be the more lucrative investing technique for 7/8 holdings, with an average return of 1.57% /yr greater than the static return. Among the 7 stocks in which the model performed better, the average return was 3.02% /yr greater than the static return.\nRecommendations\nWhile the model appears to have performed well, I would not recommend using this to guide any investment decisions. The use of averaging over many years and multiple economic cycles is naiive, expecially considering the lack of information used in the decision making process of these models. This model can serve as a guide, but it should not be used to pick which securities to buy. The purpose of this model is to maximize the returns of a holding, not to give input on which stocks will outperform others.\n""], 'url_profile': 'https://github.com/DesiPilla', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'Scotland, United Kingdom', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Data-Analytics-Project---Classification-with-Python\nI created a repository for my Data Analytics Project at the University of Stirling.\nThis repository includes a notebook and the outcome report of my project.\nAll experiments for Exploratory Data Analysis (EDA), data manipulation and visualisation, implementation of the models and their evaluation are implemented using Python programming language.\nIn the notebook, I predict which customers are more likely to respond positively to a bank marketing call by setting up a regular savings deposit or subscribing the term “made_deposit”.\nThree classification algorithms will be developed in order to predict the target variable. Logistic Regression, Decision Tree and Multi-Layer Perceptron (MLP).\nThe analysis of the project includes Data Summary, Data Preparation, Modelling, Results and Errors using Evaluation Metrics, Confusion Matrices and ROC Curve.\nMore information about the dataset, the process I followed and the evaluation of the classification models will be found on the report.\n'], 'url_profile': 'https://github.com/stella-spyrou', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tapasbhadra', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated Apr 27, 2020', 'Python', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'R', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'HTML', 'MIT license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'JavaScript', 'Updated Apr 28, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['Demand-Forecast-Walmart\n'], 'url_profile': 'https://github.com/SamratSengupta', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Abhishekdaw-Coronavirus-Testing-Probability-Detector\nHi Guys!I have come up with a new solution for the Coronavirus Challenge by MHRD.Here is case:Lets take a case where there are 10,000 coronavirus suspects and we just have 5,000 testing kits available for that day.I have developed a logistic regression model where i have included features like Fever,runnynose,bodypain etc.This model will predict whether a person should be tested immediately for Covid-19 or not(If probability is >50%).The data that i have taken is random,there is no official source.\n'], 'url_profile': 'https://github.com/Abhishekdaw', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['Lending_Club_Analysis_and_Modeling: Predicting bad loans from Lending Club data\nThe purpose of this project is to understand what makes a user default a loan from LendingClub.\nEach line item in the data corresponded to a loan, and had various features relating to loan amount, employment information of the borrower, payments made and so on. I will layout the most influencing parameters by considering both the statistical and lexicon based NLP modeling for the prediction.\nApart from that, an exploratory data analysis was performed on the data, to look for outliers and individual distributions of the variables. Following which, the interactions between these variables were studied to weed out highly correlated variables. There are very few representation of defaults in the data compared to the fully paid class. Therefore, this was treated as an imbalanced class problem, wherein traditional random sampling would not yield optimal results.\nTo overcome this problem, I performed SMOTE for data class balance. To best classify which loans would likely default from the given dataset, various statistical learning techniques, such as Regression, Tree-based methods- standalone, boosting and bagging ensemble methods, Support Vector Machines and Neural Networks were employed. Amongst these classifiers, Gradient boosting was observed to have the best performance, although with further fine tuning, Deep Neural Networks could possibly classify better.\n'], 'url_profile': 'https://github.com/solharsh', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SiddharthKalla', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Milan, Italy', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Forecasting-SARS-CoV.10\nThis is a time series forecasting for predicting the increase of confirmed cases of SARS-CoV-19.  Different models as SVR, SGBoost, or Random Forest Regressors have been applied.  Also provides a different perspective and interpretability on the results relating it to Contingency Measures.\n'], 'url_profile': 'https://github.com/miriviere', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '186 contributions\n        in the last year', 'description': [""Price prediction of used cars\n\nPrice prediction of used cars\n\nDemo\nOverview\n\nDataset Descrption\n\n\nkey technical aspects\nBest Model selection\n\nMetric\nModel Pipeline\nParameter search space\nBest parametes\nValiditon results:\nFeature importances\n\n\nFalsk App Deployment\nTechnologies Used\nCreator:\n\n\n\nDemo\nAWS link: http://usedcarpricepredict-env.eba-jdefnbzx.us-east-1.elasticbeanstalk.com/\n\nworking App developed using Flask\nOverview\nCars are more than just a utility for many. We all have different tastes when it comes to owning a car or at least when thinking of owning one. Some fit in our budget and some lauxury brands are heavy on our pockets. But that should not stop us from owning it, atleast used ones. The goal of this project to predict the costs of used cars to enable the buyers to make informed purchase using the data collected from various sources and distributed across various locations in India.\nDataset Descrption\nDataset used here is from a hackathon hosted by MachineHack. Go to the hackathon homepage to know more about the dataset. The dataset set contains features like Location, Manufacture details, car features such as Fuel type, Engine, and usage parameters. Below is the app in Working condition.\n\n\nSize of training set: 6,019 records | Size of test set: 1,234 records\n\n\nFeatures:\n\nName: The brand and model of the car.\nLocation: The location in which the car is being sold or is available for purchase.\nYear: The year or edition of the model.\nKilometers_Driven: The total kilometres driven in the car by the previous owner(s) in KM.\nFuel_Type: The type of fuel used by the car. Transmission: The type of transmission used by the car.\nOwner_Type: Whether the ownership is Firsthand, Second hand or other.\nMileage: The standard mileage offered by the car company in kmpl or km/kg\nEngine: The displacement volume of the engine in cc.\nPower: The maximum power of the engine in bhp.\nSeats: The number of seats in the car.\nNew_Price: The price of a new car of the same model.\nPrice: The price of the used car in INR Lakhs.\n\n\n\nkey technical aspects\nAfter data exploration and visualization various data prepossing steps are selected after of data. Following are noticeable ones among them.\n\nNew_Price feature dropped due to significant missing values.\nName column split into Brand and Model features.\nContinuos variables including target feature are Log transformed to make their distribution symetrical.\nKilometers_Driven and Mileage are multiplied together to form new feature as this interaction show high correlation with target feature price.\nBrand,Model, and Location are encoded using Target encoding as they have lot of categories.\nFuel_Type, Transmission, and Owner_Type are one-hot encoded.\nYear columns are deducted by current year to introduce aging effect (current year - edition year).\n\nBest Model selection\nThe data is trained on Linear Regression, KNN, SVM, Decision Tree,Random Forest, GBDT and XGBoost with hyper-parmeter tuning. GBDT turns out be best model with lowest loss of 0.033.\nMetric\n\n\nRoot Mean Squared Logarithmic Error (RMSLE) is used as metric.\n\n\nRMSLE is usually used when you don't want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers. Rather we have to focus on percent error relative to the actual values.\n\n\nModel Pipeline\npipeline = Pipeline([\n    ('features',DFFeatureUnion([\n        ('numerics', Pipeline([\n            ('extract',ColumnExtractor(con_cols)),\n            ('log', Log1pTransformer()),\n            ('col_Interact',DFadd_ColInteraction('Kilometers_Driven','Mileage'))\n        ])),\n        ('nominal_OneHot',Pipeline([\n            ('extract',ColumnExtractor(One_hot_cols)),\n            ('dummy',DummyTransformer())])),\n        ('nominal_Target', Pipeline([\n            ('extract',ColumnExtractor(Tar_cols)),\n            ('Mean_Enc',TargetEncoder())])),\n        ('Year',Pipeline([\n            ('extract',ColumnExtractor(Year)),\n            ('Shift',ShiftTranformer(par=2019))])),\n        ('Seats',Pipeline([\n            ('extract',ColumnExtractor(Seats)),\n            ('Select_OneHot',DF_OneHotEncoder(filter_threshold=0.05))]))\n        ])),\n    ('Model_fit',GradientBoostingRegressor())])\nParameter search space\npipe_params= {\n    'Model_fit__n_estimators': [100,150,200,250,500,750],\n    'Model_fit__learning_rate': [0.01,0.1,0.5,1],\n    'Model_fit__subsample': [0.1,0.2,0.5,1.0],\n}\nBest parametes\n{'Model_fit__learning_rate': 0.1,\n  'Model_fit__n_estimators': 500,\n  'Model_fit__subsample': 0.5}\nValiditon results:\n\n\nGradient boosting algo with lowest loss 0.033 is finally selected.\n\nFeature importances\n\nFalsk App Deployment\nAll the files reqiured for setting up Flask deployment are in webapp folder. To deploy app in your local, Clone the git repo to local system and execute the following commands in your terminal setting project folder as working directory.\nconda create -n CarPricePredict python=3.7 # create environment under 'CarPricePredict' name\nconda activate CarPricePredict # activate environment\ncd webApp/ # change directory to App deploymnet setup folder\npip3 install -r requirements.txt # install dependencies\npython3 application.py # launch application\nTechnologies Used\n\n\n\n\n\n\n\n\nCreator:\n\n""], 'url_profile': 'https://github.com/Skumarr53', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""Pred2Town - Prediction of Homicides in Urban Centers: A Machine Learning Approach\nAuthors:\nJosé Ribeiro - site: https://sites.google.com/view/jose-sousa-ribeiro\nLair Meneses - site: http://lattes.cnpq.br/5112686666929641\nDenis Costa - site: http://lattes.cnpq.br/8063713696787401\nWando Miranda - site: http://lattes.cnpq.br/6925939035060395\nRonnie Alves (Orientador) - site: https://sites.google.com/site/alvesrco/\nSearch summary\nRelevant research has been highlighted in the computing community to develop machine learning models capable of predicting the occurrence of crimes, analyzing contexts of crimes, extracting profiles of individuals linked to crime, and analyzing crimes over time. However, models capable of predicting specific crimes, such as homicide, are not commonly found in the current literature. This research presents a machine learning model to predict homicide crimes, using a dataset that uses generic data (without study location dependencies) based on incident report records for 34 different types of crimes, along with time and space data from crime reports. Experimentally, data from the city of Belém - Pará, Brazil was used. These data were transformed to make the problem generic, enabling the replication of this model to other locations. In the research, analyses were performed with simple and robust algorithms on the created dataset. With this, statistical tests were performed with 11 different classification methods and the results are related to the prediction’s occurrence and non-occurrence of homicide crimes in the month subsequent to the occurrence of other registered crimes, with 76% assertiveness for both classes of the problem, using Random Forest. Results are considered as a baseline for the proposed problem.\nImportant\nThe disclosure of statistical information present in this study is duly authorized by the Secretariat of Public Security and Social Defense of the State of Pará, Brazil. Document available at:\nAutorization\nThe procedures performed in this research can be divided into 5 parts:\nNote, in order not to expose the personal information of individuals, we do not provide in this repository information a raw database of reports of occurrence reports of the city of study.\nDue to issues related to information confidentiality, only tabular data, as a result of the cleaning and pre-processing processes carried out by this research, can be provided in this repository.\n1 - Pre-processing of raw data from police reports.\nThe main pre-processing and data transformations carried out were:\n\nExclusion of 9 sparse attributes (with unregistered data) and id;\nExclusion of 21 attributes not directly related to the crime context;\nExclusion of 2 attributes related to personal data of registered individuals;\nExclusion of 2 attributes related to the location of the crime that occurred due to inconsistency;\nExclusion of 2 attributes of crime’s georeferencing (latitude and longitude), due to inconsistency;\nRemoving special characters (such as: @ # $% ˆ & * áíóúç?! ºª • §∞ ¢ £ ™ ¡);\nConsolidation of neighborhoods in the city of study;\nExclusion of records related to occurrences in neighborhoods located on islands or in rural areas due to high inconsistency;\nExclusion of police reports instances considered non-crimes;\nExclusion of duplicate occurrence records (since a crime can be registered in more than one police report, in this case by different people);\n\n2 - Pre-processing of tabular data.\nThe pre-processing performed in the tabular database based on time and space, defended by this research are listed below:\n\nSelection of records for urbanized neighborhoods (removal of records for small islands).\nNormalization application (min-max) of quantitative attributes;\nApplication of transforming categorical data to ordinals (neighborhood);\nCreation of a binary Class attribute based on the number of homicides that occurred in the following month equal to zero (non-homicide) and greater than zero (homicide occurrence);\n\nThe data is in: dataset\nThe translation of data labels is in: Portuguese to English.\n3 - Tunning process.\nTable 2 presents all the best parameters found from the execution of the grid search process based on cross-validation with folds size equal to 7, and the metric used to measure the performance of each fold execution was the Area Under ROC – AUC. It was decided to use cross-validation at this stage of creation to identify the most stable machine learning models in the face of data as input.\n\n4 - Measurement of performances and statistical analysis.\nThe tests performed with the 11 algorithms were divided into two moments:\nA) Performance Analysis: Comparison of performances based on Accuracy - ACC and Confusion Matrices;\nB) Statistical Analysis: based on the Friedman test and score AUC.\nAll statistical analyzes of the 11 algorithms were performed using the Spyder 4.0.1 development environment through the python script developed by the authors:\nNotebook_Pred2Town_XAI_1.1\n5 - Visualization of RF results.\nIn order to present the performance of the RF algorithm in a contextualized way with the space of the study city, in Figure 1, a visualization layer created in the model's output is presented, which distributes each tested instance according to the neighborhood it belongs to. In this way, it is possible to have an understanding of which are the neighborhoods that the algorithm hits the most and in which it misses the most. Note that even without an algorithm receiving the neighborhood attribute as data entry, it manages to learn the patterns of the occurrence of crimes and thus can predict the occurrence of homicide.\n\nFigure 1 - Map with the results of the Random Forest algorithm.\nReferences\nAll references of research.\n""], 'url_profile': 'https://github.com/josesousaribeiro', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '233 contributions\n        in the last year', 'description': [""House Price Investigation\nGraduate Course: Quantitative Analysis for Business\nThis was a group project from a course on Quantitative Analysis for Business. Here's a brief summary of the project:\n\nOn an individual level, the real estate markets form as one of the most reliable ways for investors to grow their personal wealth. They qualify as a safe and potential investment option to secure income stability. On a much more global scale, property prices are becoming increasingly important and beneficial, the ability to identify price trends in properties serve as markers for indicating the overall market condition and the economic health of a country. This makes it a somewhat interesting investment category for analysis as there exists no standard source of real estate valuation.\nOur analysis emphasizes on investigating the different factors that are correlated with the price of a house in ten cities in the state of Washington. The cities that we studied were Seattle, Renton, Bellevue, Redmond, Kirkland, Issaquah, Kent, Auburn, Sammamish and Federal Way. We built a multiple linear regression model with variables such as number of bedrooms, number of bathrooms, square footage of living area, square footage of lot, number of floors, city, and age of the house out of 18 different variables in the beginning. After our analysis, we found that only square footage of lot does not add much to the average natural log of house price values when other features of the house remain same. The detailed explanation is provided in the report.\n\nHere's some information about all the files included in this repository:\n\nDataset - Data used for this analysis that was retrieved from Kaggle.com.\nData File (.sav) - Data file that was generated using SPSS Statistics software to perform the analysis.\nOutput File (.spv) - Output file in .spv format that was generated using SPSS Statistics software.\nProject Report - In-depth analysis details.\nPresentation - Final presentation summarizing the analysis for non-technical audience.\n\n""], 'url_profile': 'https://github.com/SagarBansal7', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '233 contributions\n        in the last year', 'description': [""Stroke and Health Status\nGraduate Course: Intermediate Statistical Modeling for Business\nThis was a group project from a course on Intermediate Statistical Modeling for Business. Here's a brief summary of the project:\n\nWith the improvements of living standards, people nowadays pay more attention to their health. This report talks about stroke, one of the biggest health problems in the U.S., focusing on the pre-existing health factors that will potentially rise one’s risk of getting a stroke. We investigated relationships between stroke and other factors including age, hypertension, heart disease, average glucose level, body mass index, and smoking status. With the logistic regression model we fitted, it is clear that within the six factors, body mass index is the only factor that has little association with whether a person will get a stroke or not. The cluster analysis model showed\nthat age and average glucose level are important factors that helped conclude the four different groups of people.\n\nHere's some information about all the files included in this repository:\n\nDataset - Data used for this analysis that was retrieved from Kaggle.com. The original dataset was published by Mckinsey & Company for one of their case studies.\nSource Code - Solution code that was written in SAS programming language.\nProject Report - In-depth analysis details.\nPresentation - Final presentation summarizing the analysis for non-technical audience.\n\n""], 'url_profile': 'https://github.com/SagarBansal7', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['DecisionTree_Zoo_Matlab\nLes arbres de décision sont une méthode d\'apprentissage supervisé non paramétrique pouvant être utilisée à la fois pour la classification et la régression. Les arbres de décision codent essentiellement un ensemble de règles « if - then - else » qui peuvent être utilisées pour prédire la variable cible en fonction des caractéristiques de données. Ces règles sont formées à l\'aide du jeu de données d\'apprentissage dans le but de satisfaire autant d\'instances de données d\'apprentissage que possible.\nLe problème de notre étude est de classifier des animaux selon leurs types, pour le bu prévoir si l’animale appartient à la classe Fish ou autres par exemple.\nDescription de la BDD:\nla base de données contient 17 attributs de valeur booléenne. L\'attribut ""type"" semble être l\'attribut de classe. 101 instances regroupée en 7 classes qui sont : Mamma, Bird, Reptile, Fish, Amphibian, Bug et invertebrate\nla base de données étant assez cohérente aucune normalisation n’a été appliquée.\nInformations sur les attributs:\n\nnom de l\'animal: unique pour chaque cas\npoil: booléen\nplumes: booléen\noeufs: booléen\nlait: booléen\naérien: booléen\naquatique: booléen\nprédateur: booléen\ndenté: booléen\n10 . épine dorsale: booléenne\nrespire: booléenne\nvenimeuse: booléenne\nnageoires: booléenne\njambes: numérique (ensemble de valeurs: {0,2,4,5,6,8})\nqueue: booléenne\ndomestique: booléen\ntaille du chat: booléen\ntype: numérique (valeurs entières dans la plage [1,7])\n\n'], 'url_profile': 'https://github.com/NouicerAniss', 'info_list': ['Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Nov 17, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', '1', 'Updated Sep 28, 2020', '1', 'SAS', 'Updated Sep 28, 2020', 'MATLAB', 'Updated Apr 28, 2020']}"
"{'location': 'Gurgaon , Haryana', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dhruvsaini00', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', '6', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'www.zerihunassociates.com ', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Predicting-ICU-Patient-Clinical-Deterioration---Report\nFor this project, I used publicly available Electronic Health Records (EHRs) datasets. The MIT Media Lab for Computational Physiology has developed MIMIC-IIIv1.4 dataset based on 46,520 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center of Boston between 2001 and 2012. MIMIC-IIIv1.4 dataset is freely available to researchers across the world. A formal request should be made directly to www.mimic.physionet.org, to gain access to the data. There is a required course on human research ‘Data or Specimens Only Research’ prior to data access request. I have secured one here -www.citiprogram.org/verify/?kb6607b78-5821-4de5-8cad-daf929f7fbbf-33486907. We built flexible and better performing model using the same 17 variables used in the SAPS II severity prediction model. The question ‘Can we improve the prediction performance of widely used severity scores using a more flexible model?’ is the central question of our project.\nLinked repositories: Exploratory-Data-Analysis-Clinical-Deterioration, Data-Wrangling-MIMICIII-Database, Clinical-Deterioration-Prediction-Model--Inferential-Statistics, Clinical-Deterioration-Prediction-Model--Ensemble-Algorithms-, Clinical-Deterioration-Prediction-Model--Logistic-Regression, Clinical-Deterioration-Prediction-Model---KNN\n'], 'url_profile': 'https://github.com/abebual', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', '6', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""Credit-Consumption-Prediction-Challenge\nProblem Statement\nUnderstanding the consumption pattern for credit cards at an individual consumer level is important for customer relationship management. This understanding allows banks to customize for consumers and make strategic marketing plans. Thus it is imperative to study the relationship between the characteristics of the consumers and their consumption patterns. Here the dataset is of some XYZ Bank that has given a sample of their customers, along with their details like age, gender and other demographics. Also shared are information on liabilities, assets and history of transactions with the bank for each customer. In addition to the above, data has been provided for a particular set of customers' credit card spend in the previous 3 months (April, May & June) and their expected average spend in the coming 3 months (July, August & September). The average spend for different set of customers needs to be predicted in the test set for the coming 3 months.\nData Dictionary\nid Unique ID for every Customer\naccount_type Account Type – current or saving\ngender Gender of customer-M or F\nage Age of customer\nregion_code Code assigned to region of residence (has order)\ncc_cons_apr Credit card spend in April\ndc_cons_apr Debit card spend in April\ncc_cons_may Credit card spend in May\ndc_cons_may Debit card spend in May\ncc_cons_jun Credit card spend in June\ndc_cons_jun Debit card spend in June\ncc_count_apr Number of credit card transactions in April\ncc_count_may Number of credit card transactions in May\ncc_count_jun Number of credit card transactions in June\ndc_count_apr Number of debit card transactions in April\ndc_count_may Number of debit card transactions in May\ndc_count_jun Number of debit card transactions in June\ncard_lim Maximum Credit Card Limit allocated\npersonal_loan_active Active personal loan with other bank\nvehicle_loan_active Active Vehicle loan with other bank\npersonal_loan_closed Closed personal loan in last 12 months\nvehicle_loan_closed Closed vehicle loan in last 12 months\ninvestment_1 DEMAT investment in june\ninvestment_2 fixed deposit investment in june\ninvestment_3 Life Insurance investment in June\ninvestment_4 General Insurance Investment in June\ndebit_amount_apr Total amount debited for April\ncredit_amount_apr Total amount credited for April\ndebit_count_apr Total number of times amount debited in april\ncredit_count_apr Total number of times amount credited in april\nmax_credit_amount_apr Maximum amount credited in April\ndebit_amount_may Total amount debited for May\ncredit_amount_may Total amount credited for May\ncredit_count_may Total number of times amount credited in May\ndebit_count_may Total number of times amount debited in May\nmax_credit_amount_may Maximum amount credited in May\ndebit_amount_jun Total amount debited for June\ncredit_amount_jun Total amount credited for June\ncredit_count_jun Total number of times amount credited in June\ndebit_count_jun Total number of times amount debited in June\nmax_credit_amount_jun Maximum amount credited in June\nloan_enq Loan enquiry in last 3 months (Y or N)\nemi_active Monthly EMI paid to other bank for active loans\ncc_cons (Target) Average Credit Card Spend in next three months\nEvaluation Metric\nSubmissions are evaluated on Root Mean Squared Logarithmic Error(RMSLE) between the predicted credit card consumption and the observed target.\nApproach\nAt first, I conducted exploratory data analysis of the dataset to gain a deeper understanding of the data. Next, I did feature engineering to create new variables.Then I tried some scikit-learn models out of which XGBoost and Random Forest gave good RMSLE. In the end I created a stacked model of those two with Linear Regression and it has been selected as the final model.\nRMSLE: 115.02\n""], 'url_profile': 'https://github.com/iftekarpatel', 'info_list': ['1', 'Python', 'Updated Apr 27, 2020', '6', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}",,,,,,,
