"{'location': 'Golden, Colorado, USA', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': [""Resampling for regression (resreg)\nResreg is a Python package for resampling imbalanced distributions in regression problems.\nIf you find resreg useful, please cite the following article:\n\nGado, J.E., Beckham, G.T., and Payne, C.M (2020). Improving enzyme optimum temperature prediction with resampling strategies and ensemble learning. J. Chem. Inf. Model. 60(8), 4098-4107.\n\nIf you use RO, RU, SMOTER, GN, or WERCS methods, also cite\n\nBranco, P., Torgo, L., and Ribeiro, R.P. (2019). Pre-processing approaches for imbalanced distributions in regression. Neurocomputing. 343, 76-99.\n\nIf you use REBAGG, also cite\n\nBranco, P., Torgo, L., and Ribeiro, R.P. (2018). REBAGG: Resampled bagging for imbalanced regression. In 2nd International Workshop on Learning with Imbalanced Domains: Theory and Applications. pp 67-81.\n\nIf you use precision, recall, or F1-score for regression, also cite\n\nTorgo, L. and Ribeira, R.P. (2009). Precision and recall for regression. In International Conference on Discovery Science. pp332-346\n\n\nInstallation\nPreferrably, install from GitHub source. The use of a virtual environment is strongly advised.\ngit clone https://github.com/jafetgado/resreg.git\ncd resreg\npip install -r requirements.txt\npython setup.py install\n\nOr, install with pip (less preferred)\npip install resreg\n\n\nPrerequisites\n\nPython 3\nNumpy\nScipy\nPandas\nScikit-learn\n\n\nUsage\nA regression dataset (X, y) can be resampled to mitigate the imbalance in the distribution with any of six strategies: random oversampling, random undersampling, SMOTER, Gaussian noise, WERCS, or Rebagg.\n\nRandom oversampling (RO): randomly oversample rare values selected by the user via a relevance function.\nRandom undersampling (RU): randomly undersample abundant values.\nSMOTER: randomly undersample abundant values; oversample rare values by interpolation between nearest neighbors.\nGaussian noise (GN): randomly undersample abundant values; oversample rare values by adding Gaussian noise.\nWERCS: resample the dataset by selecting instances using user-specified relevance values as weights.\nREBAGG: Train an ensemble of Scikit-learn base learners on independently resampled bootstrap subsets of the dataset.\n\nSee the tutorial for more details.\n\nExamples\nimport resreg\nfrom sklearn.metrics import train_test_split\nfrom sklearn.metrics import RandomForestRegressor\n\n# Split dataset to training and testing sets\nX_train, X_test, y_train, y_test = resreg.train_test_split(X, y, test_size=0.25)\n\n# Resample training set with random oversampling\nrelevance = resreg.sigmoid_relevance(y, cl=None, ch=np.percentile(y, 90))\nX_train, y_train = resreg.random_oversampling(X_train, y_train, relevance, relevance_threshold=0.5,\n                                              over='balance')\n\n# Fit regressor to resampled training set\nreg = RandomForestRegressor()\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_train, y_train)\n""], 'url_profile': 'https://github.com/jafetgado', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Monterrey, Mexico', 'stats_list': [], 'contributions': '418 contributions\n        in the last year', 'description': ['Analisis de dataset de videojuegos con Regression Lineal y metodos de regularizacion\nEl proyecto se divide en 2 partes\nI. Explicacion e intuicion detras de los métodos, utilizando pocos datos\n\n\n\n\nII. Analisis de dataset de videojuegos\nLink del dataset: https://www.kaggle.com/xapulc/jpsales\nGracias Victor Kharlamov por crear el dataset.\nAntes de alimentar el modelo se hace un proceso de ingenieria de datos para seleccionar features, remover outliers, datos faltantes, etc.\n\n\nPara la prediccion se usan los features de\n\nVentas en Europa\nVentas en el resto del mundo\nCalificacion de los usuario\n\nY se busca predecir las ventas en Norteamerica (NA_Sales).\nPrimero se utiliza regresion lineal, y después los metodos de regularizacion ridge, lasso y elastic net regression.\n\n\n'], 'url_profile': 'https://github.com/Baltazar-Ortega', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshitroy2605', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['postLogistic\nTools after Logistic Regression\nThe package contains three functions, all of which focus on the threshold of a binary classifier, namely the cut point of the predicted probabilities.\nAll the functions take in the same two main parameters: the predicted probabilities and the ""truth"".\nROC()\nThe function returns an R data frame, each row containing the True Positive Rate (tpr) and False Positive Rate (fpr), according to the same threshold.\nplotROC()\nThe function plots the ROC curve.\nthreshold()\nThe function searches the optimal threhold, according to three different metrics, Accuracy, Balanced Accuracy and F1 Score. It returns an R data frame containing the three optimal thresholds.\n'], 'url_profile': 'https://github.com/ZhengAndyTan', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Florianópolis, Brazil', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/igortg', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '1,013 contributions\n        in the last year', 'description': ['Breast-cancer-detection\nBreast cancer detection using 4 different models i.e. Logistic Regression, KNN, SVM and Decision Tree Machine Learning models and optimising them for even a better accuracy. This project is started with the goal use machine learning algorithms and learn how to optimize the tuning params and also and hopefully to help some diagnoses.\nProblem Statement\nThis project focuses in investigating the probability of predicting the chances of breast cancer from the given   characteristics of breast mass computed from dataset. This project will examine the data available and attempt to predict the possibility that a breast cancer. To achieve this goal, the following steps are identified:\n•\tDownload the breast cancer data from UCI repository\n•\tFamiliarize with the data by looking at its shape, the relations between variables, their possible correlations, and other attributes of the dataset.\n•\tPreprocess data if needed\n•\tSplit the data into testing and training samples\n•\tEmploy various classifiers (K-neighbors, Decision trees, SVC and Random Forest classifier) to predict the data with different sets of training samples (100, 200, 300, and 400).\n•\tOnce the best predicting model is identified, will reduce the training set in size to see what is the limit for this classifier to best predict these data.\n•\tCompare the best identified classifier with evaluation metric stated at the beginning of the project.\n•\tWrite conclusions.\nProject Overview\nAccording to the Centers for Disease Control and Prevention (CDC) breast cancer is the most common type of cancer for women regardless of race and ethnicity (CDC, 2016). Around 220,000 women are diagnosed with breast cancer each year in the United States (CDC, 2016). Although we may not be aware of all the factors contributing in developing breast cancer, certain attributes such as family history, age, obesity, alcohol and tobacco use have been identified from research studies on this topic (DeSantis, Ma, Bryan, & Jemal, 2014). Breast images procedures such as mammography have been found to be quite effective in early identification cases of breast cancer (Ball, 2012).  When breast images procedures are not utilized, patients can find out late about their diagnosis to be able to treat it.  Similar work on attempting to find the best way to predict the type of cancer based on images of mammograms has identified Support Vector Machine as the best predictor after tuning parameters.\nLibraries used\nimport numpy as np #for linear algebra\nimport pandas as pd #for chopping, processing\nimport csv #for opening csv files\n%matplotlib inline \nimport matplotlib.pyplot as plt #for plotting the graphs\nfrom scipy import stats #for statistical info\nfrom time import time\n\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split # to split the data in train and test\nfrom sklearn.model_selection import KFold # for cross validation\nfrom sklearn.grid_search import GridSearchCV  # for tuning parameters\nfrom sklearn import metrics  # for checking the accuracy \n\n#Classifiers \n\nfrom sklearn import svm #for Support Vector Machines\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report , confusion_matrix #for Logistic regression\nfrom sklearn.svm import SVC # for support vector classifier\nfrom sklearn.neighbors import NearestNeighbors #for nearest neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier # for K neighbor classifier\nfrom sklearn.tree import DecisionTreeClassifier #for decision tree classifier\nfrom sklearn.ensemble import RandomForestClassifier #for Random Forest\n###How to\nTo run the scripts you just type:\npython script_name.py\nAs result of execution the reached accuracy will print\nDataset and Inputs\nThe characteristics of the cell nuclei have been captured in the images and a classification methods which uses linear programming to construct a decision line. The dataset is published by Kaggle and taken from the University of California Irvine (UCI) machine learning repository.  The data is taken from the Breast Cancer Wisconsin Center. It includes ten (10) attributes taken from each cell nucleus as well as ID and the diagnosis (M=malignant, B=benign).  The dataset has 570 cases and 31 variables.\n\nthe dataset can be found here\n\n'], 'url_profile': 'https://github.com/Bhard27', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '708 contributions\n        in the last year', 'description': ['Predicting House Prices in King County\nThis multiple linear regression project was completed as part of Flatiron School\'s Data Science Bootcamp (Module 2 Final Project).\nProblem statement\nAs a junior data scientist at real estate company PropertiesInc., I have been tasked with investigating house sales in the King County area and building a model to predict sale price. Key executives are keen to launch an advertising campaign directed towards home owners in that area who might consider selling their house, focusing on higher-end residential properties.\nBefore building the model, we will address the following descriptive questions through data exploration:\n\n\nWhich locations within the King County area have the highest average house prices?\n\n\nWhich house attributes increase sale price?\n\n\nDoes time of the year have an impact on house sales?\n\n\nComponents\n\nJupyter Notebook\n\nThe Jupyter Notebook is our key deliverable and contains details of our approach and methodology, data cleaning, exploratory data analysis and model building and validation. The key models have also been saved using Pickle.\nI recommend using nbviewer to view the Jupyter Notebook.\n\nPresentation\n\nThe presentation gives a high-level overview of our approach, findings and recommendations for non-technical stakeholders. It is aimed to be between 5 and 10 minutes long.\n\nData\n\nThe dataset can be found in the file ""kc_house_data.csv"" in the Data folder, in this repository. It was originally provided in the following repository.\n\nMisc\n\nWe found a GEOJSON file with zip code borders, which was used as part of data exploration. It is included in the repository.\n\nBlog Post\n\nA blog post was created as part of this project.\nTechnologies/ Packages\n\nPython version: 3.6.9\nMatplotlib version: 3.1.3\nSeaborn version: 0.9.0\nPandas version: 0.25.1\nNumpy version: 1.16.5\nStatsmodels version: 0.10.1\nScikit-learn version: 0.21.2\nBokeh version: 2.0.2\nFolium version: 0.9.1\nGeopandas version: 0.7.0\nGeopy version: 1.21.0\nReverse_geocoder version: 1.5.1\nPickleshare version: 0.7.5\n\nTo get started\n\nClone this repository - guidance.\nDataset can be found in the file ""kc_house_data.csv"".\nCheck requirements in Technologies section above and download libraries if necessary.\n\nSummary of EDA\nQ1 Location\n\nWaterfront living is key, with the median house price for a house with a waterfront view being almost double that of one that does not have this feature.\nWe recommend focusing the campaign on the following neighbourhoods: Medina, Clyde Hill and Mercer Island (the most expensive neighbourhoods).\nLocation within King County is important with a noticeable disparity amongst zipcodes. The median house price ranges from $235,000 in 98002 up to $1,260,000 in 98039.\n\n\nQ2 House attributes\n\nWe recommend targetting the campaign towards houses with a higher bedroom count. However for a given house depending on its square-footage, note that adding an additional bedroom does not necessarily result in a a sale price increase.\nThere does not appear to be a clear relationship between the ratio sqft_living/sqft_lot and price. This indicates that there is unlikely to be an idea area to allocate to living space within a plot.\nThe median house price increases with grade indicating that these features are positively correlated. We suspect grade will be a good indicator of price.\nFor the campaign we would recommend looking at houses with a grade of 10 or above.\n\n\nQ3 Time of the year\n\nHouse prices do not appear to be affected by sale month or quarter, with the median house price being almost constant throughout the year.\nApril and May are the most popular months for house sales. In contrast, January and February have the lowest number of sales Q2 alone accounts for 31.3% of house sales.\nWe would recommend launching the campaign in March/April, to gather interest with a view of completing the sale in Q2.\n\n\nFinal model details\nModel A\n\n17 features\nAdjusted  R-squared  of 0.701 (70% of variations explained by our model)\nUses zip code tiers instead of actual zipcodes\nBetter for generalising to other areas\nRMSE of 132,444 (mean RMSE with 10-fold cross-validation)\n\nModel B\n\n87 features\nNo interacting terms or polynomials\nAdjusted R-squared  of 0.832 (83% of variations explained by our model)\nRMSE of 99,654 (mean RMSE with 10-fold cross-validation)\n\nInterpreting Model coefficients\n\nA grade 12 house on average is worth USD 52,000 more than grade 11 (model A)\nBeing on the waterfront is valued at USD 277,442 (model A)\nFor every additional squarefoot of living space, the price inscreases by USD 123.5 (model B)\n\nContributors:\n\n\n\nName\nGitHub\n\n\n\n\nNadine Amersi-Belton\nhttps://github.com/nadinezab\n\n\n\nContact\n\nIf you have any questions, you can contact me at nzamersi@gmail.com\n\n'], 'url_profile': 'https://github.com/nadinezab', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Noida', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/parthsahai04', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '303 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aliensmart', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Regression\nBasic code templates for ;\n\nSimple Linear Regression\n\n'], 'url_profile': 'https://github.com/GizemCidal', 'info_list': ['7', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Oct 8, 2020', '3', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '4', 'R', 'Updated Apr 23, 2020', '9', 'Python', 'Updated Apr 22, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Aug 3, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated May 28, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/george200150', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Regression Analysis\nProgram utilizes regressiona and MSE to predict the number of days a lake will be frozen over in ice. Dataset includes years and number of days frozen in a year since 1855 to present.\nUsing iterative gradient descent and iteravtive stochastic gradient descent to reduce the MSE and find a near optimal regression fit\n'], 'url_profile': 'https://github.com/guskalivas', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yas2020', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shravankumar649', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Geethu1502', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Liverpool', 'stats_list': [], 'contributions': '890 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/peterprescott', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/genggengjing', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'San Francisco, Bay Area', 'stats_list': [], 'contributions': '606 contributions\n        in the last year', 'description': ['Regression\nYou must install TensorFlow before running the project. Refer to this page on how to do that\nSome more references. Model training\nUsing raw tensorflow operations. Tensorflow\n'], 'url_profile': 'https://github.com/ByCyril', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Sao Paulo, Brazil', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['Regression\nRegression is simple and efficient statistics learning algorithm, used as a baseline for other more complex methods. This repository contains the code and algorithm of linear regression, regularization and logistic regression for binary and multiclass classification.\nLogistic regression implements the sigmoid function for binary classification and softmax for multiclass classification. Gradient descent is used to calculate the variable learning rate.\nContents\ncode.py- Python code.\nReport - The report describes the theoretical fundamentals, datasets, algorithm and result analysis. Contains the PDF and R Markdown files.\ndiabetes: Dataset used to train the model.\n'], 'url_profile': 'https://github.com/vivamoto', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Lakshaygrvr', 'info_list': ['1', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Python', 'Updated Apr 26, 2020', 'TeX', 'MIT license', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', '1', 'MIT license', 'Updated Apr 26, 2020', '1', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amrBelasy', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['{\n""cells"": [\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""# 线性回归的概念""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""1、线性回归的原理  \\n"",\n""  \\n"",\n""2、线性回归损失函数、代价函数、目标函数  \\n"",\n""  \\n"",\n""3、优化方法(梯度下降法、牛顿法、拟牛顿法等)  \\n"",\n""  \\n"",\n""4、线性回归的评估指标  \\n"",\n""  \\n"",\n""5、sklearn参数详解""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""## 1、线性回归的原理\\n"",\n""\\n"",\n""\\n""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""进入一家房产网，可以看到房价、面积、厅室呈现以下数据：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""\\n"",\n""\\n"",\n""    \\n"",\n""        \\n"",\n""        \\n"",\n""        \\n"",\n""    \\n"",\n""    \\n"",\n""        \\n"",\n""        \\n"",\n""        \\n"",\n""    \\n"",\n""    \\n"",\n""        \\n"",\n""        \\n"",\n""        \\n"",\n""    \\n"",\n""    \\n"",\n""        \\n"",\n""        \\n"",\n""        \\n"",\n""    \\n"",\n""    \\n"",\n""        \\n"",\n""        \\n"",\n""        \\n"",\n""    \\n"",\n""    \\n"",\n""        \\n"",\n""        \\n"",\n""        \\n"",\n""    ""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""我们可以将价格和面积、厅室数量的关系习得为$f(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2$，使得$f(x)\\approx y$，这就是一个直观的线性回归的样式。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""### 线性回归的一般形式：\\n"",\n""有数据集$\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$,其中,$x_i = (x_{i1};x_{i2};x_{i3};...;x_{id}),y_i\\in R$ \\n"",\n""其中n表示变量的数量，d表示每个变量的维度。  \\n"",\n""可以用以下函数来描述y和x之间的关系：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""\\begin{align*}\\n"",\n""f(x) \\n"",\n""&= \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_dx_d  \\\\\\n"",\n""&= \\sum_{i=0}^{d}\\theta_ix_i \\\\\\n"",\n""\\end{align*}""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""如何来确定$\\theta$的值，使得$f(x)$尽可能接近y的值呢？均方误差是回归中常用的性能度量，即：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$J(\\theta)=\\frac{1}{2}\\sum_{j=1}^{n}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$  ""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""我们可以选择$\\theta$，试图让均方误差最小化：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""### 极大似然估计（概率角度的诠释）""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""下面我们用极大似然估计，来解释为什么要用均方误差作为性能度量""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""我们可以把目标值和变量写成如下等式：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""y^{(i)} = \\theta^T x^{(i)}+\\epsilon^{(i)}\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$\\epsilon$表示我们未观测到的变量的印象，即随机噪音。我们假定$\\epsilon$是独立同分布，服从高斯分布。（根据中心极限定理）""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2}\\right)$$\\n""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""因此，""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""p(y^{(i)}|x^{(i)};\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2}\\right)\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""我们建立极大似然函数，即描述数据遵从当前样本分布的概率分布函数。由于样本的数据集独立同分布，因此可以写成""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""L(\\theta) = p(\\vec y | X;\\theta) = \\prod^n_{i=1}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2}\\right)\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""选择$\\theta$，使得似然函数最大化，这就是极大似然估计的思想。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""为了方便计算，我们计算时通常对对数似然函数求最大值：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""\\begin{align*}\\n"",\n""l(\\theta) \\n"",\n""&= log L(\\theta) = log \\prod^n_{i=1}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{(y^{(i)}-\\theta^T x^{(i)})^2} {2\\sigma^2}\\right) \\\\\\n"",\n""& = \\sum^n_{i=1}log\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2}\\right) \\\\\\n"",\n""& = nlog\\frac{1}{{\\sqrt{2\\pi}\\sigma}} - \\frac{1}{\\sigma^2} \\cdot \\frac{1}{2}\\sum^n_{i=1}((y^{(i)}-\\theta^T x^{(i)})^2\\n"",\n""\\end{align*}""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""显然，最大化$l(\\theta)$即最小化 $\\frac{1}{2}\\sum^n_{i=1}((y^{(i)}-\\theta^T x^{(i)})^2$。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""这一结果即均方误差，因此用这个值作为代价函数来优化模型在统计学的角度是合理的。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""## 2、线性回归损失函数、代价函数、目标函数\\n"",\n""* 损失函数(Loss Function)：度量单样本预测的错误程度，损失函数值越小，模型就越好。\\n"",\n""* 代价函数(Cost Function)：度量全部样本集的平均误差。\\n"",\n""* 目标函数(Object Function)：代价函数和正则化函数，最终要优化的函数。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""常用的损失函数包括：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数等；常用的代价函数包括均方误差、均方根误差、平均绝对误差等。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""#### 回答：\\n"",\n""当模型复杂度增加时，有可能对训练集可以模拟的很好，但是预测测试集的效果不好，出现过拟合现象，这就出现了所谓的“结构化风险”。结构风险最小化即为了防止过拟合而提出来的策略，定义模型复杂度为$J(F)$，目标函数可表示为：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\underset{f\\in F}{min}\\, \\frac{1}{n}\\sum^{n}{i=1}L(y_i,f(x_i))+\\lambda J(F)$$""\n]\n},\n{\n""attachments"": {\n""image.png"": {\n""image/png"": ""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""\n}\n},\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""例如有以上6个房价和面积关系的数据点，可以看到，当设定$f(x)=\\sum{j=0}^{5}\\theta_jx_j$时，可以完美拟合训练集数据，但是，真实情况下房价和面积不可能是这样的关系，出现了过拟合现象。当训练集本身存在噪声时，拟合曲线对未知影响因素的拟合往往不是最好的。\\n"",\n""通常，随着模型复杂度的增加，训练误差会减少；但测试误差会先增加后减小。我们的最终目的时试测试误差达到最小，这就是我们为什么需要选取适合的目标函数的原因。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""## 3、线性回归的优化方法""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""### 1、梯度下降法""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""设定初始参数$\\theta$,不断迭代，使得$J(\\theta)$最小化：\\n"",\n""$$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial{J(\\theta)}}{\\partial\\theta}$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""\\begin{align*}\\n"",\n""\\frac{\\partial{J(\\theta)}}{\\partial\\theta} \\n"",\n""&= \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2}\\sum_{i=1}^{n}(f_\\theta(x)^{(i)}-y^{(i)})^2 \\\\\\n"",\n""&= 2*\\frac{1}{2}\\sum_{i=1}^{n}(f_\\theta(x)^{(i)}-y^{(i)})\\frac{\\partial}{\\partial\\theta_j}(f_\\theta(x)^{(i)}-y^{(i)}) \\\\\\n"",\n""&= \\sum_{i=1}^{n}(f_\\theta(x)^{(i)}-y^{(i)})\\frac{\\partial}{\\partial\\theta_j}(\\sum_{j=0}^{d}\\theta_jx_j^{(i)}-y^{(i)}))\\\\\\n"",\n""&= \\sum_{i=1}^{n}(f_\\theta(x)^{(i)}-y^{(i)})x_j^{(i)} \\\\\\n"",\n""\\end{align*}""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""即：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""\\theta_j = \\theta_j + \\alpha\\sum_{i=1}^{n}(y^{(i)}-f_\\theta(x)^{(i)})x_j^{(i)}\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""注：下标j表示第j个参数，上标i表示第i个数据点。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""将所有的参数以向量形式表示，可得：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""\\theta = \\theta + \\alpha\\sum_{i=1}^{n}(y^{(i)}-f_\\theta(x)^{(i)})x^{(i)}\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""由于这个方法中，参数在每一个数据点上同时进行了移动，因此称为批梯度下降法，对应的，我们可以每一次让参数只针对一个数据点进行移动，即：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""\\theta = \\theta + \\alpha(y^{(i)}-f_\\theta(x)^{(i)})x^{(i)}\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""这个算法成为随机梯度下降法，随机梯度下降法的好处是，当数据点很多时，运行效率更高；缺点是，因为每次只针对一个样本更新参数，未必找到最快路径达到最优值，甚至有时候会出现参数在最小值附近徘徊而不是立即收敛。但当数据量很大的时候，随机梯度下降法经常优于批梯度下降法。""\n]\n},\n{\n""attachments"": {\n""image.png"": {\n""image/png"": ""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""\n}\n},\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""当J为凸函数时，梯度下降法相当于让参数$\\theta$不断向J的最小值位置移动""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""梯度下降法的缺陷：如果函数为非凸函数，有可能找到的并非全局最优值，而是局部最优值。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""### 2、最小二乘法矩阵求解""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""令\\n"",\n""$$ X = \\left[ \\begin{array} {cccc}\\n"",\n""(x^{(1)})^T\\\\\\n"",\n""(x^{(2)})^T\\\\\\n"",\n""\\ldots \\\\\\n"",\n""(x^{(n)})^T\\n"",\n""\\end{array} \\right] $$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""其中，""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$x^{(i)} = \\left[ \\begin{array} {cccc}\\n"",\n""x_1^{(i)}\\\\\\n"",\n""x_2^{(i)}\\\\\\n"",\n""\\ldots \\\\\\n"",\n""x_d^{(i)}\\n"",\n""\\end{array} \\right]$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""由于""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$Y = \\left[ \\begin{array} {cccc}\\n"",\n""y^{(1)}\\\\\\n"",\n""y^{(2)}\\\\\\n"",\n""\\ldots \\\\\\n"",\n""y^{(n)}\\n"",\n""\\end{array} \\right]$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$h_\\theta(x)$可以写作""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$h_\\theta(x)=X\\theta$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""对于向量来说，有""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$z^Tz = \\sum_i z_i^2$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""因此可以把损失函数写作""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$J(\\theta)=\\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y)$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""为最小化$J(\\theta)$,对$\\theta$求导可得：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""\\begin{align*}\\n"",\n""\\frac{\\partial{J(\\theta)}}{\\partial\\theta} \\n"",\n""&= \\frac{\\partial}{\\partial\\theta} \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) \\\\\\n"",\n""&= \\frac{1}{2}\\frac{\\partial}{\\partial\\theta} (\\theta^TX^TX\\theta - Y^TX\\theta-\\theta^T X^TY - Y^TY) \\\\\\n"",\n""\\end{align*}""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""中间两项互为转置，由于求得的值是个标量，矩阵与转置相同，因此可以写成""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""\\begin{align*}\\n"",\n""\\frac{\\partial{J(\\theta)}}{\\partial\\theta} \\n"",\n""&= \\frac{1}{2}\\frac{\\partial}{\\partial\\theta} (\\theta^TX^TX\\theta - 2\\theta^T X^TY - Y^TY) \\\\\\n"",\n""\\end{align*}""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""令偏导数等于零，由于最后一项和$\\theta$无关，偏导数为0。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""因此，""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\frac{\\partial{J(\\theta)}}{\\partial\\theta}  = \\frac{1}{2}\\frac{\\partial}{\\partial\\theta} \\theta^TX^TX\\theta - \\frac{\\partial}{\\partial\\theta} \\theta^T X^TY\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""利用矩阵求导性质，  \\n"",\n""\\n"",\n""\\n""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""\\frac{\\partial \\vec x^T\\alpha}{\\partial \\vec x} =\\alpha \\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""和\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\frac{\\partial A^TB}{\\partial \\vec x} = \\frac{\\partial A^T}{\\partial \\vec x}B + \\frac{\\partial B^T}{\\partial \\vec x}A$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n"" ""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""\\n"",\n""\\begin{align*}\\n"",\n""\\frac{\\partial}{\\partial\\theta} \\theta^TX^TX\\theta \\n"",\n""&= \\frac{\\partial}{\\partial\\theta}{(X\\theta)^TX\\theta}\\\\\\n"",\n""&= \\frac{\\partial (X\\theta)^T}{\\partial\\theta}X\\theta + \\frac{\\partial (X\\theta)^T}{\\partial\\theta}X\\theta \\\\\\n"",\n""&= 2X^TX\\theta\\n"",\n""\\end{align*}""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\frac{\\partial{J(\\theta)}}{\\partial\\theta} = X^TX\\theta - X^TY\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""令导数等于零，""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$X^TX\\theta = X^TY$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\theta = (X^TX)^{(-1)}X^TY\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""注：CS229视频中吴恩达的推导利用了矩阵迹的性质，可自行参考学习。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""### 3、牛顿法""\n]\n},\n{\n""attachments"": {\n""image.png"": {\n""image/png"": ""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""\n}\n},\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""通过图例可知(参考吴恩达CS229),""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$f(\\theta)\' = \\frac{f(\\theta)}{\\Delta},\\Delta = \\theta_0 - \\theta_1$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$可求得，\\theta_1 = \\theta_0 - \\frac {f(\\theta_0)}{f(\\theta_0)\'}$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""重复迭代，可以让逼近取到$f(\\theta)$的最小值""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""当我们对损失函数$l(\\theta)$进行优化的时候，实际上是想要取到$l\'(\\theta)$的最小值，因此迭代公式为：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""\\theta :=\\theta-\\frac{l\'(\\theta)}{l\'\'(\\theta)}\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$\\n"",\n""当\\theta是向量值的时候，\\theta :=\\theta - H^{-1}\\Delta_{\\theta}l(\\theta)\\n"",\n""$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""其中，$\\Delta_{\\theta}l(\\theta)$是$l(\\theta)$对$\\theta_i$的偏导数，$H$是$J(\\theta)$的海森矩阵，\\n"",\n""$$H_{ij} = \\frac{\\partial ^2l(\\theta)}{\\partial\\theta_i\\partial\\theta_j}$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""问题：请用泰勒展开法推导牛顿法公式。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""Answer：将$f(x)$用泰勒公式展开到第二阶，""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$f(x) = f(x_0) + f\'(x_0)(x - x_0)+\\frac{1}{2}f\'\'(x_0)(x - x_0)^2$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""对上式求导，并令导数等于0，求得x值""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$f\'(x) = f\'(x_0) + f\'\'(x_0)x -f\'\'(x_0)x_0 = 0$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""可以求得，""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$x = x_0 - \\frac{f\'(x_0)}{f\'\'(x_0)}$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""牛顿法的收敛速度非常快，但海森矩阵的计算较为复杂，尤其当参数的维度很多时，会耗费大量计算成本。我们可以用其他矩阵替代海森矩阵，用拟牛顿法进行估计，""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""### 4、拟牛顿法""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""拟牛顿法的思路是用一个矩阵替代计算复杂的海森矩阵H，因此要找到符合H性质的矩阵。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""要求得海森矩阵符合的条件，同样对泰勒公式求导$f\'(x) = f\'(x_0) + f\'\'(x_0)x -f\'\'(x_0)x_0$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""令$x = x_1$，即迭代后的值，代入可得：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$f\'(x_1) = f\'(x_0) + f\'\'(x_0)x_1 - f\'\'(x_0)x_0$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""更一般的，""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$f\'(x_{k+1}) = f\'(x_k) + f\'\'(x_k)x_{k+1} - f\'\'(x_k)x_k$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$f\'(x_{k+1}) - f\'(x_k)  = f\'\'(x_k)(x_{k+1}- x_k)= H(x_{k+1}- x_k)$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$x_k$为第k个迭代值""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""即找到矩阵G，使得它符合上式。\\n"",\n""常用的拟牛顿法的算法包括DFP，BFGS等，作为选学内容，有兴趣者可自行查询材料学习。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""## 4、线性回归的评价指标""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""均方误差(MSE):$\\frac{1}{m}\\sum^{m}{i=1}(y^{(i)} - \\hat y^{(i)})^2$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""均方根误差(RMSE)：$\\sqrt{MSE} = \\sqrt{\\frac{1}{m}\\sum^{m}{i=1}(y^{(i)} - \\hat y^{(i)})^2}$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""平均绝对误差(MAE)：$\\frac{1}{m}\\sum^{m}{i=1} | (y^{(i)} - \\hat y^{(i)} | $""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""但以上评价指标都无法消除量纲不一致而导致的误差值差别大的问题，最常用的指标是$R^2$,可以避免量纲不一致问题""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""$$R^2: = 1-\\frac{\\sum^{m}{i=1}(y^{(i)} - \\hat y^{(i)})^2}{\\sum^{m}{i=1}(\\bar y - \\hat y^{(i)})^2} =1-\\frac{\\frac{1}{m}\\sum^{m}{i=1}(y^{(i)} - \\hat y^{(i)})^2}{\\frac{1}{m}\\sum^{m}{i=1}(\\bar y - \\hat y^{(i)})^2} = 1-\\frac{MSE}{VAR}$$""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""我们可以把$R^2$理解为，回归模型可以成功解释的数据方差部分在数据固有方差中所占的比例，$R^2$越接近1，表示可解释力度越大，模型拟合的效果越好。""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""## 5、sklearn.linear_model参数详解：""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""fit_intercept : 默认为True,是否计算该模型的截距。如果使用中心化的数据，可以考虑设置为False,不考虑截距。注意这里是考虑，一般还是要考虑截距\\n"",\n""\\n"",\n""normalize: 默认为false. 当fit_intercept设置为false的时候，这个参数会被自动忽略。如果为True,回归器会标准化输入参数：减去平均值，并且除以相应的二范数。当然啦，在这里还是建议将标准化的工作放在训练模型之前。通过设置sklearn.preprocessing.StandardScaler来实现，而在此处设置为false\\n"",\n""\\n"",\n""copy_X : 默认为True, 否则X会被改写\\n"",\n""\\n"",\n""n_jobs: int 默认为1. 当-1时默认使用全部CPUs ??(这个参数有待尝试)\\n"",\n""\\n"",\n""可用属性：\\n"",\n""\\n"",\n""coef:训练后的输入端模型系数，如果label有两个，即y值有两列。那么是一个2D的array\\n"",\n""\\n"",\n""intercept_: 截距\\n"",\n""\\n"",\n""可用的methods:\\n"",\n""\\n"",\n""fit(X,y,sample_weight=None):\\n"",\n""X: array, 稀疏矩阵 [n_samples,n_features]\\n"",\n""y: array [n_samples, n_targets]\\n"",\n""sample_weight: 权重 array [n_samples]\\n"",\n""在版本0.17后添加了sample_weight\\n"",\n""\\n"",\n""get_params(deep=True)： 返回对regressor 的设置值\\n"",\n""\\n"",\n""predict(X): 预测 基于 R^2值\\n"",\n""\\n"",\n""score： 评估\\n"",\n""\\n"",\n""参考https://blog.csdn.net/weixin_39175124/article/details/79465558""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""<table align =""left"";background-color=""#87CEEB"">\\n"",\n""\\n"",\n""    <td bgcolor=""#87CEEB"">练习题：请用以下数据（可自行生成尝试，或用其他已有数据集）\\n"",\n""\\n"",\n""\\n"",\n""<td  bgcolor=""#87CEEB"">1、首先尝试调用sklearn的线性回归函数进行训练；\\n"",\n""\\n"",\n""\\n"",\n""<td bgcolor=""#87CEEB"">2、用最小二乘法的矩阵求解法训练数据；\\n"",\n""\\n"",\n""    \\n"",\n""<td  bgcolor=""#87CEEB"">3、用梯度下降法训练数据；\\n"",\n""\\n"",\n""\\n"",\n""    <td  bgcolor=""#87CEEB"">4、比较各方法得出的结果是否一致。\\n"",\n""\\n"",\n""面积($x_1$)厅室数量($x_2)$价格(万元)(y)6432255931856532081164508………………\\n"",\n""    \\n"",\n""小练习：这是国内一个房产网站上任意搜的数据，有兴趣可以找个网站观察一下，还可以获得哪些可能影响到房价的因素？可能会如何影响到实际房价呢？\\n"",\n""思考题：既然代价函数已经可以度量样本集的平均误差，为什么还要设定目标函数？""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""生成数据""\n]\n},\n{\n""cell_type"": ""code"",\n""execution_count"": 2,\n""metadata"": {},\n""outputs"": [],\n""source"": [\n""#生成数据\\n"",\n""import numpy as np\\n"",\n""#生成随机数\\n"",\n""np.random.seed(1234)\\n"",\n""x = np.random.rand(500,3)\\n"",\n""#构建映射关系，模拟真实的数据待预测值,映射关系为y = 4.2 + 5.7x1 + 10.8x2，可自行设置值进行尝试\\n"",\n""y = x.dot(np.array([4.2,5.7,10.8]))""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""#### 1、先尝试调用sklearn的线性回归模型训练数据""\n]\n},\n{\n""cell_type"": ""code"",\n""execution_count"": 3,\n""metadata"": {},\n""outputs"": [\n{\n""name"": ""stdout"",\n""output_type"": ""stream"",\n""text"": [\n""估计的参数值为：[ 4.2  5.7 10.8]\\n"",\n""R2:1.0\\n"",\n""预测值为: [85.2]\\n""\n]\n}\n],\n""source"": [\n""import numpy as np\\n"",\n""from sklearn.linear_model import LinearRegression\\n"",\n""import matplotlib.pyplot as plt\\n"",\n""%matplotlib inline\\n"",\n""\\n"",\n""# 调用模型\\n"",\n""lr = LinearRegression(fit_intercept=True)\\n"",\n""# 训练模型\\n"",\n""lr.fit(x,y)\\n"",\n""print(""估计的参数值为：%s"" %(lr.coef_))\\n"",\n""# 计算R平方\\n"",\n""print(\'R2:%s\' %(lr.score(x,y)))\\n"",\n""# 任意设定变量，预测目标值\\n"",\n""x_test = np.array([2,4,5]).reshape(1,-1)\\n"",\n""y_hat = lr.predict(x_test)\\n"",\n""print(""预测值为: %s"" %(y_hat))\\n""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""#### 2、最小二乘法的矩阵求解""\n]\n},\n{\n""cell_type"": ""code"",\n""execution_count"": 4,\n""metadata"": {},\n""outputs"": [\n{\n""name"": ""stdout"",\n""output_type"": ""stream"",\n""text"": [\n""估计的参数值：[ 4.2  5.7 10.8]\\n"",\n""预测值为: [85.2]\\n""\n]\n}\n],\n""source"": [\n""class LR_LS():\\n"",\n""    def init(self):\\n"",\n""        self.w = None      \\n"",\n""    def fit(self, X, y):\\n"",\n""        # 最小二乘法矩阵求解\\n"",\n""        #============================= show me your code =======================\\n"",\n""        self.w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\\n"",\n""        #============================= show me your code =======================\\n"",\n""    def predict(self, X):\\n"",\n""        # 用已经拟合的参数值预测新自变量\\n"",\n""        #============================= show me your code =======================\\n"",\n""        y_pred = X.dot(self.w)\\n"",\n""        #============================= show me your code =======================\\n"",\n""        return y_pred\\n"",\n""\\n"",\n""if name == ""main"":\\n"",\n""    lr_ls = LR_LS()\\n"",\n""    lr_ls.fit(x,y)\\n"",\n""    print(""估计的参数值：%s"" %(lr_ls.w))\\n"",\n""    x_test = np.array([2,4,5]).reshape(1,-1)\\n"",\n""    print(""预测值为: %s"" %(lr_ls.predict(x_test)))\\n"",\n""\\n"",\n""    ""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""3、梯度下降法""\n]\n},\n{\n""cell_type"": ""code"",\n""execution_count"": 5,\n""metadata"": {\n""scrolled"": true\n},\n""outputs"": [\n{\n""name"": ""stdout"",\n""output_type"": ""stream"",\n""text"": [\n""估计的参数值为：[ 4.20000001  5.70000003 10.79999997]\\n"",\n""预测值为：[85.19999995]\\n""\n]\n}\n],\n""source"": [\n""class LR_GD():\\n"",\n""    def init(self):\\n"",\n""        self.w = None     \\n"",\n""    def fit(self,X,y,alpha=0.02,loss = 1e-10): # 设定步长为0.002,判断是否收敛的条件为1e-10\\n"",\n""        y = y.reshape(-1,1) #重塑y值的维度以便矩阵运算\\n"",\n""        [m,d] = np.shape(X) #自变量的维度\\n"",\n""        self.w = np.zeros((d)) #将参数的初始值定为0\\n"",\n""        tol = 1e5\\n"",\n""        #============================= show me your code =======================\\n"",\n""        while tol > loss:\\n"",\n""            h_f = X.dot(self.w).reshape(-1,1) \\n"",\n""            theta = self.w + alphanp.mean(X(y - h_f),axis=0) #计算迭代的参数值\\n"",\n""            tol = np.sum(np.abs(theta - self.w))\\n"",\n""            self.w = theta\\n"",\n""        #============================= show me your code =======================\\n"",\n""    def predict(self, X):\\n"",\n""        # 用已经拟合的参数值预测新自变量\\n"",\n""        y_pred = X.dot(self.w)\\n"",\n""        return y_pred  \\n"",\n""\\n"",\n""if name == ""main"":\\n"",\n""    lr_gd = LR_GD()\\n"",\n""    lr_gd.fit(x,y)\\n"",\n""    print(""估计的参数值为：%s"" %(lr_gd.w))\\n"",\n""    x_test = np.array([2,4,5]).reshape(1,-1)\\n"",\n""    print(""预测值为：%s"" %(lr_gd.predict(x_test)))""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""## 参考""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""吴恩达 CS229课程""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""周志华 《机器学习》""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""李航 《统计学习方法》""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""https://hangzhou.anjuke.com/""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""https://www.jianshu.com/p/e0eb4f4ccf3e""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""https://blog.csdn.net/qq_28448117/article/details/79199835""\n]\n},\n{\n""cell_type"": ""markdown"",\n""metadata"": {},\n""source"": [\n""https://blog.csdn.net/weixin_39175124/article/details/79465558""\n]\n},\n{\n""cell_type"": ""code"",\n""execution_count"": null,\n""metadata"": {},\n""outputs"": [],\n""source"": []\n}\n],\n""metadata"": {\n""hide_input"": false,\n""kernelspec"": {\n""display_name"": ""Python 3"",\n""language"": ""python"",\n""name"": ""python3""\n},\n""language_info"": {\n""codemirror_mode"": {\n""name"": ""ipython"",\n""version"": 3\n},\n""file_extension"": "".py"",\n""mimetype"": ""text/x-python"",\n""name"": ""python"",\n""nbconvert_exporter"": ""python"",\n""pygments_lexer"": ""ipython3"",\n""version"": ""3.7.6""\n},\n""toc"": {\n""base_numbering"": 1,\n""nav_menu"": {},\n""number_sections"": true,\n""sideBar"": true,\n""skip_h1_title"": false,\n""title_cell"": ""Table of Contents"",\n""title_sidebar"": ""Contents"",\n""toc_cell"": false,\n""toc_position"": {},\n""toc_section_display"": true,\n""toc_window_display"": false\n},\n""varInspector"": {\n""cols"": {\n""lenName"": 16,\n""lenType"": 16,\n""lenVar"": 40\n},\n""kernels_config"": {\n""python"": {\n""delete_cmd_postfix"": """",\n""delete_cmd_prefix"": ""del "",\n""library"": ""var_list.py"",\n""varRefreshCmd"": ""print(var_dic_list())""\n},\n""r"": {\n""delete_cmd_postfix"": "") "",\n""delete_cmd_prefix"": ""rm("",\n""library"": ""var_list.r"",\n""varRefreshCmd"": ""cat(var_dic_list()) ""\n}\n},\n""types_to_exclude"": [\n""module"",\n""function"",\n""builtin_function_or_method"",\n""instance"",\n""_Feature""\n],\n""window_display"": false\n}\n},\n""nbformat"": 4,\n""nbformat_minor"": 2\n}\n'], 'url_profile': 'https://github.com/qqzhuang', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Regression\nThe rendered version of this report can be accessed here\nFirst exploratory approach to the Belkin Elago df\nAbout the df:\nThe file QuestionsBelkinComplete contains the questions made to the customers, the other file BelkinComplete contains the answers to the questions formulated.\n'], 'url_profile': 'https://github.com/Luis-VAR', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'Nanjing, China', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hassenBaw', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NAMRATHABATHULA', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahmalianto', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Decorrelated-Weighted-Regression\nStable Prediction with Model Misspecification and Agnostic Distribution Shift\nThe main code of our Decorrelated-Weighted-Regression (DWR) algorithm can be found in ""DWR.py"".\nReferences\nKuang K, Xiong R, Cui P, Athey S, and Li B. Stable Prediction with Model Misspecification and Agnostic Distribution Shift[C]//Thirty-Fourth AAAI Conference on Artificial Intelligence. 2020.\n'], 'url_profile': 'https://github.com/KunKuang', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'Cuttack,Odisha,India', 'stats_list': [], 'contributions': '680 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prasenjitghose36', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['R-CODES\nRegression\n'], 'url_profile': 'https://github.com/JITENDRA4048', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['ML-\nregression\n'], 'url_profile': 'https://github.com/Rukmini2001', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'HTML', 'Updated Jun 5, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', '3', 'Python', 'Updated Apr 26, 2020', '3', 'Python', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Updated Apr 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['predict\nLinear Regression, Logist Regression and Time Series \n2 Regressão Linear \n3 Regressão Logística \n4 Séries Temporais\n'], 'url_profile': 'https://github.com/mjoventino', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Bangalore,India', 'stats_list': [], 'contributions': '458 contributions\n        in the last year', 'description': ['HousePricePrediction\nHouse dataset is collected and datacleaned by preprocessing.\nThe  model is created  based on linear regression which predicts house price..\nLibraries:\nNumpy\nPandas\nSeaborn\nMatplotlib.\n'], 'url_profile': 'https://github.com/deepakkapse', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deepanjanbhol', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MathengeKen', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Logisticreg\nLogistic Regression From Scratch ASA\n'], 'url_profile': 'https://github.com/Aliyu-haidar', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'pune', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['ML_Model_for_COVID-19_Deaths_Prediction\nAuthor: Parameswara rao\nDate: 04 April 2020.\nI implimented a simple Linear Regression model using corona_cases.csv data set for predicting COVID-19 death cases.This project is organised as follows:\n(a) Extract data from CSV file.\n(b) Data preprocessing and variable selection.\n(c) Analyse Pearsons correlation between indipendant and dependant variables.\n(d) Basic regression model.\n(e) monitor model summary and results\ncorona_cases.csv: data set for model building.\nhtml_table_to_csv_table_extraction.py: used to extract and creat data from website html tables into csv table.\nResult:\n\nObserved correlation between those by using Heat maps and got Pearson’s correlation factor: 0.969.\nattained R^2: 0.940, i.e. I can predict daily death cases with 94% accuracy.\n\n'], 'url_profile': 'https://github.com/parameswar-ds', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/genggengjing', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Programming Exercise 2: Logistic Regression\nIn this exercise, you will implement logistic regression and apply it to two\ndifferent datasets.\nFiles included in this exercise\nex2.m - Octave/MATLAB script that steps you through the exercise\nex2 reg.m - Octave/MATLAB script for the later parts of the exercise\nex2data1.txt - Training set for the first half of the exercise\nex2data2.txt - Training set for the second half of the exercise\nsubmit.m - Submission script that sends your solutions to our servers\nmapFeature.m - Function to generate polynomial features\nplotDecisionBoundary.m - Function to plot classifier’s decision boundary\nplotData.m - Function to plot 2D classification data\nsigmoid.m - Sigmoid Function\ncostFunction.m - Logistic Regression Cost Function\npredict.m - Logistic Regression Prediction Function\ncostFunctionReg.m - Regularized Logistic Regression Cost\n'], 'url_profile': 'https://github.com/aayushi363', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Trivandrum', 'stats_list': [], 'contributions': '223 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/a-anand-91119', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression model on simple dataset\n'], 'url_profile': 'https://github.com/prathamesh1499', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 26, 2020', '3', 'Updated Oct 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Updated Apr 22, 2020', 'MATLAB', 'Updated Apr 23, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""MA346-Project\nLinear Regression\nData\nA Tsanas, MA Little, PE McSharry, LO Ramig (2009)\n'Accurate telemonitoring of Parkinson.s disease progression by non-invasive\nspeech tests',\nIEEE Transactions on Biomedical Engineering (to appear).\nSample output:\n>> run('Code/EfficiencyTester.m')\n\nans =\n\n  1×5 table\n\n      MAE       MSE       RMSE      R_sq      R_sq_adj\n    _______    ______    ______    _______    ________\n\n    0.12065    85.523    9.2479    0.25292    0.25037 \n\n\nans =\n\n  1×5 table\n\n      MAE       MSE       RMSE      R_sq      R_sq_adj\n    _______    ______    ______    _______    ________\n\n    0.12066    85.539    9.2487    0.25278    0.25023 \n\n\nans =\n\n  1×5 table\n\n      MAE       MSE      RMSE      R_sq      R_sq_adj\n    _______    _____    ______    _______    ________\n\n    0.12066    85.53    9.2482    0.25286    0.25031 \n\n\nans =\n\n  1×5 table\n\n      MAE       MSE      RMSE      R_sq      R_sq_adj\n    _______    ______    _____    _______    ________\n\n    0.12386    90.136    9.494    0.21262    0.20993 \n\n\nmdl = \n\n\nLinear regression model:\n    y ~ 1 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20\n\nEstimated Coefficients:\n                   Estimate       SE         tStat        pValue   \n                   ________    _________    ________    ___________\n\n    (Intercept)        35.1       3.0572      11.481      3.446e-30\n    x1               0.2636     0.010775      24.464    6.2546e-126\n    x2              0.31867     0.014363      22.186    8.7419e-105\n    x3               -4.812      0.31257     -15.395     1.8608e-52\n    x4              0.01594    0.0022738      7.0103     2.6465e-12\n    x5              -255.55       203.16     -1.2579        0.20848\n    x6               -44609       9447.5     -4.7218      2.392e-06\n    x7               -26030        44530    -0.58455        0.55887\n    x8              -166.92       180.52    -0.92464        0.35519\n    x9               9062.4        14844     0.61049        0.54156\n    x10              14.126       61.868     0.22833         0.8194\n    x11            -0.58858       4.6154    -0.12753        0.89853\n    x12              -14998        44727    -0.33532        0.73739\n    x13              49.607       52.793     0.93965        0.34744\n    x14              9.7274       23.711     0.41025        0.68164\n    x15              4949.3        14909     0.33197        0.73992\n    x16             -23.717       5.9443     -3.9899     6.6903e-05\n    x17            -0.48577      0.06584     -7.3781     1.8297e-13\n    x18              1.6927       1.7377     0.97413        0.33003\n    x19              -36.34       2.2067     -16.468     1.3424e-59\n    x20              15.485       2.7851      5.5601      2.815e-08\n\n\nNumber of observations: 5875, Error degrees of freedom: 5854\nRoot Mean Squared Error: 9.26\nR-squared: 0.253,  Adjusted R-Squared: 0.25\nF-statistic vs. constant model: 99.1, p-value = 0```\n""], 'url_profile': 'https://github.com/Hariharan-V', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Alohomora25', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anna1027', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayguillo', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['LinearRegression\nFirst Linear Regression Task\n'], 'url_profile': 'https://github.com/chirag-wadhwa', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Ithaca, NY', 'stats_list': [], 'contributions': '563 contributions\n        in the last year', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/merb92', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Decision_Tree_Regression_using_Python-\nDecision Tree regression technique\n'], 'url_profile': 'https://github.com/amitdivekar30', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Bangalore,India', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nSimple Linear Regression\n'], 'url_profile': 'https://github.com/himanshumishra-ds', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Athens, Greece', 'stats_list': [], 'contributions': '1,265 contributions\n        in the last year', 'description': ['logistic-regression\nLogistic Regression in Matlab\n'], 'url_profile': 'https://github.com/victor-timofei', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Nanded,Maharashtra', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['Fake-news-classifier\nAbove model is build on simple logistic regression by tfidfvectorizer.\nGiven dataset is from https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction\n'], 'url_profile': 'https://github.com/swapniladnak2510', 'info_list': ['MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated Oct 31, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neeraj2296', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Nalagarh', 'stats_list': [], 'contributions': '288 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dikshitkaushal', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Boulder, CO', 'stats_list': [], 'contributions': '692 contributions\n        in the last year', 'description': ['repo_test\nGit regression test for Milestone 1\n'], 'url_profile': 'https://github.com/him-28', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': [""Logistic-regression\nlogistic regression attempt\nThis is my first try using a logistic regression algorithm. It uses a heart diseases dataset for predicting the probability of future heart diseases. I tried it using Linear regression and then later on realised that it has a categorical target variables, so i switched over to logistic regression algorithm. Still I'm getting very low accuracy for the model and I do not know why that is. Hope I could improve it in the future!\n""], 'url_profile': 'https://github.com/Kirang96', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'IIT mandi', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Linear regression\ncode of linear regression(Multi variate)\n&Plotting using matplotlib.\n'], 'url_profile': 'https://github.com/Rupesh-rkgit', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YimingZhou22', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/markchangliu', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AdhipathiPandiyan-S', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Chittagong, Bangladesh', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Logistic Regression\nLogistic Regression Projects\n'], 'url_profile': 'https://github.com/ronyripan', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TheMethik', 'info_list': ['1', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Updated May 14, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Julia', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}"
"{'location': 'Chittagong, Bangladesh', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Logistic Regression\nLogistic Regression Projects\n'], 'url_profile': 'https://github.com/ronyripan', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Washington, DC', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['RegressionModels\nCoursera Regression Models\nhttps://www.coursera.org/learn/regression-models/\n'], 'url_profile': 'https://github.com/Ficke', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Chittagong, Bangladesh', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Linear Regression\n'], 'url_profile': 'https://github.com/ronyripan', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JackyGaoJ', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'FCT Abuja, Nigeria', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['The Linear Regression Algorithm\nThe linear regression algorithm is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. Wikipedia\nThe Linear Regression is one of the most basic and popular machine learning algorithm you will come across in your pursuit of a career as a data scientist or a machine learning engineer. It is intuitive and has a good range of uses and it\'s pretty straight forward and easy to understand.\n\nSo what problem does linear regression solve? Well when the target variable we\'re trying to predict is continuous, then such learning problem is a regression problem, thus the linear regression algorithm is used to solve continuous problems.\nThe linear regression algorithm was implemented in linear_regression.py and you can reference this notebook for more practical details on how the linear regression algorithm works. The focus was on implementing a simple linear regression with one explanatory variable f(x) = mx + b.\n\nThe class LinearRegression which contains several variables & methods (public and private) to carry out the relationships modeled using a linear predictor functions.\nclass LinearRegression:\n    def __init__(self, x, y, alpha=0.01, num_iter=1000, verbose=False):\n        pass\nAt initialization of the Linear Regression model:\n\nx will be the input feature which should be a (Xm, 1) matrix.\ny will be the target feature which should be a (Ym, 1) matrix.\n\nalpha is the learning rate and num_iter the number of iterations used in gradient descent. verbose if True will produce the detailed output of the cost function for diagnostic purposes.\nNB: ""m"" is the number of training examples.\nThe choice of numpy array was to perform vectorization on the data, thus avoiding the constant use of excessive for loops and thus optimizing the program.\nThe Hypothesis of a simple linear regression is given as:\nhθ(x) = θo x + θ1x\nwhere x is the input variable.\nThe cost function or mean squared error is used to measure the accuracy of our hypothesis. This takes the average difference of all the result of the with the inputs from and the actual output y\'s.\nThe mean of the cost function is halved as a convenience for the computation of gradient descent.\ndef fit(self, timeit=True, count_at=100):\n    pass\nSo when we have our hypothesis function and we have a way of measuring how well it fits into the data. We then need to estimate the parameters in the hypothesis function and this is where Gradient Descent comes in and this process goes on for a period of time until the cost converge to a global minimum.\nFollow me on Twitter @GM_Olalekan\n'], 'url_profile': 'https://github.com/ganiyuolalekan', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['House-Price-Prediction\nLinear Regression Algorithm\n'], 'url_profile': 'https://github.com/sw473', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Athens, Greece', 'stats_list': [], 'contributions': '1,265 contributions\n        in the last year', 'description': ['logistic-regression-with-regularization\nLogistic Regression with Regularization\n'], 'url_profile': 'https://github.com/victor-timofei', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kd0104', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Israel', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/roipolanitzer', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '271 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sebsm', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'MATLAB', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['QuantReg\nQuantile Regression test\n'], 'url_profile': 'https://github.com/felixkung47', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VirgileF', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Mechine-Learning-Models-and-Algorithms\nApplying regression algorithms (Liner regression, decision tree, random forest, SVR), classification models(Logistic Regression,KNN,Decision Tree Classifier,SVM) and visualising the data of dataset\n'], 'url_profile': 'https://github.com/aarinp', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/urvins03', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Covid-Prediction\nPrediction using simple linear regression\n'], 'url_profile': 'https://github.com/vamsi-bel', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Madrid', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['linear-regression-scratch\nLinear Regression Implementation from scratch\n'], 'url_profile': 'https://github.com/franAlg', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lirunkui', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pkumar2244', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['DevSim_reg_suite\nRegression suite code for devsim\n'], 'url_profile': 'https://github.com/bharatgandhi1104', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Bandung', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ParalysisSyndrome', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pkumar2244', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['DevSim_reg_suite\nRegression suite code for devsim\n'], 'url_profile': 'https://github.com/bharatgandhi1104', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'Bandung', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ParalysisSyndrome', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Linear_Regression\nGenerate Random Numbers for X and Y axis and plot Scatter diagram\nUsing y=mx +c , predict values for X and store in Yhat and draw straight line\nCheck if this line hits how many points in Y\nCheck different between actual and predicted values\nIterate 10 times above steps from 4 to 7\nFor better results keep changing m and c and plot\n'], 'url_profile': 'https://github.com/ashok-python', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Automated loess regression\nIn regression problems, a parametric model for the response (or dependent) variable as a function of the predictor (or independent) ones is not always available or known. In those cases, a non-parametric regression is highly recommended because it avoids the assumption of a heuristic model. However, the price to pay is that more data are needed to perform a fit, compared to the case of parametric regressions.\nOne of those non-parametric regressions is loess (Cleveland et al. 1992, chapter 8 of Statistical Models in S), which performs polynomial fits over local intervals along the domain. To perform a loess regression with one predictor, it is necessary to specify the order of the local polynomial (deg), which can be 1 (linear) or 2 (quadratic), and the smoothing parameter (alpha), which determines how much of the data is used to fit each local polynomial.\nIn Rodríguez et al. (2019), I developed an automated routine to perform loess regressions to data with one predictor variable, called Automated Loess Regression (ALR). This routine takes into account the observed and intrinsic errors, along with the presence of possible outliers. The routine uses by default an order 2 for the local polynomials in order to give more freedom to the loess fitting procedure. In order to estimate an optimal alpha value, ALR uses the ""an"" information criterion (AIC, Akaike 1974). To test whether response errors can account for the observed dispersion around the ALR fit, the code computes its log-likelihood. If an intrinsic error is necessary to maximize the log-likelihood, then it is added in quadrature to the response errors and the code performs again the ALR fit. This process is repeated until an intrinsic error is not necessary. For the outliers detection, ALR uses the Tukey (1977) rule, where values below Q1-1.5(Q3-Q1) or above Q3+1.5(Q3-Q1) (known as inner fences, where Q1 and Q3 are the first and third quartile, respectively) are considered as outliers. Finally, the code computes the errors around the ALR fit through simulations.\nWhen loess is not able to perform a regression (e.g., only few data points are available), then the ALR just performs a linear interpolation between points.\nI developed the ALR with the purpose of fitting light curves of Type II supernovae (SNe II) during the first ~100 days. However, in principle, it can be applied to any set of data with one predictor variable, with or without errors on the response variable.\nFor any question, email me at olrodrig@gmail.com\nIf you use the ALR in your work, please cite Rodríguez et al. (2019).\n'], 'url_profile': 'https://github.com/olrodrig', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '340 contributions\n        in the last year', 'description': ['SkyFi_ML_assignment\nLinear Regression on Boston Dataset\n'], 'url_profile': 'https://github.com/amandewatnitrr', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'Manipal Institute Of Technology', 'stats_list': [], 'contributions': '265 contributions\n        in the last year', 'description': ['Supervised Learning |-> Regression |-> Simple Regression\nData: https://www.kaggle.com/m2skills/datasets-and-tutorial-kernels-for-beginners\n'], 'url_profile': 'https://github.com/Sanjay9921', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['QuasiconvexLSE\nThis is an R package to compute the multivariate quasiconvex (and decreasing) nonparametric LSE as described in ""Least Squares Estimation of a Monotone Quasiconvex Regression Function"" by Somabha Mukherjee, Rohit K. Patra, Andrew L. Johnson, and Hiroshi Morita, which can be found at the following link:\nhttps://arxiv.org/abs/2003.04433\nTo download the R package use the following in R:\nlibrary(devtools)\ninstall_github(repo = ""somabhamukherjee/QuasiconvexLSE"")\nlibrary(QuasiconvexLSE)\n\nNote that the above package requires CPLEX and the R-package RCPLEX.\nFiles in the folder titled ""ReplicationCode"" replicate Figure 5 of https://arxiv.org/abs/2003.04433.\nReferences\nSomabha Mukherjee, Rohit K. Patra, Andrew L. Johnson, and Hiroshi Morita. Least Squares Estimation of a Monotone Quasiconvex Regression Function. 2020. arXiv:2003.04433\n'], 'url_profile': 'https://github.com/somabhamukherjee', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nairababayan', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/orgoca', 'info_list': ['Updated Apr 21, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Apr 25, 2020']}"
"{'location': 'Gurgaon', 'stats_list': [], 'contributions': '755 contributions\n        in the last year', 'description': ['Assumptions-of-Linear-Regression\nAll Assumptions of Linear Regression\n'], 'url_profile': 'https://github.com/piyushpathak03', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'New Jersey', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': [""Results\nBoth Logistic Regression and SVC models were use to train the exoplanet data. The results find that the SVM model has an 88% testing accuracy while the Logistic Regression model's accuracy is 87%.\nThe Logistic Regression and the SVC model both had high recall and percision for all predicited values meaning that a majority of the predicitions are accuarate and are positivley confirmed.\nAt the same time both models did have high precision for False Positive results meaning that the prediction was that it was not an exoplanet when it actually was an exoplanet. This precision being 0.98 for both the SVC and Logistic Regression models with a recall rate of 1. From my understadning this means that these tests are incorrect when predicting whether or not a hidden planet is a new exoplanet.\nOne would think that these models are missing information and are incorrectly predicting results. In this instance all of the False Positive results are confirmed meaning that the model is predicting that the result is the opposite of the actual result. In my opinion, the easiest way to correct this mistake is to identify and switch all of the False Positive results to display the opposite result of the False Positive report.\n""], 'url_profile': 'https://github.com/jackharv7', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'Chandigarh - IN', 'stats_list': [], 'contributions': '349 contributions\n        in the last year', 'description': ['Logistic-Regression-Scratch\n(Implemented Logistic Regression from Scratch)\nOptimizers used (along with implementation):\n\nMini - Batch SGD (Stochastic Gradient Descent)\nSGD with Momentum\nRMSprop\n\nSoon, I will also add implementation of Adam.\n'], 'url_profile': 'https://github.com/iamdhruvsharma', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '3,124 contributions\n        in the last year', 'description': ['Visual Regression Testing Example Repository\nThis repository is an example for Andrew Taylors talk/workshop Automating your QA with Visual Regression Testing and David Needham\'s talk/workshop Visual Regression Testing with BackstopJS. Andrew\'s slides that accompany this repository can be found here and David\'s can be found here.\nBackstopJS is used for the visual regression testing. The app itself is built with Node JS, commander.js, and Inquirer.js.\nPrerequisites\nYou will need:\n\nA local development environment with Node JS/NPM\nGoogle Chrome\nA live, web-accessible website\nAnother environment of the website above (e.g. local, staging, etc.)\n\nGetting The Code\nCreate a new repository from this template and then either using Git clone or download the .zip file of your copy.\nInstructions\nAfter setting up the repository locally (see above) you will need to:\n\nRun the command npm install to download dependencies\n\nThis only needs to be done once\n\n\nRun the command npm run start\n\nSelect the site you want to test from the list\nNote: npm run start can be used anytime you want to run the app\n\n\nCheck out the results from the sample test\n\nThey should open in your browser automatically\n\n\nEdit inc/sitesToTest.js\n\nThis is where the list of sites to test is stored\nTry changing to one (or more) of your sites\nnonProductionBaseUrl is your non-production environment (local, staging, etc.) URL\nproductionBaseUrl is your production site URL\nAdjust pathsToTest, which is the array of URIs to test for each site\n\n\nEdit inc/backstopConfig.js to adjust viewports, delay, hidden selectors, etc.\nRun the command npm run start.\n\nSelect the site you want to test from the list\n\n\n\nTroubleshooting\nIf you are having issues with the script hanging or BackstopJS taking a long time there may be headless Chrome instances that didn\'t close properly.\nTry pkill -f ""(chrome)?(--headless)"" on Mac/Linux or Get-CimInstance Win32_Process -Filter ""Name = \'chrome.exe\' AND CommandLine LIKE \'%--headless%\'"" | %{Stop-Process $_.ProcessId} in Windows PowerShell.\nRegression-testing-practice\n'], 'url_profile': 'https://github.com/TomSpencerLondon', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'salem,tamilnadu,india', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['code\nIt consist of polynomial regression.\nAnd consist of simple linear regression\n'], 'url_profile': 'https://github.com/subhashini-mariappan', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""my-first-project-of-linear-regression-\nLinear Regression on Diabetes Dataset\nImport some Libraries\nimport pandas as pd\nimport numpy as np\nimport csv\nimport pandas\nfrom sklearn import dataset\nLoad the Dataset\nfrom sklearn.datasets import load_diabetes\ndiabetes_data = load_diabetes()\ndf = pd.DataFrame(data=diabetes_data['data'], columns=diabetes_data['feature_names'])\ndf['target'] = diabetes_data['target']\nprint(df.shape)\nprint(df.head())\ndefining our x and y variables\nx= df.iloc[:,:-1].values\ny= df.iloc[:,-1].values\nm ,n = x.shape\nprint('x[0]={},y[0]={}'.format(x[0],y[0]))\n#Appending column of ones to make it for intercept term\nz= np.ones(m)\nz= z.reshape(m,1)\nx = np.append(z,x , axis =1)\nprint('x[0]={} ,y[0]={}'.format(x[0],y[0]))\nDeclaring values of Theta\nm,n = x.shape\ntheta = np.zeros(n)\ntheta = theta.reshape(n,1)\ny = y.reshape(-1,1)\n#computing the cost\ndef computeCost(x, y, theta):\ntemp = np.dot(x, theta) - y\nreturn np.sum(np.power(temp, 2)) / (2*m)\nJ = computeCost(x, y, theta)\nprint(J)\nFinding the optimal parameters using Gradient Descent\niterations = 50000\nalpha = 0.01\ndef gradientDescent(x, y, theta, alpha, iterations):\nfor _ in range(iterations):\ntemp = np.dot(x, theta) - y\ntemp = np.dot(x.T, temp)\ntheta = theta - (alpha / m) * temp\nreturn theta\ntheta = gradientDescent(x, y, theta, alpha, iterations)\nprint(theta)\nWe now have optimized values of Thetas (intercept and slope ). so we can find minimum cost\nJ = computeCost(x, y, theta)\nprint(J)\n""], 'url_profile': 'https://github.com/ujavaid123', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iruxandra', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'waterloo,ON', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gty3310', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['RegressionHP\nData Source : https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\nThis data set has 79 factors which describe different aspects of residential houses in Iowa, and has train data with “SalesPrice” column, test data without. Both train and test data have “Id” column to identify the houses. Our task is to build some models to analyze the data and finally predict the price of each house. The House.ipynb file is the ML regression model created by Sydney. Medium article can be found https://medium.com/analytics-vidhya/simple-regression-model-by-using-python-scikit-learn-house-prices-advanced-regression-techniques-66841559aa4c\n'], 'url_profile': 'https://github.com/SydneyChen222', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Context\nYou work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:\n\n“Is an automatic or manual transmission better for MPG”\n""Quantify the MPG difference between automatic and manual transmissions""\n\nQuestion\nTake the mtcars data set and write up an analysis to answer their question using regression models and exploratory data analyses.\nYour report must be:\n\nWritten as a PDF printout of a compiled (using knitr) R markdown document.\nBrief. Roughly the equivalent of 2 pages or less for the main text. Supporting figures in an appendix can be included up to 5 total pages including the 2 for the main report. The appendix can only include figures.\nnclude a first paragraph executive summary.\n\nUpload your PDF by clicking the Upload button below the text box.\nPeer Grading\n\nThe criteria that your classmates will use to evaluate and grade your work are shown below. \nEach criteria is binary: (1 point = criteria met acceptably; 0 points = criteria not met acceptably)\nour Course Project score will be the sum of the points and will count as 40% of your final grade in the course. \n\n'], 'url_profile': 'https://github.com/ankurkhaitan', 'info_list': ['1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 24, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 22, 2020', '2', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Context\nYou work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:\n\n“Is an automatic or manual transmission better for MPG”\n""Quantify the MPG difference between automatic and manual transmissions""\n\nQuestion\nTake the mtcars data set and write up an analysis to answer their question using regression models and exploratory data analyses.\nYour report must be:\n\nWritten as a PDF printout of a compiled (using knitr) R markdown document.\nBrief. Roughly the equivalent of 2 pages or less for the main text. Supporting figures in an appendix can be included up to 5 total pages including the 2 for the main report. The appendix can only include figures.\nnclude a first paragraph executive summary.\n\nUpload your PDF by clicking the Upload button below the text box.\nPeer Grading\n\nThe criteria that your classmates will use to evaluate and grade your work are shown below. \nEach criteria is binary: (1 point = criteria met acceptably; 0 points = criteria not met acceptably)\nour Course Project score will be the sum of the points and will count as 40% of your final grade in the course. \n\n'], 'url_profile': 'https://github.com/ankurkhaitan', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,442 contributions\n        in the last year', 'description': [""Regressions KPI\nHow to use\nPut your token in a file named token.txt at the root of the project.\nEnvironment variables\n\n\n\nParameter\nDescription\nMandatory ?\n\n\n\n\nVERSION\nVersion of PrestaShop to use (patch version or .0 version)\nYes\n\n\nFREEZE_DATE\nArbitrary freeze date (YYYY-MM-DD)\nYes\n\n\nRELEASE_DATE\nArbitrary release date (YYYY-MM-DD)\nNo (default to today's date)\n\n\n\nExamples of use\nTo get all the information about a patch release:\nVERSION=1.7.6.2 php getPatchVersionData.php\nTo get all the information about a minor release:\nVERSION=1.7.6.0 FREEZE_DATE=2019-04-08 RELEASE_DATE=2019-06-01 php getMinorVersionData.php\nMake sure to use a version number ending in 0 (1.7.6.0, 1.7.7.0, and so on).\n""], 'url_profile': 'https://github.com/boubkerbribri', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['R_Projects\n'], 'url_profile': 'https://github.com/saikumar786', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '495 contributions\n        in the last year', 'description': ['This repository contains the Machine Learning for Imaging coursework from the Department of Computing, Imperial College London, Academic Year 2019-2020, delivered by Dr Ben Glocker and Professor Daniel Rueckert. The coursework was developed with their PhD students.\nThis coursework and git repo is my joint work with Alvaro Prat.\nPlease find the coursework description in the specs.pdf file, the developed solution in Brain-Age-Regression.ipynb and the final report in report.pdf.\n'], 'url_profile': 'https://github.com/Theosau', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Machine Learning Practice\n\n'], 'url_profile': 'https://github.com/qpg93', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Lab 1. Team name: RankingUP\nAuthor: 吳少凱 108552010、彭信穎 108552018\nOverview of House Prices: Advanced Regression Techniques\n主要參考以下兩篇\nComprehensive data exploration with Python\nhttps://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python/notebook\nStacked Regressions : Top 4% on LeaderBoard\nhttps://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n資料表格式參考\nhttps://chtseng.wordpress.com/2017/12/26/kaggle-house-price/\n優化參考\nhttps://www.kaggle.com/niteshx2/top-50-beginners-stacking-lgb-xgb\nhttps://www.kaggle.com/jesucristo/1-house-prices-solution-top-1?scriptVersionId=12846740\nhttps://www.kaggle.com/agehsbarg/top-10-0-10943-stacking-mice-and-brutal-force\n其他參考\nhttps://medium.com/%E7%84%A1%E9%82%8A%E6%8B%BC%E5%9C%96%E6%A1%86/kaggle-house-price-prediction-competition-%E5%AF%A6%E6%88%B0-ff1c846a9f14\n書籍參考： Feature Engineering for Machine Learning\n短報告：\nhttps://docs.google.com/document/d/1M6Ke5PB8j09mOlH29Ziq1bTU3C_5zpD7l32n_0c_f_0/edit?usp=sharing\n參考 Hair et al. (2013)書中所描述，對於多變量統計分析，需要驗證四個假設：\n\nNormality 常態分佈\nHomoscedasticity 等分散性\nLinearity 線性\nAbsence of correlated errors 不具有相關誤差\n\n接下來將對每個變數進行上述幾項處裡\n1. 資料預處裡 (Data Pre-Processing)\n1.1 SalePrice\n本kaggle competetion的主要目標為使用78個參數，進行房屋SalePrice的預測。\n本次將對SalePrice進行預測。但在進行訓練之前，對於資料的前置處裡是必不可少的。\n1.1.1 離群值 OutLiers - 去除少量偏移過大的數值。\n1.1.2. 直方圖 & 常態機率圖 Histogram  & Normal probability plot - 檢查峰度(Kurtosis) 以及 偏度(skewness)、資料分布需接近對角線以符合常態分佈。\n我們使用SeaBorn對SalePrice繪圖並檢查資料分布狀況。\n1.1.1 離群值 OutLiers\n先將訓練資料讀入pandas dataframe\ntrain = pd.read_csv(\'./train.csv\')\ntest = pd.read_csv(\'./test.csv\')\n\n\'StandardScaler().fit_transform\'之後的SalePrice資料具有兩個700,000以上的上界離群值，由於離群值會影響資料訓練時的準確性，而房產價格又受到地域分布的極大影響，因此我們將此兩樣離群值刪去。\n#show all plots\nfig, ax = plt.subplots()\nax.scatter(x = train[\'GrLivArea\'], y = train[\'SalePrice\'])\nplt.ylabel(\'SalePrice\', fontsize=13)\nplt.xlabel(\'GrLivArea\', fontsize=13)\n\n#Deleting outliers\ntrain = train.drop(train[(train[\'GrLivArea\']>4000) & (train[\'SalePrice\']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train[\'GrLivArea\'], train[\'SalePrice\'])\nplt.ylabel(\'SalePrice\', fontsize=13)\nplt.xlabel(\'GrLivArea\', fontsize=13)\nplt.show()\n\n將離群值移除\n\n1.1.2 直方圖 & 常態機率圖 Histogram  & Normal probability plot\n此處僅為log變換處理的展示，我們將會在 2.1 Pipeline 管線處理章節中，一次對所有特徵數值的常態機率分佈進行處理。\nSalePrice的分佈很明顯非高斯分布，具有極值、正偏度(skewness)、且不在對角線上、峰度為1.882876、偏度為6.536282。我們進行簡單的資料變換(Data transformation)來解決上述問題。\n#skewness and kurtosis\nprint(""Skewness: %f"" % df_train[\'SalePrice\'].skew())\nprint(""Kurtosis: %f"" % df_train[\'SalePrice\'].kurt())\n\n對數變換前 brfore log transformations\n#histogram\nfig = plt.figure(figsize=(10, 5))\nplt.subplot(1,2,1)\nsns.distplot(train[\'SalePrice\'],fit=norm)\nax=plt.subplot(1,2,2)\nax.yaxis.tick_right()\nres = stats.probplot(train[\'SalePrice\'], plot=plt)\n\n對數變換後 after log transformations\n#applying log transformation\ny_log = np.log(train.SalePrice)\n\n#transformed histogram and normal probability plot\nsns.distplot(y_log, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y_log, plot=plt)\n\n\n從指數變換過後的直方圖以及常態機率圖可以發現，分佈更加趨近常態分佈。\n在對於我們即將預測的資料進行觀察後，我們進行下一步，特徵工程 (Features engineering)的處裡。\n1.2特徵工程 Features engineering\n特徵工程是將原始資料轉化成更易於表達問題本質之特徵的過程，讓這些特徵運用到預測模型時，提高對不可見資料的預測精度。同時也是資料預測模型開發中最耗時、最重要的一步。\n因此這裡將著重在各特徵參數的評估、轉換、以及補全。\n由於現實中資料的獲得並不完美，有時會有缺損，或是定義不清。造成評估時的困擾，因此我們也將Na分類以及0數值進行適度修改。\n先列出特徵參數以及樣本數量，並事先移除header ID colum、SalePrice欄位。\n因此共有79個特徵參數，每個特徵參數共有2917個數值(Train & Test set合併計算)\n#check the numbers of samples and features\nprint(""The train data size before dropping Id feature is : {} "".format(train.shape))\nprint(""The test data size before dropping Id feature is : {} "".format(test.shape))\n\n#Save the \'Id\' column\ntrain_ID = train[\'Id\']\ntest_ID = test[\'Id\']\n\n#Now drop the  \'Id\' colum since it\'s unnecessary for  the prediction process.\ntrain.drop(""Id"", axis = 1, inplace = True)\ntest.drop(""Id"", axis = 1, inplace = True)\n\n#check again the data size after dropping the \'Id\' variable\nprint(""\\nThe train data size after dropping Id feature is : {} "".format(train.shape)) \nprint(""The test data size after dropping Id feature is : {} "".format(test.shape))\n\nThe train data size before dropping Id feature is : (1460, 81) \nThe test data size before dropping Id feature is : (1459, 80) \n\nThe train data size after dropping Id feature is : (1460, 80) \nThe test data size after dropping Id feature is : (1459, 79) \n\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop([\'SalePrice\'], axis=1, inplace=True)\nprint(""all_data size is : {}"".format(all_data.shape))\n\nall_data size is : (2917, 79)\n\n再來，我們將進行以下步驟以處裡各個特徵參數，並分章節詳加描述：\n1.2.1. 處理空缺數值 - 分析空缺資料並填入適當數值或分類\n1.2.2. 資料相關度 - 相關性矩陣 heatmap(熱圖)\n1.2.3. 數值轉類別 - 將實際上為類別資料的數值參數進行轉換\n1.2.4. 類別轉數值 - Label encoding 方便進行分析\n1.2.5. 新增參數 - TotalSF\n1.2.6. 偏度校正 - Box Cox Transformation\n1.2.7. 一位有效編碼 - 將類別轉為無序 Dummy variables\n1.2.8. 特徵組合 - 利用groupby函式進行特徵組合。\n1.2.1. 空缺數值處理\n共有30/78個參數資料有缺失欄位，比例如下所示，接下來將依照對於資料的理解，填入適當內容。\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({\'Missing Ratio\' :all_data_na})\nmissing_data.head(30)\n\n\n\nPoolQC (泳池品質)\n在資料說明中，NA代表沒有泳池，對照PoolArea也為0，由資料中可以看到，99%以上的房產沒有泳池。\n\nall_data[""PoolQC""] = all_data[""PoolQC""].fillna(""None"")\n\n\nMiscFeature (其他特色)\n資料說明中，NA代表沒有其他特色，替換為None。\n\nall_data[""MiscFeature""] = all_data[""MiscFeature""].fillna(""None"")\n\n\nAlley (鄰近巷子)\n資料說明中，NA代表沒有鄰近的巷弄，替換為None。\n\nall_data[""Alley""] = all_data[""Alley""].fillna(""None"")\n\n\nFence (圍籬)\n資料說明中，NA代表沒有圍籬，替換為None。\n\nall_data[""Fence""] = all_data[""Fence""].fillna(""None"")\n\n\nFireplaceQu (壁爐品質)\n資料說明中，NA代表沒有壁爐，Fireplace也為0。替換為None。\n\nall_data[""FireplaceQu""] = all_data[""FireplaceQu""].fillna(""None"")\n\n\nLotFrontage (前方街道寬度)\n\n有兩個方式：\n方法1: 通常許多鄰近房屋的街道寬度接近，因此填入附近房產的LotFrontage中位數值\n方法2: 若NA代表此房產隱藏在巷弄中，會劇烈影響最後的售價，相關討論可參考以下文章：LotFrontage Imputation for House Price Competition。\n此處先用方法1：\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[""LotFrontage""] = all_data.groupby(""Neighborhood"")[""LotFrontage""].transform(lambda x: x.fillna(x.median()))\n\n\nGarageType, GarageFinish, GarageQual, GarageCond (車庫位置、車庫內部完工程度、車庫品質、車庫狀況)\n資料說明中，此處數值為Na代表沒有車庫。\n\nfor col in (\'GarageType\', \'GarageFinish\', \'GarageQual\', \'GarageCond\'):\n    all_data[col] = all_data[col].fillna(\'None\')\n\n\nGarageYrBlt, GarageArea, GarageCars (車庫年分、車庫大小、可放幾台車)\n將無資料的欄位填入0。\n\nfor col in (\'GarageYrBlt\', \'GarageArea\', \'GarageCars\'):\n    all_data[col] = all_data[col].fillna(0)\n\n\nBsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath(地下室相關的數值參數，為零代表沒有地下室)\n\nfor col in (\'BsmtFinSF1\', \'BsmtFinSF2\', \'BsmtUnfSF\',\'TotalBsmtSF\', \'BsmtFullBath\', \'BsmtHalfBath\'):\n    all_data[col] = all_data[col].fillna(0)\n\n\nBsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 (地下室高度、地下室評級、地下室牆面狀況、地下室完工評比、地下室完工評比2)\n此處的地下室類別參數中，Na代表沒有地下室。\n\nfor col in (\'BsmtQual\', \'BsmtCond\', \'BsmtExposure\', \'BsmtFinType1\', \'BsmtFinType2\'):\n    all_data[col] = all_data[col].fillna(\'None\')\n\n\nMasVnrArea and MasVnrType (裝飾外牆面積、裝飾外牆種類)\nNa代表房產沒有石工裝飾外牆，種類填入None，面積填入0。\n\nall_data[""MasVnrType""] = all_data[""MasVnrType""].fillna(""None"")\nall_data[""MasVnrArea""] = all_data[""MasVnrArea""].fillna(0)\n\n\nMSZoning (房屋區域分類)\n由於RL為最常見的數值，將缺失欄位填入RL。\n\nall_data[\'MSZoning\'] = all_data[\'MSZoning\'].fillna(all_data[\'MSZoning\'].mode()[0])\n\n\nUtilities (水電瓦斯供應)\n除了兩筆NA，一筆NoSeWa以外，其餘皆為AllPub。由僅有兩種資料差異，且位於training set當中，因此將整個參數移除。\n\nall_data = all_data.drop([\'Utilities\'], axis=1)\n\n\nFunctional (房屋功能性)\n資料說明中，預設為Typ，因此將Na設為Typ\n\nall_data[""Functional""] = all_data[""Functional""].fillna(""Typ"")\n\n\nElectrical (電力系統)\n具有一筆Na數值，其餘大多數為SBrkr，因此將缺失欄位改為SBrkr。\n\nall_data[\'Electrical\'] = all_data[\'Electrical\'].fillna(all_data[\'Electrical\'].mode()[0])\n\n\nKitchenQual (廚房品質)\n與Electrical相同，僅具有一筆Na，因此將其設定為最常見的TA。\n\nall_data[\'KitchenQual\'] = all_data[\'KitchenQual\'].fillna(all_data[\'KitchenQual\'].mode()[0])\n\n\nExterior1st, Exterior2nd (房屋外觀材質1、房屋外觀材質2)\n若外觀材質1 or 2 僅缺一個參數，因此填入最常見的數值。\n\nall_data[\'Exterior1st\'] = all_data[\'Exterior1st\'].fillna(all_data[\'Exterior1st\'].mode()[0])\nall_data[\'Exterior2nd\'] = all_data[\'Exterior2nd\'].fillna(all_data[\'Exterior2nd\'].mode()[0])\n\n\nSaleType (交易方式與類型)\n填入最常見的參數WD。\n\nall_data[\'SaleType\'] = all_data[\'SaleType\'].fillna(all_data[\'SaleType\'].mode()[0])\n\n\nMSSubClass (住宅類型)\nNa 代表缺乏建築分類資料，以None取代。\n\nall_data[\'MSSubClass\'] = all_data[\'MSSubClass\'].fillna(""None"")\n\n檢查是否有遺漏的NaN資料\n#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({\'Missing Ratio\' :all_data_na})\nmissing_data.head()\n\n\n\n\n\nMissing Ratio\n\n\n\n\n已經沒有未修正的Na數值\n\n\n\n\n1.2.2. 資料相關度\n所有參數相關性矩陣\n利用seaborn提供的heatmap(熱圖)，能夠輕鬆做出相關性矩陣，vmax是指最大值超過限度的方塊，為白色（square=True保证了图中都是方块；顏色越淺，相關性越大）\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\n\n\n這張圖中，我們會注意到兩個白色區塊，分別為：\n\n\'TotalBsmtSF\' 及 \'1stFlrSF\'\n\'GarageCars\' 及 \'GarageArea\'\n其實我們仔細觀察此兩組變數，就可以發現它們提供的幾乎是一樣的資訊。熱圖對於此種高度共線性，並在選取重要參數時具有很好的辨識能力。\n此外我們可以發現SalePrice與上面討論過的\'GrLivArea\'，\'TotalBsmtSF\'，以及 \'OverallQual\'相關性較高，但不僅這三項，我們接下來就是要處理這個部分。\n\n1.2.3. 數值轉類別\n\'MSSubClass\'雖然是數值欄位，但實際上是以數值的方式，分16種類別；同樣的，\'OverallCond\'以數值分了1-10等級；\'YrSold\'、\'MoSold\'銷售年分以及月份皆以類別的方式處理。\nall_data[\'MSSubClass\'] = all_data[\'MSSubClass\'].apply(str)\n\n#Changing OverallCond into a categorical variable\nall_data[\'OverallCond\'] = all_data[\'OverallCond\'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data[\'YrSold\'] = all_data[\'YrSold\'].astype(str)\nall_data[\'MoSold\'] = all_data[\'MoSold\'].astype(str)\n\n1.2.4. 類別轉數值\n我們將有序文字類別轉為數值，方便計算使用。\n大多數的類別格式皆為文字(str)資料，這對我們進行資料計算時會產生很多不方便的地方，因此轉為數值就是一個好用的手段。\n但由於數值隱含了大小階層關係，但原始的類別資料不一定有這層意義，因此無階層關係的類別將會在接下來的7. 一位有效編碼 (One Hot Encoding)中進行轉換。\n那些參數具有階層關係呢？如各種品質、房屋狀態、道路鋪面狀況、中央空調有無、建築類型&年分、售出年份等具有優劣分別的資料，因此需要對於特徵參數以及內涵的資料具有一定的掌握度。\nfrom sklearn.preprocessing import LabelEncoder\ncols = (\'FireplaceQu\', \'BsmtQual\', \'BsmtCond\', \'GarageQual\', \'GarageCond\', \'ExterQual\', \'ExterCond\',\'HeatingQC\', \'PoolQC\', \'KitchenQual\', \'BsmtFinType1\', \'BsmtFinType2\', \'Functional\', \'Fence\', \'BsmtExposure\', \'GarageFinish\', \'LandSlope\', \'LotShape\', \'PavedDrive\', \'Street\', \'Alley\', \'CentralAir\', \'MSSubClass\', \'OverallCond\', \'YrSold\', \'MoSold\')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint(\'Shape all_data: {}\'.format(all_data.shape))\n\n1.2.5. 新增參數\n由於屋內空間對於售價具有決定性的影響力，因此我們新增一個室內總面積的參數，包含地下室、一樓、二樓的面積。\n# Adding total sqfootage feature \nall_data[\'TotalSF\'] = all_data[\'TotalBsmtSF\'] + all_data[\'1stFlrSF\'] + all_data[\'2ndFlrSF\']\n\n1.2.6. 偏度校正\n由於大多數的參數資料分布並不符合常態分布，因此將其以Box Cox Transformation進行偏度校正。\nnumeric_feats = all_data.dtypes[all_data.dtypes != ""object""].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(""\\nSkew in numerical features: \\n"")\nskewness = pd.DataFrame({\'Skew\' :skewed_feats})\nskewness.head(10)\n\n1.2.7. 一位有效編碼\n由於某些類別參數不帶有優劣、順序關係，因此不能單純以數字代替，而需要以one-hot encoding的方式來進行編碼。get_dummies()會將欄位中的object轉換為無序的Dummy variables。\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)\n\n1.2.8. 特徵組合\n許多特徵資料雖然是以數值的方式呈現，但其展示的其實是類別特徵，比如說\'MSSubClass\'以數字的方式將房屋類型分為16類，而不同類別的方屋對於售價有較大的影響，因此我們先將MSSubClass/SalePrice列出，再合理分類，以此方式最大化特徵參數。\nall_data.groupby([\'MSSubClass\'])[[\'SalePrice\']].agg([\'mean\',\'median\',\'count\'])\n\n\n以上述方式分類下列特徵類別：\n#from ALL YOU NEED IS PCA\nfull=all_data.copy()\ndef map_values():\n    full[""oMSSubClass""] = full.MSSubClass.map({\'180\':1, \'150\':1,\n                                        \'30\':2, \'45\':2, \n                                        \'190\':3, \'50\':3, \'90\':3, \n                                        \'85\':4, \'40\':4, \'160\':4, \n                                        \'70\':5, \'20\':5, \'75\':5, \'80\':5, \n                                        \'120\': 6, \'60\':6})\n    \n    full[""oMSZoning""] = full.MSZoning.map({\'C (all)\':1, \'RH\':2, \'RM\':2, \'RL\':3, \'FV\':4})\n    \n    full[""oNeighborhood""] = full.Neighborhood.map({\'MeadowV\':1,\n                                               \'IDOTRR\':2, \'BrDale\':2,\n                                               \'OldTown\':3, \'Edwards\':3, \'BrkSide\':3,\n                                               \'Sawyer\':4, \'Blueste\':4, \'SWISU\':4, \'NAmes\':4,\n                                               \'NPkVill\':5, \'Mitchel\':5,\n                                               \'SawyerW\':6, \'Gilbert\':6, \'NWAmes\':6,\n                                               \'Blmngtn\':7, \'CollgCr\':7, \'ClearCr\':7, \'Crawfor\':7,\n                                               \'Veenker\':8, \'Somerst\':8, \'Timber\':8,\n                                               \'StoneBr\':9,\n                                               \'NoRidge\':10, \'NridgHt\':10})\n    \n    full[""oCondition1""] = full.Condition1.map({\'Artery\':1,\n                                           \'Feedr\':2, \'RRAe\':2,\n                                           \'Norm\':3, \'RRAn\':3,\n                                           \'PosN\':4, \'RRNe\':4,\n                                           \'PosA\':5 ,\'RRNn\':5})\n    \n    full[""oBldgType""] = full.BldgType.map({\'2fmCon\':1, \'Duplex\':1, \'Twnhs\':1, \'1Fam\':2, \'TwnhsE\':2})\n    \n    full[""oHouseStyle""] = full.HouseStyle.map({\'1.5Unf\':1, \n                                           \'1.5Fin\':2, \'2.5Unf\':2, \'SFoyer\':2, \n                                           \'1Story\':3, \'SLvl\':3,\n                                           \'2Story\':4, \'2.5Fin\':4})\n    \n    full[""oExterior1st""] = full.Exterior1st.map({\'BrkComm\':1,\n                                             \'AsphShn\':2, \'CBlock\':2, \'AsbShng\':2,\n                                             \'WdShing\':3, \'Wd Sdng\':3, \'MetalSd\':3, \'Stucco\':3, \'HdBoard\':3,\n                                             \'BrkFace\':4, \'Plywood\':4,\n                                             \'VinylSd\':5,\n                                             \'CemntBd\':6,\n                                             \'Stone\':7, \'ImStucc\':7})\n    \n    full[""oMasVnrType""] = full.MasVnrType.map({\'BrkCmn\':1, \'None\':1, \'BrkFace\':2, \'Stone\':3})\n    \n    full[""oExterQual""] = full.ExterQual.map({\'Fa\':1, \'TA\':2, \'Gd\':3, \'Ex\':4})\n    \n    full[""oFoundation""] = full.Foundation.map({\'Slab\':1, \n                                           \'BrkTil\':2, \'CBlock\':2, \'Stone\':2,\n                                           \'Wood\':3, \'PConc\':4})\n    \n    full[""oBsmtQual""] = full.BsmtQual.map({\'Fa\':2, \'None\':1, \'TA\':3, \'Gd\':4, \'Ex\':5})\n    \n    full[""oBsmtExposure""] = full.BsmtExposure.map({\'None\':1, \'No\':2, \'Av\':3, \'Mn\':3, \'Gd\':4})\n    \n    full[""oHeating""] = full.Heating.map({\'Floor\':1, \'Grav\':1, \'Wall\':2, \'OthW\':3, \'GasW\':4, \'GasA\':5})\n    \n    full[""oHeatingQC""] = full.HeatingQC.map({\'Po\':1, \'Fa\':2, \'TA\':3, \'Gd\':4, \'Ex\':5})\n    \n    full[""oKitchenQual""] = full.KitchenQual.map({\'Fa\':1, \'TA\':2, \'Gd\':3, \'Ex\':4})\n    \n    full[""oFunctional""] = full.Functional.map({\'Maj2\':1, \'Maj1\':2, \'Min1\':2, \'Min2\':2, \'Mod\':2, \'Sev\':2, \'Typ\':3})\n    \n    full[""oFireplaceQu""] = full.FireplaceQu.map({\'None\':1, \'Po\':1, \'Fa\':2, \'TA\':3, \'Gd\':4, \'Ex\':5})\n    \n    full[""oGarageType""] = full.GarageType.map({\'CarPort\':1, \'None\':1,\n                                           \'Detchd\':2,\n                                           \'2Types\':3, \'Basment\':3,\n                                           \'Attchd\':4, \'BuiltIn\':5})\n    \n    full[""oGarageFinish""] = full.GarageFinish.map({\'None\':1, \'Unf\':2, \'RFn\':3, \'Fin\':4})\n    \n    full[""oPavedDrive""] = full.PavedDrive.map({\'N\':1, \'P\':2, \'Y\':3})\n    \n    full[""oSaleType""] = full.SaleType.map({\'COD\':1, \'ConLD\':1, \'ConLI\':1, \'ConLw\':1, \'Oth\':1, \'WD\':1,\n                                       \'CWD\':2, \'Con\':3, \'New\':3})\n    \n    full[""oSaleCondition""] = full.SaleCondition.map({\'AdjLand\':1, \'Abnorml\':2, \'Alloca\':2, \'Family\':2, \'Normal\':3, \'Partial\':4})            \n                \n                        \n                        \n    \n    return ""Done!""\n\nmap_values()\n\n2. 模型選擇與評估\n這個部分我們處理模型產出的相關流程，由建立管線流程，以lasso選取相關特徵最高的幾個項目，加入新的特徵參數後納入標準管線流程。並使用Weight Average、stacking方式完成本次submit的基礎模型。最後再使用融合方式，合併其他優秀模型的成果，期望能達到最佳的Leaderboard Ranking排名！\n2.1 Pipeline 管線處理\n利用Pipeline可以方便的減少coding的行數，並重複利用整個流程，對於參數調整以及試誤都有很好的功效。\n定義pipeline中的labelenc以及skew_dummies 函式，並在其中處裡新增特徵數值skew>=0.5的案例，再求對數使其符合正態分布。\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass labelenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        lab=LabelEncoder()\n        X[""YearBuilt""] = lab.fit_transform(X[""YearBuilt""])\n        X[""YearRemodAdd""] = lab.fit_transform(X[""YearRemodAdd""])\n        X[""GarageYrBlt""] = lab.fit_transform(X[""GarageYrBlt""])\n        return X\n\nfrom scipy.stats import skew\nclass skew_dummies(BaseEstimator, TransformerMixin):\n    def __init__(self,skew=0.5):\n        self.skew = skew\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[""object""])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        X = pd.get_dummies(X)\n        return X\n\n\n\n建立pipeline流程\n# build pipeline\nfrom sklearn.pipeline import Pipeline, make_pipeline\npipe = Pipeline([\n    (\'labenc\', labelenc()),\n    (\'skew_dummies\', skew_dummies(skew=1)),\n    ])\n\n以lasso選取關係度較高的特徵，並再度增加特徵數量\n由於上述的特徵工程可能仍然不足夠，結合不同的特徵通常是個好方法，但是我們沒辦法確定該選擇哪一個，好在我們可以使用一些模型提供特徵的選擇，這邊運用lasso算法來進行訓練集的特徵選擇\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\nlasso=Lasso(alpha=0.001)\nlasso.fit(X_scaled,y_log)\n\nLasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection=\'cyclic\', tol=0.0001, warm_start=False)\n\nFI_lasso = pd.DataFrame({""Feature Importance"":lasso.coef_}, index=data_pipe.columns)\nFI_lasso.sort_values(""Feature Importance"",ascending=False)\n\n\nFI_lasso[FI_lasso[""Feature Importance""]!=0].sort_values(""Feature Importance"").plot(kind=""barh"",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.show()\n\n\n得到特徵重要性圖之後就可以進行特徵選擇與重做\nclass add_feature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.additional==1:\n            X[""TotalHouse""] = X[""TotalBsmtSF""] + X[""1stFlrSF""] + X[""2ndFlrSF""]   \n            X[""TotalArea""] = X[""TotalBsmtSF""] + X[""1stFlrSF""] + X[""2ndFlrSF""] + X[""GarageArea""]\n            \n        else:\n            X[""TotalHouse""] = X[""TotalBsmtSF""] + X[""1stFlrSF""] + X[""2ndFlrSF""]   \n            X[""TotalArea""] = X[""TotalBsmtSF""] + X[""1stFlrSF""] + X[""2ndFlrSF""] + X[""GarageArea""]\n            \n            X[""+_TotalHouse_OverallQual""] = X[""TotalHouse""] * X[""OverallQual""]\n            X[""+_GrLivArea_OverallQual""] = X[""GrLivArea""] * X[""OverallQual""]\n            X[""+_oMSZoning_TotalHouse""] = X[""oMSZoning""] * X[""TotalHouse""]\n            X[""+_oMSZoning_OverallQual""] = X[""oMSZoning""] + X[""OverallQual""]\n            X[""+_oMSZoning_YearBuilt""] = X[""oMSZoning""] + X[""YearBuilt""]\n            X[""+_oNeighborhood_TotalHouse""] = X[""oNeighborhood""] * X[""TotalHouse""]\n            X[""+_oNeighborhood_OverallQual""] = X[""oNeighborhood""] + X[""OverallQual""]\n            X[""+_oNeighborhood_YearBuilt""] = X[""oNeighborhood""] + X[""YearBuilt""]\n            X[""+_BsmtFinSF1_OverallQual""] = X[""BsmtFinSF1""] * X[""OverallQual""]\n            \n            X[""-_oFunctional_TotalHouse""] = X[""oFunctional""] * X[""TotalHouse""]\n            X[""-_oFunctional_OverallQual""] = X[""oFunctional""] + X[""OverallQual""]\n            X[""-_LotArea_OverallQual""] = X[""LotArea""] * X[""OverallQual""]\n            X[""-_TotalHouse_LotArea""] = X[""TotalHouse""] + X[""LotArea""]\n            X[""-_oCondition1_TotalHouse""] = X[""oCondition1""] * X[""TotalHouse""]\n            X[""-_oCondition1_OverallQual""] = X[""oCondition1""] + X[""OverallQual""]\n            \n           \n            X[""Bsmt""] = X[""BsmtFinSF1""] + X[""BsmtFinSF2""] + X[""BsmtUnfSF""]\n            X[""Rooms""] = X[""FullBath""]+X[""TotRmsAbvGrd""]\n            X[""PorchArea""] = X[""OpenPorchSF""]+X[""EnclosedPorch""]+X[""3SsnPorch""]+X[""ScreenPorch""]\n            X[""TotalPlace""] = X[""TotalBsmtSF""] + X[""1stFlrSF""] + X[""2ndFlrSF""] + X[""GarageArea""] + X[""OpenPorchSF""]+X[""EnclosedPorch""]+X[""3SsnPorch""]+X[""ScreenPorch""]\n\n    \n            return X\n\n將所有的特徵處理納入Pipeline流程中、並重建pipeline數據\npipe = Pipeline([\n    (\'labenc\', labelenc()),\n    (\'add_feature\', add_feature(additional=2)),\n    (\'skew_dummies\', skew_dummies(skew=1)),\n    ])\nfull_pipe = pipe.fit_transform(full)\nfull_pipe.shape\n\nn_train=train.shape[0]\nX = full_pipe[:n_train]\ntest_X = full_pipe[n_train:]\ny= train.SalePrice\n\nX_scaled = scaler.fit(X).transform(X)\ny_log = np.log(train.SalePrice)\ntest_X_scaled = scaler.transform(test_X)\n\n2.2 PCA處理\n由於我們從原本的資料新增了許多特徵參數，一定會有很多共線性特徵出現，因此這裡加入主成分分析流程 (Principal Component Analysis, PCA)\nfrom sklearn.decomposition import PCA, KernelPCA\npca = PCA(n_components=410)\nX_scaled=pca.fit_transform(X_scaled)\ntest_X_scaled = pca.transform(test_X_scaled)\nX_scaled.shape, test_X_scaled.shape\ntest_X_scaled\n\n\n2.3 DNN\n由於上課時有提到可以使用keras-DNN進行訓練，因此我們將DNN流程也運用進來。但由於結果不理想(最終score約: 0.14)，因此這邊僅作紀錄之用。\n# col_train = list(train.columns)\n# col_train_bis = list(train.columns)\n# COLUMNS = col_train\n# FEATURES = col_train_bis\n# LABEL = ""SalePrice""\n# feature_cols = FEATURES\n# # Training set and Prediction set with the features to predict\n# training_set = train[COLUMNS]\n# prediction_set = train.SalePrice\n\n# import keras\n# import numpy as np\n# import pandas as pd\n# from keras.models import Sequential\n# from keras.layers import Dense\n# from keras.wrappers.scikit_learn import KerasRegressor\n# model = keras.models.Sequential()\n\n# # model = Sequential()\n# # model.add(Dense(200, input_dim=410, kernel_initializer=\'normal\', activation=\'relu\'))\n# # model.add(Dense(100, kernel_initializer=\'normal\', activation=\'relu\'))\n# # model.add(Dense(50, kernel_initializer=\'normal\', activation=\'relu\'))\n# # model.add(Dense(25, kernel_initializer=\'normal\', activation=\'relu\'))\n# # model.add(Dense(1, kernel_initializer=\'normal\'))\n\n\n# model = keras.models.Sequential([\n# # keras.layers.Flatten(input_dim=410),\n# # keras.layers.BatchNormalization(),\n# keras.layers.Dense(200, input_dim=410,activation=""relu"", kernel_initializer=""normal""),\n# keras.layers.BatchNormalization(),\n# keras.layers.Dense(100, activation=""relu"", kernel_initializer=""normal""),\n# keras.layers.BatchNormalization(),\n# keras.layers.Dense(50, activation=""relu"", kernel_initializer=""normal""),\n# keras.layers.BatchNormalization(),\n# keras.layers.Dense(25, activation=""relu"", kernel_initializer=""normal""),\n# keras.layers.BatchNormalization(),\n# keras.layers.Dense(1, kernel_initializer=\'normal\')\n# ])\n\n\n# # Compile model\n# model.compile(loss=\'mean_squared_error\', optimizer=keras.optimizers.Adadelta())\n\n# model.fit(X_scaled, y_log, epochs=1000, batch_size=10,callbacks=[keras.callbacks.EarlyStopping(patience=3)])\n\n# model.evaluate(X_scaled, y_log)\n\n# #y_predict = model.predict(test_X_scaled)\n# y_predict = np.exp(model.predict(test_X_scaled))\n# ID = test_ID\n\n# import itertools\n# def to_submit(pred_y,name_out):\n#     y_predict = list(itertools.islice(pred_y, test.shape[0]))\n#     y_predict = pd.DataFrame((np.array(y_predict).reshape(len(y_predict),1)), columns = [\'SalePrice\'])\n#     y_predict = y_predict.join(ID)\n#     y_predict.to_csv(name_out + \'.csv\',index=False)\n    \n# to_submit(y_predict, ""DNN_submission_v02"")\n\n\n2.4 模型選取、評估\n2.4.1 Cross Validation\n定義Cross Validation函式。將資料切為五份，隨機打亂(shuffle)後平均分數，以避免訓練時資料選取造成的Bias。\n按照比賽要求定義基於 RMSE 的交叉驗證評估指標\n# define cross validation strategy\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=""neg_mean_squared_error"", cv=5))\n    return rmse\n\n2.4.2 模型選取\n我們採用以下13個模型，分別為：\n\nLinearRegression\nRidge\nLasso\nRandom Forrest\nGradient Boosting Tree\nSupport Vector Regression\nLinear Support Vector Regression\nElasticNet\nStochastic Gradient Descent\nBayesianRidge\nKernelRidge\nExtraTreesRegressor\nXgBoost\n\n引入函式庫、定義model的基本參數\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\nmodels = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel=\'polynomial\', degree=2, coef0=2.5),\n          ExtraTreesRegressor(),XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)]\n\n評估每個模型的預測效果\nnames = [""LR"", ""Ridge"", ""Lasso"", ""RF"", ""GBR"", ""SVR"", ""LinSVR"", ""Ela"",""SGD"",""Bay"",""Ker"",""Extra"",""Xgb""]\nfor name, model in zip(names, models):\n    score = rmse_cv(model, X_scaled, y_log)\n    print(""{}: {:.6f}, {:.4f}"".format(name,score.mean(),score.std()))\n\n2.4.2 模型參數調整\n參數調整，建立一個調參的方法，這裏的評估指標是RMSE，所以打印出的分數也要是RMSE。定義交叉方式，先指定模型後指定參數，方便測試多個模型，使用網格交叉驗證\nclass grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=""neg_mean_squared_error"")\n        grid_search.fit(X,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_[\'mean_test_score\'] = np.sqrt(-grid_search.cv_results_[\'mean_test_score\'])\n        print(pd.DataFrame(grid_search.cv_results_)[[\'params\',\'mean_test_score\',\'std_test_score\']])\n\nLasso()的參數調整結果\ngrid(Lasso()).grid_get(X_scaled,y_log,{\'alpha\': [0.0004,0.0005,0.0007,0.0009],\'max_iter\':[10000]})\n\nRidge()的參數調整結果\ngrid(Ridge()).grid_get(X_scaled,y_log,{\'alpha\':[35,40,45,50,55,60,65,70,80,90]})\n\nSVR()的參數調整結果\ngrid(SVR()).grid_get(X_scaled,y_log,{\'C\':[11,13,15],\'kernel\':[""rbf""],""gamma"":[0.0003,0.0004],""epsilon"":[0.008,0.009]})\n\nKernel Ridge() 的參數調整結果\nparam_grid={\'alpha\':[0.2,0.3,0.4], \'kernel\':[""polynomial""], \'degree\':[3],\'coef0\':[0.8,1]}\ngrid(KernelRidge()).grid_get(X_scaled,y_log,param_grid)\n\nElasticNet() 的參數調整結果\ngrid(ElasticNet()).grid_get(X_scaled,y_log,{\'alpha\':[0.0008,0.004,0.005],\'l1_ratio\':[0.08,0.1,0.3],\'max_iter\':[10000]})\n\n經過多輪測試，最終選擇以下六個模型及對應的最優參數，進行加權平均集成方法\nlasso = Lasso(alpha=0.0007,max_iter=10000)\nridge = Ridge(alpha=80)\nsvr = SVR(gamma= 0.0004,kernel=\'rbf\',C=11,epsilon=0.008)\nker = KernelRidge(alpha=0.2 ,kernel=\'polynomial\',degree=3 , coef0=1)\nela = ElasticNet(alpha=0.005,l1_ratio=0.1,max_iter=10000)\nbay = BayesianRidge()\n\n3. 模型整合\n由於比起建立新的模型建立方式，以多個模型取各自優點可以取得最佳成績，以下將使用加權平均法以及堆疊法(Stacking)嘗試求得最低rmsel結果。\n3.1 加權平均方法\n根據權重對各個模型加權平均\nclass AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n        \n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.mod]\n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w\n\n定義6個初始權重\n# assign weights based on their gridsearch score\nw1 = 0.02\nw2 = 0.2\nw3 = 0.25\nw4 = 0.3\nw5 = 0.03\nw6 = 0.2\n\n依照權重進行分配\nweight_avg = AverageWeight(mod = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])\nscore = rmse_cv(weight_avg,X_scaled,y_log)\nprint(score.mean())\n\n若我們僅取最佳的兩個模型進行分配，將會得到更好的結果\nweight_avg = AverageWeight(mod = [svr,ker],weight=[0.5,0.5])\nscore = rmse_cv(weight_avg,X_scaled,y_log)\nprint(score.mean())\n\n3.2 模型堆疊方法\n\nclass stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,mod,meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n        \n    def fit(self,X,y):\n        self.saved_model = [list() for i in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        \n        for i,model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X,y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index,i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train,y)\n        return self\n    \n    def predict(self,X):\n        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self,X,y,test_X):\n        oof = np.zeros((X.shape[0],len(self.mod)))\n        test_single = np.zeros((test_X.shape[0],5))\n        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n        for i,model in enumerate(self.mod):\n            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index],y[train_index])\n                oof[val_index,i] = clone_model.predict(X[val_index])\n                test_single[:,j] = clone_model.predict(test_X)\n            test_mean[:,i] = test_single.mean(axis=1)\n        return oof, test_mean\n\n預處理後才能放到堆疊模型計算\n# must do imputer first, otherwise stacking won\'t work, and i don\'t know why.\nfrom sklearn.impute import SimpleImputer\na = SimpleImputer().fit_transform(X_scaled)\nb = SimpleImputer().fit_transform(y_log.values.reshape(-1,1)).ravel()\n\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)\nscore = rmse_cv(stack_model,a,b)\nprint(score.mean())\n\n0.12639169939493294\n\n最後，將stacking產出的特徵與原先的特徵合併，最佳化原本的預測分數。\nX_train_stack, X_test_stack = stack_model.get_oof(a,b,test_X_scaled)\nX_train_add = np.hstack((a,X_train_stack))\nX_test_add = np.hstack((test_X_scaled,X_test_stack))\nX_train_add.shape, X_test_add.shape\nscore = rmse_cv(stack_model,X_train_add,b)\nprint(score.mean())\n\n0.11007661117525458\n\n3.3 submittion 成果繳交\n輸出submittion.csv至kaggle上繳以後，分數為：\n\n# This is the final model I use\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)\nstack_model.fit(a,b)\nresult=pd.DataFrame({\'id\':test_ID, \'SalePrice\':pred})\nresult.to_csv(""submission_4PCA.csv"",index=False)\n\n4. 混合法\n由於此份題目的主旨為提升Ranking的成績，我們還有另一個方法可以提高名次，雖然不符合一般使用的方法，但可以有效的提升排名！\n我們提交此份名次後，獲得了 0.10677 分，超越三個融合成果中成績最高的\'House_Prices_submit.csv: 0.10985\'!!\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n\nsubmission = pd.read_csv(""./sample_submission.csv"")\nsubmission.iloc[:,1] = np.exp(stack_model.predict(test_X_scaled))\n\n# this kernel gave a score 0.115\n# let\'s up it by mixing with the top kernels\n\n#print(\'Blend with Top Kernals submissions\', datetime.now(),)\nsub_1 = pd.read_csv(\'./House_Prices_submit.csv\')\nsub_2 = pd.read_csv(\'./hybrid_solution.csv\')\nsub_3 = pd.read_csv(\'./lasso_sol22_Median.csv\')\n\nsubmission.iloc[:,1] = np.floor((0.05 * np.exp(stack_model.predict(test_X_scaled)) + \n                                (0.85 * sub_1.iloc[:,1]) + \n                                (0.05 * sub_2.iloc[:,1]) + \n                                (0.05 * sub_3.iloc[:,1])))\n                                \nq1 = submission[\'SalePrice\'].quantile(0.0045)\nq2 = submission[\'SalePrice\'].quantile(0.99)\n\nsubmission[\'SalePrice\'] = submission[\'SalePrice\'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission[\'SalePrice\'] = submission[\'SalePrice\'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv(""submission_0418_v04.csv"", index=False)\n#print(\'Save submission\', datetime.now(),)\n\n5. Data Leakage 問題\n由於kaggle: House Price上大多數的kernel都將train.csv以及test.csv整合起來進行特徵工程處理。但在特徵學習的角度上，這樣會產生嚴重的 data leackage 問題。因此我們另外將以上的方式扣除test.csv並再度進行處理。主要的差異在處理空缺數值、偏度校正、特徵組合以及特徵選擇部分，若納入test set，會將test set中的特徵值以不同的方式影響到原先train data的評估準確度。\n但由於我們在最後的融合階段，tune出的權重大幅降低此處訓練出結果的比重，因此在比較去除test set的差異時，僅使用融合之前的結果。\n在將test set移除後，由kaggle scoring的rmsle分數，可以看到score上的差異量約為0.00032。\n在去除了data leakage的問題後，我們可以較為自信的表示，我們有達到data prediction中所需求的目標。\n\n'], 'url_profile': 'https://github.com/WuSKai403', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['Heart Disease - Classification Problem\nSolving a classification problem using Logistic Regression, LDA, and Random Forest.\n'], 'url_profile': 'https://github.com/emora034', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sejongkang', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhong94', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/william-tiritilli', 'info_list': ['HTML', 'Updated Apr 20, 2020', '1', 'PHP', 'Updated Apr 23, 2020', 'R', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', '1', 'Python', 'Updated Apr 28, 2020', 'Updated Apr 22, 2020']}"
"{'location': 'İstanbul', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['Polynomial-Regression-\nPolynomial regression algoritm with numerical analysis\nTurkish descripion of the algorithm : https://medium.com/@keyoguzhan/polinomsal-regresyon-algoritması-ve-covi̇d-19-veri-seti-implementasyonu-1cbe25bae1e9\nImplementation on a data science project : https://www.kaggle.com/oguzhanercan/polynomial-regression-algorithm-with-cov-d-19\n'], 'url_profile': 'https://github.com/Oguzhanercan', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Regression Week 2: Multiple Regression (Interpretation)\nThe goal of this first notebook is to explore multiple regression and feature engineering with existing Turi Create functions.\nIn this notebook you will use data on house sales in King County to predict prices using multiple regression. You will:\n\nUse SFrames to do some feature engineering\nUse built-in Turi Create functions to compute the regression weights (coefficients/parameters)\nGiven the regression weights, predictors and outcome write a function to compute the Residual Sum of Squares\nLook at coefficients and interpret their meanings\nEvaluate multiple models via RSS\n\nFire up Turi Create\nimport turicreate\nLoad in house sales data\nDataset is from house sales in King County, the region where the city of Seattle, WA is located.\nsales = turicreate.SFrame(\'home_data.sframe/\')\nSplit data into training and testing.\nWe use seed=0 so that everyone running this notebook gets the same results.  In practice, you may set a random seed (or let Turi Create pick a random seed for you).\ntrain_data,test_data = sales.random_split(.8,seed=0)\nLearning a multiple regression model\nRecall we can use the following code to learn a multiple regression model predicting \'price\' based on the following features:\nexample_features = [\'sqft_living\', \'bedrooms\', \'bathrooms\'] on training data with the following code:\n(Aside: We set validation_set = None to ensure that the results are always the same)\nexample_features = [\'sqft_living\', \'bedrooms\', \'bathrooms\']\nexample_model = turicreate.linear_regression.create(train_data, target = \'price\', features = example_features, \n                                                    validation_set = None)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 17384\nNumber of features          : 3\nNumber of unpacked features : 3\nNumber of coefficients    : 4\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 1.018871     | 4146407.600631     | 258679.804477                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\nNow that we have fitted the model we can extract the regression weights (coefficients) as an SFrame as follows:\nexample_weight_summary = example_model.coefficients\nprint(example_weight_summary)\n+-------------+-------+---------------------+--------------------+\n|     name    | index |        value        |       stderr       |\n+-------------+-------+---------------------+--------------------+\n| (intercept) |  None |   87910.0724923957  | 7873.338143401634  |\n| sqft_living |  None |  315.40344055210005 | 3.4557003258547296 |\n|   bedrooms  |  None | -65080.215552827525 | 2717.4568544207045 |\n|  bathrooms  |  None |  6944.020192638836  | 3923.114931441481  |\n+-------------+-------+---------------------+--------------------+\n[4 rows x 4 columns]\n\nMaking Predictions\nIn the gradient descent notebook we use numpy to do our regression. In this book we will use existing Turi Create functions to analyze multiple regressions.\nRecall that once a model is built we can use the .predict() function to find the predicted values for data we pass. For example using the example model above:\nexample_predictions = example_model.predict(train_data)\nprint(example_predictions[0]) # should be 271789.505878\n271789.5058780301\n\nCompute RSS\nNow that we can make predictions given the model, let\'s write a function to compute the RSS of the model. Complete the function below to calculate RSS given the model, data, and the outcome.\ndef get_residual_sum_of_squares(model, data, outcome):\n    # First get the predictions\n    predicted = model.predict(data);\n    # Then compute the residuals/errors\n    errors = outcome-predicted;\n    # Then square and add them up    \n    RSS = (errors*errors).sum();\n    return(RSS)    \nTest your function by computing the RSS on TEST data for the example model:\nrss_example_train = get_residual_sum_of_squares(example_model, test_data, test_data[\'price\'])\nprint(rss_example_train) # should be 2.7376153833e+14\n273761538330193.0\n\nCreate some new features\nAlthough we often think of multiple regression as including multiple different features (e.g. # of bedrooms, squarefeet, and # of bathrooms) but we can also consider transformations of existing features e.g. the log of the squarefeet or even ""interaction"" features such as the product of bedrooms and bathrooms.\nYou will use the logarithm function to create a new feature. so first you should import it from the math library.\nfrom math import log\nNext create the following 4 new features as column in both TEST and TRAIN data:\n\nbedrooms_squared = bedrooms*bedrooms\nbed_bath_rooms = bedrooms*bathrooms\nlog_sqft_living = log(sqft_living)\nlat_plus_long = lat + long\nAs an example here\'s the first one:\n\ntrain_data[\'bedrooms_squared\'] = train_data[\'bedrooms\'].apply(lambda x: x**2)\ntest_data[\'bedrooms_squared\'] = test_data[\'bedrooms\'].apply(lambda x: x**2)\n# create the remaining 3 features in both TEST and TRAIN data\ntrain_data[\'bed_bath_rooms\'] = train_data[\'bedrooms\']*train_data[\'bathrooms\'];\ntest_data[\'bed_bath_rooms\'] = test_data[\'bedrooms\']*test_data[\'bathrooms\'];\n\ntrain_data[\'log_sqft_living\'] = train_data[\'sqft_living\'].apply(lambda x: log(x));\ntest_data[\'log_sqft_living\'] = test_data[\'sqft_living\'].apply(lambda x: log(x));\n\ntrain_data[\'lat_plus_long\'] = train_data[\'lat\'] + train_data[\'long\'];\ntest_data[\'lat_plus_long\'] = test_data[\'lat\'] + test_data[\'long\'];\n\nSquaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this feature will mostly affect houses with many bedrooms.\nbedrooms times bathrooms gives what\'s called an ""interaction"" feature. It is large when both of them are large.\nTaking the log of squarefeet has the effect of bringing large values closer together and spreading out small values.\nAdding latitude to longitude is totally non-sensical but we will do it anyway (you\'ll see why)\n\nQuiz Question: What is the mean (arithmetic average) value of your 4 new features on TEST data? (round to 2 digits)\nprint(\'bedrooms_squared: \' + str(test_data[\'bedrooms_squared\'].mean()));\nprint(\'bed_bath_rooms: \' + str(test_data[\'bed_bath_rooms\'].mean()));\nprint(\'log_sqft_living: \' + str(test_data[\'log_sqft_living\'].mean()));\nprint(\'lat_plus_long: \' + str(test_data[\'lat_plus_long\'].mean()));\nbedrooms_squared: 12.446677701584301\nbed_bath_rooms: 7.503901631591394\nlog_sqft_living: 7.550274679645938\nlat_plus_long: -74.65333497217307\n\nLearning Multiple Models\nNow we will learn the weights for three (nested) models for predicting house prices. The first model will have the fewest features the second model will add one more feature and the third will add a few more:\n\nModel 1: squarefeet, # bedrooms, # bathrooms, latitude & longitude\nModel 2: add bedrooms*bathrooms\nModel 3: Add log squarefeet, bedrooms squared, and the (nonsensical) latitude + longitude\n\nmodel_1_features = [\'sqft_living\', \'bedrooms\', \'bathrooms\', \'lat\', \'long\']\nmodel_2_features = model_1_features + [\'bed_bath_rooms\']\nmodel_3_features = model_2_features + [\'bedrooms_squared\', \'log_sqft_living\', \'lat_plus_long\']\nNow that you have the features, learn the weights for the three different models for predicting target = \'price\' using turicreate.linear_regression.create() and look at the value of the weights/coefficients:\n# Learn the three models: (don\'t forget to set validation_set = None)\nmodel_1 = turicreate.linear_regression.create(train_data, target = \'price\', features = model_1_features, \n                                                    validation_set = None);\nmodel_2 = turicreate.linear_regression.create(train_data, target = \'price\', features = model_2_features, \n                                                    validation_set = None);\nmodel_3 = turicreate.linear_regression.create(train_data, target = \'price\', features = model_3_features, \n                                                    validation_set = None);\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 17384\nNumber of features          : 5\nNumber of unpacked features : 5\nNumber of coefficients    : 6\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.042022     | 4074878.213132     | 236378.596455                   |\n| 2         | 3        | 0.082033     | 4074878.213108     | 236378.596455                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 17384\nNumber of features          : 6\nNumber of unpacked features : 6\nNumber of coefficients    : 7\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.039762     | 4014170.932952     | 235190.935429                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 17384\nNumber of features          : 9\nNumber of unpacked features : 9\nNumber of coefficients    : 10\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.011139     | 3193229.177890     | 228200.043155                   |\n| 2         | 3        | 0.025022     | 3193229.177873     | 228200.043155                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n# Examine/extract each model\'s coefficients:\nprint(model_1.coefficients);\nprint(model_2.coefficients);\nprint(model_3.coefficients);\n+-------------+-------+---------------------+--------------------+\n|     name    | index |        value        |       stderr       |\n+-------------+-------+---------------------+--------------------+\n| (intercept) |  None |  -56140675.74114427 | 1649985.420135553  |\n| sqft_living |  None |  310.26332577692136 | 3.1888296040737765 |\n|   bedrooms  |  None |  -59577.11606759667 | 2487.2797732245012 |\n|  bathrooms  |  None |  13811.840541653264 | 3593.5421329670735 |\n|     lat     |  None |  629865.7894714845  | 13120.710032363884 |\n|     long    |  None | -214790.28516471002 | 13284.285159576597 |\n+-------------+-------+---------------------+--------------------+\n[6 rows x 4 columns]\n\n+----------------+-------+---------------------+--------------------+\n|      name      | index |        value        |       stderr       |\n+----------------+-------+---------------------+--------------------+\n|  (intercept)   |  None |  -54410676.1071702  | 1650405.1652726454 |\n|  sqft_living   |  None |  304.44929805407946 |  3.20217535637094  |\n|    bedrooms    |  None | -116366.04322451768 | 4805.5496654858225 |\n|   bathrooms    |  None |  -77972.33050970349 | 7565.059910947983  |\n|      lat       |  None |  625433.8349445503  | 13058.353097300462 |\n|      long      |  None | -203958.60289731968 | 13268.128370009661 |\n| bed_bath_rooms |  None |  26961.624907583264 | 1956.3656155588428 |\n+----------------+-------+---------------------+--------------------+\n[7 rows x 4 columns]\n\n+------------------+-------+---------------------+--------------------+\n|       name       | index |        value        |       stderr       |\n+------------------+-------+---------------------+--------------------+\n|   (intercept)    |  None |  -52974974.06892153 | 1615194.942821453  |\n|   sqft_living    |  None |  529.1964205687523  | 7.699134985078978  |\n|     bedrooms     |  None |  28948.527746351134 | 9395.728891110177  |\n|    bathrooms     |  None |  65661.20723969836  | 10795.338070247015 |\n|       lat        |  None |  704762.1484430869  |        nan         |\n|       long       |  None | -137780.02000717327 |        nan         |\n|  bed_bath_rooms  |  None |  -8478.364107167803 | 2858.9539125640354 |\n| bedrooms_squared |  None |  -6072.384661904947 | 1494.9704277794906 |\n| log_sqft_living  |  None |  -563467.7842801767 | 17567.823081204006 |\n|  lat_plus_long   |  None |  -83217.19791002883 |        nan         |\n+------------------+-------+---------------------+--------------------+\n[10 rows x 4 columns]\n\nQuiz Question: What is the sign (positive or negative) for the coefficient/weight for \'bathrooms\' in model 1?\nQuiz Question: What is the sign (positive or negative) for the coefficient/weight for \'bathrooms\' in model 2?\nThink about what this means.\nComparing multiple models\nNow that you\'ve learned three models and extracted the model weights we want to evaluate which model is best.\nFirst use your functions from earlier to compute the RSS on TRAINING Data for each of the three models.\n# Compute the RSS on TRAINING data for each of the three models and record the values:\nprint(get_residual_sum_of_squares(model_1, train_data, train_data[\'price\']));\nprint(get_residual_sum_of_squares(model_2, train_data, train_data[\'price\']));\nprint(get_residual_sum_of_squares(model_3, train_data, train_data[\'price\']));\n971328233545434.4\n961592067859822.1\n905276314551640.9\n\nQuiz Question: Which model (1, 2 or 3) has lowest RSS on TRAINING Data? Is this what you expected?\nNow compute the RSS on on TEST data for each of the three models.\n# Compute the RSS on TESTING data for each of the three models and record the values:\nprint(get_residual_sum_of_squares(model_1, test_data, test_data[\'price\']));\nprint(get_residual_sum_of_squares(model_2, test_data, test_data[\'price\']));\nprint(get_residual_sum_of_squares(model_3, test_data, test_data[\'price\']));\n226568089093160.56\n224368799994313.0\n251829318963157.28\n\nQuiz Question: Which model (1, 2 or 3) has lowest RSS on TESTING Data? Is this what you expected? Think about the features that were added to each model from the previous.\n'], 'url_profile': 'https://github.com/garabaya', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': [""There are multiple ways of implementing Least Squares\nThe Calculus way :\nGradient Descent :\nGradient Descent has been implemented on the mean squared error function\nThe following image is a graph between the cost function and the number of steps taken by the parameter x.\n\nThe Linear Algebra way :\nThe matrix representation of a system of linear equations is of the form Ax = b .\nThe above equation can be solved without any hassle if b belongs to the column space of A. If b does not belong to the column space of A then the equation cannot be solved. The following algorithms can be used to compute an approximate solution in such cases.\ni) Normal Solution :\nThe operation used to compute the Normal Solution is the following:\nx^ = (ATA)-1ATb\nThis solution can be obtained by projecting the vector b onto the column space of A (p) and then solving for x\nusing the equation Ax^ = p, where x^ is the approximate solution.\nThis solution can also be obtained by taking the derivative of square of the magnitude of the error vector i.e. b-Ax^\nand setting it to 0.\nii) Gram Schmidt :\nThis is similar to the Normal Solution. An orthogonal basis for the column space is computed using the  Gram Schmidt algorithm to make certain computations like the inverse of a matrix less expensive. A matrix can be decomposed into an orthogonal matrix Q and an upper triangular matrix R.\nA = QR (QR Factorization)\nThis factorization can be substituted into the Normal solution to obtain the following solution:\nx^ = R-1(QTb)\nOrthogonal Matrices also have the following property which is computationally convinient:\nQTQ = I\nIt is always ideal to have an orthogonal basis as it simplifies matrix operations.\niii) Pseudo Inverse :\nThe matrix representation of a system of linear equations is of the form Ax = b .\nWhy not simply solve for x^ by taking the inverse of A and multiply it with the b vector?\nIt's because in most cases A is not a square matrix and for an inverse to exist, the Matrix must be a square matrix and it's rank must be equal to its dimensions.\nIn the case of a rectangular matrix which has independent columns, we can compute its pseduo inverse by decomposing it using Singular Value Decomposition (SVD).\nThe Pseduo inverse of a matrix A is denoted by A+.\nx^ can then be computed using the below operation:\nx^ = A+ b.\nImplementing Least Squares the Linear Algebra way comes with its own advantages and disadvantages.\nAdvantages:\n\nIt is a non-iterative algorithm. Unlike gradient descent, this algorithm does not have to train the model in an interative manner to predict results.\nThe equation to compute the solution comes in a very compact form. Irrespective of the number of features, the equation remains the same as all of the information is packed into a Matrix.\n\nThen why use Gradient Descent?\nDisadvantages:\nThe closed solution form is preferred in every case in which it is not too computationally expensive to compute.\nIf the data points and the training feautures are large in number (sometimes in a machine learning algorithm we can end up with number of data points >1,000,000 and number of variables > 1000), the size of the Matrix becomes huge and it becomes computationally expensive to even initialize the matrix and on top of that a lot of expensive operations have to be performed on the matrix.\nIn such scenarios the gradient descent algorithm is computationally less expensive and saves a lot of time on calculations.\n""], 'url_profile': 'https://github.com/sathvikswaminathan', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/adityanair239', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Regression Week 2: Multiple Regression (gradient descent)\nIn the first notebook we explored multiple regression using Turi Create. Now we will use Turi Create along with numpy to solve for the regression weights with gradient descent.\nIn this notebook we will cover estimating multiple regression weights via gradient descent. You will:\n\nAdd a constant column of 1\'s to a Turi Create SFrame to account for the intercept\nConvert an SFrame into a Numpy array\nWrite a predict_output() function using Numpy\nWrite a numpy function to compute the derivative of the regression weights with respect to a single feature\nWrite gradient descent function to compute the regression weights given an initial weight vector, step size and tolerance.\nUse the gradient descent function to estimate regression weights for multiple features\n\nFire up Turi Create\nMake sure you have the latest version of Turi Create\nimport turicreate\nLoad in house sales data\nDataset is from house sales in King County, the region where the city of Seattle, WA is located.\nsales = turicreate.SFrame(\'home_data.sframe/\')\nIf we want to do any ""feature engineering"" like creating new features or adjusting existing ones we should do this directly using the SFrames as seen in the other Week 2 notebook. For this notebook, however, we will work with the existing features.\nConvert to Numpy Array\nAlthough SFrames offer a number of benefits to users (especially when using Big Data and built-in Turi Create functions) in order to understand the details of the implementation of algorithms it\'s important to work with a library that allows for direct (and optimized) matrix operations. Numpy is a Python solution to work with matrices (or any multi-dimensional ""array"").\nRecall that the predicted value given the weights and the features is just the dot product between the feature and weight vector. Similarly, if we put all of the features row-by-row in a matrix then the predicted value for all the observations can be computed by right multiplying the ""feature matrix"" by the ""weight vector"".\nFirst we need to take the SFrame of our data and convert it into a 2D numpy array (also called a matrix). To do this we use Turi Create\'s built in .to_dataframe() which converts the SFrame into a Pandas (another python library) dataframe. We can then use Panda\'s .as_matrix() to convert the dataframe into a numpy matrix.\nimport numpy as np # note this allows us to refer to numpy as np instead \nNow we will write a function that will accept an SFrame, a list of feature names (e.g. [\'sqft_living\', \'bedrooms\']) and an target feature e.g. (\'price\') and will return two things:\n\nA numpy matrix whose columns are the desired features plus a constant column (this is how we create an \'intercept\')\nA numpy array containing the values of the output\n\nWith this in mind, complete the following function (where there\'s an empty line you should write a line of code that does what the comment above indicates)\ndef get_numpy_data(data_sframe, features, output):\n    data_sframe[\'constant\'] = 1 # this is how you add a constant column to an SFrame\n    # add the column \'constant\' to the front of the features list so that we can extract it along with the others:\n    features = [\'constant\'] + features # this is how you combine two lists\n    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n    features_sframe = data_sframe[features];\n    # the following line will convert the features_SFrame into a numpy matrix:\n    feature_matrix = features_sframe.to_numpy()\n    # assign the column of data_sframe associated with the output to the SArray output_sarray\n    output_sarray = data_sframe[output];\n    # the following will convert the SArray into a numpy array by first converting it to a list\n    output_array = output_sarray.to_numpy()\n    return(feature_matrix, output_array)\nFor testing let\'s use the \'sqft_living\' feature and a constant as our features and price as our output:\n(example_features, example_output) = get_numpy_data(sales, [\'sqft_living\'], \'price\') # the [] around \'sqft_living\' makes it a list\nprint(example_features[0,:]) # this accesses the first row of the data the \':\' indicates \'all columns\'\nprint(example_output[0]) # and the corresponding output\n[1.00e+00 1.18e+03]\n221900.0\n\nPredicting output given regression weights\nSuppose we had the weights [1.0, 1.0] and the features [1.0, 1180.0] and we wanted to compute the predicted output 1.0*1.0 + 1.0*1180.0 = 1181.0 this is the dot product between these two arrays. If they\'re numpy arrays we can use np.dot() to compute this:\nmy_weights = np.array([1., 1.]) # the example weights\nmy_features = example_features[0,] # we\'ll use the first data point\npredicted_value = np.dot(my_features, my_weights)\nprint(predicted_value)\n1181.0\n\nnp.dot() also works when dealing with a matrix and a vector. Recall that the predictions from all the observations is just the RIGHT (as in weights on the right) dot product between the features matrix and the weights vector. With this in mind finish the following predict_output function to compute the predictions for an entire matrix of features given the matrix and the weights:\ndef predict_output(feature_matrix, weights):\n    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n    # create the predictions vector by using np.dot()\n    predictions = np.dot(feature_matrix, weights)\n    return(predictions)\nIf you want to test your code run the following cell:\ntest_predictions = predict_output(example_features, my_weights)\nprint(test_predictions[0]) # should be 1181.0\nprint(test_predictions[1]) # should be 2571.0\n1181.0\n2571.0\n\nComputing the Derivative\nWe are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\nSince the derivative of a sum is the sum of the derivatives we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n(w[0]*[CONSTANT] + w[1]*[feature_1] + ... + w[i] *[feature_i] + ... +  w[k]*[feature_k] - output)^2\nWhere we have k features and a constant. So the derivative with respect to weight w[i] by the chain rule is:\n2*(w[0]*[CONSTANT] + w[1]*[feature_1] + ... + w[i] *[feature_i] + ... +  w[k]*[feature_k] - output)* [feature_i]\nThe term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n2*error*[feature_i]\nThat is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\nRecall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors.\nWith this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).\ndef feature_derivative(errors, feature):\n    # Assume that errors and feature are both numpy arrays of the same length (number of data points)\n    # compute twice the dot product of these vectors as \'derivative\' and return the value\n    derivative = 2*np.dot(errors, feature)\n    return(derivative)\nTo test your feature derivartive run the following:\n(example_features, example_output) = get_numpy_data(sales, [\'sqft_living\'], \'price\') \nmy_weights = np.array([0., 0.]) # this makes all the predictions 0\ntest_predictions = predict_output(example_features, my_weights) \n# just like SFrames 2 numpy arrays can be elementwise subtracted with \'-\': \nerrors = test_predictions - example_output # prediction errors in this case is just the -example_output\nfeature = example_features[:,0] # let\'s compute the derivative with respect to \'constant\', the "":"" indicates ""all rows""\nderivative = feature_derivative(errors, feature)\nprint(derivative)\nprint(-np.sum(example_output)*2) # should be the same as derivative\n-23345850022.0\n-23345850022.0\n\nGradient Descent\nNow we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of increase and therefore the negative gradient is the direction of decrease and we\'re trying to minimize a cost function.\nThe amount by which we move in the negative gradient direction  is called the \'step size\'. We stop when we are \'sufficiently close\' to the optimum. We define this by requiring that the magnitude (length) of the gradient vector to be smaller than a fixed \'tolerance\'.\nWith this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent we update the weight for each feature befofe computing our stopping criteria\nfrom math import sqrt # recall that the magnitude/length of a vector [g[0], g[1], g[2]] is sqrt(g[0]^2 + g[1]^2 + g[2]^2)\ndef regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n    converged = False \n    weights = np.array(initial_weights) # make sure it\'s a numpy array\n    while not converged:\n        # compute the predictions based on feature_matrix and weights using your predict_output() function\n        predictions = predict_output(feature_matrix, weights)\n        # compute the errors as predictions - output\n        errors = predictions - output\n        gradient_sum_squares = 0 # initialize the gradient sum of squares\n        # while we haven\'t reached the tolerance yet, update each feature\'s weight\n        for i in range(len(weights)): # loop over each weight\n            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n            # compute the derivative for weight[i]:\n            derivative_i = feature_derivative(errors, feature_matrix[:,i])\n            # add the squared value of the derivative to the gradient sum of squares (for assessing convergence)\n            gradient_sum_squares += derivative_i**2\n            # subtract the step size times the derivative from the current weight\n            weights[i] -= step_size*derivative_i\n        # compute the square-root of the gradient sum of squares to get the gradient magnitude:\n        gradient_magnitude = sqrt(gradient_sum_squares)\n        if gradient_magnitude < tolerance:\n            converged = True\n    return(weights)\nA few things to note before we run the gradient descent. Since the gradient is a sum over all the data points and involves a product of an error and a feature the gradient itself will be very large since the features are large (squarefeet) and the output is large (prices). So while you might expect ""tolerance"" to be small, small is only relative to the size of the features.\nFor similar reasons the step size will be much smaller than you might expect but this is because the gradient has such large values.\nRunning the Gradient Descent as Simple Regression\nFirst let\'s split the data into training and test data.\ntrain_data,test_data = sales.random_split(.8,seed=0)\nAlthough the gradient descent is designed for multiple regression since the constant is now a feature we can use the gradient descent function to estimate the parameters in the simple regression on squarefeet. The folowing cell sets up the feature_matrix, output, initial weights and step size for the first model:\n# let\'s test out the gradient descent\nsimple_features = [\'sqft_living\']\nmy_output = \'price\'\n(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\ninitial_weights = np.array([-47000., 1.])\nstep_size = 7e-12\ntolerance = 2.5e7\nNext run your gradient descent with the above parameters.\nfinal_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)\nprint(final_weights)\n[-46999.88716555    281.91211912]\n\nHow do your weights compare to those achieved in week 1 (don\'t expect them to be exactly the same)?\nweek 1:\nIntercept: -47116.076574939885\nSlope: 281.9588385676974\nQuiz Question: What is the value of the weight for sqft_living -- the second element of ‘simple_weights’ (rounded to 1 decimal place)?\nUse your newly estimated weights and your predict_output() function to compute the predictions on all the TEST data (you will need to create a numpy array of the test feature_matrix and test output first:\n(test_simple_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)\nNow compute your predictions using test_simple_feature_matrix and your weights from above.\nmy_prediction = predict_output(test_simple_feature_matrix, final_weights)\nprint(my_prediction)\n[356134.44317093 784640.86422788 435069.83652353 ... 663418.65300782\n 604217.10799338 240550.4743332 ]\n\nQuiz Question: What is the predicted price for the 1st house in the TEST data set for model 1 (round to nearest dollar)?\nmy_prediction[0]\n356134.4431709297\n\nNow that you have the predictions on test data, compute the RSS on the test data set. Save this value for comparison later. Recall that RSS is the sum of the squared errors (difference between prediction and output).\nRSS = ((my_prediction - test_output)**2).sum()\nRSS\n275400047593155.94\n\nRunning a multiple regression\nNow we will use more than one actual feature. Use the following code to produce the weights for a second model with the following parameters:\nmodel_features = [\'sqft_living\', \'sqft_living15\'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \nmy_output = \'price\'\n(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\ninitial_weights = np.array([-100000., 1., 1.])\nstep_size = 4e-12\ntolerance = 1e9\nUse the above parameters to estimate the model weights. Record these values for your quiz.\nfinal_weights_multiple = regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance)\nprint(final_weights_multiple)\n[-9.99999688e+04  2.45072603e+02  6.52795277e+01]\n\nUse your newly estimated weights and the predict_output function to compute the predictions on the TEST data. Don\'t forget to create a numpy array for these features from the test set first!\n(test_multiple_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)\nmultiple_prediction = predict_output(test_multiple_feature_matrix, final_weights_multiple)\nmultiple_prediction\narray([366651.41203656, 762662.39786164, 386312.09499712, ...,\n       682087.39928241, 585579.27865729, 216559.20396617])\n\nQuiz Question: What is the predicted price for the 1st house in the TEST data set for model 2 (round to nearest dollar)?\nmultiple_prediction[0]\n366651.4120365591\n\nWhat is the actual price for the 1st house in the test data set?\ntest_data[0][\'price\']\n310000.0\n\nQuiz Question: Which estimate was closer to the true price for the 1st house on the TEST data set, model 1 or model 2?\nmodel 1 prediciton: 356134.4431709297\nNow use your predictions and the output to compute the RSS for model 2 on TEST data.\nRSS2 = ((multiple_prediction - test_output)**2).sum()\n**Quiz Question: Which model (1 or 2) has lowest RSS on all of the TEST data? **\nprint(\'model 1: \' + str(RSS))\nprint(\'model 2: \' + str(RSS2))\nmodel 1: 275400047593155.94\nmodel 2: 270263446465244.06\n\n'], 'url_profile': 'https://github.com/garabaya', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Deploy ML model with Flask to Heroku\nClick the button below to quickly clone and deploy into your own Heroku acount.\nIf you don\'t have one it\'ll prompt you to setup a free one.\n\nOnce deployed to your Heroku instance run the following:\ncurl -s -XPOST \'https://<name-of-your-heroku-app>.herokuapp.com/\' -d \'{""Pclass"":3,""Age"":2,""SibSp"":1,""Fare"":50}\' -H \'accept-content: application/json\'\nAlternatively a simple python script:\nimport requests\nimport json\nurl = \'https://<name-of-your-heroku-app>.herokuapp.com/\'\ndata = {""Pclass"":3, ""Age"":2, ""SibSp"":1, ""Fare"":50}\nresponse = requests.post(url, json.dumps(data))\nprint(response.json())\nArticle\nYou can read the full article that was published on Towards Data Science\n\nArticle: Towards Data Science: Create an API to Deploy Machine Learning Models using Flask\nAuthor: Elizabeth Ter Sahakyan\n\nAcknowlegements\nThank you to Josh Peak who added the button for heroku deployment\n'], 'url_profile': 'https://github.com/hanumanthsistla', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['linear_regression\nLinear Regression to predict player performance.\n'], 'url_profile': 'https://github.com/spartanovo', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dulal-CH', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Multi Variate Linear Regression\nI am attempting to learn how machine learning works from the ground up so decided to implement multi variate linear regression from scratch.\n'], 'url_profile': 'https://github.com/sxmarth', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ishikaahuja', 'info_list': ['Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'Updated Apr 21, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 24, 2020']}"
"{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Multi Variate Linear Regression\nI am attempting to learn how machine learning works from the ground up so decided to implement multi variate linear regression from scratch.\n'], 'url_profile': 'https://github.com/sxmarth', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Logistic-Regression-as-a-non-linear-classifier\nLogistic Regression as a non linear classifier\n'], 'url_profile': 'https://github.com/sumitjotrao', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jukamala', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '708 contributions\n        in the last year', 'description': ['airbnb-kaggle-comp\nRegression Practice using NYC AirBnB dataset\n'], 'url_profile': 'https://github.com/nadinezab', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'www.zerihunassociates.com ', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': [""Clinical Deterioration Prediction Model: Logistic Regression\nData\nThe final dataset used for the inferential statistics project includes unique ICU admission of 46,234 patients’ demographic (age), vital (blood pressure, heart rate, body temperature, and Glasgow Comma Scale), underlying conditions (HIV, metastatic cancer, and hematologic malignancy), admission type (scheduled surgical, medical, or unscheduled surgical), renal (urinary output, and Blood Urea Nitrogen), and others (serum bicarbonate level, sodium level, potassium level, and bilirubin level) data. This dataset is build based on the commonly used mortality prediction tool, Simplified Acute Physiology Score II (SAPSII).\nFirst, run logistics regression using saps2 (the sum of all features) as explantory variable and death at ICU (hdeath - hospital death) as target variable.\nHyperparameter Tuning\nThe model has some hyperparameters we can tune for hopefully better performance. In Logistic Regression, the most important parameter to tune is the regularization parameter C. Note that the regularization parameter is not always part of the logistic regression model. The regularization parameter is used to control for unlikely high regression coefficients, and in other cases can be used when data is sparse, as a method of feature selection. We may not need this for our model but worth checking.\nUsing the cv_score function (5-fold cross validation) for a basic logistic regression model without regularization,the score on the held-out data (test data) is 0.908, 91%.\nBased on the training set the best model parameter is 0.9079345258458951 for a C value of 0.01.\nRunning the model with C=0.01 gives as the same accuracy results on the test data as the deafult. This is not always the case hence important to experment with the hyperparameters that works best with new data.\nBest score on training data: 0.9080219037022492 using {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}. It gives a diffrent best value of C - this time 0.1. The GridSearchCV performs slightly better on test data (0.9036 vs 0.9044), almost the same.\nLet's first set some code up for classification that we will need for further discussion on the math. We first set up a function cv_optimize which takes a classifier clf, a grid of hyperparameters (such as a complexity parameter or regularization parameter) implemented as a dictionary parameters, a training set (as a samples x features array) Xtrain, and a set of labels ytrain. The code takes the traning set, splits it into n_folds parts, sets up n_folds folds, and carries out a cross-validation by splitting the training set into a training and validation section for each foldfor us. It prints the best value of the parameters, and retuens the best classifier to us.\nCross Validation Score\nwe should evaluate the performance of an algorithm rigorously by using resampling approaches (e.g. 100 times 5-fold cross-validation) to get some measurement of the variability in the performance of the algorithm. Maybe on a particular hold-out set, two algorithms have very similar performance but the variability of their estimates is massively different. That has serious implication on when we deploy our model in the future or use it to draw conclusion about future performance.\nPlotting an ROC curve - receiver operating characteristic\nWeighted Logistic Regression for Imbalanced Dataset\nXGBoost\nXGBoost Python api provides a method to assess the incremental performance by the incremental number of trees. It uses two arguments: “eval_set” — usually Train and Test sets — and the associated “eval_metric” to measure your error on these evaluation sets.\n""], 'url_profile': 'https://github.com/abebual', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/justincessna', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['_proj-Predict-Bike-Sharing-Patterns\nBasic Neural Network for Predicting Bike Sharing\nThis was an early project in the Udacity curriculum in which we constructed a simple fully connected neural net, c/w hand coded activation functions (sigmoid for non-linearity), and excluded the use of any deep learning frameworks.\nDatasources\nNote, there may be some differences from this dataset and the one included in the folders.\n\n\nKaggle: Bike Sharing Demand Dataset\nSub-headings within the main notebook\n\nProject Overview\nLoad and prepare the data\nChecking out the data\nDummy Variables\nScaling Target Variables\nSplitting the data into training, testing, and validation sets\nBuild the network\nUnit Tests\nTraining the Network\nChoose the number of iterations\nChoose the learning rate\nChoose the number of hidden nodes\nCheck out the predictions\nConclusion: Thinking about the results\n\n'], 'url_profile': 'https://github.com/murraymack', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Japan', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/moriitkys', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cclearnings', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['Dessigion-Tree-Modeal-Prediction\nMovie prediction using decision tree Regression Method\n'], 'url_profile': 'https://github.com/khanakbar145', 'info_list': ['Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}"
"{'location': 'National Institue of Technology, Hamirpur', 'stats_list': [], 'contributions': '376 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ParthPant', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['TitanicDatasetAnalysis\nTitanic Data set Analysis using Logistic Regression.\n'], 'url_profile': 'https://github.com/riddhisharma2000', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['Neural-Networks-and-Regression\nPython Code for Neural Networks and Regression\nWeek3 code is the best and most interesting\n'], 'url_profile': 'https://github.com/jaichaudhry323', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'Bangalore,India', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-\nMultiple Linear Regression with Boston Data set\nols regression\nvariable reduction ( pairwise co relation ,p value , t-value)\n'], 'url_profile': 'https://github.com/himanshumishra-ds', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'Rajkot , Gujarat', 'stats_list': [], 'contributions': '966 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nipun214', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['Logistic-Regression\nIntuition of Logistic Regression on custom data\n'], 'url_profile': 'https://github.com/prathamesh1499', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['Olutomilayo Amazing-Grace Logistics_Regression_from_Scratch\nImplementing standard logistic regression from scratch\nLogistic Regression is a generalized Linear Regression in which we do not output the weighted\nsum of inputs directly, but is passed through a function (sigmoid) that can map any real value between 0\nand 1.\nIn the Logistic regression class; the hypothesis (sigmoid, net_input, probability functions), the cost function, gradient descent, train (fit, predict funtions) and accuracy were written.\nThe created model was tested on marks.txt data\nThe numpy module was used for mathematical calculations\nThe matplotlib module was used for plotting generated data\nThe scipy module was used to compute the minimum value in relation to the gradient descent\n'], 'url_profile': 'https://github.com/AmazingGrace-D', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'Portugal', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Ethereum-Data-Analysis\nSimple data analysis and Linear Regression\n'], 'url_profile': 'https://github.com/marcoramosw', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'Nancy, FRANCE', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': ['Titanic classifier\nWorking on some EDA, features engineering, dataviz skills on the good old titanic dataset in the first part of the project.\nThen some hypothesis testing and trying out scikit learn pipelines.\nFinally puting it all together by creating a logistic regression classifier and evaluating it.\n'], 'url_profile': 'https://github.com/qangelot', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['GPLAR\n25/04/2020: Simple initialisation of inducing inputs and outputs\nInitialization of fixed inducing inputs and outputs, i.e. Z=[z,u_{1},...,u_{l-1}]. Firstly, randomly sample M(<<N) data points from training sets. The corresponding observations y_{l} will be inducing outputs which is concatenated to be inputs to next layer.\nThe mean of the variational distributions are intializaed to be the corresponding observations of the inducing points, the variances are intialized to identity matrix * very small numbers, i.e. start with near deterministic.\n29/04/2020: Handle missing data\nHave missing data in say the first / second output, but subsequent outputs are observed.\nThis can be implemented fairly simply by removing the reconstruction term in the ELBO E( log p(y_1|z_1 )) for the missing outputs.\nComparing with GPAR with impute, the square sum of error for GPLAR is lower. But GPLAR is also easy to be trapped into local minimum.\nProblems:\n\nGPLAR is not producing correct uncertainty, it still seems over-confident in second and thrid output\nGPLAR is not certain in first output, while it should. Because the current pseudo-points strategies, only close-downwards observations can be chosen as pseudo-points.\n\n12/05/2020:\n\nUse different numbers of inducing points per layer. It did reduce the unnecessary uncertainty in first layer.\nUse GPAR posterior predictive mean as initial value for q_mu. I tried normal long time series and longer time series, and also only with 50 inducing points. It is observed in all cases that even when GPAR posterior predictive mean fail, GPLAR can correct them.\nTry GPLAR using method in above 2, on real dataset, EEG and Exchange rate. Although the smse metrics are similar for the two models, we can see GPLAR has better calibrated uncertainty in EEG F1 and Exchange USD/AUD.\n\n22/05/2020:\n\n\nIncrease observation noise on synthetic data, and compare smse and log-density of GPAR and GPLAR. Line graphs are shown. Log-density of GPLAR is very steady for all three outputs. smse performance are similar.\n\n\nI try to make observations that are close-downwards or close-upwards. GPLAR still fails in ""close-upwards""  area missing observations, and the uncertainty estimates can be terrible. I suspect the reason is because, \'F4,F5,FZ,F1,F2\' (later five) outputs can perform perfectly only given input x(time), such that the q_mu and the q_sqrt of the first two outputs are not updated. Hence, I try to remove the kernel over inputs for those later outputs without missing data, i.e., \'F4, F5, FZ, F1, F2\' only has kernels over preceding outputs. and GPLAR seems to work. However, uncertainty in the subsequent outputs will be unnecessarily high.\nTwo method can make GPLAR work on close-upwards observations:\n\nLet some outputs only use kernels over preceding outputs, i.e. only learn through ""cross-channel""\nInitialize ""q_mu"" on missing areas from zeros.\n\n\n\n02/06/2020:\n\n\nGenerate synthetic data directly from probabilistic model. I have tried different kernels, such as non-linear over inputs + non-linear over outputs, or + linear over outputs. Both GPAR and GPLAR can fit to those data. GPLAR performs better when a). there is large noise in the observations, b). different noise level in different outputs.\n\n\nI have looked into weird behavior of GPAR over the synthetic third output last time in the log-density vs observation noise, such that log-density of the third output can be extremely high sometimes. I have made a mistake that I didn’t calculate the log-density over held-out datasets, but over training datasets and GPAR can sometimes overfit servely. Instead, I run the held-out log-likelihood vs noise using the synthetic data generated from first task. And this time log-likelihood is calculated over test dataset never observed during training. 100 trials of different noise seed are run and np.percentile are used to produce 2.5-97.5%bounds. Again, the third output held-out loglikelihoods for GPAR are weird when noise variance is close to zero. I checked why, and it turned out that GPAR’s predictive variance for third output are all near zero, (1e-7 level), making the log-likelihood extremely negative if the true value is slightly away from the predictive mean. But GPLAR’s predictive variance is at an appropriate level.\n\n\nTo make GPLAR work in close-upwards observations.\n\nI have tried a). completely remove temporal kernels, b). using additive kernels over outputs. Both methods cannot work.\nI also sanity check that when q_mu is initialized over missing areas using the true observations, “q_mu” would not run away from them during optimization.\nHence, I implemented the ""bi-directional"" version of GPLAR, such that a DGP run in reverse is added. And the model can produce good results as shown below, without losing its good performance on close-downwards observations. (I first initialized q_mu to be the corresponding output posterior mean of GPAR, but it trained rather slowly, and then I realized in normal DGP, all q_mu is initialized to zero, which will train much faster with similar time as when reverse DGP are not added. Waste some time on realizing this).\n\n\n\n'], 'url_profile': 'https://github.com/XiaRui1996', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jonmartz', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Regressions\nIn theis repository, a simple implimentatios of the following methods in Jupiter Notebook are presented:\n\nSimple Linear Regression\nMulti Linear Regression\nPolynomial Regression\nSupport Vector Regression\nDecision Tree Regression\nRandom Forest Regression\n\n'], 'url_profile': 'https://github.com/mehdikhosravian', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'GHAZIABAD, INDIA', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anjani-123', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Idaho', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShaneWeinstock', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'São Paulo', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Shiny APP - PAT Controle Peso vs Força Compressão\nPreencha os valores de força e peso de cada camada.\n\nRepositório - Repo\nShiny APP\n\nAuthor\n\nMarcelo Nobrega - Initial work - Marcelo\n\n'], 'url_profile': 'https://github.com/marcelomedre', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Taipei', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': [""house-price-prediction\nDescription\nThe dataset used on this predictive model is the result of the work of the the StatLib library about housing values in suburbs of Boston. This dataset can be accessed from the following website: Boston Housing Data\nList of Features\n1) CRIM :     per capita crime rate by town\n2) ZN :       proportion of residential land zoned for lots over 25,000 sq.ft.\n3) INDUS :    proportion of non-retail business acres per town\n4) CHAS :     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n5) NOX :      nitric oxides concentration (parts per 10 million)\n6) RM :       average number of rooms per dwelling\n7) AGE :      proportion of owner-occupied units built prior to 1940\n8) DIS :      weighted distances to five Boston employment centres\n9) RAD :      index of accessibility to radial highways\n10) TAX :     full-value property-tax rate per $10,000\n11) PTRATIO : pupil-teacher ratio by town\n12) B :       1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n13) LSTAT :   % lower status of the population\n\nDependent Variable\n14) MEDV :    Median value of owner-occupied homes in $1000's\n\nDependencies\nYou can use pip or conda to install the dependencies:\n\ntensorflow\nmatplotlib\njupyter\npandas\nseaborn\nscikit-learn\n\nUsage\nIf you want to try this program, download this repo and launch jupyter to run it on your machine.\n- - - TODO  - - -\n\n\nENVIRONMENT PREPARATION\n\nInstall library dependencies\nDocument installation and usage\n\n\n\nDATA EXPLORATION\n\nAdd dataset description\nPreview the structure of the dataset\nAdd data visualizations\n\n\n\nDATA PREPROCESSING\n\nApply standarization to feature data\nApply one-hot encoding to categorical data\nSplit data into training and testing sets\nOutput preprocessed data for faster preloading\n\n\n\nDATA ANALYSIS\n\nDefine network parameters\nDefine network structure\nAdd different network configurations\n\nDefine learning rate with different decaying methods\nSet up cost, optimizer, and accuracy function with different configurations\n\n\nDefine model execution\nVisualize evolution of MSE on training and testing datasets through epoch iteration\nVisualize evolution of loss function\nVisualize evolution of learning rate\nAdd log and summary writer\nAdd Tensorboard visualization\nAdd checkpoints for model restoration\n\n\n\nMODEL DEPLOYMENT\n\nLoad a pretrained model\nTest it with new data\n\n\n\nOTHERS\n\nUpdate README files\nUpdate all nbviewer links\nCreate to-do document\nAdd Tensorflow 1.x, Tensorflow 2.x, keras, tf.keras, and scikit-learn data analysis notebooks\n\n\n\n""], 'url_profile': 'https://github.com/igerardoh', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Kenya', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['RegressionWithSummaryOutput\nStatsModels for output and Sklearn for model score\n'], 'url_profile': 'https://github.com/Waweru007', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Kansas City, MO, USA', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['RegressionFromScratch\nC# code example implementing sample regression logic for informational purposes.\n'], 'url_profile': 'https://github.com/bruce-dunwiddie', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['regression-model\n'], 'url_profile': 'https://github.com/CLozy', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['polynomial-regression-example\nexample of a polynomial regression model ML\n'], 'url_profile': 'https://github.com/SnehaKedia', 'info_list': ['Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'R', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'C#', 'Apache-2.0 license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020']}"
"{'location': 'www.zerihunassociates.com ', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Clinical Deterioration Prediction Model - Selection of Ensemble Algorithms\n'], 'url_profile': 'https://github.com/abebual', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '326 contributions\n        in the last year', 'description': ['PREDICT BREAST CANCER  \nDeployed Notebook on Binder. Click Binder Logo.\nAnswer the following questions:\n\nWhat are the factors that predict malignant cancer? (i.e. which variables significantly predict malignancy, p < 0.05)\nCreate a classification report and confusion matrix of predicted and observed values. What is the accuracy,\nprecision, recall and F1-score of the model on the (a) training and (b) test data?\nPlot a Receiver Operating Characteristic (ROC) curve on the test data.\nWhat is overdispersion?\n\n'], 'url_profile': 'https://github.com/Shalom91', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Koshila1319', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['RegressionUsingSciKitLearn\nThis repository include machine learning models which are examples of how to use different estimator APIs of Scikit Learn in building Regression models\n'], 'url_profile': 'https://github.com/subhankar453', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Egypt', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['\nClassical and quantum regression analysis for the optoelectronic performance of NTCDA\nDataset Visualization\n\n\n\nThe optical absorbance of NTCDA thin film on a quartz substrate was measured at normal incidence of the light at room temperature in the spectral range of 190–2500 nm using a double beam spectrophotometer (JASCO model V-570 UV-VIS-NIR). The Photoelectrical properties – of the fabricated Au/NTCDA/p-Si/Al photodiode – were investigated by measuring the I-V characteristic curve at room temperature from +3.5 to -3.4, using Keithley electrometer model 6517B under the influence of UV light of wavelength 194 nm. The intensity of incident light was measured using Radiometer/Photometer model IL1400A.\n\nEnvironment Setup\nThese are the packages used in the code development\npip install pennylane==0.10.0 strawberryfields==0.14.0 pennylane-sf==0.9.0 tensorflow==2.2.0 tpot==0.11.5 xgboost==0.90 keras\n\nNotebooks\n\nANN\nKNN\nTPOT\nQNN\n\nAuthors\nAhmed M. El-Mahalawy, Kareem H. El-Safty\nHow to Cite\nIf you extend or use this work, please cite the paper where it was introduced:\n@article{el2020classical,\n  title={Classical and quantum regression analysis for the optoelectronic performance of NTCDA/p-Si UV photodiode},\n  author={El-Mahalawy, Ahmed M and El-Safty, Kareem H},\n  journal={arXiv preprint arXiv:2004.01257},\n  year={2020}\n}\n\nLicense\nThis research is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n'], 'url_profile': 'https://github.com/kareem1925', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['RatFun-Regression\nA spreadhseet with visual basic code for fast and accurate single-parameter symbolic regression with rational functions. Rational functions are ideal for approximating complex analytical formulae, and for regressions to empirical data such as thermodynamic properties. They have the following form:\ny = (a0 + a1·x + a2·x² +...+ an·xn) / (b0 + b1·x + b2·x² +...+ bm·xm)\nWhere either coefficient a0 or b0 is set to 1:\n\nFor Padé approximant: b0=1, and a0 can have any value (also zero). The simplest such equation is\ny = x.\nFor reciprocal Padé approximant: a0=1, and b0 can have any value (also zero). This regression is only applied in cases where all x and y values are non-zero. The simplest such equation is y = 1/x.\n\nHow to use the program\n\nSTEP 1: Enter a data set (x and y values) in columns A & B of sheet 1.Data.  If the y-values have different weights or uncertainties, enter these in column C.\nSTEP 2: Adjust the user-definable settings in sheet 2.Options.  These settings include the maximum exponent of x in the rational function, and the maximum number of polynomial coefficients permitted.\nSTEP 3: Click on the tab for sheet 3.Analyze, and wait for results to appear in sheet 4.Results:\n\nThe solver to generates all combinations of rational functions with up to the given number of coefficients.\nFor each function, the polynomial coefficients are fitted by SVD (Singular-Value Decomposition) that can omit outliers with a user-defined tolerance.\nAll fitted equations are tested for singularities in the denominator.\nAll fitted equations that pass the test are output in Horner\'s nested form, for fastest possible calculation.\nFor each equation, statistics are reported including R², X², mean and maximum absolute error, and Akaike Information Criterion.\n\n\nSTEP 4: Select your preferred equation by sorting the results in sheet 4.Results by a statistic of your choice in columns B-G. The best candidate equations are flagged in column Pareto.\n\nInstallation and activation\n\nSimply download the spreadsheet file and open it in Microsoft Excel. No installation or registration is needed.\nHowever, this is a Visual Basic macro-enabled spreadhseet. You must activate macros for it to function:\n\nWhen you open the file for the first time in Excel, you will see a yellow bar at the top of the window, with the message ""PROTECTED VIEW Be careful... [Enable Editing]"". Click on the \'Enable Editing\' button.\nNext, depending on the security settings on your installation of Microsoft Excel, a red bar may appear at the top of the window, with the message ""BLOCKED CONTENT Macros in this document have been disabled..."". This can be solved by moving the file to a directory on your PC that you designate for files that you trust, then open the file in Excel. To designate a \'trusted directory\', click on File > Options > Trust Center > Trust Center Settings > Trusted Locations > Add new location, then browse to a directory, e.g. C:\\TEMP. Finally check that Trust Center Settings > Trusted Documents > Disable Trusted Documents  is not ticked.\nWhen macros are properly activated, BEMS-hourly-values shows a small square splash-screen when you open the file. This splash screen shows the licence info, and advises you if an update is available for download from GitHub. Simply press \'Close\' button to close the splash-screen.\nIf still no splash-screen appears, then you might be able to fix it by menu option File > Options > Trust Center > Trust Center Settings > Macro Settings > Disable all macros with notification, which is a suitable level of security.\n\n\n\nLicense & warranty\n\nDistributed under the GLP v3 lisence (https://www.gnu.org/licenses/gpl-3.0.en.html). Please acknowledge/attribute use of this software in your report/publication with an appropriate author citation and URL to this site.\nProvided without warranty of any kind.\n\nAuthor & copyright owner\n© Peter.Schild@OsloMet.no\n'], 'url_profile': 'https://github.com/SchildCode', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Solapur', 'stats_list': [], 'contributions': '363 contributions\n        in the last year', 'description': ['Kaggle-s-House-Prices-Advanced-Regression-Tecniques\n'], 'url_profile': 'https://github.com/nileshchilka1', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['This RMarkdown will explore 3 topics in the analysis of discrete response\n\nConfidence intervals for the parameter of a binomial distribution\n\nFor small sample sizes and/or values of $\\pi$ close to 0 or 1, the normal distribution is a poor approximation for the binomial. Due to the finite outcome space of discrete distributions, a finite number of confidence intervals for any calculation procedure exists for any given data set of fixed size $n$. The true confidence level of these intervals are rarely achieved under these circumstances. We will explore empirical confidence intervals for the binomial distribution, and compare their claim confidence level to their true confidence level.\n\nInference with Logistic Regression\n\nLogistic regression is a commonly used classfier in machine learning. Here, rather than focusing on its predictive power as binary classifier, we will focus on inference and interpretibility of logistic regression models. We will demonstrate on three separate problems coefficient interpretation, various hypothesis testing procedures, confidence interval generation, and problem-solving.\n\nImplementation of MLE for Multinomial Regression\n\nFinally, we will implement maximum likelihood estimation (MLE) with multinomial regression. MLE is an important concept that is used for many parameter estimation procedures, and the theoretical outline presented, as well as MLE calculation code, can be generalized any generalized linear model regression problem where the model parameters are a function of a set of collected observables.\nData is provided by Chris Bilder, as part of his textbook: Analysis of Categorical Data with R\nhttp://www.chrisbilder.com/categorical/programs_and_data.html\n'], 'url_profile': 'https://github.com/siduojiang', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '571 contributions\n        in the last year', 'description': ['Machine-Learning-Projects\n'], 'url_profile': 'https://github.com/shahshubh', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Titanic_Survivor_Prediction\nThe Data analytics and (logistic regression and regression tree algorithm) has been applied on the popular titanic dataset\n'], 'url_profile': 'https://github.com/Prasiddhanarayan', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Java', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 26, 2020', '5', 'Jupyter Notebook', 'Updated Aug 8, 2020', '1', 'Updated Apr 30, 2020', '2', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 21, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020']}"
"{'location': 'Montréal', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Regressions\n'], 'url_profile': 'https://github.com/dpatry', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['tfLinearRegression\nA test of tensorflow.js using linear regression\n'], 'url_profile': 'https://github.com/ryanwdaly', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Microsoft-Company-Project\nClustering and linear regression for a project with Microsoft\n'], 'url_profile': 'https://github.com/AntoninKS', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Linear-Regression\nTemplates for Linear Regression using Scikit Learn and from Scratch\n'], 'url_profile': 'https://github.com/atusneem', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'Sao Carlos, SP Brazil', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['credit-card-approval-predictor-practice\nSimple one day credit card approval predictor using Logistic Regression\n'], 'url_profile': 'https://github.com/pingu1m', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Predicting-Treatment-costs-at-Mission-Hospital-using-Multiple-Regression\nPredicting Treatment costs at Mission Hospital using Multiple Regression\nAnalyzed 248 patient’s health data containing 41 features and identified 8 key factors that affect the treatment cost.\nBuilt a multiple regression model using Mission Hospital data for predicting the package price/treatment costs at the time of patient admission.\nData cannot be provided due to NDA.\n'], 'url_profile': 'https://github.com/VIgneshGV91', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '441 contributions\n        in the last year', 'description': ['Stats-Proj\nTechnology: Python, Google Colab\nLinear Regression perfomed on Facebook Posts from UCI database\n'], 'url_profile': 'https://github.com/Inh3ritance', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['What is Linear Regression?\nLinear regression is a common Statistical Data Analysis technique. It is used to determine the extent to which there is a linear\nrelationship between a dependent variable and one or more independent variables.\n1 The first Problem here is a simple linear regression using first principle\n'], 'url_profile': 'https://github.com/Ashitha97', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '103 contributions\n        in the last year', 'description': ['DSI CUEPID Competition\nRidge Regression Model for Predicting State Social Distancing Adherence Rates\nThis Ridge regression model aims to provide insights into what socioeconomic and clinical demographics contribute to statewide mobility prectices and social distancing adherence (SoDA). Because of the colinearity of socioeconomic data, ridge regression was used to counteract this colinearity and provide a more accurate model. The model then uses these socioecomic clinical factors to predict the SoDA scores of states. The purpose of this algorithm was to highlight possible disparities in social distancing and anticipate which states would be slower to come out of social distancing protocols despite federal guidance.\nGetting Started\nDownload the code, data, and dump folders onto your local machine.\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\nPrerequisites\nPython 3.3.6 and above\nR 1.1 and above\nPython Packages necessary:\nPython\n----\nNumpy\nPandas\nPathlib\nsklearn\nMatplotlib.pyplot\nR\n----\nggplot\n\nTO RUN\n\nrun python soc_dist_regress.py to the performance of the model under different factors\nTo see the prediction values and the coefficients use comp_pred_df(save=True) and get_coef_df(save=True), respectively\n\nDescription of Code and Data\nUS_Demographics\nLocation of tables for the us demographics data used as model features.\n\nmaster_soc_dist_dataset_trim.csv contains all of the data used in the most recent version of the ridge regression model\nAll other files contain tables used to make master_soc_dist and the links to where the data was extracted\n\nCode\nsoc_dist_regress.py: Executes the ridge regression model, generates dataframes for beta coefficients and model results\n\ncomp_pred_df Gives the prediction values of three variations of the model and the model score of each variation. The three variations include: all features used simultaneously in a multivariate ridge regression, only total COVID count per state in a single variable ridge regression, and the top 11 most impactful fatures determined by the absolute value of the coefficients found by get_coef_df.\nget_coef_df: Creates the dataframe of the beta coeffients by doing single variable ridge regressions for each variable in ps.m_dataset\nscatter_plot_features: Used to vizualize the features vs. SoDA score per state\n\nsoc_dist_presets.py: The logistics of the model and datasheets used in the model. Cleaning of the data was also done in this sheet. Data used includes:\n\nstates demographics\nthe number of cumulative COVID cases in a state using CDC data\nthe SoDA scores derived by taking absolute value the mean of the percent mobility change from baseline in public areas (parks, recreation areas, retail stores, workplaces, grocery stores, pharmacies, places of transit) averaged over March 16th 2020 to April 11th. The baseline for this data was mobility trends in January and February. Vist https://www.google.com/covid19/mobility/ for more information on the mobility trend data.\n\nsoc_dist_graphs.py: functions for visualizing the SoDA data per state, the mobility trends per state, and model performance results\nridge_regression.R: functions for visualizing the beta coefficients of the single ridge regressions of each variable\nBuilt With\nPython 3.3.6\nAuthors\n\nMyles Ingram\nAshley Zahabian\n\nAcknowledgments\n\nGoogle Global Mobility Data\nCensus.gov\nPolitico\nStatista\nCNN\nCDC\n\n'], 'url_profile': 'https://github.com/Ingrammyles8', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}","{'location': 'Seoul, South Korea', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Maxhin', 'info_list': ['R', 'Updated Jun 24, 2020', 'JavaScript', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['An Analysis of College Admissions Trends via Supervised Machine Learning Models\nBy Arnav Garg and Parth Saxena\nA full description of the problem, methods, and results can be found here.\nDependencies\nmatplotlib\npandas\nnumpy\n'], 'url_profile': 'https://github.com/arnavgarg', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Chicago IL', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': [""Plot\nPearson's Correlation Coeffeicent (r) is a measure of the linearity of a data set.\nLinear Regression is a technique for fitting a line to a data set, with the slope of the line being represented by β.\nWe can see in this animation how r and β relate, particularly them being equal when a dataset is standardized\n""], 'url_profile': 'https://github.com/trevorData', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '820 contributions\n        in the last year', 'description': ['A Flask App\nThis web app was built with Flask, I used the load_boston data in sklearn to predict house price in BOSTON. \nI deployed the model to the web app.\n'], 'url_profile': 'https://github.com/Babatunde13', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Customer-churn\nIt consists of predicting customer churn using LOGISTIC REGRESSION.\n'], 'url_profile': 'https://github.com/nikhilmshebannavar', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['circle-parameter-estimation\nrealization of circle parameter estimation based on support vector regression\nIntroduction\nSVR.m : the main function includes loading datas, constrcuting optimization objectives, constructing constraint condition and processing stage\n1 optimization objectives : minimize f(x) = C*H*X\n\n2 constraint condition : A*X <= b\n\n3 processing stage : calculating parameter iteratively based on Augmented Lagrangian Method\n\ndata\nThere are five group datas in ""data"" folder, every group contains many point coordinates, and these points\nare located in the edge of a cicle. The algorithm is used to estimate the circle parameter based on the points.\n'], 'url_profile': 'https://github.com/littlewhitesea', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Campinas', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Genetic Algorithms for Linear Regression\nThis mini-project is an exercise for training Genetic Algorithms.\nGiven a set of 2 points, it will train to find the line that best fit them.\nCurrently it selects the best and worst individuals.\nTo-do: create the next generation of individuals.\n'], 'url_profile': 'https://github.com/viniciusares', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Kaggle-Ames\nPlaying with regression and TensorFlow on Ames dataset\nKaggle comp\n'], 'url_profile': 'https://github.com/himberger', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Yogyakarata Indonesia', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zikr99', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Animesh-Bagchi', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['Topic: Climate Change Analysis\n'], 'url_profile': 'https://github.com/pranav0904', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 11, 2020', '1', 'Updated Apr 23, 2020', 'C++', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['LinearRegression_iris_test\nusing sklearn and its database to test Linear Regression\nIt pick the sepal length (cm) and petal length (cm)\tto test this regression\n'], 'url_profile': 'https://github.com/azsxdc881', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['King County is a county located in the U.S. state of Washington. The population was 2,149,970 in a 2016 census estimate. King is the most populous county in Washington, and the 13th-most populous in the United States. The county seat is Seattle, which is the state’s largest city. King County is one of three Washington counties that are included in the Seattle-Tacoma-Bellevue metropolitan statistical area. About two-thirds of King County’s population lives in the city’s suburbs. As of 2011, King County was the 86th highest-income county in the United States. This document addresses the factors concerning the “house sale prices” in King County sold between May 2014 and May 2015.\n'], 'url_profile': 'https://github.com/ruturajH', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['wine-evalulator-project\nProject 3\nEvaluating Wine\n'], 'url_profile': 'https://github.com/tveeder', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'Israel', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maorseg', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'Columbus, OH', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['R-shiny-test\n'], 'url_profile': 'https://github.com/TarikuJBeyene', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '224 contributions\n        in the last year', 'description': ['Regression-Wrapper-Function-in-Python-Sklearn\nUsing Scikit-Learn library in Python, I created a wrapper function that automates regression with a single line of code. It takes in two arguments: data and location where it will store results in csv and rds format. The function runs 17 different statistical and machine learning algorithms from linear regression to multilayer perceptron. The function returns the following two items in the given location:\n\nsummary table of evaluation metrics (RMSE, R2, etc) of every regression model on test set\nsave every model\n\n'], 'url_profile': 'https://github.com/sapiensvisionem', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['Linear Regression\n\nUses a least mean squares learning rule to implement gradient descent to fit a linear function to arbitrary user data\nA faster (but not interactive) example of linear regression is implemented in a different repository of mine where the regression is instantly computed instead of using an incremental gradient descent algorithm\n'], 'url_profile': 'https://github.com/SAXTEN2011', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Titanic logistic regression\nMy take on titanic dataset with simple logistic regression:\nhttps://www.kaggle.com/c/titanic\n'], 'url_profile': 'https://github.com/kucharykw', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sharangdev75', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Varad0612', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated May 5, 2020', 'R', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'JavaScript', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'C++', 'Updated May 6, 2020']}"
"{'location': 'Karlsruhe, Germany', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': ['AdvancedHousePrices-kaggle\nML approach to solve Advanced Housing Prices Regression\n'], 'url_profile': 'https://github.com/sanjit1995', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'Gießen, Germany', 'stats_list': [], 'contributions': '2,337 contributions\n        in the last year', 'description': ['HH-Modelica-ref\nThis repository stores reference simulation data for regression tests in HH-modelica.\nIt is a separate repository so that it can be used as shallow submodule to keep the main repository small in size.\n'], 'url_profile': 'https://github.com/CSchoel', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['CPI-Predictions\nA linear regression model to predict future Canadian CPI values.\nThis was done early in my data science self-education as a ""Hello World"" style introduction to predictive modelling.\n'], 'url_profile': 'https://github.com/modernfaust', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'Greater Boston', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Boston-Weather-Logistic-Regression-Model\nPredicting Boston weather using logistic regression model in R\n'], 'url_profile': 'https://github.com/MariamSerag', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'pune', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['Data-Preprocessing-and-Linear-Regression\nData Preprocessing and Linear Regression of the campus recruitment dataset\nCampus_Placement.ipynb is having all the python code of the Data Pre-Processing and Multiple Linear Regression.\nPlacement_data.csv is the dataset of the Campus placement in Banglore.\n'], 'url_profile': 'https://github.com/sanky2501', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Diabetes-Prediction\nDiabetes prediction using K-means clustering + Logistic Regression on PIMA Indians Diabetes dataset.\n'], 'url_profile': 'https://github.com/annjc', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['SVM-Movie_regression\nPrediction of movie Collection Using Support Vector Machine Regression Method\n'], 'url_profile': 'https://github.com/khanakbar145', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['House-Price-Predictions\nPrediciting Boston House Prices using multi-variable Linear Regression.\n'], 'url_profile': 'https://github.com/satindersingh10', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'London/Barcelona', 'stats_list': [], 'contributions': '774 contributions\n        in the last year', 'description': ['Using Logistic Regression to Predict Hospital Patient Readmittance\nThe high readmission level of hospital patients after they are discharged from the hospital is a significant concern for the US health care system. It is estimated that 20% of all hospitalized Medicare patients are readmitted within 30 days of hospitalization and 34% are readmitted within 90 days. The estimated cost of hospital readmissions is about $17.4 billion annually. To address the problem, the 2010 Affordable Care Act established a hospital readmissions reduction program (HRRP). The program created financial incentives for hospitals to reduce readmissions by linking Medicare reimbursements to a hospital’s risk-adjusted readmission rate. For 2012, penalties could be as much 1% of the total reimbursements a hospital received for the three target conditions. In the first year of the program, 2,225 hospitals were subject to reduced payment penalties, with penalties totaling $225 million nationwide. The maximum penalties were set to increase to 3% of reimbursements by 2014. This project studies the hospitals readmission rate and the costs associated with the Medicare penalties. The paper also considers a CareTracker program that aims to help paitents post care and reduce readmission rates in order to reduce Medicare penalties. A logistic regression model is applied to determine the best possible threshold, based on patient readmittence probabiloty, to apply when deciding to offer the CareTracker program or not.\n'], 'url_profile': 'https://github.com/JaumeClave', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Houses_Kc_Price_Prediction\nProblem Statement:\nHouse Data Price Prediction Using Multiple Regression and Backward Elimination Technique\nDescription:\nHere in this business problem, we want to find out the prices of Houses in King County,USA. Here we are given many parameters which directly influence the prices of the houses. We need to find the best quoted value for the price considering all the dependent parameters and we need to observe which parameters plays a key role in price determination of the houses.\nSince this is a problem of PREDICTION ANALYSIS we will use MULTIPLE REGRESSION ANALYSIS WITH BACKWARD ELIMINATION TECHNIQUE.\nWhat is Multiple Regression and Backward Elimination Technique:\nMULTIPLE REGRESSION: Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of an dependent variable based on the value of two or more other independent variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).\nBACKWARD ELIMINATION TECHNIQUE:Backward elimination is a feature selection technique while building a machine learning model. It is used to remove those features(independent variables) that do not have a significant effect on the dependent variable or prediction of output.\nSteps involved in backward Elimination Technique:\n\nSelect a significance level to stay in the model (eg. SL = 0.05)\nFit the model with all possible independent variables.\nConsider the independent variable with the highest P-value. If P>SL, go to point 4.).\nRemove this independent variable .\nFit the model without this variable and repeat the step 3.) until the condition becomes false.\n\nDATA\nThe kc_house_data excel file contains our complete dataset. This data is colleceted through surveys and by estimation of analysts.\nWe have total 21613 historical data prices for houses in USA.\nIndependent Variables/Features: All are numerical Data\n\nid\ndate\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\nDependent Variable - PRICE\nWe have test data whose size is one-third of our training dataset.\nThe dataset is uniform and consitent and the data have ratio scale of measurement.\nData Pre-Processing:\n\nLoad all the relevat python libraries.\nLoad the excel file kc_house_data.\nThere were some null values in tha dataset that have been removed , this is an important step so that regression model can be run successfully.\nAfter that we observe that id and date column does not play a key role in price determination , so we need to drop these two columns.\nNow our data is clean and ready to be processed under Regression Model in Python.\n\nLibraries Required :\n\nSklearn\nMatplotlib\npandas\nnumpy\nSeaborn\nCollections\n\nSteps :\n\nImporting relevant python Libraries\nExploring the Dataset\nExploratory Data Analysis\n\n\n\nUnivariate Analysis\n\n\n\nData Preprocessing\nModel Building\n\n\n\nRegression Model\nBackward Elimination Technique\n\n\n\nValidation\n\n\n*Regression Matrix (p test-Significance Test)\n\n\nConclusion\n\nEDA (Exploratory Data Analysis):\nFor EDA , Seaborn Data Visualization Library is used which is based on Matplotlib . Several distributions and plots have been depicted to understand the pattern of housing industry.\nWe have used scikit-learn library for running regression model.\nAnd Finally we have used OLS (Ordinary Least Square Method) for our regression Model. For this we have used Statsmodels Module, which is built on on top of numerical library Numpy. We get a Final Model after running a regression Model with an excellent R squared value of 0.905.\nOur Model is consistent with an excellent R squared value of 0.905. This clearly shows that most of the variation in the prices are very well expalined by these independent features. We have retained all those features whose p value < 0.05.\nModel Building:\n\nSplit dependent and independent features.\nSplit the data into train and test in 67 : 33 ratio.\nFit Multiple Regression and Backward Elimination Technique on training set, predict and check accuracy.\n\nInstall\n\nSupported Python version\n\nPython version used in this Project : 3.5+\n\n\n\nRun\n- To run this project use - Anaconda, which provides support for running .ipynb files (Jupyter Notebook).\n- After making sure you have that, you can run from a terminal or cmd next lines :\n- ipython notebook kc_housing_price.ipynb\n\nConclusion:\nOur Model is consistent with an excellent R squared value of 0.905. This clearly shows that most of the variation in the prices are very well expalined by these independent features. We have retained all those features whose p value < 0.05.\n'], 'url_profile': 'https://github.com/ADDYBOY', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Updated Aug 6, 2020', '1', 'Python', 'Updated Aug 13, 2020', 'Updated Apr 21, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Feb 16, 2021', 'HTML', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 28, 2020']}"
"{'location': 'London/Barcelona', 'stats_list': [], 'contributions': '774 contributions\n        in the last year', 'description': ['Using Logistic Regression to Predict Hospital Patient Readmittance\nThe high readmission level of hospital patients after they are discharged from the hospital is a significant concern for the US health care system. It is estimated that 20% of all hospitalized Medicare patients are readmitted within 30 days of hospitalization and 34% are readmitted within 90 days. The estimated cost of hospital readmissions is about $17.4 billion annually. To address the problem, the 2010 Affordable Care Act established a hospital readmissions reduction program (HRRP). The program created financial incentives for hospitals to reduce readmissions by linking Medicare reimbursements to a hospital’s risk-adjusted readmission rate. For 2012, penalties could be as much 1% of the total reimbursements a hospital received for the three target conditions. In the first year of the program, 2,225 hospitals were subject to reduced payment penalties, with penalties totaling $225 million nationwide. The maximum penalties were set to increase to 3% of reimbursements by 2014. This project studies the hospitals readmission rate and the costs associated with the Medicare penalties. The paper also considers a CareTracker program that aims to help paitents post care and reduce readmission rates in order to reduce Medicare penalties. A logistic regression model is applied to determine the best possible threshold, based on patient readmittence probabiloty, to apply when deciding to offer the CareTracker program or not.\n'], 'url_profile': 'https://github.com/JaumeClave', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jedrek369', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['AirQualityPrediction\nAir Quality Data set prediction using Linear Regression.\n'], 'url_profile': 'https://github.com/riddhisharma2000', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Vellore. Tamil Nadu', 'stats_list': [], 'contributions': '486 contributions\n        in the last year', 'description': ['Gradient-Descent-Algorithm-Implementation\nEstimating Multiple Linear Regression Parameters using GD Algorithm\n'], 'url_profile': 'https://github.com/Tekraj15', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Indianapolis', 'stats_list': [], 'contributions': '226 contributions\n        in the last year', 'description': ['MetisBaseball\nSimple regression analysis of Major League Baseball attendance, 1920-2019\n'], 'url_profile': 'https://github.com/PAGiesting', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NBvsLogRes\nNaive Bayesian and Logistic Regression performance comparison on classification.\n'], 'url_profile': 'https://github.com/emre-tasci', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['ML-Classification-Problem\nA Simple Two Class Classification Problem with Logistic Regression\nRead the training set from a json file -> run logistic regression with gradient descent -> plot the cost function versus numer of iterations to see if improvements are required -> plot the training set along with the boundary obtained -> predict classes based on the obtained parameters.\n'], 'url_profile': 'https://github.com/photonPrograms', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['House_Prices\nKaggle Educational Competition: House Prices: Advanced Regression Techniques\n'], 'url_profile': 'https://github.com/staschistyakov', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['BUAN 6341: Applied Machine Learning\nSGEMM-GPU-Kernel-Performance-Prediction\nImplementing Linear and Logistic regression using Gradient Descent algorithm\nA linear regression and logistic regression model implemented using the gradient descent algorithm with batch update (all training examples used at once) having squared error normalized by 2*number of samples [J(β0, β1) = (1/2m)[Σ(yᶺ(i) – y(i))2] as your cost and error measures, to predict the GPU run time.\nThe following tasks were carried out:\nPart 1: Download the dataset and partition it randomly into train and test set using a good train/test split percentage.\nPart 2: Design a linear regression model to model the average GPU run time. Include your regression model equation in the report.\nPart 3: Implement the gradient descent algorithm with batch update rule. Use the same cost function as in the class (sum of squared error). Report your initial parameter values.\nPart 4: Convert this problem into a binary classification problem. The target variable should have two categories. Implement logistic regression to carry out classification on this data set. Report accuracy/error metrics for train and test sets.\n'], 'url_profile': 'https://github.com/pradeepselvamp', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['first-ml-model\nLogistic regression model on the classic titanic data set.\nThis was built using Python and the pandas and sklearn packages on Jupyter notebooks\nThis version does not include visualisation of any sort.\n'], 'url_profile': 'https://github.com/Pythondroid', 'info_list': ['Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated May 25, 2020', 'HTML', 'Updated Apr 26, 2020', '1', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', '1', 'Jupyter Notebook', 'Updated Jun 2, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['what is Logistic Regression?\nLogistic Regression is used to solve the classification problems, so it’s called as Classification Algorithm that models the\nprobability of output class. It is a classification problem where your target element is categorical Unlike in Linear Regression,\nin Logistic regression the output required is represented in discrete values like binary 0 and 1. It estimates relationship\nbetween a dependent variable (target) and one or more independent variable (predictors) where dependent variable is categorical/nominal.\n'], 'url_profile': 'https://github.com/Ashitha97', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Prostate-Cancer-Survival-Analysis\nProstate Cancer Survival Analysis using Forward Step-wise Logistic Regression\nAnalyzed 8955 patient records and their 34 features and identified the characteristics of prostate cancer survival after 7 years. Built a forward stepwise logistic regression model to identify the prostate cancer patients and predict the odds and risk of prostate cancer survival.\nData cannot be disclosed due to NDA.\n'], 'url_profile': 'https://github.com/VIgneshGV91', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Mexico City', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/IamMultivac', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PeterHuang024', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['capstone1\nCapstone Project [House Price Prediction using Advanced Regression Techniques]\nBy:- Priyam Kumar\nAll the relevant documents pertaining to this capstone project could be found out here.\n'], 'url_profile': 'https://github.com/priyam81', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['PrivacyPreservingLogisticRegression\nA simulation project for PPSC-Gossip distributed logistic regression algorithm\n'], 'url_profile': 'https://github.com/lyyanjiu1jia1', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2,210 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gogbajbobo', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'pune', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['ML_Model_for_Halloween_Candy_Power_Ranking_Classification\nAuthor: Parameswara rao\nDate: 04 April 2020\nI build a simple model using candy-data.csv data set for predicting candy is chocolate or not based on its other features. This project is Organised as follows:\n(a) extract featues from data set.\n(b) data preprocessing and split into train and test data for a given model.\n(C) hyper parameters tuning\n(d) basic logistic regression classification model\n(e) summary of model output.\ncandy-data.csv: ncludes attributes for each candy along with its ranking. For binary variables, 1 means yes, 0 means no. The  data contains the following fields:\n\nchocolate: Does it contain chocolate?\nfruity: Is it fruit flavored?\ncaramel: Is there caramel in the candy?\npeanutalmondy: Does it contain peanuts, peanut butter or almonds?\nnougat: Does it contain nougat?\ncrispedricewafer: Does it contain crisped rice, wafers, or a cookie component?\nhard: Is it a hard candy?\nbar: Is it a candy bar?\npluribus: Is it one of many candies in a bag or box?\nsugarpercent: The percentile of sugar it falls under within the data set.\npricepercent: The unit price percentile compared to the rest of the set.\nwinpercent: The overall win percentage according to 269,000 matchups.\n\nThis dataset is Copyright (c) 2014 ESPN Internet Ventures and distributed under an MIT license. Check out the analysis and write-up here: The Ultimate Halloween Candy Power Ranking. Thanks to Walt Hickey for making the data available.\nResult:\nAccuracy on test data for a given model is 0.9615384615384616\n'], 'url_profile': 'https://github.com/parameswar-ds', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Animesh-Bagchi', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Succubae', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated May 18, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Oct 28, 2020', 'Python', 'Updated Jan 9, 2021', '1', 'Python', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 24, 2020']}"
"{'location': 'Alverca do Ribatejo', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Regression_polynomial_ann\nRegression_polynomial_ann\n'], 'url_profile': 'https://github.com/RFJC21', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['MultipleRegression\n'], 'url_profile': 'https://github.com/shrikantrepository', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'Istanbul, TURKEY', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tekinadem', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['fragon_regression\nRegression tests for Fragon.\n'], 'url_profile': 'https://github.com/huwjenkins', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Multilinear-regression-\n'], 'url_profile': 'https://github.com/gayatri1990', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Assemble7', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'Spain', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/josemanuelmiras', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'Vienna', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Znerual', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['Linear Regression with NumPy and Python\n'], 'url_profile': 'https://github.com/shubhansu31', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}","{'location': 'Madeira Is.', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': [""Comparative Analysis of Levenberg--Marquardt Algorithm and Particle Swarm Optimisation for Time Series and Regression Tasks\nAbstract:\nThis paper analyses how the Levenberg--Marquardt backpropagation algorithm (LMA) and the Particle Swarm Optimisation (PSO) can be used as a training algorithm of Artificial Neural Networks. In this view, four data sets (two of them for time-series analysis and the other two for fitting applications) were tested with different ANN architectures. The comparison between the two algorithms was made in terms of lower Mean Squared Error (MSE) value and in terms of the coefficient  of  correlation (R). Our results showed that the LMA worked better for time-series analysis and PSO for data fitting tasks.\nResults:\nThis study highlights two important concepts: time-series analysis and data fitting. It includes using two data sets: milk production and urban traffic behaviours in the São Paulo data set for time-series analysis and QSAR fish toxicity and wine data set for data fitting.\nThe experiments were conducted altering the number of neurons in the hidden layer. Two optimisation algorithms (Levenberg-Maquardt algorithm and Particle Swarm Optimisation) were used for both tasks. LMA worked better for time-series analysis and PSO for data fitting tasks. This could be due to the ability of PSO to handle high dimensionality of data than LMA.\nOn the one hand, the LMA algorithm was more stable than the PSO algorithm, especially in regression tasks; however, for the same accuracy, the PSO algorithm required a lower number of hidden units in the red and white wine data sets, as can be seen in the following table:\n\n\n\n\nLMA\n\n\nPSO\n\n\n\n\n\n\nData set\nNo. Hidden\nBest MSE\nR\nNo. Hidden\nBest MSE\nR\n\n\nTraffic\n4\n0.0584\n0.9870\n4\n101.7226\n0.2068\n\n\nMilk\n4\n1.2323\n0.9937\n12\n2152.8343\n0.8150\n\n\nFish\n4\n0.6901\n0.7446\n7\n0.7354\n0.7289\n\n\nRed wine\n15\n0.4813\n0.5534\n7\n0.4779\n0.5430\n\n\nWhite wine\n20\n0.4788\n0.4660\n7\n0.5020\n0.4176\n\n\n\nAs future work, we are going to test the PSO algorithm with different parameters, different velocity update equations and different topologies of swarms.\nBefore using the algorithms to find the optimal ANN's weights, feature selection methods were used for this study and were based on the variance of each feature and on its rank based on its p-values of F-test statistics.\nAlthough for other data sets, combination of the average MSE and R value have suggested neural networks in the way that both were pointing to the same network architecture, this may not always be the case. Finally, it can be concluded that selection of algorithm is dependent on the type of task, as seen in the study.\n""], 'url_profile': 'https://github.com/Dntfreitas', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 20, 2020', 'Fortran', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'MATLAB', 'Updated May 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['regression-kaggle\n'], 'url_profile': 'https://github.com/ayuzva', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '758 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/MphoKhotleng', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neilwrawlins', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['LINEAR-REGRESSION\nPROJECT ON HOUSE PRICING\n'], 'url_profile': 'https://github.com/kaushalsingh584', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '161 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic regression classifier model built using a single sigmoid activation funtion to recognize cat images. It is implemented with a neural network mindset.\n'], 'url_profile': 'https://github.com/jerin-ignatious', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/parth967', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nilarghya', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['RidgeRegression\n'], 'url_profile': 'https://github.com/alexlcy', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'Charlottesville, VA', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Linear Regression\nThis repo consists of relevant R codes for STAT 5120: Linear Regression. It includes simple/multiple regressions, statistical inferences, model selection techniques, as well as logistic regression.\n'], 'url_profile': 'https://github.com/yl4df', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/shrikantrepository', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'R', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jhl9617', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['linearRegression\nHarvard Data Science Course Projects/Assignments\n'], 'url_profile': 'https://github.com/raunakbardia', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'Cincinnati, OH', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Logistic-Regression-R-Census-Income\nData set source is UCI Machine Learning Repository. http://archive.ics.uci.edu/ml/datasets/Census+Income.\nThe model predicts whether the income is hgher than  50k or not for individuals given their age, race, native country, sex etc.\n'], 'url_profile': 'https://github.com/karayigit-beyza', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Linear Regression project\n'], 'url_profile': 'https://github.com/shubham171019', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/GusZandy', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Abhishek9076', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'Great Vancouver, Coquitlam', 'stats_list': [], 'contributions': '194 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/sepehrvafaei', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jindalnishant', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""Boston house price prediction\nI took the data set from kaggle boston house price prediction\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset\ni got better r square value for sgd regressor\ntrained model is the final file\ni saved predicted values for test set as predicted values\nhope that you will like\npls like and comment ur valuable suggestions\n""], 'url_profile': 'https://github.com/santhoshsans189', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/digitalSculpt', 'info_list': ['Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nilarghya', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'Cincinnati, OH', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Logistic-Regression-R-Census-Income\nData set source is UCI Machine Learning Repository. http://archive.ics.uci.edu/ml/datasets/Census+Income.\nThe model predicts whether the income is hgher than  50k or not for individuals given their age, race, native country, sex etc.\n'], 'url_profile': 'https://github.com/karayigit-beyza', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Linear Regression project\n'], 'url_profile': 'https://github.com/shubham171019', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/GusZandy', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Abhishek9076', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'Great Vancouver, Coquitlam', 'stats_list': [], 'contributions': '194 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/sepehrvafaei', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jindalnishant', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""Boston house price prediction\nI took the data set from kaggle boston house price prediction\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset\ni got better r square value for sgd regressor\ntrained model is the final file\ni saved predicted values for test set as predicted values\nhope that you will like\npls like and comment ur valuable suggestions\n""], 'url_profile': 'https://github.com/santhoshsans189', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/digitalSculpt', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AswanMordor', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', 'HTML', 'Updated Apr 26, 2020', 'Java', 'Updated Apr 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['CS 595 Project: Reddit Regression\n\nAuthors\n\nJustin Duhaime\nKaitlyn Leta\n\nFiles\n\nProject_Reddit.ipynb\n\nJupyter Notebook containing the ML algorithms and results\n\n\nsan.py\n\nPython script to sanitize the raw reddit data\n\n\n\nHow to run from repo\n\nDownload the Project_Reddit.ipynb and out0.json file to the same working directory\nOpen the Project_Reddit.ipynb file as a jupyter notebook\nRun the notebook\n\nHow to run from scratch\n\nGo to https://files.pushshift.io/reddit/submissions/ and download a repository of reddit submissions\nExtract the file to a working direcotry\nAlter the san.py file to open the extracted file (line 12)\nRun the san.py file\n\nThis will create multiple files in the current working directory, each with 100,000 reddit posts each\n\n\nOpen the Project_Reddit.ipynb file in a jupyter notebook\nRun the notebook\n\nYou can change the filename in the first code block to any of those that were created by the san.py script for further regression testing\n\n\n\n'], 'url_profile': 'https://github.com/kleta1', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Linear-regression-\nThis is a project form the predictive analysis course.\nThe Linear regression model is made and testing on Alteryx Designer x64\n'], 'url_profile': 'https://github.com/Shariq17', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['ml-regression\n'], 'url_profile': 'https://github.com/UfukAltanUA', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/syedbasit24', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/willpm99', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['Regression-Analysis using Sci-kit learn\nRegression Analysis using Boston Housing Prices dataset.\nRegression Used:\n\nLinear Regression\nRidge Regression\nLasso Regression\n\n'], 'url_profile': 'https://github.com/HassanShahzad7', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['polynomial_regression\n'], 'url_profile': 'https://github.com/vaish28', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['linear_regression\n'], 'url_profile': 'https://github.com/warmice99', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chethanrao', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""Regression Case Study\nContents\n\nGoals\nData\nGithub Workflow\nRegression\nConclusions\n\nThe goal of the contest is to predict the sale price of a particular piece of heavy equipment at auction based on its usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.\n\nGoals\n\nIn this project we were given a team and equiptment data and we were asked to use different types of regression to predict the most accurate price while minimizing Root Mean Squared Log Error.\nWe worked with Linear Regression, Logisitic Regression, and even attempted Regularized Regression to minimize our RMSLE.\nThis README will go over some of the challenges we faced, along with our Regression results.\n\nData\n\nThe data for this case study are in ./data. There you will find both training and testing data sets. We trained on the training set and evaluated our final model only on the testing dataset. For model selection and model comparison, we used Cross Fold Validation to evaluate each model and to avoid overfitting.\nWe read in the data using pandas pd.read_csv('data/Train.zip').\nThis training data had 401,000 rows and 53 features. 27 of the 53 features contained over 300,000 null values.\nThe biggest challenges we had was feature selection. The sale date of some of the equiptment was dated all the way back to 1919, which would scew results drascially. Besides feature selection we chose to impute missing values for features that had only a few null values, we used different metrics depending on the feature like the mode or mean.\nThis dataset required a lot of preprocessing and data cleaning.\n\nGithub Workflow\n\nOur Workflow is as follows:\nAs the team lead on this Regression Project, I started our master repo and my teammates cloned my repository on Github and created their separate branches.\nWe all began EDA and feature exploration to come up with the best way to deal with our dataset.\nI monitored each team members progress by utilizing git fetch <your feature branch> and through this we were able to maintain a steady pace throughout our project.\nEverytime we completed an atomic piece of work: git add -p git commit -m git push <your feature branch>\nAfter our changes to our separate branches were made, I merged all of our production code from each branch into the master branch.\n\nRegression\n\nThe evaluation of our model will be based on Root Mean Squared Log Error.\nWhich is computed as follows:\n\nwhere pi are the predicted values and ai are the\ntarget values.\nThis loss function is sensitive to the ratio of predicted values to\nthe actual values, a prediction of 200 for an actual value of 100 contributes\napproximately the same amount to the loss as a prediction of 2000 for an actual\nvalue of 1000.\nWhen learning a predictive model, we would like you to use only regression\nmethods for this case study.  The following techniques are legal\nImportant Notes\n\nThis data is messy. We initially overestimated our time limits to clean the entire dataset and defaulted to cleaning and utilizing only features that we knew would be predictive in out model.\nBecause of the restriction to linear models, we were forced to consider how to transform continuous predictors in our model.\nIt's possible some columns in the test data will take on values not seen in\nthe training data. Plan accordingly.\nUse your intuition to think about where the strongest signal about a price\nis likely to come from. If you weren't fitting a model, but were asked to use\nthis data to predict a price what would you do? Can you combine the model with\nyour intuitive instincts?  This is important because it can be done without\nlooking at the data; thinking about the problem has no risk of overfitting.\nWe remembered that we were evaluated on a loss function that is only sensitive to\nthe ratios of predicted to actual values, so we used different model scoring to observe this ratio.\n\n""], 'url_profile': 'https://github.com/morganabbitt', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/syedbasit24', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/willpm99', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['Regression-Analysis using Sci-kit learn\nRegression Analysis using Boston Housing Prices dataset.\nRegression Used:\n\nLinear Regression\nRidge Regression\nLasso Regression\n\n'], 'url_profile': 'https://github.com/HassanShahzad7', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['polynomial_regression\n'], 'url_profile': 'https://github.com/vaish28', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'Nalagarh', 'stats_list': [], 'contributions': '288 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dikshitkaushal', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/praveen6993', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Logistic-Regression-\n'], 'url_profile': 'https://github.com/gayatri1990', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'USTC', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zht1999', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['LinearRegression\nThis a linear regression model using linear regression algorithm. I just build this small model from basic knowledge of Machine learning. this web app predict the chances of student admission in the university.\n'], 'url_profile': 'https://github.com/Niranjan-10', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mritunjay11196', 'info_list': ['Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 23, 2020', 'JavaScript', 'Updated Dec 4, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nskk-1662', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swami1894', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '275 contributions\n        in the last year', 'description': ['Linear_Regression-\n'], 'url_profile': 'https://github.com/sakshijainn', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'Kerala, IN', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kelvin-jose', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['linear_regression from scratch, no sklearn\n'], 'url_profile': 'https://github.com/Abelmarumo', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'Lucknow, Uttar Pradesh', 'stats_list': [], 'contributions': '220 contributions\n        in the last year', 'description': ['                                                    Linear-regression\n\nLinear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range, (e.g. housing prices) rather than trying to classify them into categories (e.g. breast cancer malignant or benign).\nIn Linear regression if we have only one feature and we have to predict the output(Only Simple regression).\nWe use hypothesis function to predict the value\n                h(x) = theta(0) + theta(1)(x)\n\nTo minimize the error between predicted value and the actual value we use cost function also called as squared error function\n                J(theta(0), theta(1)) = ((1/2*m) * (h(x) - y)^2)\n\nTo minimize the cost function gradient descent came into practice where we have to simultaneously update the theta value.\n                repeat until convergence\n                {\n                theta = theta - alpha*(partial derivative(J(theta)))\n                }\n\nhere alpha is the learning rate which needs to be very small.\nIf alpha is too large it may overshoot the minimum and it can fail to converge or diverge.\nSome Important Points related to Gradient Descent\n\nIf alpha is too small, then gradient descent may take  very long time to converge.\nIf theta(0) and theta(1) are not initialized at local minima then one iteration will not change their value.\nIf the first few iterations of gradient descent cause cost function to increase rather than decerease then the moost likely cause is that we have to set the learning rate to too large.\n\n'], 'url_profile': 'https://github.com/palak2610', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'Gurgaon', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['LogisticRegression\nCode for Logistic Regression - (Project - Banking, Customer will subscribe or not)\n'], 'url_profile': 'https://github.com/ashishsangwan18', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jciba', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/igor-barsukov', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}","{'location': 'Mexico City', 'stats_list': [], 'contributions': '183 contributions\n        in the last year', 'description': [""Data Scientist Associate - Delivered by Dell EMC\nThe tech stack covered in the course/certificate:\n\nWhat you'll find in this repo is (R in Jupyter Notebook):\n\nR Basics\nDescriptive and Inferential Statistics\nMachine Learning Algorithms (K-Means, Linear Regression, Logistic Regression, Naive Bayes, Text Analysis, etc)\n\nThe rest of the content of this course/certificate was (you can find the PDF in this repo with the full content)\n\nHadoop: HDFS, Yarn, Map Reduce, Pig, Hive\nAdvanced SQL (Transact-SQL, )\n\n""], 'url_profile': 'https://github.com/mattssll', 'info_list': ['Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Linear-Regression-from-scratch\nLinear_Regression Practice code for to find the relationship between the Head Size and Brain weights\nWhat is Linear Regression?\nLinear Regression is a method used to define a relationship between a dependent variable (Y) and independent variable (X)\nwhich is written as y = mx + c\nWhere y is the dependent variable, m is the scale factor or coefficient(slope of the line), c being the bias coefficient(y-intercept) and X being the independent variable\nCredit - https://towardsdatascience.com/linear-regression-from-scratch-cd0dee067f72?gi=cc5d9a4a3af5\n'], 'url_profile': 'https://github.com/sourabhsugandhi', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jimetony', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Pune, Maharashtra', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anushka1096', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Colombia', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['LogisticRegression Breast Cancer\nEjercicio Realizado en el Track de MachineLearning Curso Supervised Learning con DataCamp\n\nen este caso se trata de un Algoritmo que usa Regresion Logistica SKLearn para predecir si un paciente tiene cancer de mama o no.\n\n'], 'url_profile': 'https://github.com/MrAnZa', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YigitDeniz', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/heinzalex', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rethabilethulo', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/const-ntino', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '487 contributions\n        in the last year', 'description': ['Yelp-Regression-Project\n'], 'url_profile': 'https://github.com/Mohit-Gaur', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Logistic-Regression-from-scratch\nThis is the legendary Titanic dataset– the best, first challenge for you to dive into ML competitions and familiarize yourself\nuse machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”\ntest.csv` dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes.\nUsing the patterns we’ll be trying to predict a classification- survival or deceased.\nTitanic dataset available @ https://www.kaggle.com/c/titanic\nRequirements\nNumpy - Array manipulations and computations\nPandas - Creating data frames and exploring Dataset\nMatplotlib and Seaborn - Visualizing dataset and creating different insightful plots\nScikit-learn - Importing Regression Model and different evaluation metrices.\nCredit - https://medium.com/@anishsingh20/logistic-regression-in-python-423c8d32838b\n'], 'url_profile': 'https://github.com/sourabhsugandhi', 'info_list': ['Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Python', 'Updated Apr 28, 2020', '1', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LiuJun523', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Logistic-Regression-using-R\n'], 'url_profile': 'https://github.com/jainil24', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['multiple-linear-regression-project\n'], 'url_profile': 'https://github.com/jsagawa10589', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Esbo, Finland', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Romuruotsalainen', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alex938', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Poojau22', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '449 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/williamtnguyen', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/mphipps2-ML', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '898 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/juanektbb', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""Predicting the Sale Price of Bulldozers using Machine Learning\nIn this notebook, we're going to go through an example machine learning project with the goal of predicting the sale price of bulldozers.\n1. Problem defition\n\nHow well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar bulldozers have been sold for?\n\n2. Data\nThe data is downloaded from the Kaggle Bluebook for Bulldozers competition: https://www.kaggle.com/c/bluebook-for-bulldozers/data\nThere are 3 main datasets:\n\nTrain.csv is the training set, which contains data through the end of 2011.\nValid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\nTest.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n\n3. Evaluation\nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\nFor more on the evaluation of this project check: https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation\nNote: The goal for most regression evaluation metrics is to minimize the error. For example, our goal for this project will be to build a machine learning model which minimises RMSLE.\n4. Features\nKaggle provides a data dictionary detailing all of the features of the dataset. You can view this data dictionary on Google Sheets: https://docs.google.com/spreadsheets/d/18ly-bLR8sbDJLITkWG7ozKm8l3RyieQ2Fpgix-beSYI/edit?usp=sharing\n""], 'url_profile': 'https://github.com/hari047', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 24, 2020', 'R', 'Updated Apr 24, 2020', 'Python', 'Updated May 7, 2020', 'Python', 'Updated Nov 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}"
"{'location': 'Dallas', 'stats_list': [], 'contributions': '725 contributions\n        in the last year', 'description': ['This is a R based Multiple Linear Regression Project\nPredict the PSA level for a patient\nConsider the prostate cancer dataset prostate cancer.csv. It consists of\ndata on 97 men with advanced prostate cancer. A description of the variables is given in Figure 1.\nWe would like to understand how PSA level is related to the other predictors in the dataset.\nBuild a ""reasonably good"" linear model for these data by taking PSA level as the response variable.\nUse the final model to predict the PSA level for a patient whose quantitative predictors are at the sample\nmeans of the variables and qualitative predictors are at the most frequent category.\nvesinv: qualitative variable(aka, categorical)\ngleason: quantitative variable(aka, numeric)\n\nSelect the data by p-value\nadd dummy variable for qualitative variable\nFind the suitable transformation for data\nCheck the QQplot for transformation.\nEstimate the PSA\n\n\n'], 'url_profile': 'https://github.com/dryadd44651', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/q890003', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['ft_linear_regression\nIt is the simple iOS application for training linear regression with gradient descent optimization algorithm. You can change parameters to see how do they affect learning.\nTested with:\n\nmacOS Catalina - 10.15.5 Beta\nXcode - 11.4.1\niPhone 8\niOS - 13.3.1\n\nGet started:\ngit clone https://github.com/Gleonett/ft_linear_regression.git\ncd ft_linear_regression\ngit submodule update --init --recursive\n\n\nOpen ft_linear_regression/ft_linear_regression.xcodeproj\nDrag the Charts/Charts.xcodeproj to the project\nGo to your target\'s settings, hit the ""+"" under the ""Embedded Binaries"" section, and select the Charts.framework\nBuild and run\n\nExplanation\nMain view\n\nMain view contains visualization for model, dataset and buttons:\n\nParameters - Link to Parameters view\nReset - Reset already trained linear regression model parameters\nTrain - Train model\nPredict - Link to Prediction view\n\nParameters view\n\nLearning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function\nlearningRate = initinalLearningRate / (decay * currentEpoch + 1)\n\nModel will train either until the end of epochs or until:\nabs(InterceptN - InterceptN-1) < AccuracyThreshold\nand\nabs(BiasN - BiasN-1) < AccuracyThreshold\n\nPrediction view\n\nregressor is the value we want to predict the dependent\n'], 'url_profile': 'https://github.com/Gleonett', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['ML_SupervisedLearning_Regression\n'], 'url_profile': 'https://github.com/PaolaArz', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hsagnep', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'Hyderabad, Telangana', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Ridge-Lasso-Regression\nUsing boston dataset available in sklearn.datasets library\n'], 'url_profile': 'https://github.com/syedkhaja', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/linhl019', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['linear-regression-for-beginers\n#==============================================================================================\nSuppose you are the CEO of a restaurant franchise and are considering different cities for opening a new\noutlet. The chain already has trucks in various cities and you have data for\nprofits and populations from the cities.\n#==============================================================================================\nfunctions we will need:::\n#1- Plotting the Data:\nfunction plotData(x, y)\n%PLOTDATA Plots the data points x and y into a new figure\n%   PLOTDATA(x,y) plots the data points and gives the figure axes labels of\n%   population and profit.\n% ====================== information ======================\n% Instructions: Plot the training data into a figure using the\n%               ""figure"" and ""plot"" commands. Set the axes labels using\n%               the ""xlabel"" and ""ylabel"" commands. Assume the\n%               population and revenue data have been passed in\n%               as the x and y arguments of this function.\n%\n% Hint: You can use the \'rx\' option with plot to have the markers\n%       appear as red crosses. Furthermore, you can make the\n%       markers larger by using plot(..., \'rx\', \'MarkerSize\', 10);\nfigure; % open a new figure window\nplot(x, y, \'rx\', \'MarkerSize\', 10); % Plot the data\nylabel(\'Profit in $10,000s\'); % Set the y\U00100000axis label\nxlabel(\'Population of City in 10,000s\');\nend\n#======================================================================================================\n#2-Computing the cost J(theta):\nfunction J = computeCost(X, y, theta)\n% ====================== information ======================\n%COMPUTECOST Compute cost for linear regression\n%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the\n%   parameter for linear regression to fit the data points in X and y\n% Initialize some useful values\n% You need to return the following variables correctly\n% Instructions: Compute the cost of a particular choice of theta\n%               You should set J to the cost.\nm = length(y); % number of training examples\nJ = 0;\nh=Xtheta;\nsqrError=(h-y).^2;\nJ=1/(2m)*sum(sqrError);\nend\n#======================================================================================================\n3-Gradient descent:\nfunction [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)\n%GRADIENTDESCENT Performs gradient descent to learn theta\n%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by\n%   taking num_iters gradient steps with learning rate alpha\n% Initialize some useful values\nm = length(y); % number of training examples\nJ_history = zeros(num_iters, 1);\nfor iter = 1:num_iters\nx=X(:,2);\nh=theta(1)+(theta(2)x);\n##theta(1)=theta(1)-alpha(1/m)sum(h-y);\n##theta(2)=theta(2)-alpha(1/m)*sum((h-y).x); wrong\ntheta_zero=theta(1)-alpha(1/m)sum(h-y);\ntheta_one=theta(2)-alpha(1/m)*sum((h-y).*x);\ntheta=[theta_zero;theta_one];\n% Save the cost J in every iteration\nJ_history(iter) = computeCost(X, y, theta);\nend\nend\n#=======================================================================================================================\nthe main program:\n%% Machine Learning Online Class - Exercise 1: Linear Regression\n%  Instructions\n%  ------------\n%\n%  This file contains code that helps you get started on the\n%  linear exercise. You will need to complete the following functions\n%  in this exericse:\n%\n%     warmUpExercise.m\n%     plotData.m\n%     gradientDescent.m\n%     computeCost.m\n%     gradientDescentMulti.m\n%     computeCostMulti.m\n%     featureNormalize.m\n%     normalEqn.m\n%\n%  For this exercise, you will not need to change any code in this file,\n%  or any other files other than those mentioned above.\n%\n% x refers to the population size in 10,000s\n% y refers to the profit in $10,000s\n%\n%% Initialization\nclear ; close all; clc\n%% ==================== Part 1: Basic Function ====================\n% Complete warmUpExercise.m\nfprintf(\'Running warmUpExercise ... \\n\');\nfprintf(\'5x5 Identity Matrix: \\n\');\nwarmUpExercise()\nfprintf(\'Program paused. Press enter to continue.\\n\');\npause;\n%% ======================= Part 2: Plotting =======================\nfprintf(\'Plotting Data ...\\n\')\ndata = load(\'ex1data1.txt\');\nX = data(:, 1); y = data(:, 2);\nm = length(y); % number of training examples\n% Plot Data\n% Note: You have to complete the code in plotData.m\nplotData(X, y);\nfprintf(\'Program paused. Press enter to continue.\\n\');\npause;\n%% =================== Part 3: Cost and Gradient descent ===================\nX = [ones(m, 1), data(:,1)]; % Add a column of ones to x\ntheta = zeros(2, 1); % initialize fitting parameters\n% Some gradient descent settings\niterations = 1500;\nalpha = 0.01;\nfprintf(\'\\nTesting the cost function ...\\n\')\n% compute and display initial cost\nJ = computeCost(X, y, theta);\nfprintf(\'With theta = [0 ; 0]\\nCost computed = %f\\n\', J);\nfprintf(\'Expected cost value (approx) 32.07\\n\');\n% further testing of the cost function\nJ = computeCost(X, y, [-1 ; 2]);\nfprintf(\'\\nWith theta = [-1 ; 2]\\nCost computed = %f\\n\', J);\nfprintf(\'Expected cost value (approx) 54.24\\n\');\nfprintf(\'Program paused. Press enter to continue.\\n\');\npause;\nfprintf(\'\\nRunning Gradient Descent ...\\n\')\n% run gradient descent\ntheta = gradientDescent(X, y, theta, alpha, iterations);\n% print theta to screen\nfprintf(\'Theta found by gradient descent:\\n\');\nfprintf(\'%f\\n\', theta);\nfprintf(\'Expected theta values (approx)\\n\');\nfprintf(\' -3.6303\\n  1.1664\\n\\n\');\n% Plot the linear fit\nhold on; % keep previous plot visible\nplot(X(:,2), X*theta, \'-\')\nlegend(\'Training data\', \'Linear regression\')\nhold off % don\'t overlay any more plots on this figure\n% Predict values for population sizes of 35,000 and 70,000\npredict1 = [1, 3.5] theta;\nfprintf(\'For population = 35,000, we predict a profit of %f\\n\',...\npredict110000);\npredict2 = [1, 7] * theta;\nfprintf(\'For population = 70,000, we predict a profit of %f\\n\',...\npredict2*10000);\nfprintf(\'Program paused. Press enter to continue.\\n\');\npause;\n%% ============= Part 4: Visualizing J(theta_0, theta_1) =============\nfprintf(\'Visualizing J(theta_0, theta_1) ...\\n\')\n% Grid over which we will calculate J\ntheta0_vals = linspace(-10, 10, 100);\ntheta1_vals = linspace(-1, 4, 100);\n% initialize J_vals to a matrix of 0\'s\nJ_vals = zeros(length(theta0_vals), length(theta1_vals));\n% Fill out J_vals\nfor i = 1:length(theta0_vals)\nfor j = 1:length(theta1_vals)\nt = [theta0_vals(i); theta1_vals(j)];\nJ_vals(i,j) = computeCost(X, y, t);\nend\nend\n% Because of the way meshgrids work in the surf command, we need to\n% transpose J_vals before calling surf, or else the axes will be flipped\nJ_vals = J_vals\';\n% Surface plot\nfigure;\nsurf(theta0_vals, theta1_vals, J_vals)\nxlabel(\'\\theta_0\'); ylabel(\'\\theta_1\');\n% Contour plot\nfigure;\n% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\ncontour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))\nxlabel(\'\\theta_0\'); ylabel(\'\\theta_1\');\nhold on;\nplot(theta(1), theta(2), \'rx\', \'MarkerSize\', 10, \'LineWidth\', 2);\n#======================================================================================\ntraininng data\n6.1101,17.592\n5.5277,9.1302\n8.5186,13.662\n7.0032,11.854\n5.8598,6.8233\n8.3829,11.886\n7.4764,4.3483\n8.5781,12\n6.4862,6.5987\n5.0546,3.8166\n5.7107,3.2522\n14.164,15.505\n5.734,3.1551\n8.4084,7.2258\n5.6407,0.71618\n5.3794,3.5129\n6.3654,5.3048\n5.1301,0.56077\n6.4296,3.6518\n7.0708,5.3893\n6.1891,3.1386\n20.27,21.767\n5.4901,4.263\n6.3261,5.1875\n5.5649,3.0825\n18.945,22.638\n12.828,13.501\n10.957,7.0467\n13.176,14.692\n22.203,24.147\n5.2524,-1.22\n6.5894,5.9966\n9.2482,12.134\n5.8918,1.8495\n8.2111,6.5426\n7.9334,4.5623\n8.0959,4.1164\n5.6063,3.3928\n12.836,10.117\n6.3534,5.4974\n5.4069,0.55657\n6.8825,3.9115\n11.708,5.3854\n5.7737,2.4406\n7.8247,6.7318\n7.0931,1.0463\n5.0702,5.1337\n5.8014,1.844\n11.7,8.0043\n5.5416,1.0179\n7.5402,6.7504\n5.3077,1.8396\n7.4239,4.2885\n7.6031,4.9981\n6.3328,1.4233\n6.3589,-1.4211\n6.2742,2.4756\n5.6397,4.6042\n9.3102,3.9624\n9.4536,5.4141\n8.8254,5.1694\n5.1793,-0.74279\n21.279,17.929\n14.908,12.054\n18.959,17.054\n7.2182,4.8852\n8.2951,5.7442\n10.236,7.7754\n5.4994,1.0173\n20.341,20.992\n10.136,6.6799\n7.3345,4.0259\n6.0062,1.2784\n7.2259,3.3411\n5.0269,-2.6807\n6.5479,0.29678\n7.5386,3.8845\n5.0365,5.7014\n10.274,6.7526\n5.1077,2.0576\n5.7292,0.47953\n5.1884,0.20421\n6.3557,0.67861\n9.7687,7.5435\n6.5159,5.3436\n8.5172,4.2415\n9.1802,6.7981\n6.002,0.92695\n5.5204,0.152\n5.0594,2.8214\n5.7077,1.8451\n7.6366,4.2959\n5.8707,7.2029\n5.3054,1.9869\n8.2934,0.14454\n13.394,9.0551\n5.4369,0.61705\n'], 'url_profile': 'https://github.com/HadeerArafa', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Logistic_Regression_Classifier\nThis repo contains a jupyter notebook that builds the Logistic Rgeression classifier in Python. It uses the student.csv dataset which is categorical in nature.\n'], 'url_profile': 'https://github.com/Sanj98', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bahramhashmi', 'info_list': ['Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', '1', 'Swift', 'MIT license', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 24, 2020']}"
"{'location': 'indonesia', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ghalyrizqi', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mauricio-Physics', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gjain307', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'tokyo, japan', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['Support-Vector-Regression\nanalyzing the salary of a job hunter using machine learning model.\n'], 'url_profile': 'https://github.com/Mayaz9156', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Austin', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear-Regression-Ecommerce-Company\nThe data belongs to an Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions.\nThe company is trying to decide whether to focus their efforts on their mobile app experience or their website.\nI will work with the Ecommerce Customers csv file. It has Customer info, such-as Email, Address, and their color Avatar. Then it also has numerical value columns:\n\nAvg. Session Length: Average session of in-store style advice sessions.\nTime on App: Average time spent on App in minutes\nTime on Website: Average time spent on Website in minutes\nLength of Membership: How many years the customer has been a member.\n\n'], 'url_profile': 'https://github.com/AmoghKatwe', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'San Jose, CA', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Wilmer-Tejada', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Russas', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ltiufc', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['SimpleLinearRegression\n'], 'url_profile': 'https://github.com/shrikantrepository', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Nalagarh', 'stats_list': [], 'contributions': '288 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dikshitkaushal', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Colombia', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['LinearRegression BostonHousing\nEjercicio Realizado en el Track de MachineLearning Curso Supervised Learning con DataCamp\n\nen este caso se trata de un Algoritmo que usa Regresion lineal SKLearn para predecir el precio de una casa en boston de acuerdo al numero de habitaciones de la misma\n\n'], 'url_profile': 'https://github.com/MrAnZa', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'HTML', 'Updated Apr 22, 2020', 'R', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020']}"
"{'location': 'Querétaro, Mex.', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['RNR_Linear_Regression\nThis is a basic code from compute the math equation who help me calculate with linear regression prediction model the reach expected based on amount spent for Rock n Ribs Facebook campaigns.\n'], 'url_profile': 'https://github.com/axelrocha-py', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YigitDeniz', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'UK', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Polynomial Regression Calculator (Swift)\nA tool written in Swift to find an approximating polynomial of a known degree for a given data.\nThis is useful for non-linear data sets, which tend to follow a curve.  The charts below show why Polynomial regression is better suited to non-linear data sets.\nThis tool was created with reference to the Polynomial Regression page at RosettaCode.org and converted to Swift\nhttps://rosettacode.org/wiki/Polynomial_regression\nUsage\nYou just need the PolynomicalRegressionCalculator.swift struct, although a simple Xcode project has been provided as a demonstration.\nX and Y values must be provided in Arrays of Doubles, with corresponding values at the same index.\nAt least 3 values must be provided in order to make calculations.\nlet xValues: [Double] = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nlet yValues: [Double] = [1, 20, 25, 48, 65, 70, 95, 170, 210, 290, 340]\nlet prCalc = PolynomialRegressionCalculator(xValues: xValues, yValues: yValues)\n\nlet queryXValue = 110.0\n\nif let result = prCalc.predictYValue(at: queryXValue) {\n    print(""For X value \\(queryXValue) the predicated Y value is: \\(result)"")\n} else {\n    print(""Unable to provide result."")\n}\n\n// result = 417.61212121212174\n\n\n\n\n'], 'url_profile': 'https://github.com/robbaldwin', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Ukraine', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Oleksandr-Soloshenko', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnunayGupta', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['ScreenerRegressionCode\nScreenerRegressionCode\n'], 'url_profile': 'https://github.com/Nageshstuffs', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'Amsterdam, Netherlands', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': [""bayesian-quantile-regression\nIn this repo I would like to learn more about (deep) Bayesian quantile regression. In order to do this. The learning is mainly done in the notebooks. Where possible, the knowledge gathered will be put together in proper code. My interest was sparked by Yu Ri Tan's blog on prediction uncertainty.\nExplanation about notebooks\nThe notebooks will be able to be read standalone, but there will be a progression in knowledge.\nNotebook 1 - Quantile Regression\nThe first notebook is completely dedicated to quantile regression. It assumes a bit of knowledge about linear regression as well as a bit of (convex) optmization. The quantile regression problem is rewritten into a convex optimization problem which allows for optimal solving of quantile regression. The derivation followed in the notebook is based in large part on an answer given on Stats Stackexchange. This is subsequently worked out in a scikit-learn type class. The second part makes a quick example of how quantile regression can be used in a homoscedastic setting (just like linear regression) as well as in a heteroscedastic setting. If multiple quantiles are estimated, the heteroscadestic properties of the data can be taken into account.\nNotebook 2 - Linear Bayesian Regression with PyMC3\nWork in progres...\nThe notebook will be based on the blog series of twiecki.io\n""], 'url_profile': 'https://github.com/GianmarcoDisario', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nadaAlqahtani', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Logistic-Regression-2\n'], 'url_profile': 'https://github.com/GusZandy', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Logistic-Regression-3\n'], 'url_profile': 'https://github.com/GusZandy', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Swift', 'MIT license', 'Updated Apr 23, 2020', 'R', 'Updated Apr 21, 2020', 'MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated May 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnunayGupta', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['ScreenerRegressionCode\nScreenerRegressionCode\n'], 'url_profile': 'https://github.com/Nageshstuffs', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'Austin', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear-Regression-Ecommerce-Company\nThe data belongs to an Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions.\nThe company is trying to decide whether to focus their efforts on their mobile app experience or their website.\nI will work with the Ecommerce Customers csv file. It has Customer info, such-as Email, Address, and their color Avatar. Then it also has numerical value columns:\n\nAvg. Session Length: Average session of in-store style advice sessions.\nTime on App: Average time spent on App in minutes\nTime on Website: Average time spent on Website in minutes\nLength of Membership: How many years the customer has been a member.\n\n'], 'url_profile': 'https://github.com/AmoghKatwe', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'San Jose, CA', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Wilmer-Tejada', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'Russas', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ltiufc', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['SimpleLinearRegression\n'], 'url_profile': 'https://github.com/shrikantrepository', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'Nalagarh', 'stats_list': [], 'contributions': '288 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dikshitkaushal', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'Colombia', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['LinearRegression BostonHousing\nEjercicio Realizado en el Track de MachineLearning Curso Supervised Learning con DataCamp\n\nen este caso se trata de un Algoritmo que usa Regresion lineal SKLearn para predecir el precio de una casa en boston de acuerdo al numero de habitaciones de la misma\n\n'], 'url_profile': 'https://github.com/MrAnZa', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'NC', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alx1056', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['R-Linear-Regression-Modeling\nUsing Real Estate Data containing various factors such as square footage, location, number of bathrooms etc. a linear Regression Model is formed to predict Total Price.\n'], 'url_profile': 'https://github.com/KobbeH', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 20, 2020', 'HTML', 'Updated May 1, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 25, 2020', 'Python', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 22, 2020', 'HTML', 'Updated Apr 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['House_price_regression\ntask : To predict prices of house given 79 explanatory variables describing (almost) every aspect of residential homes\nAchieved RMSE of 5.45\n'], 'url_profile': 'https://github.com/Anjali2720', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Erlangen, Germany', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mahadev1995', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Jaipur,Rajasthan', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['House_price_prediction-Regression-\n'], 'url_profile': 'https://github.com/shishir7654', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Regression-with-multivariable\n'], 'url_profile': 'https://github.com/Bassantraafat98', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Пример расчета прогноза потребления электроэнергии с помощью алгоритмов машинного обучения из БД Oracle\nАлгоритмы регрессии широко используются для решения множества задач, в частности для прогнозирования объема продаж, определения значимости клиентов или LTV (life-time-value), прогноза выхода оборудования из строя и многих других.\nВ данном воркшопе рассматривается алгоритм GLM (generalized linear model) из встроенной в БД Oracle библиотеки алгоритмов машинного обучения для расчета прогноза потребления электроэнергии. Для оценки качества модели используется среднеквадратичное отклонение, которое автоматически рассчитывается в БД Oracle. Также рассматриваются возможности библиотеки DBMS_CLOUD для импорта данных из файлов с разделителями.\nВоркшоп реализован в рамках облачного сервиса Oracle Autonomous DataWarehouse с использованием встроенной платформы Apache Zeppelin. Для выполнения воркшопа достаточно бесплатного аккаунта Oracle Always Free.\n'], 'url_profile': 'https://github.com/OlegS1978', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karthikchandu', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['ASSG1_LINEAR_REGRESSION\n'], 'url_profile': 'https://github.com/KiNgSLayer-10', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/muddassiir', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RezaGhari', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/desaizeeshan22', 'info_list': ['Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Python', 'Updated Apr 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['Regression-Classification-Clustring\nContain implementation of various machine learning algorithms\nUsage : run flask api\n$python server.py\nopen another terminal to send request to api\n$python request.py\n'], 'url_profile': 'https://github.com/avitomar12', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/mphipps2-ML', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['abalone-regression-study\nDataset\nThe abalone dataset used in this project was downloaded from http://mlr.cs.umass.edu/ml/datasets/Abalone > Data Folder > abalone.data on April 17, 2020. It can be found locally as path-to-repository/abalone_data.csv (e.g. Desktop/abalone-regression-study/abalone_data.csv). path-to-repository/abalone_data.csv contains the same, unaltered data as when downloaded.\nDependencies\nTo locally run this study, you will need the following installed.\n\nPython 2.7\nnumpy library\npandas library\nscikit-learn library\nmath library\n\nHow to Run\n\nDownload this repository to your preferred directory (e.g. Desktop)\nOpen path-to-repository/abalone_regression.py (e.g. Desktop/abalone-regression-study/abalone_regression.py)\nChange regType = ""lr to desired algorithm (Choices: lr for Linear Regression, krr for Kernel Ridge Regression, knr for k-Neighbors Regression, and nn for a Neural Network)\nGo to your preferred directory on your command line (e.g. cd ~/Desktop/abalone-regression-study)\nRun python abalone_regression.py to generate CSVs and print scores and statistics\nCSVs generated by your run will appear as path-to-repository/krr_i.csv, path-to-repository/knr_i.csv, and/or path-to-repository/nn_i.csv, where ""i"" corresponds to the nested cross-validation outer-loop fold number. Note: Linear Regression will only print a single score and will not generate CSVs.\n\nResults\nThe analysis is located in path-to-repository/report.pdf (e.g. Desktop/abalone-regression-study/report.pdf).\nCSVs generated from my run are located in path-to-repository/results/raw/ (e.g. Desktop/abalone-regression-study/results/raw/). Tables found in the analysis were trimmed from abridged copies of the CSVs located in path-to-repository/results/report_analysis/ (e.g. Desktop/abalone-regression-study/results/report_analysis/). Screenshots of printed scores and statistics are also located in path-to-repository/results/report_analysis/.\n'], 'url_profile': 'https://github.com/rpatelpj', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'Lyon', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/agiordan101', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Regression-with-one-variable-\n'], 'url_profile': 'https://github.com/Bassantraafat98', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/orgoca', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'London, U.K. ', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Simple Linear Regression (C++)\nThis is a C++ class for calculating a simple linear regression model and producing the results as an object. In the code file there is also a main function to demonstrate an example of usage.\nUsage\n\nData should be given in the format of arrays of float, along with sample size, for example: ""slr regression (samplesize, datax, datay);"" where ""slr"" is the name of the class, ""regression"" is the name of the object we want to create with the results, ""samplesize"" is an int, ""datax"" and ""datay"" are arrays of float.\nResults are provided as public variables within the regression object, as follows:\n\n.sample = the sample size\n.xbar = the mean of the x values\n.ybar = the mean of the y values\n.r = the Pearson Correlation between x and y (r)\n.rsquared = the r-squared for the model\n.alpha = the alpha coefficient\n.beta = the beta coefficient\n.SEBeta = the standard error for beta\n.t = the t-value for beta\n.residualSE = the standard error for the residuals\n.residualmean = the average distance between observed values of y and predicted values of y (will always be positive)\n.residualmax = the largest residual difference\n.residualmin = the smallest residual difference\n.SSR = the sum of squares for the residuals\n.SSE = the sum of squares explained by the model\n.SST = the total sum of squares\n.residuals[] = an array of float (equal in size to the sample) containing individual residual values\n\n\n\nAbout\nThis class was constructed by George Maier from the London School of Economics. Twitter: @GeorgeMaier\n'], 'url_profile': 'https://github.com/georgemaier', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'México ', 'stats_list': [], 'contributions': '1,174 contributions\n        in the last year', 'description': [""Danielle's Delicious Delicacies, yelp_regression_project\ncodeacademy project\nhttps://nbviewer.jupyter.org/github/256bitsmx/yelp_regression_project/blob/master/yelp_regression.ipynb\n""], 'url_profile': 'https://github.com/dsm0109x', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['R code for the functional continuum regression\nInstruction\n\nTested at RStudio.\nCreate two folders named after Rimage and figure.\nThe folder data includes real data involved in the paper.\nRun R scripts main.realdata.R and main.simulate.R for real and simulated data analysis, respectively.\nR script functions.R contains all the supporting functions.\nR image-files and figures yielded go into Rimage and figure, respectively.\n\nReference\nZhou, Z. (2019). Functional continuum regression. Journal of Multivariate Analysis, 173, 328-346.\ndoi:10.1016/j.jmva.2019.03.006\n'], 'url_profile': 'https://github.com/ZhiyangGeeZhou', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Validation_-_Simple_Linear_Regression\nIntroduction:\nIn this notebook I will predict employee salaries from different employee characteristics or features.I am going to use a simple supervised learning technique: linear regression.To build a simple model to determine how well Years Worked predicts an employee’s salary\nContents :\nQuestion 1 :\n•Importing And Exploring Data Analysis.\n•Examining missing values\n•Visualising and interpretring distributions\n•Splitting data into training and testing\n•Examining extreme values\nQuestion 2 :\n•Running A Simple Linear Regression to the Training\n•Finding the report the amount of variance explained (R^2) and significance value (p)\n•Percentage of the variance\nQuestion 3 :\n•The unstandardized coefficient (B or ‘coef’ in statsmodels)\nQuestion 4 :\n•The 95% confidence intervals [0.025, 0.975]\nQuestion 5 :\n•The expected salary for someone with 12 years’ work experience\nQuestion 6 :\n•The expected salary for someone with years’ work experience\nQuestion 7 :\n•Other employee characteristics might influence salary\nComparing model when running it on the test set\n•Root Mean Square Error\nConclusion :\nI can conclude that the model is overfitted.The test root mean square error is more than the training root mean square error.This means that the model is not fitted for testing dataset.To avoid overfitting I should draw a random sample instead of modelling with a contant  that is large enough to handle all of the terms that I expect to include in my model.\n'], 'url_profile': 'https://github.com/rethabilethulo', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Jul 27, 2020', 'Python', 'Updated Nov 22, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 28, 2020', 'HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'C++', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020']}"
"{'location': 'Bengaluru', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Advanced-Multiple-Regression-House-Price-Prediction\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price.\nYou are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/akhilsn', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Sylhet, Bangladesh', 'stats_list': [], 'contributions': '848 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hasan-moni-321', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '350 contributions\n        in the last year', 'description': ['Bulldozer-Prizing-LinearRegression\nBuilding a linear - regression machine-learning model to predict the sales-price of a bulldozer.\nContent\n\nTools used\nData used\nPreview graphical insights\nWork with the project\n\nTools\nThe tools used in this project are:\n\nnumpy\npandas\nmatplotlib\nscikit-learn\nJupyter\nSeaborn\n\nData\nThe data used in this project comes from kaggle.\nThe data was part of a kaggle - competition which aim was to find a suitable model to predict the prices of bulldozers.\nThe data is split into three different sets:\nTrain.csv\n\nis the training set, which contains data through the end of 2011.\n\nValid.csv\n\nis the validation set, which contains data from January 1, 2012 - April 30, 2012. You make predictions on this set.\n\nTest.csv\n\nis the test set. It contains data from May 1, 2012 - November 2012.\n\nPreview: Graphical insights from the dataset\nAt this point I share some of the insights the EDA revealed. For further graphical insights please\nlook into the jupyter notebook or into the plots folder.\nPreviews\n\nThis histogram plot shows shows the distribution of the Sale - Year variable.\n\n\n\nThis histogram plot shows the distribution of the Manufacturing - Year variable.\n\n\nWork with the project\nThe environment.yml contains the\nconda environment(the tools) being used in this project.\nThere are several ways to build the environment in order to work with the notebook in this project.\n\nFrom scratch:\n\n\nconda create --prefix ./env pandas numpy matplotlib scikit-learn jupyter\n\n\nFrom environment.yml file:\n\n\nconda env create --prefix -f ./env -f Path/to/yaml/file\n\nEach of those commands will create the environment within an env-folder relative to your current directory.\n\nWhen you have successfully created your environment folder you can list all your environments with:\n\nconda env list\n\nYou then have to activate the environment using:\n\nconda activate path/to/your/environment/folder\nIf successfull, the base - prompt should change to your environment-path.\n\nThen you can run jupyter notebook from within the anaconda prompt which should open a jupyter dashboard in your default-browser.\n\nFrom that you can open the jupyter notebook and work with it.\n'], 'url_profile': 'https://github.com/Ritsch1', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Ukraine', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/darktova', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'IIT BHU, India', 'stats_list': [], 'contributions': '609 contributions\n        in the last year', 'description': ['simple-multivariate-logistic-regression\nThis repository contains the code and datasets to be used for a simple logistic regression which categorizes data under 4 different labels.\nIn this we have over 7-8 variables in the datasets which determine the overall outcome into 4 different categories namely 0, 1, 2 and 3. We can be using any such labels depending on the requirements.\nFor this project, we have used numpy and scitik learn libraries and imported their functions for making our tasks easier.\n'], 'url_profile': 'https://github.com/shubham1710', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bin0110001', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'San Francisco ↔️ Prague ', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': [""apprentice-chef-regression-analysis\nBuilding a regression-based predictive model and providing insights based on Apprentice Chef's data on their customers and orders. The story of the analysis was told through exploratory data analysis, feature treatment and engineering, and utilizing appropriate modeling techniques. Multiple machine learning algorithms such as linear regression, logistic regression, and kNN were applied to come up with the best model.\n""], 'url_profile': 'https://github.com/thanhtamluu', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Aba', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/charlespreshy', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '571 contributions\n        in the last year', 'description': ['ml-linearRegression\n'], 'url_profile': 'https://github.com/Beasty786', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}","{'location': 'Medellín, Colombia ', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['LogisticRegression-test\nRegresion Logistica para detectar fallos en la configuracion lanzada por un optimizador multi-objetivo\npara los climatizadores del teatro real de España\nV1\nLos datos de entrenamiento estan clasificados por temperatura y las bombas en funcionamiento\n'], 'url_profile': 'https://github.com/damesaa201710054010', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Python', 'Updated May 2, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 23, 2020']}"
"{'location': 'Pune, Maharashtra', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anushka1096', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['LogisticRegression_Octave\n'], 'url_profile': 'https://github.com/NazarKobuk', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['multiple-regression-with-python\n'], 'url_profile': 'https://github.com/Gichere', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/concussion20', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'Arlington, VA', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['The Ames House Price Predection Analysis with Python\nMunkhjargal Tsogtsaikhan | Email: tsmj2013@gmail.com | linkedIn: https://www.linkedin.com/in/munkhjargal1/\nThis project is a part of the springboard capstone two project.\nIntro\nHouse price is a core influence to buy a dream house for a home buyer. Proper and justified house prices can bring in a lot of transparency and trust into the real estate industry in long term. If real estate companies have accurate measures of house prices, they can increase their profits through saving time, effort and advertising and maintaining expenses, and increasing sale revenues. Therefore, they should to predict house prices appropriately and offer a proper price for buyers and make a right decision to buy houses at the right moment.\nThis analysis’s goal is to help real estate companies in Ames make the best investment decisions through building the best house price prediction model using regression techniques for the Ames House Price dataset.\nDataset:\n\nThis dataset describes the sale of individual residential property in Ames, Iowa.\nThree Excel files contains 2919 observations and 80 explanatory variables (43 categorical and 37 numerical) involved in assessing home values.\nData sources: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n\n'], 'url_profile': 'https://github.com/Munkh-Ts', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PoojaReddy05', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': [""kc_house\nmodel Linear Regression to predict kc_house\nOverview\nDependencies\nUsage\nInstall jupyter here.\nCredits\nCredits for this code go to RUdrakshTuwani, Siraj Raval's YouTube\n""], 'url_profile': 'https://github.com/Maneenoot', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'Assam', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Logistic-Regression-Using-Python\nLogistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.\nLogistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.\n'], 'url_profile': 'https://github.com/rajansahu713', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'Paris - France', 'stats_list': [], 'contributions': '370 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SeyfGoumeida', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EzequielHumaB', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'MATLAB', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PoojaReddy05', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': [""kc_house\nmodel Linear Regression to predict kc_house\nOverview\nDependencies\nUsage\nInstall jupyter here.\nCredits\nCredits for this code go to RUdrakshTuwani, Siraj Raval's YouTube\n""], 'url_profile': 'https://github.com/Maneenoot', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Assam', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Logistic-Regression-Using-Python\nLogistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.\nLogistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.\n'], 'url_profile': 'https://github.com/rajansahu713', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Paris - France', 'stats_list': [], 'contributions': '370 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SeyfGoumeida', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Pleasant Hill, California', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Telco_Customer_Churn_Analysis\nby Yueh-Han Chen\nDataset\n\n""Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs."" [IBM Sample Data Sets] https://www.kaggle.com/blastchar/telco-customer-churn Each row represents a customer, each column contains customer’s attributes described on the column Metadata. The raw data contains 7043 rows (customers) and 21 columns (features). The “Churn” column is our target.\n\nFile Explanation\n\nThe main analysis file is Telco_Customer_Churn_Analysis.ipynb\nThe final visualization is in Visualization.slides.html (you can download it to see the result in slides.)\n\nLimitation\n\nLimitation 1 : In this dataset, we can only see one type of each variables instead of real world situation of changing different options as time passes, e.g., in real world, people might wanna try streaming service, but they might change their mind to leave the service next month.\n\n\nLimitation 2 : We cannot only see these variables as whole factors to understand the exact reasons why customers left because they might leave for better price offered by competitors or the bad economy in certian time, etc. We also cannot see the time they leaked, so it\'s hard to infer those external situation.\n\nKey Insights for Presentation\n\n\nThe average LTV of 80% of those who unsubscribed is 750 dollars. On the other hand, the average LTV of top 20% of those who unsubscribed is 4750 dollars. And the ratio by the sum of total LTV by each groups is 750*4 : 4750 = 1 : 1.6, which suggests we should focus on serving those 20% customers with high LTV, which brought 60%(1.6/2.6) of our revenue from leaked customers\n\n\n\n\n80%(low LTV) of leaked customers only stayed under 10 months.\n\n\n\n\nIn 80%(low LTV) of leaked customers, only 5% of them used internet service.\n\n\n\n\n81% of those who have and had stayed for more than 50 months tends to use multiple lines.\n\n\n\n\n100% of those who brought 20% of extremely high LTV used and are using internet service.\n\n\n\n\nIn 80% of data of current customers, all the subsets of internet services are equally used by 40% of people.\nExcept for 80% of data of current customers, in the rest of the data, streaming movies and streaming TV are two top subsets of internet service people use, and device protection and online backup are the second place, and tech support and online security are both third place, the gaps between places are near 10% of whole data except for 80% of current customers.\n\n\n\n\nIn 80% of current customers, those whose LTV are more than the average LTV of 80% of leaked customers are 10% more likely to use multiple lines and internet service than the average 80% of current customers.\n\n\n\n\n89% of those who leaked used monthly contract, while only 42% of current customers are using monthly contract.\n\n\n\n\nWhether people use internet service is the most important factor to create high LTV, and yearly contract is second place.\n\n\n\n\nAmong all the subsets of internet services, online backup is the most important factor to create high LTV\n\n\n'], 'url_profile': 'https://github.com/YuehHanChen', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Relativistic-Electron-Flux-Prediction\nThis project is for the paper: Relativistic Electron Flux Prediction at Geosynchronous Orbit Based on the Neural Network and the Quantile Regression Method.\nThe project is based on the python language.\nThe detailed information of the project structure can be found in the README(project structure).PNG.\nThe processed data, and all the codes are provided in the project.\nThe primitive data files are too large (1.3G) to provide.\nThe results of the models are recorded in the log files in the folder ""result2"".\nHowever, the early results of models in our early work are not recorded in an well-organized way, which we haven\'t provided here.\nThe ""train"", ""test"", ""show"" in the programs means train set, validation set, and test set.\nHow to use?\nFirstly, set the hyperparameters of the neural network in config.py. Secondly, run the model (For example, run FFN_model.py) . The model result will be recorded in the log file in the folder ""/result2"". Thirdly, check the result in the log file. (The detailed information of different models can be found in the project structure file README(project structure).PNG)\nIf you have any questions, please contact me through zhanghui_skysee@pku.edu.cn\n'], 'url_profile': 'https://github.com/studyalltheway', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Animesh-Bagchi', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Tree-based-methods\nApplications of some tree-based methods in classification and regression prediction tasks\n'], 'url_profile': 'https://github.com/ngduong', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Data-Science-coureswork\nHouse Price Prediction by Using Regression techniques/kaggle房价预测\n'], 'url_profile': 'https://github.com/fernandovan', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '181 contributions\n        in the last year', 'description': ['\nPredicting rain in Australia using logistic regression\nPerforming an EDA and building a logistic regression to predict rain occurrence\nWhy this dataset?\nThere are several reasons why weather forcasting is important. Not only does it make an uncertain future a little more certain, it also helps people to plan ahead for their day. On a more serious note, without accurate weather forecasts, people end up in dangerous situations they were unprepared for and end up injured or worse. Pilots need to know the weather to plan their flights, sailors need to know what the weather will be like to plan their sails, and farmers need to know what the weather will be like to help them plan watering, fertilizer and pesticide application, and harvest activities, to name a few.\nDataset\nYou can download the dataset here(14 MB).\nContributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\nLicense\nMIT\n'], 'url_profile': 'https://github.com/kianweelee', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 31, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020', '2', 'Jupyter Notebook', 'Updated May 6, 2020', '2', 'Python', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 27, 2020', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['ARIMA modeling of CO2 Time Series\nThe purpose of this report is to apply regression and primarily ARIMA models to fitting and forecasting Carbon Dioxide ($\\text{CO}_2$) levels from the 1960s to present (Feb. 2020). $\\text{CO}_2$ levels since the 1960s display both a trend and seasonal component. Here, we perform:\n\nrigorous Exploratory Data Analysis in R of both yearly and weekly data, including statistical testing for stationarity,\nmodel selection based on residual analysis, in-sample fit, and pseudo-out-of-sample fit, for both yearly and weekly data, and for both non- and seasonally-adjusted data, and\nforecasting and evaluation including confidence level generation.\n\nThe modeling package we use is fpp3 written by Rob J Hyndman and George Athanasopoulos:\nhttps://otexts.com/fpp3\nYearly $\\text{CO}_2$ data (of uptake in grass plants) is provided directly within R:\nhttps://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/CO2\nWeekly $\\text{CO}_2$ is provided by the National Oceanic and Atmospheric Administration (NOAA) and is available for free use by the public:\nhttps://www.esrl.noaa.gov/gmd/ccgg/trends\n'], 'url_profile': 'https://github.com/siduojiang', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Predict_Used_Car_Price\nUsed statistical methods & multiple linear regression to predict selling price of used cars (SAS)\n'], 'url_profile': 'https://github.com/aneesa-noorani', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Final Project C11\nA simple program that implements PCA, regression, cross validation on the oil500 dataset. The PCA coefficients are fed as inputs to the regression model (which can be linear or quadratic). Cross validation is used to verify the ideal subspace dimension to use as well as the dimension of the model\n'], 'url_profile': 'https://github.com/AbChatt', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Credit-Risk-Modeling\nCredit Risk Modeling  on Lendingclub data using logistic regression, decision trees.\n'], 'url_profile': 'https://github.com/kpravesh', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': [""China-s-GDP\nIt consists of China's GDP prediction using NON LINEAR REGRESSION.\n""], 'url_profile': 'https://github.com/nikhilmshebannavar', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '1,016 contributions\n        in the last year', 'description': [""Fish Weight Prediction Model:\n\nRequirements\nHello! Welcome to the famous Tsukiji fish market of Tokyo, Japan! We came here to collect data on some of the fish they have here\nbut we didn't wake up at 5am for the tuna auction and by the time we showed up they were only left with a few species of fish.\nWe got to work and gathered measurements from a few different species of fish and want you to train a regression model to predict\nthe weight of a fish using some of the features we were able to measure. We have no idea which features will be good predictors.\nWe will hold out 30% of the data before we hand it to you and we will use that csv for scoring.\nHere's what we need from you:\n\nA function that accepts a csv path and returns the predictions of your regression model using our csv. The csv we use will contain all the columns.\nUse a pipenv and scikit learn to submit the final model. You may use R for model selection.\n\nWith this function and it's output, we will rank the students by how well their model performed on predicting weight based on naive data.\nYour grade will be determined by ranking according to Mean-squared Error.\nIf your function does not return a list of predictions or we cannot compute the accuracy of your model that it will be an automatic F.\nUsed\n\nLinear Regression\nLasso Regression\nRandom Forest\nScikit Learn\nPython\nJoblib\nMatplotlib\nPipenv\n\n""], 'url_profile': 'https://github.com/esharma3', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'Barcelona', 'stats_list': [], 'contributions': '466 contributions\n        in the last year', 'description': ['House Prices Prediction Project 🚀\nA repository to store all the material used to build the Final Project of the Data Science Postgraduate Course in the University of Barcelona.\n\nDescription of the project 🔊\nIn this project, we have conducted data analysis to determine whether the machine learning techniques can be used to predict the house pricing for a specific location in the United States. The data sets used in this project have been obtained from a Kaggle competition entitled House Prices Prediction: Advanced Regression Techniques.\nTo achieve it, we formed a working team (bcnDataScience) in the Leaderboard of the Kaggle competition to participate in it and get the best possible result. To do this, we have analyzed the data and created some regressions models based on Logistics Regression, Random Forest Regressor and Artificial Neural Networks.\nFolders Structure & Content 📋\nThe material contained in this repository is divided into 2 main folders:\nHouse Prices Prediction 📂\nHere we store the material with which we have been working most of the life of the project. On one hand, documents with explanations of the challenge and data downloaded from the kaggle platform are available.\nOn another hand, we have created several Jupyter Notebooks to carry out detailed analysis and visualizations and testing some functionalities and performance of the models generated in python, specially the Logistics Regression and Random Forest. Finally, we have saved the csv and pkl files generated with the different notebooks in various subfolders to classify the information and avoid confusion.\nKaggle Competition 📂\nHere there are available the Jupyter Notebooks where we have built the last versions of the regression models (Logistics Regression, Random Forest and ANN) and final results predicted by the models that we have delivered in the Kaggle Leaderboard platform. Furthermore, we have collected the results of each of the submissions made to review the performance of the models and the development of the project.\nBelow you can find the details of the Notebooks generated for the Kaggle competition:\n\nHouse Prices Prediction - Logistics Regression &d Random Forest Regressor\nHouse Prices Prediction - Artificial Neural Networks\nOverview Kaggle Submissions & Results\n\nProject Presentation 🎬\nIn the following PowerPoint file you can find a short summary about the development and final results achieved in the project:\nHouse Prices Prediction Project - September 2020\nProject Duration ⏳\nFrom May 2020 until September 2020.\nAuthors ✒️\n\nLorena Méndez Otero - lmendezotero\nNuria Sanchez - NuriaSanchezc\nTheresa Kothe - TheKothe\nChristian Tipantuña - Christian Tipantuña\n\n'], 'url_profile': 'https://github.com/lmendezotero', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Prediction of Movie Ratings Among Young Adults Using Linear Regression and Deep Learning\nThe goal of this project is to investigate the features of movies published since the year 2000 that lead to higher ratings among adults between the ages of 18 and 30. Linear regression and deep neural network models are applied on the publicly available IMDb movie dataset.\nThe code used to analyze the dataset is available in the Jupyter Notebook, and a PDF report explaining the project in more detail is also available in this GitHub repository. This is my final project for 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning.\nTechnologies\n\nPython version 3.7\nNumPy\nPandas\nScikit-learn\nTensorFlow\nKeras\n\nTable of Contents\n\nJupyter Notebook (Prediction_of_Movie_Ratings_Among_Young_Adults.ipynb) containing Python code, machine learning models, and data analysis\nProject report (Prediction_of_Movie_Ratings_Among_Young_Adults.pdf) explaining the project in more detail\n\nOverview of Methods\nThis project involves two main steps which will be described in more detail below:\n\nData preprocessing\nTraining models\n\nData Preprocessing\nWill be added soon!\nTraining Models\nWill be added soon!\nProject To-Dos\n\n Update Data Preprocessing and Training Models sections of README.md\n\nSources\n\nIMDb movie dataset: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset\n\n'], 'url_profile': 'https://github.com/k8xu', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Big-Mart-Sales\nLinear Regression, Decision Tree, and Random Forest Algorithms implemented on bigmart sales data\n'], 'url_profile': 'https://github.com/Prasiddhanarayan', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}","{'location': 'Ranchi, India', 'stats_list': [], 'contributions': '419 contributions\n        in the last year', 'description': ['ml-deploy\nLogistic regression model deployment for iris dataset using flask, docker and aws.\nCheck At- http://ec2-18-221-206-206.us-east-2.compute.amazonaws.com/\n'], 'url_profile': 'https://github.com/Abhilasha06', 'info_list': ['R', 'Updated Apr 26, 2020', 'SAS', 'Updated Apr 24, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Python', 'Updated Apr 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['GWAS pipeline\nDate: April 2020\nLast updated: 22/04/2020\nAuthors: Manuela Tan\nGeneral description and purpose\nSteps for running a basic linear or logistic regression model GWAS in plink or rvtests.\npipeline for running a GWAS. This covers:\nThank you to the Laboratory of Neurogenetics group at NIH. This GWAS pipeline is largely built on their steps.\nhttps://github.com/neurogenetics/GWAS-pipeline\n'], 'url_profile': 'https://github.com/huw-morris-lab', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['GPR for MQCLE\nThis project uses Gaussian Process Regression (GPR) for the evolution of Mixed Quantum-Classical Liouville Equation (MQCLE).\nOverview\nWe includes\n\nthe documents, i.e., the reading notes of literatures, and\nexact solutions, i.e., grided solution of Schrodinger Equation (SE) and MQCLE and their plotting codes, and\ntest codes, which test whether the GPR works well for depicting the phase space distribution (of Liouville equation solution) at a moment, and\nthe main program, which uses GPR to depict the phase space distribution at each moment, and sampling from the distribution, where the chosen points are used for evolution to the next time step.\n\nThose in boldslash style are works that have not yet finished.\nThings to Be Done\n\naltogether makefile\ncombined exact solutions (SE and MQCLE) into one subfolder, with common interface\nmain program, commented in Doxygen style\n\nReference\n\n\nColbert D T, Miller W H. A novel discrete variable representation for quantum mechanical reactive scattering via the S-matrix Kohn method. J. Chem. Phys., 1992, 96(3): 1982-1991.\nManolopoulos D E. Derivation and reflection properties of a transmission-free absorbing potential. J. Chem. Phys., 2002, 117(21): 9552-9559.\nGonzalez-Lezana T, Rackham E J, Manolopoulos D E. Quantum reactive scattering with a transmission-free absorbing potential. J. Chem. Phys., 2004, 120(5): 2247-2254.\nKapral R. Quantum dynamics in open quantum-classical systems. J. Phys.: Condens. Matter, 2015, 27(7): 073201.\nWilliams C K I, Rasmussen C E. Gaussian processes for machine learning. Cambridge, MA: MIT press, 2006.\nSoeren Sonnenburg, Heiko Strathmann, Sergey Lisitsyn, Viktor Gal, Fernando J. Iglesias García, Wu Lin, … Björn Esser. (2019, July 5). shogun-toolbox/shogun: Shogun 6.1.4 (Version shogun_6.1.4). Zenodo. http://doi.org/10.5281/zenodo.591641\n\n\n'], 'url_profile': 'https://github.com/kaigu1997', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ADDYBOY', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': [""Apply-feature-Engineering-on-Churn-Data\nComparing the two Naive Baye's model  and logistic Regression model using f1 score precision and recall\nexplaining each one\nhappy to share comments and discuss the notebook\n""], 'url_profile': 'https://github.com/EmanDiab', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['Logistic Regression Analysis of Challenger Failure\nThis report will use logistic regression to analyze partial data available before the Challenger Space Shuttle failure. We will focus on both the predictive power of logistic regression, and inferential procedures around coefficient interpretation and confidence interval generation. The central question are:\n\nShould the launch have been delayed for a later date with higher temperature?\nGiven past data, what does the LR predict as the odds and probability, as associated confidence intervals, of shuttle failure?\n\nNotably, we will implement a parametric bootstrapping procedure for estimating the confidence interval of logistic regression parameters, and compare its interval width to both the the Wald and profile LR confidence intervals.\n'], 'url_profile': 'https://github.com/siduojiang', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Apurvachoudhary88', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/animalran', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'USA, MA', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['Machine-Leanrning-Project1\nPart One: Logistic Regression for Digit Classication\nYou have been given data corresponding to images of handwritten digits (8 and 9 in particular).\nEach row of the input data consists of pixel data from a (28 * 28) image with gray-scale values between 0.0 (black) and 1.0 (white);\nthis pixel data is thus represented as a single feature-vector of length 28*28 = 784 such values.\nThe output data is a binary label, with 0 representing an 8, and 1 representing a 9.\nTasks\n\nBuild a logistic regression model with max_iter set ( i = 1,2,...,40 ), then produce two plots showing the track of accuracy and the log loss.\nUsing coef_ atrribute you can access the weights which the model assigns to each feature in the data. Produce a plot with the values of iter as x-axis and eature weight as y.\nBuild and evaluate model for each value C, the inverse penalty strength. Record the result and also to confusion matrix on test data.\nPlot the images that are false positive or false negative.\nAnalyze all of the final weights produced by your classier. Reshape the weight coefficients into a matrix, corresponding to the pixels of the original images, and plot the result.\n\nPart Two: Sneakers vs Sandals\nYou have been given data from Fashion MNIST data-set, your task is to build a logistic regression classiffer for this data. You should explore different feature transformations, transforming the input features (in any way you see fit) that are given to the regression classiffer.\n2.1  Introduction:\nThe data provided has sparse matrix form, which means there are lots of number 0 in the CSV but little number above 0 can we find. This is the characteristic of data in image processing.\nThe problems caused by sparse matrix are apparently, our model can not fit the test data well enough,and the processing or training part is annoying and wasting time. We need to wait for a long time but do lots of useless counting work to train our model, since there are many meaningless 0 in the data, but we need to consider every point in the data set.\nAs I known, there are two useful patterns to process the data, which means standardization and normalization. Python gives us some useful tools to lower the dimension of data set or say compress the information, but consider we need to use logistic regression in this work, I will not try those tricky tools.\nIn this part, I tried to use two kinds of standardization ways, one is Scalar from sklearn preprocessing part, the other is simple but useful, the 01 standardization.As for feature preprocessing part, I consider to add the square terms, cubic terms, histogram information and spatial information such as the local maximum and average.\n2.2 Data Training and Comparision\n2.2.1 01 Standardization\nIdea: If the input value is greater than 0, then set it as 1.\nThis can increase the characters of the data and make it easier for machine to learn.\n2.2.2 StandardScaler (with_std=False)\nIdea: By this method, we can center and scale independently to each feature by computing the relevant statistics on the samples in the training set. \nConsider the data in this situation only range 0 to 1 so we set the parameter with_std as false.\nBut after some counting and comparison, I found this way is not good enough as 01-way, the scalar performances bad after many kinds of preprocessing. So for the best training model, I did not choose it as my final model.\n2.2.3 Best C\nSurprisingly, I found the parameter C plays a great role with standard scaler data, I randomly tried so number of C to train my logistic regression model and they performs well in the result. \nI think for future study, we can combine part one and part two, which means we can use the X_test data in order to look for the best C and then we might get higher accuracy in the end.\n2.2.4 Add Histogram Information X = [X, c^🇹]\nIdea: In this way we compute every positive feature and add this to the end of the data set.\nThis way performs really well, it reduce the error rate apparently. I was amazed by its performance but don’t know exactly why. I think it is a good way to empersize every feature in every vector, so for this kind of data, the training model can find out the “special characters” well.\n2.2.5 Add Spatial Information X_max or X_avg\nAdd the local maximum or average is a basic idea for image processing. Since we found that for every pixel in a image, the point near by will look likely as the point itself, which is background CNN model considered. \nJust as the histogram method below, we compute the neighbors for every pixel and add this information into the vector.\n2.2.6 Extend with Square. X = [ X, X² ]\nIdea: for every X_i, we compute its square and add it to the end in order to extend the data.\nThis and the following method is an initial way to extend the data set. But I found it is not good for spare matrix. There are 2 reasons:\n\nsince there are many 0 in data set,most of the computing part it’s just a waste of time\nextend information by only adding the square does not help to explore the features’ characteristics. The number is range 0 to 1, which means the square one will be less than the original one, that doesn’t help a lot for machine to learn.\n\n2.2.7 Extend with Square and Cube. X = [ X , X², X³]\nidea is the same as writen before.\n2.3 Result\n2.3.1 Show the ROC Curve, Coefficient plot and image.\n2.3.2 Comparison Form ( Base on error rate of test data)\n\n\n\nPreprocessing\\Standardization\n01-Standardization\nScalar Standardization(with_std = False)\nResult with different C after Scalar Standardization\n\n\n\n\nHistogram information\nX = [01,x_hist] Error Rate: 0.03300 C=0.5 Error Rate: 0.03200\nX = [scaler,x_hist] Error Rate: 0.04049\nX = [scaler,x_hist] C = 0.316 Error Rate: 0.03649\n\n\nCounting Maximun\nX = [01,x_max]Error Rate: 0.04749 X = [original,x_max]Error Rate: 0.04749\nX = [scaler,x_max] C = 0.5Error Rate: 0.04449\nX = [scaler,x_hist]C = 0.5 Error Rate: 0.03600\n\n\nCounting Average\nX = [01,x_max] Error Rate: 0.04149 X = [original,x_max]Error Rate: 0.04149\nX = [scaler,x_avg]C = 0.5Error Rate: 0.03749\nX = [scaler,x_hist]C = 0.75Error Rate: 0.03749999999999998\n\n\nCounting the Square\nX = [original,x_square] Error Rate: 0.0410\nX = [scaler,x_square] Error Rate: 0.04149\nX = [scaler,x_hist] C = 0.9Error Rate: 0.03949\n\n\nCounting the Cube\nX = [original,x_square,x_cube]Error Rate: 0.04049\n\nX = [scaler,x_hist] C = 1Error Rate: 0.04449\n\n\n\nNote: if not mention, the paramter C is by defult.\n'], 'url_profile': 'https://github.com/JanetLau0310', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Syracuse, NY', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ezair', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Titanic-dataset\nExploring the Titanic data set and predicting the survival of the passengers using Logistic Regression\n'], 'url_profile': 'https://github.com/Vyshnave', 'info_list': ['Updated Apr 22, 2020', 'C++', 'Updated Dec 24, 2020', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 26, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}"
"{'location': 'Mumbai', 'stats_list': [], 'contributions': '481 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GauravSahani1417', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['SuperMega ML =)\nОписание\nПроект представляет собой мини-библиотеку Python3, предназначенную для решения задач прогнозирования, предсказания и выявления зависимостей методом линейной регрессии.\nСистема способна работать с достаточно большим количеством примеров (1000-100000), имеющих достаточно большое (100-1000) число признаков.\n\nТакже реализована функциональная программа с графическим интерфейсом.\nОбщее представление\nЗадача линейной регрессии -- задача поиска, выявления произвольной зависимости некоторого показателя от набора признаков на множестве объектов. Это задача аппроксимации неизвестной зависимости некоторой известной гиперплоскостью.\nИными словами, мы можем построить, к примеру, такую прямую, которая максимально точно приблизится к зависимости стоимости квартиры от её площади, что также позволит предсказывать стоимость для тех квартир, для которых известна только прощадь.\nПроект предоставляет не только библиотеку, как набор классов, предназначенных для произведения регрессионного анализа, но и графический интерфейс, опозволяющий настроить регрессию, обучить её и визуализировать результаты обучения регрессионной модели -- посмотреть на выявленную зависимость.\nПо сути, с помощью реализованного проекта можно производить анализ групп людей, строить гиперплоскости ""тренда"", и многое другое.\nВнешности\nФункциональная программа с графическим интерфейсом PyQt5 предоставляет возможности конфигурирования регрессии, выбора метода оптимизации и изменения его параметров, выбора данных для анализа, обучения модели и просмотра результата.\n\n\nНа картинках выше видно, как регерссионная модель приблизила красную плоскость к синим точкам примеров. В данном случае мы имеем дело с данными, в которых необходимо выявить зависимость одного значения (предикторного) от двух других (признаковых). Видно, что модель хорошо справилась с задачей.\nКонкретный пример -- это зависимость количества посещений научно-популяных сообществ людьми от двух искуственных метрик.\nВнутренности\nВ общем и целом о внутреннем устройства проекта отлично расскажут UML-диаграммы.\nПроцесс взаимодействия пользователя с программой, реализующей возможности разработанной библиотеки, отображён на следующей диаграмме:\n\nВнутренная логика (регрессия, оптимизация и яже с ними) представлена двумя модулями: Frontend и Backend, внутри которых реализованы соответствующие классы:\n\n\n\nМатематика\nВ общем и целом алгоритм работы регрессионной модели достаточно прост и изящен: в начале процесса обучения модель только получает входной датасет с $M$ примерами, у каждого из которых $N$ признаков и одно предсказываемое значение. Далее создается случайный вектор $\\vec{W}$ размера $N$. Затем в цикле некоторое количество раз выполняется следующая операция:\n\nГде  -- поправочный вектор, а lr -- скорость обучения модели.\nПоправочный вектор в данном случае вычисляется с помощью градиента одним из следующих методов:\n\nКлассический\nМоменты\nСтохастический-пакетный\nСо случайной добавкой\n\nКаждый из этих методов имеет некоторую оптимизацию классического градиента, призванную ускорить сходимость в процессе градиентного спуска.\nНиже приведён процесс вычисления классического градиента:\n\nГде  -- функция ошибки в данной точке графика ошибки, X -- матрица признаков для некоторого пула примеров,  -- вектор ответов для соответствующих примеров матрицы примеров.\nКод\nС исходным кодом можно ознакомиться в репозитории проекта на GitHub.\nЗависимости\nPython3\n\nМодуль регрессионного анализа (Backend)\n\nNumpy\n\n\nМодуль графического интерфейса (Frontend)\n\nNumpy\nPandas\nMatplitlib\nPyQt5\n\n\n\n'], 'url_profile': 'https://github.com/pushsla', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Welcome to GitHub\nWelcome to GitHub—where millions of developers work together on software. Ready to get started? Let’s learn how this all works by building and publishing your first GitHub Pages website!\nRepositories\nRight now, we’re in your first GitHub repository. A repository is like a folder or storage space for your project. Your project\'s repository contains all its files such as code, documentation, images, and more. It also tracks every change that you—or your collaborators—make to each file, so you can always go back to previous versions of your project if you make any mistakes.\nThis repository contains three important files: The HTML code for your first website on GitHub, the CSS stylesheet that decorates your website with colors and fonts, and the README file. It also contains an image folder, with one image file.\nDescribe your project\nYou are currently viewing your project\'s README file. README files are like cover pages or elevator pitches for your project. They are written in plain text or Markdown language, and usually include a paragraph describing the project, directions on how to use it, who authored it, and more.\nLearn more about READMEs\nYour first website\nGitHub Pages is a free and easy way to create a website using the code that lives in your GitHub repositories. You can use GitHub Pages to build a portfolio of your work, create a personal website, or share a fun project that you coded with the world. GitHub Pages is automatically enabled in this repository, but when you create new repositories in the future, the steps to launch a GitHub Pages website will be slightly different.\nLearn more about GitHub Pages\nRename this repository to publish your site\nWe\'ve already set-up a GitHub Pages website for you, based on your personal username. This repository is called hello-world, but you\'ll rename it to: username.github.io, to match your website\'s URL address. If the first part of the repository doesn’t exactly match your username, it won’t work, so make sure to get it right.\nLet\'s get started! To update this repository’s name, click the Settings tab on this page. This will take you to your repository’s settings page.\n\nUnder the Repository Name heading, type: username.github.io, where username is your username on GitHub. Then click Rename—and that’s it. When you’re done, click your repository name or browser’s back button to return to this page.\n\nOnce you click Rename, your website will automatically be published at: https://your-username.github.io/. The HTML file—called index.html—is rendered as the home page and you\'ll be making changes to this file in the next step.\nCongratulations! You just launched your first GitHub Pages website. It\'s now live to share with the entire world\nMaking your first edit\nWhen you make any change to any file in your project, you’re making a commit. If you fix a typo, update a filename, or edit your code, you can add it to GitHub as a commit. Your commits represent your project’s entire history—and they’re all saved in your project’s repository.\nWith each commit, you have the opportunity to write a commit message, a short, meaningful comment describing the change you’re making to a file. So you always know exactly what changed, no matter when you return to a commit.\nPractice: Customize your first GitHub website by writing HTML code\nWant to edit the site you just published? Let’s practice commits by introducing yourself in your index.html file. Don’t worry about getting it right the first time—you can always build on your introduction later.\nLet’s start with this template:\n<p>Hello World! I’m [username]. This is my website!</p>\n\nTo add your introduction, copy our template and click the edit pencil icon at the top right hand corner of the index.html file.\n\nDelete this placeholder line:\n<p>Welcome to your first GitHub Pages website!</p>\n\nThen, paste the template to line 15 and fill in the blanks.\n\nWhen you’re done, scroll down to the Commit changes section near the bottom of the edit page. Add a short message explaining your change, like ""Add my introduction"", then click Commit changes.\n\nOnce you click Commit changes, your changes will automatically be published on your GitHub Pages website. Refresh the page to see your new changes live in action.\n🎉 You just made your first commit! 🎉\nWork with GitHub on your computer using GitHub Desktop\nGitHub Desktop is a free app from GitHub for Windows and Mac that allows you to easily work with your GitHub repositories from your computer. You just saw how you can commit to a repository from GitHub.com, but most developers do the majority of their work from their computer (locally) before pushing it up to GitHub. So let’s try that out!\nDownload GitHub Desktop\nPractice: Use GitHub Desktop and an editor to make a change from your computer\nStart by downloading GitHub Desktop if you haven’t already done so, and install it on your computer. Go through the GitHub Desktop onboarding steps, and when you get to the “Let’s get started” screen, go ahead and choose the repository you were just working with on GitHub.com, and click “Clone.”\nUsing an editor to make changes\nLet’s make sure you have a text editor on your computer - this is what you\'ll use to actually make changes to your files. If you already know you have an editor, then skip to the next step. Otherwise, download and install either Visual Studio Code or Atom and restart GitHub Desktop before proceeding to the next step.\nLet’s make a change to your GitHub Pages site, just like you did on GitHub.com, except this time we’re going to do it all from your computer. From GitHub Desktop, click the “Open in…” button in the middle of the screen to “open the repository in your external editor” that you just downloaded.\n\nIn the left sidebar, click the index.html file to open it, and go ahead and add another line. Maybe, “Building websites is fun! You should try it too!” or whatever you want to add.\n\nNow switch back to GitHub Desktop, and you should see the change you made.\n\nCommit your changes\nNow you can commit your changes by typing a message in the Summary box at the bottom left, and then click the blue Commit button below that.\n\nPush your changes to GitHub.com\nOne of the great things about working on things on your computer is that you get to control when other people see them. Now let’s push your commit to GitHub.com as well so it’s saved there and you can publish it to your site. Click the “Push origin” button to push your commit to GitHub.com.\n\nNow click the “View on GitHub” button to get back to your repository’s page on GitHub.com.\n\nDeploy and see your changes live on your GitHub Pages website!\nOnce you commit your changes, they are automatically published on your GitHub Pages website. Refresh your browser to see it live!\nCelebrate!\nHooray! Now you have your repository linked between your computer and GitHub.com. In the future, you can use GitHub Desktop to push any changes you decide to make from your computer.\nExtra Credit: Keep on building!\nChange the placeholder Octocat gif on your GitHub Pages website by creating your own personal Octocat emoji or choose a different Octocat gif from our logo library here. Add that image to line 12 of your index.html file, in place of the <img src= link.\nWant to add even more code and fun styles to your GitHub Pages website? Follow these instructions to build a fully-fledged static website.\n\nEverything you need to know about GitHub\nGetting started is the hardest part. If there’s anything you’d like to know as you get started with GitHub, try searching GitHub Help. Our documentation has tutorials on everything from changing your repository settings to configuring GitHub from your command line.\n'], 'url_profile': 'https://github.com/ashutoshpatil603', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Machine-Learning\nBy Ellek Linton\nA library of clean and efficient implementations of various machine learning models and an automatic and dynamic cross-validation automation library.\nIncluded Models:\n\nLDA (Linear Discriminant Analysis)\nQDA (Quadratic Discriminant Analysis)\nLogistic Regression\nDecision Trees\nRandom Forests\n\nIncluded Utilities:\n\nCross-Validation (vanilla)\nCross-Validation (automated tuning of hyperparameters for any Model implementation)\n\n'], 'url_profile': 'https://github.com/elleklinton', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'mumbai', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SHAIKHHUZEFA', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Arlington,Texas', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': [""training-iris\nThis project is part of the Machine Learning coursework. Training the iris dataset via linear regression. Using Supervised Learning to do classification. Using k-fold cross validation here.\nApplication Used: Spyder\nLanguage Used: Python 3.7\nLibraries used: Numpy, sklearn(cross_val_predict, cross_val_score, cross_val_predict, Kfold, train_test_split)\nInput file: iris.data\nProgram File: code.py\nInput:\npython code.py\nOutput:\nrunfile('C:/Users/Shreyas Mohan/Documents/Fall 19/Machine Learning/Project1_sxm9806/code.py', wdir='C:/Users/Shreyas Mohan/Documents/Fall 19/Machine Learning/Project1_sxm9806')\n[-0.04558273 -0.63329621  0.52108015 -0.45601488  1.91854987]\n2 -fold Cross-validation scores: 1.0\n3 -fold Cross-validation scores: 1.0\n4 -fold Cross-validation scores: 1.0\n5 -fold Cross-validation scores: 1.0\n6 -fold Cross-validation scores: 1.0\n7 -fold Cross-validation scores: 1.0\n8 -fold Cross-validation scores: 1.0\n9 -fold Cross-validation scores: 1.0\n""], 'url_profile': 'https://github.com/Shreysm', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'National Institue of Technology, Hamirpur', 'stats_list': [], 'contributions': '376 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ParthPant', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Selangor, Malaysia', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Kaggle-Titanic-Challenge-using-Logistic-Regression-77.90-\nThis is the submission code for Kaggle Titanic Challenge using Logistic Regression. Score is 77.90.\nFor more detailed explanation, please open the IPYTHON File.\nDatasets can be found at Kaggle Website \n'], 'url_profile': 'https://github.com/LeeSinLiang', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Logistic-Regression-using-scikit-learn\nImplemented logistic regression to predict the admission of students based on score of two exams\n'], 'url_profile': 'https://github.com/sumitjotrao', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['Car-Evaluation\nCar Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:\nCAR car acceptability\n. PRICE overall price\n. . buying buying price\n. . maint price of the maintenance\n. TECH technical characteristics\n. . COMFORT comfort\n. . . doors number of doors\n. . . persons capacity in terms of persons to carry\n. . . lug_boot the size of luggage boot\n. . safety estimated safety of the car\nInput attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts: PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples (for these examples sets see [Web Link]).\nThe Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.\nBecause of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\n'], 'url_profile': 'https://github.com/hamzabell', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Oct 11, 2020', '1', 'Python', 'Updated Apr 26, 2020', '1', 'Python', 'Updated Apr 22, 2020', '1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Machine-Learning\nDecision Tree, SVM, KNN, K Means clustering, Linear, Logistic and Naive Bayes regression\nThis repository is for Lab work and ML projects that have been worked.\n'], 'url_profile': 'https://github.com/Surya8291', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': [""Indian Demographic Projections for 2050\nThis repository contains the Jupyter notebook required to run the demographic projections as well as visulizaions. Simply run the notebook in GitHub or if that's not working, then clone this repo and run 'jupyter-notebook' through the terminal after navigating to the repo's directory and click on regression.ipynb to run the notebook.\nIf you wish to use any part of this code or require any assistance, please feel free to reach out to me at vidur.p11gmail.com.\nAnd don't forget to give due credits! ;)\nSources: Govt. of India Census, 2001 District-Wise\nPublications\nSex Composition Sex Composition of the Population of the Population\nHANDBOOK OF STATISTICS ON INDIAN STATES\n""], 'url_profile': 'https://github.com/MeanManMachineMan', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Adegoke_Toluwani_Bolu\nlearnt building logistic regression in python from scratch and decided to apply\n'], 'url_profile': 'https://github.com/beejhay31', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': ['WC_AT\nsimple Linear Regression. Adipose Tissue size prediction based on Waist using simple Linear Regression.\nhttps://waist-adiposetissue-api.herokuapp.com/\n'], 'url_profile': 'https://github.com/yvak90', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Cypress-Framework-Template\nBasic cypress framework template with page object pattern and visual regression.\nThe perfect structure to start automating!\ncontact: r-podraza@wp.pl\n'], 'url_profile': 'https://github.com/rafi9898', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': [""Machine-Learning-Project\nColumbia FinTech Machine Learning Project - Using NLP / Logistic Regression to Predict Future Stock Movement\nProgram that allows a user to choose a company from the S&P 500 and run a logistic regression model to predict the price movement of this company's stock on the next trading day based on current sentiment (Vader) of Reuters news articles related to this company.\nNOTE: You must have active keys from the following APIs to run this program:\n\nIEX Finance: https://iexcloud.io/\nNews API: https://newsapi.org/\n\nFile to run:\nproject_code > master_function\n""], 'url_profile': 'https://github.com/bwacker1', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'Tokyo, Japan', 'stats_list': [], 'contributions': '336 contributions\n        in the last year', 'description': ['Churn_prediction_using_logistic_regression\nIntroduction\nCustomer churn, also known as customer attrition, occurs when customers stop doing business with a company. The companies are interested in identifying segments of these customers because the price for acquiring a new customer is usually higher than retaining the old one. For example, if Netflix knew a segment of customers who were at risk of churning they could proactively engage them with special offers instead of simply losing them.\nLogistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.\nMotivation\nI was curious to apply logistic regression to predict customer churn.\nData\nIn this notebook, a customer churn prediction model is built using Telco Customer Churn  dataset .\nGetting started\nYou need an installation of Python, plus the following libraries:\n\nnumpy\npandas\nmatplotlib.pyplot\nseaborn\nsklearn\nimblearn\nstatsmodels.api\n\nSummary and key findings\n\nThis logistic regression model can predict churn with 0.82 accuracy, which can help to retain old users;\nThe model gives 1,786 correct and 379 incorrect predictions of churn;\nIn order to implement the model, Synthetic Minority Oversampling Technique (SMOT) and Recursive Feature Elimination (RFE) were applied to balance the data and select the important features;\nAfter apllying RFE, we chose parameters with p-value less than 0.05. Overall, the model was implemented using the following parameters: ""gender_Female"", ""gender_Male"", ""SeniorCitizen_No"", ""SeniorCitizen_Yes"", ""Dependents_No"",""MultipleLines_No"", ""MultipleLines_Yes"", ""StreamingMovies_No internet service"", ""Contract_One year"", ""Contract_Two year"", ""PaperlessBilling_Yes"", ""PaymentMethod_Bank transfer (automatic)"", ""PaymentMethod_Credit card (automatic)"", ""PaymentMethod_Electronic check"", ""PaymentMethod_Mailed check"".\n\n'], 'url_profile': 'https://github.com/aigera2007', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Final Project C11 Question 2\nA simple program that implements multi class logistic regression on the orchid dataset.\n'], 'url_profile': 'https://github.com/AbChatt', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'Porto Alegre, Brazil', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Programming Exercise 2: Logistic Regression\nThis repository provides a Python implementation that solves both the linear and non-linear regression excercises proposed in ""Programming Exercise 2: Logistic Regression"" from Stanford\'s Professor Andrew Ng at Coursera.\nA brief analysis of the results is provided in Portuguese. It was submited as an assignment of a graduate course named Connectionist Artificial Intelligence at UFSC, Brazil.\n'], 'url_profile': 'https://github.com/fredericoschardong', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}","{'location': 'San Jose, CA', 'stats_list': [], 'contributions': '223 contributions\n        in the last year', 'description': ['CNN\n\n• Outcome of the project:\n• Built a regression model to predict the positive cases of the COVID-19, as it has become a pandemic, leveraged the applications of Convolutional Neural Network, ARIMA and FB Prophet algorithms to come up with useful insights\n• Visualized the data by grouping the COVID-19 cases statewise, using matplotlib, plotly and seaborn.\n• Increased the base line accuracy of the convolutional neural networks to 98% from 50% model using Keras with TensorFlow as backend by regularizing (Kernel regularization), increasing training data, pre-trained model (VGG-16), flipping, rotating, adding  noise and dropouts to normalize the model to the data.\n• Visualized the chest xray images using Image.\n• The model is learning through each epoch, and the accuracy of the training model is consistently increasing which shows a solid training model. Confusion matrix was plotted. The model is doing great with good ration of tp,tn>fp,fn.\n• The training accuracy of the CNN model is: 94%, The test accuracy of the CNN model is 98%, and the validation accuracy is 97-98%\n'], 'url_profile': 'https://github.com/sajithgowthaman', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Apr 29, 2020', 'JavaScript', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Python', 'MIT license', 'Updated Apr 25, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['Iris-Classification-With-Feature-Selection\nCategorizing the iris species using linear regression , and finding the most and least important feature\n'], 'url_profile': 'https://github.com/nachi-hebbar', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['CO2-Emission-Linear-Regression\nUsing a linear regression with different variables to calculate the CO2 Emissions of cars.\n'], 'url_profile': 'https://github.com/saee-ghulepatil', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '361 contributions\n        in the last year', 'description': ['House-price-predictions-using-advanced-regression-techniques\nThis is my submission of the Kaggle competition House price predictions using advanced regression techniques.\n'], 'url_profile': 'https://github.com/devanshu125', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '330 contributions\n        in the last year', 'description': ['advertising-sales-RFE-MLR\nAdvertising and Sales Case Study using Recursive Feature Elimination (RFE) and Multiple Linear Regression Model\n'], 'url_profile': 'https://github.com/junmoan', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '513 contributions\n        in the last year', 'description': ['clogistic\n\n\n\n\nLogistic regression with bound and linear constraints. L1, L2 and Elastic-Net regularization.\nThis is a Python implementation of the constrained logistic regression with a scikit-learn like API. This library uses CVXPY and scipy optimizer L-BFGS-B. Currently, only binary classification is supported.\n\nInstallation\nTo install the current release of clogistic from PyPI:\npip install clogistic\n\nTo install from source, download or clone the git repository\ngit clone https://github.com/guillermo-navas-palencia/clogistic.git\ncd clogistic\npython setup.py install\n\n\nDependencies\n\ncvxpy>=1.0.31\nnumpy\nscikit-learn>=0.20.0\nscipy\n\n\nExamples\nclogistic can flawlessly replace the scikit-learn LogisticRegression import when bounds or linear constraints are required:\n# from sklearn.linear_models import LogisticRegression\nfrom clogistic import LogisticRegression\n\nL1-norm / Elastic-Net\nIn the unconstrained problem, the L-BFGS-B solver supports both L1 and Elastic-Net regularization.\n>>> from clogistic import LogisticRegression\n>>> from sklearn.datasets import load_breast_cancer\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = LogisticRegression(solver=""lbfgs"", penalty=""l1"")\n>>> clf.fit(X, y)\nLogisticRegression(C=1.0, class_weight=None, fit_intercept=True, l1_ratio=None,\n                   max_iter=100, penalty=\'l1\', solver=\'lbfgs\', tol=0.0001,\n                   verbose=False, warm_start=False)\n>>> clf.predict(X[:5, :])\narray([0, 0, 0, 1, 0])\n>>> clf.predict_proba(X[:5, :])\narray([[1.00000000e+00, 1.77635684e-14],\n       [9.99999984e-01, 1.61472709e-08],\n       [9.99999651e-01, 3.48756416e-07],\n       [1.99686878e-01, 8.00313122e-01],\n       [9.99992767e-01, 7.23307080e-06]])\n>>> clf.score(X, y)\n0.9384885764499121\n>>> clf.coef_\narray([[ 0.21636945,  0.24114984,  0.60707879, -0.02554191,  0.        ,\n        -0.01089683, -0.02143886, -0.00094761,  0.        ,  0.        ,\n         0.        ,  0.04741039, -0.04362739, -0.08740847,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.22373333, -0.33820163, -0.30848864, -0.00795973,  0.        ,\n        -0.06749937, -0.08757346, -0.01489128, -0.00660756,  0.        ]])\n>>> clf.intercept_\narray([0.02357148])\n>>> clf = LogisticRegression(solver=""lbfgs"", penalty=""elasticnet"", l1_ratio=0.5)\n>>> clf.fit(X, y)\nLogisticRegression(C=1.0, class_weight=None, fit_intercept=True, l1_ratio=0.5,\n                   max_iter=100, penalty=\'elasticnet\', solver=\'lbfgs\',\n                   tol=0.0001, verbose=False, warm_start=False)\n>>> clf.score(X, y)\n0.9402460456942003\n\nL1-norm with bounds\nAdd bound constraints to force all coefficients to be negative. The intercept\nrepresents the last position of the lower and upper bound arrays lb, ub,\nin this case, it is unconstrained.\n>>> import numpy as np\n>>> from scipy.optimize import Bounds\n>>> lb = np.r_[np.full(X.shape[1], -1), -np.inf]\n>>> ub = np.r_[np.zeros(X.shape[1]), np.inf]\n>>> bounds = Bounds(lb, ub)\n>>> clf = LogisticRegression(solver=""ecos"", penalty=""l1"")\n>>> clf.fit(X, y, bounds=bounds)\nLogisticRegression(C=1.0, class_weight=None, fit_intercept=True, l1_ratio=None,\n                   max_iter=100, penalty=\'l1\', solver=\'ecos\', tol=0.0001,\n                   verbose=False, warm_start=False)\n>>> clf.score(X, y)\n0.9507908611599297\n>>> clf.coef_\narray([[ 6.42042386e-10,  6.69614517e-10,  7.49065341e-10,\n         2.47466729e-10, -7.46445480e-08, -1.66525870e-07,\n        -5.07484194e-06, -9.67293096e-08, -9.94240524e-08,\n        -5.10981877e-08, -6.24719977e-08, -2.53429851e-09,\n        -2.07856647e-08, -5.03914527e-02, -4.44953073e-08,\n        -4.26536917e-08, -4.63999149e-08, -4.53887837e-08,\n        -4.58750836e-08, -4.32208857e-08, -2.25323306e-08,\n        -2.32851192e-01, -1.56344127e-01,  4.11491956e-11,\n        -1.82998431e-07, -9.99999982e-01, -9.99999988e-01,\n        -9.99999848e-01, -9.99999947e-01, -7.78260579e-08]])\n>>> clf.intercept_\narray([25.93817947])\n\nL2-norm with bounds\nIf we choose penalty=""l2"" or penalty=""none"", the L-BFGS-B solver can handle bound constraints.\n>>> clf = LogisticRegression(solver=""lbfgs"", penalty=""l2"")\n>>> clf.fit(X, y, bounds=bounds)\nLogisticRegression(C=1.0, class_weight=None, fit_intercept=True, l1_ratio=None,\n                   max_iter=100, penalty=\'l2\', solver=\'lbfgs\', tol=0.0001,\n                   verbose=False, warm_start=False)\n>>> clf.score(X, y, bounds=bounds)\n0.9507908611599297\n>>> clf.coef_\narray([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00, -1.25630653e-01, -4.92843035e-01,\n        -5.85325868e-01, -4.06870366e-01, -1.79105954e-01,\n        -4.60000473e-02, -3.22302459e-01,  0.00000000e+00,\n         0.00000000e+00, -4.54736330e-02, -6.33875425e-03,\n        -6.32628802e-03, -2.51268348e-02, -1.17129553e-02,\n        -1.71495885e-02, -5.82817365e-04, -8.19771941e-04,\n        -2.44436774e-01, -1.53861432e-01,  0.00000000e+00,\n        -2.47266502e-01, -1.00000000e+00, -1.00000000e+00,\n        -6.42342321e-01, -5.32446169e-01, -1.41399360e-01]])\n>>> clf.intercept_\narray([25.96760162])\n\nElastic-Net with bounds and constraints\nIf solver=""ecos"", linear constraints are supported. First, we solver the\nunconstrained problem:\n>>> clf = LogisticRegression(solver=""ecos"", penalty=""elasticnet"", l1_ratio=0.5)\n>>> clf.fit(X, y)\nLogisticRegression(C=1.0, class_weight=None, fit_intercept=True, l1_ratio=0.5,\n                   max_iter=100, penalty=\'elasticnet\', solver=\'ecos\',\n                   tol=0.0001, verbose=False, warm_start=False)\n>>> clf.coef_\narray([[ 1.09515934e+00,  1.78915210e-01, -2.88199448e-01,\n         2.26253000e-02, -2.38177991e-08, -3.48595366e-08,\n        -1.11789210e-01, -5.41772242e-08, -4.46703080e-08,\n        -3.70030911e-09, -9.23360225e-09,  1.34197557e+00,\n         2.38283098e-08, -1.02639970e-01, -2.87375705e-09,\n         6.99608679e-09, -4.41159130e-09, -4.39357355e-09,\n        -4.51432833e-09,  1.46276767e-09,  1.75313422e-08,\n        -4.39081317e-01, -9.05714045e-02, -1.32670345e-02,\n        -8.77722530e-08, -4.68697190e-01, -1.91274067e+00,\n        -2.41172826e-01, -5.15782954e-01, -1.16567422e-08]])\n>>> clf.intercept_\narray([28.2732499])\n>>> clf.score(X, y)\n0.9578207381370826\nNow, we require to impose bounds and a linear constraint, for example, -coef_[0] + coef_[1] <= 0.5.\nThe constraint has the general inequality form: lb <= A^Tx <= ub.\n>>> from scipy.optimize import LinearConstraint\n>>> lb = np.array([0.0])\n>>> ub = np.array([0.5])\n>>> A = np.zeros((1, X.shape[1] + 1))\n>>> A[0, :2] = np.array([-1, 1])\n>>> A\narray([[-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n         0.,  0.,  0.,  0.,  0.]])\n>>> clf = LogisticRegression(solver=""ecos"", penalty=""elasticnet"", l1_ratio=0.5)\n>>> clf.fit(X, y, bounds=bounds, constraints=constraints)\n>>> clf.coef_\narray([[-4.99999990e-01,  2.59127065e-09,  1.57855012e-09,\n         4.92952226e-10, -9.38544504e-07, -6.23158850e-01,\n        -9.99999485e-01, -4.41393438e-02, -1.50746141e-01,\n        -2.46375497e-07, -5.86201514e-07, -3.10883675e-09,\n        -7.35173366e-07, -4.48737109e-02, -1.71421755e-07,\n        -1.67941981e-07, -1.93139045e-07, -1.77770207e-07,\n        -1.83325585e-07, -1.63464915e-07, -1.76861958e-08,\n        -2.46764081e-01, -1.08368550e-01,  2.89353186e-10,\n        -6.39569062e-01, -9.99999975e-01, -9.99999982e-01,\n        -9.99999849e-01, -9.99999935e-01, -3.52251357e-06]])\n>>> clf.intercept_\narray([28.38190371])\n>>> clf.score(X, y)\n0.9543057996485061\n'], 'url_profile': 'https://github.com/guillermo-navas-palencia', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,947 contributions\n        in the last year', 'description': ['perimeter_model\nDeep Learning Regression Model to Identify Vertices on Buildings from Google Maps\nUsing FastAI\n\n'], 'url_profile': 'https://github.com/ElliotEckholm', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['sms-spam-classification\nClassifies text messages as spam or not applying Logistic Regression and Deep Learning model.\n'], 'url_profile': 'https://github.com/RodolfoLSS', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Deployment_Heruko\nUsing heruko PAAS, I deploy the salary prediction Linear regression model and make it available globally\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Greater Philadelphia Area', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/juliettegudknecht', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mihirkumar02', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2021', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'CSS', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['CO2-emission-of-cars\nIt consists of prediction on CO2 emission of cars using LINEAR REGRESSION.\n'], 'url_profile': 'https://github.com/nikhilmshebannavar', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['airbnb_pricing_prediction\nLinear Regression Model predicting the price of Airbnb listings from NewYork and Singapore\nThe business goal is to create a model that can predict the price of listings in both Singapore and New York City.\nBenefits for hosts:\n\nMore information about how to improve listing information and set prices to attract more stayers.\nBenefits for renters:\nA useful tool to check for any overpriced deal when searching for an Airbnb in either city and can help ensure the quality of a considered listing.\n\nThree models created\n\nIntuitive Variable Selection Model:\n\n\nProvides further intuitions on the Airbnb market in summer in business context.\nIt is highly interpretable, but the relatively smaller R2 Value implies that the model is still not accurate enough to capture the whole picture.\n\n\nStepwise Regression Model\n\n\nProvides more accurate explanation of the data but is not intuitive enough or easily comprehensible.\nMoreover, it still includes too many variables that are not significant or informative.\n\n\nThe Final Model\n\n\nProvides the most accurate and intuitive explanation to the data.\nIt should be noted that the final Singapore model has a higher AIC than New York because there are less observations available.\n\n'], 'url_profile': 'https://github.com/brandon19951215', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': [""dauphine-nonparametric_reg\nSituation\nNous disposons d'un jeu de données  où les  et  sont idéalisées comme des (réalisations de) variables aléatoires réelles admettant la représentation suivante :\n pour  où les  sont indépendantes et identiquement distribuées, admettant une densité  vérifiant  et .\nLes  sont indépendantes et identiquement distribuées de densité , et indépendantes des .\nLa fonction  vérifie  pour tout .\nObjectifs\nLes objectifs de cette étude sont :\n\nReconstruire  graphiquement et étudier si  est la densité uniforme ou non.\nReconstruire  graphiquement.\nExplorer les propriétés de  et estimer .\n\n""], 'url_profile': 'https://github.com/rdetain', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Pridatex Comparison Tests\nClassification and Regression Tests between Anonymized and Original data to see their closeness.\nProperly Reading in Data\n\nIn original CSV file, change name of the output column to \'output\'\nGenerate anonymized CSV file from original CSV file.\nRename the anonymized file to ""[original filename]_anonymized.csv"".\n\nClassification Test\nUsage\n\npython [Classification_Confusion_Matrix.py] [original filename]\n\nDefault Options\nEnsemble variable for average Confusion matrix: n_ensemble = 1000\nWhen to print to update ensemble, for convenience: n_update_ensemble_print = 50\n% threshold for Confusion Matrix Exclusion: n_threshold = 0\nPrediction and Confidence interval thresholds: n_interval_prediction = 90, n_interval_confidence = 95\nAlgorithm\n\nClassification Model Accuracy trained on Anonymized Data\n\nBuild a classification model with the Anonymized Dataset\nGenerate y-predictions of Original Validation Data\'s X-values using Anonymized Dataset\'s classification model\nGenerate a confusion matrix between Anonymized Dataset\'s y-predictions and Original Validation Data\'s y-values\n\nNote: We acquire losses on Original Data because those are the real y-values.\nWe want to see how closely Anonymized Model can perform on the real data.\nClassification Model Accuracy trained on Original Data\n\nBuild a classification model with the Original Training Dataset\nGenerate y-predictions of Original Validation Data\'s X-values using Original Training Dataset\'s classification model\nGenerate a confusion matrix between Original Training Dataset\'s y-predictions and Original Validation Data\'s y-values\n\nCurrent Tests on Confusion Matrices\n\nOriginal and Anonymized Confusion Matrices\nAverage Number of Correct Class Identifications\nTwo-Way Chi-Squared Test\nOriginal and Anonymized Proportional Confusion Matrices\nX-percent Confidence Interval of percent deviation from Original Dataset for each Class\nY-percent Overall Prediction Interval of percent deviation from Original Dataset\nX-percent Overall Confidence Interval of percent deviation from Original Dataset\n\nNote: The intervals show a difference of accuracies between a model trained on the anonymized data and a model trained on the original data.\nTesting Notes\nWe will use the same validation dataset for Anonymized Dataset and Original Dataset.\nThe Confusion Matrix differences may not be exactly reproducible due to the randomness of\nselecting data for training and testing, and that the rows of the anonymized dataset do not match\nthe rows of the original data.\nRegression Test\nWork in Progress\n\nNot ensemble\nFaulty Assumptions with ANOVA\nParameterization is not complete\nNeed to implement Confidence Interval Tests\n\nUsage\n\npython [Regression_CV.py] [original filename]\n\nDefault Options\nBoolean for whether forward or backward selection: bool_forward = False\nAlgorithm\nRegression Model Accuracy trained on Anonymized Data\n\nBuild a linear model of 5 variables (subject to parameterization later) with the Anonymized Dataset using forward/backward selection\nCross-validate the model with the Original Dataset\nCalculate MSE\n\nNote: We acquire losses on Original Data because those are the real y-values.\nWe want to see how closely Anonymized Model can perform on the real data.\nRegression Model Accuracy trained on Anonymized Data\n\nBuild a linear model of 5 variables (subject to parameterization later) with the Original Dataset using forward/backward selection\nCross-validate the model with the Original Dataset\nCalculate MSE\n\nNote: Forward/Backward Selection needs the following assumptions by ANOVA\n\nData is Normally Distributed (may be covered wityh sample size > 30)\nHomogeneity of Variance - Variance among the groups should be approximately equal.\nObservations are independent of eachother (well, an inaccurate assumption we will take for this case)\n\nCurrent Tests on MSE\'s\n\nOriginal and Anonymized MSEs\nProportional Difference between Original and Anonymized MSEs\n\n'], 'url_profile': 'https://github.com/banerjeearjun77', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Predicting-Bike-Rentals\nBike sharing dataset analyzed with Multilinear Regression (see bikeSharing_MLR.R), Random Forests (see bikeSharing_RF.R)\nand Time Series (see bikeSharing_TS.R). Please also refer to the Power Point presentation for a more detailed overview of the project.\n'], 'url_profile': 'https://github.com/TeodorChiaburu', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Xiamen Fujian', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['MultiKink\nMultiKink is an R package for Estimation and Inference for Multi-kink quantile regression.\nInstallation\n# install.packages(""devtools"")\ndevtools::install_github(""ChuangWAN1994/MultiKink/MultiKink"")\n\nDescription\nEstimation and inference for multiple kink quantile regression. A bootstrap restarting iterative segmented quantile algorithm is proposed to estimate the multiple kink quantile regression model conditional on a given number of change points. The number of kinks is also allowed to be unknown. In such case, the backward elimination algorithm and the bootstrap restarting iterative segmented quantile algorithm are combined to select the number of change points based on a quantile BIC. A score-type based test statistic is also developed for testing the existence of kink effect.\nCitation\nWei Zhong, Chuang Wan and Wenyang Zhang. (2020) Estimation and inference for multi-kink quantile regression. working paper.\n'], 'url_profile': 'https://github.com/ChuangWAN1994', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Toronto - Canada', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Summary\nThis experiment aims to create a predictive model for\nestimate the demand for bicycle rentals.\nDescription:\nThis experiment aims to demonstrate the process of building a model\nregression to forecast demand for bicycle rentals. We will use a\ndata set to build and train our model.\nData:\nThe “Bike Rental UCI” data set will be used to build and train the\nmodel in this experiment. This dataset is based on real company data\nCapital Bikeshare, which operates bicycle rentals in Washington DC,\nIn the USA.\nThe dataset contains 17,379 observations and 17 variables, representing the\nnumber of bikes rented within specific hours of the day, in the\n2011 and 2012. Climatic conditions (such as temperature, humidity and speed of\nwind) were included in the dataset and the dates were categorized as holidays and\ndays of the week.\nDataset: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nobjective\nThe objective will be to predict the value of the variable cnt (count) that represents the\nnumber of bikes rented within a specific hour and whose range is\nfrom 1 to 977\nAttribute Information:\nBoth hour.csv and day.csv have the following fields, except hr which is not available in day.csv\n\ninstant: record index\ndteday : date\nseason : season (1:winter, 2:spring, 3:summer, 4:fall)\nyr : year (0: 2011, 1:2012)\nmnth : month ( 1 to 12)\nhr : hour (0 to 23)\nholiday : weather day is holiday or not (extracted from [Web Link])\nweekday : day of the week\nworkingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n\n\nweathersit :\n\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\ntemp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\natemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\nhum: Normalized humidity. The values are divided to 100 (max)\nwindspeed: Normalized wind speed. The values are divided to 67 (max)\ncasual: count of casual users\nregistered: count of registered users\ncnt: count of total rental bikes including both casual and registered\n\n'], 'url_profile': 'https://github.com/lexxconsulting', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Prediction-Covid-19\nThis program for use predict Covid-19 in the world using Linear Regression and analityc\n'], 'url_profile': 'https://github.com/afiflampard', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '782 contributions\n        in the last year', 'description': ['Building a risk score model for retinopathy in diabetes patients using logistic regression.\nSteps\n\nData preprocessing\n\nLog transformations\nStandardization\n\n\nBasic Risk Models\n\nLogistic Regression\nC-index\nInteractions Terms\n\n\n\nDiabetic Retinopathy\nRetinopathy is an eye condition that causes changes to the blood vessels in the part of the eye called the retina.\nThis often leads to vision changes or blindness.\nDiabetic patients are known to be at high risk for retinopathy.\nLogistic Regression\nLogistic regression is used for predicting the probability of a binary outcome. In our case, this would be the probability of having or not having diabetic retinopathy.\nWe see that the model is less confident in its prediction with the interaction term than without (the prediction value is lower when including the interaction term). With the interaction term, the model has adjusted for the fact that the effect of high cholesterol becomes less important for older patients compared to younger patients.\ncredits: coursera Ai in medicine course\n'], 'url_profile': 'https://github.com/kirankamatmgm', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Gradient_Descent_For_Logistic_Regression\nThis work includes implementation of loss function , gradient descent and regularization for logistic regression\n'], 'url_profile': 'https://github.com/serkanemreelci', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'R', 'Updated Apr 26, 2020', 'Updated Apr 20, 2020', 'Python', 'Updated May 1, 2020', 'R', 'Updated Apr 22, 2020', 'R', 'Updated Apr 22, 2020', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020']}"
"{'location': 'Ottawa', 'stats_list': [], 'contributions': '227 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/li000611', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'Warsaw', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': [""The regression analysis of the men's shoe prices inspired by DataWorkshop initiative (https://dataworkshop.eu/)\nChallenge - Transformation 1, organized by DataWorkshop can be found here:\nDay1: https://www.youtube.com/watch?v=7vEWK1gvu60&feature=youtu.be\nDay2: https://www.youtube.com/watch?v=guc8lS6m5ws&feature=youtu.be\nDay3: https://www.youtube.com/watch?v=BFwzT0hPx1Y&feature=youtu.be\nDay4: https://www.youtube.com/watch?v=ksNrmMC682Y&feature=youtu.be\nDay5: https://www.youtube.com/watch?v=sGM1qNL5y_4&feature=youtu.be\nFinal score:\n""], 'url_profile': 'https://github.com/kordusmonika', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': [""house_prices_prediction\nHouse Prices Prediction using Regression\nProject Organization\n├── LICENSE\n├── Makefile           <- Makefile with commands like `make data` or `make train`\n├── README.md          <- The top-level README for developers using this project.\n├── data\n│\xa0\xa0 ├── external       <- Data from third party sources.\n│\xa0\xa0 ├── interim        <- Intermediate data that has been transformed.\n│\xa0\xa0 ├── processed      <- The final, canonical data sets for modeling.\n│\xa0\xa0 └── raw            <- The original, immutable data dump.\n│\n├── docs               <- A default Sphinx project; see sphinx-doc.org for details\n│\n├── models             <- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n│\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n│\n├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n│\xa0\xa0 └── figures        <- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze > requirements.txt`\n│\n├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n├── src                <- Source code for use in this project.\n│\xa0\xa0 ├── __init__.py    <- Makes src a Python module\n│   │\n│\xa0\xa0 ├── data           <- Scripts to download or generate data\n│\xa0\xa0 │\xa0\xa0 └── make_dataset.py\n│   │\n│\xa0\xa0 ├── features       <- Scripts to turn raw data into features for modeling\n│\xa0\xa0 │\xa0\xa0 └── build_features.py\n│   │\n│\xa0\xa0 ├── models         <- Scripts to train models and then use trained models to make\n│   │   │                 predictions\n│\xa0\xa0 │\xa0\xa0 ├── predict_model.py\n│\xa0\xa0 │\xa0\xa0 └── train_model.py\n│   │\n│\xa0\xa0 └── visualization  <- Scripts to create exploratory and results oriented visualizations\n│\xa0\xa0     └── visualize.py\n│\n└── tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n\n\nProject based on the cookiecutter data science project template. #cookiecutterdatascience\n""], 'url_profile': 'https://github.com/mathusuthan', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'Shanghai, China', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['ridgevar\nPython package ""ridgevar"" for estimation of error variance via ridge regression. Provide several methods to estimate the error variance for high-dimensional linear regression models, which includes the ridge regression based method of Liu et al. (2020), the maximum likelihood based method of Dicker and Erdogdu (2016), and the moments based method of Dicker (2014).\nInstallation\npip install git+https://github.com/xliusufe/RidgeVarpy.git\n\nUsage\n\n\nThere are three functions: VAR_RR, VAR_MM, and VAR_MLE\n\n\nDetails of the usage of the package can be found in corresponding R package RidgeVar-manual.pdf\n\n\nThe correponding R package named RidgeVar can be found on GitHub.\n\n\nExample\nfrom ridgevar import *\nimport numpy as np\n\nn, p = 60, 100\nbeta = np.append(np.sqrt(0.1/p)*np.repeat(1,p/2),np.repeat(0,p/2))\nx = np.random.randn(n,p)\neps = np.random.randn(n)\ny = x.dot(beta) + eps\n\nsigma2_RR = VAR_RR(y,x)\nsigma2_MLE = VAR_MLE(y,x)\nsigma2_MM = VAR_MM(y,x)\nprint([sigma2_RR,sigma2_MLE,sigma2_MM])\n\nReferences\nDicker, L. H. (2014). Variance estimation in high-dimensional linear models.  Biometrika 101, 269-284.\nDicker, L. H. and Erdogdu, M. A. (2016). Maximum likelihood for variance estimation in high-dimensional linear models. In  Proceedings     of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016), 159-167. JMLR Workshop & Conference     Proceedings.\nFan, J., Guo, S. and Hao, N. (2012). Variance estimation using refitted cross-validation in ultrahigh-dimensional regression. Journal of Royal Statistical Society Series B 74, 37-65.\nLiu, X., Zheng, S. and Feng, X. (2020). Estimation of error variance via ridge regression. Biometrika. DOI: 10.1093/biomet/asz074\nDevelopment\nThis Python package is developed by Xu Liu (liu.xu@sufe.edu.cn).\n'], 'url_profile': 'https://github.com/xliusufe', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['LogReg\nLogReg: A method of regularized logistic regression for biomarker discovery from gene expression data and compare with other 5 RFE methods. In the Data_SPTB folder, we only give examples of the file format input by each R program. The input file only gives the first few lines, but this does not affect the results of the article.\n(1) ""Box_PRS.R""  ----  Obtain the boxplot of of preterm risk score (PRS) on the independent dataset.\n(2) ""Class_ROC.R"" ----   Verify the identified biomarkers on an independent dataset by the AUC value.\n(3) ""DEgene.R"" ----   Identify the differentially expression genes (DEGs) on a dataset\nand find candidates with adjusted P-value < 0.05.\n(4) ""GSE59491_cel.R"" ----  Solve the regularized logistic regression with seven effective penalties,\ni.e., ridge, lasso, elastic net, L0, L1/2, SCAD and MCP.\n(5) ""GSE59491_expr.R"" ----  Processing original data of GSE59491.\n(6) ""GSE73685_expr.R"" ----  Processing original data of GSE73685.\n(7) ""Hyper_test.R"" ----  Calculate the number of overlapping genes selected by hypergeometric test.\n(8) ""RFE_ROC_clear.R"" ---- SVM classifier.\n(9) ""feature_overlap_clear.R"" ---- SVM: Intersection of feature subsets obtained by 5 RFE methods to identify biomarkers.\n(10) ""feature_select_clear.R"" ---- SVM: Extract features from the test set and verification set separately.\n(11) ""class_feature.R"" ---- SVM: Perform independent data set verification and draw ROC curve.\n(12) ""cluster_clear.R"" ---- SVM: Enrichment analysis of identified biomarkers.\n(13) ""ref_knn.R"" ---- SVM: KNN-RFE.\n(14) ""ref_nnet.R.R"" ---- SVM: NN-RFE.\n(15) ""rfsvm.py"" ---- SVM: RF-RFE.\n(16) ""svmrfe.py"" ---- SVM: SVM-RFE.\n(17) ""abrfe.py"" ---- SVM: AB-RFE.\n'], 'url_profile': 'https://github.com/zpliulab', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'Kharagpur, West Bengal', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Open_IIT-DA\nIn this project, we are given the geodemographic and other relevant data\nof an Auto Insurance Company that may impact its Customer Lifetime\nValue. We have to predict the Customer Lifetime Value for this company.\nCustomer Lifetime Value is the total revenue the client will derive in its\nentire relationship with a customer. Because we don’t know how long\neach customer relationship will be, we have to make a good estimate and\nstate Customer Lifetime Value as a periodic value - that is, we usually say\n“that this customer’s 12 month(or 24 months, etc) Customer Lifetime\nValue is $x.\nThe client also wants to know the types of customers that would\ngenerally give us more revenue.\n'], 'url_profile': 'https://github.com/mrinalyadav7-atom', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': [""EDA-and-ANN-for-Advanced-Regression-Problems\nDataset\nThe kaggle link to download the dataset: \nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\nContext\nThis dataset contains the various attributes required to predict the Saleprice of different housing lots.\nContents\n•\tSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n•\tMSSubClass: The building class\n•\tMSZoning: The general zoning classification\n•\tLotFrontage: Linear feet of street connected to property\n•\tLotArea: Lot size in square feet\\\n•\tStreet: Type of road access\n•\tAlley: Type of alley access\n•\tLotShape: General shape of property\n•\tLandContour: Flatness of the property\n•\tUtilities: Type of utilities available\n•\tLotConfig: Lot configuration\n•\tLandSlope: Slope of property\n•\tNeighborhood: Physical locations within Ames city limits\n•\tCondition1: Proximity to main road or railroad\n•\tCondition2: Proximity to main road or railroad (if a second is present)\n•\tBldgType: Type of dwelling\n•\tHouseStyle: Style of dwelling\n•\tOverallQual: Overall material and finish quality\n•\tOverallCond: Overall condition rating\n•\tYearBuilt: Original construction date\n•\tYearRemodAdd: Remodel date\n•\tRoofStyle: Type of roof\n•\tRoofMatl: Roof material\n•\tExterior1st: Exterior covering on house\n•\tExterior2nd: Exterior covering on house (if more than one material)\n•\tMasVnrType: Masonry veneer type\n•\tMasVnrArea: Masonry veneer area in square feet\n•\tExterQual: Exterior material quality\n•\tExterCond: Present condition of the material on the exterior\n•\tFoundation: Type of foundation\n•\tBsmtQual: Height of the basement\n•\tBsmtCond: General condition of the basement\n•\tBsmtExposure: Walkout or garden level basement walls\n•\tBsmtFinType1: Quality of basement finished area\n•\tBsmtFinSF1: Type 1 finished square feet\n•\tBsmtFinType2: Quality of second finished area (if present)\n•\tBsmtFinSF2: Type 2 finished square feet\n•\tBsmtUnfSF: Unfinished square feet of basement area\n•\tTotalBsmtSF: Total square feet of basement area\n•\tHeating: Type of heating\n•\tHeatingQC: Heating quality and condition\n•\tCentralAir: Central air conditioning\n•\tElectrical: Electrical system\n•\t1stFlrSF: First Floor square feet\n•\t2ndFlrSF: Second floor square feet\n•\tLowQualFinSF: Low quality finished square feet (all floors)\n•\tGrLivArea: Above grade (ground) living area square feet\n•\tBsmtFullBath: Basement full bathrooms\n•\tBsmtHalfBath: Basement half bathrooms\n•\tFullBath: Full bathrooms above grade\n•\tHalfBath: Half baths above grade\n•\tBedroom: Number of bedrooms above basement level\n•\tKitchen: Number of kitchens\n•\tKitchenQual: Kitchen quality\n•\tTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n•\tFunctional: Home functionality rating\n•\tFireplaces: Number of fireplaces\n•\tFireplaceQu: Fireplace quality\n•\tGarageType: Garage location\n•\tGarageYrBlt: Year garage was built\n•\tGarageFinish: Interior finish of the garage\n•\tGarageCars: Size of garage in car capacity\n•\tGarageArea: Size of garage in square feet\n•\tGarageQual: Garage quality\n•\tGarageCond: Garage condition\n•\tPavedDrive: Paved driveway\n•\tWoodDeckSF: Wood deck area in square feet\n•\tOpenPorchSF: Open porch area in square feet\n•\tEnclosedPorch: Enclosed porch area in square feet\n•\t3SsnPorch: Three season porch area in square feet\n•\tScreenPorch: Screen porch area in square feet\n•\tPoolArea: Pool area in square feet\n•\tPoolQC: Pool quality\n•\tFence: Fence quality\n•\tMiscFeature: Miscellaneous feature not covered in other categories\n•\tMiscVal: $Value of miscellaneous feature\n•\tMoSold: Month Sold\n•\tYrSold: Year Sold\n•\tSaleType: Type of sale\n•\tSaleCondition: Condition of sale\nThe 'test.csv' file contain all same attributes as of training set except that of the SalePrice attribute. We have to predict the Saleprice of the test set.\nAuthors\nThe work on the following project was done by Piyush Singla and Nikhil Sharma.\nLink to Piyush Singla kaggle account -> https://www.kaggle.com/mpiyu20 \nLink to Nikhil Sharma kaggle account -> https://www.kaggle.com/nikhilsharma4\n""], 'url_profile': 'https://github.com/nikhilSharma4', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Heights-and-Weights-Using-Logistic-Regression\nThis is a mini-project of Springboard Data Science study about logistic regression\n'], 'url_profile': 'https://github.com/jacquelineguo', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shehan-Irteza-Pranto', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shayannoorkhan', 'info_list': ['Updated Apr 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 4, 2020', 'C', 'MIT license', 'Updated Apr 21, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Oct 15, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/seantopham', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Karan1012', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Cleaning-Dataset-For-LinearRegression\nThis project is the first part of a two-part series. In the first part, you will blend and format data and deal with outliers.  For the second part, you will use your cleaned up dataset to create another linear regression model.\nThe Business Problem\nPawdacity is a leading pet store chain in Wyoming with 13 stores throughout the state. This year, Pawdacity would like to expand and open a 14th store. Your manager has asked you to perform an analysis to recommend the city for Pawdacity’s newest store, based on predicted yearly sales.\nYour first step in predicting yearly sales is to first format and blend together data from different datasets and deal with outliers.\nWe have the following information to work with:\nThe monthly sales data for all of the Pawdacity stores for the year 2010.\nNAICS data on the most current sales of all competitor stores where total sales is equal to 12 months of sales.\nA partially parsed data file that can be used for population numbers.\nDemographic data (Households with individuals under 18, Land Area, Population Density, and Total Families) for each city and county in the state of Wyoming. For people who are unfamiliar with the US city system, a state contains counties and counties contains one or more cities.\nPlease refer to this link for the power query I used to clean and merge the data.\nhttps://github.com/gmalekar/Cleaning-Dataset-For-LinearRegression/blob/master/DatasetCleaning.pbix\nPlease refer to this for the solution.\nhttps://github.com/gmalekar/Cleaning-Dataset-For-LinearRegression/blob/master/Cleaning%20Data%20Girish.pdf\n'], 'url_profile': 'https://github.com/gmalekar', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VeeMagwaza', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chinmayjoshi789', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['The published .html can be found at https://stevenmichiels.github.io/JohnsHopkins_2RegressionModel/.\nThe projects consists of multiple linear regression on the ""Auto"" dataset.\n'], 'url_profile': 'https://github.com/stevenmichiels', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rupendra-Billa', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\nCleaned the data then used Ridge Regression with an alpha value of 3.25 to predict the Sale Price.\n'], 'url_profile': 'https://github.com/ajepstein', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': [""FedEx Cup Regular Season Performance\nLinear regression model to predict a PGA Tour player's regular season FedEx Cup points total using player statistics scraped from PGATour.com.\n""], 'url_profile': 'https://github.com/akedilok', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ishtiaqmahmud', 'info_list': ['R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'HTML', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020']}"
"{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': [""Simple Linear Regression & Multiple Linear Regression\nScikit learn\nTable of Contents\n\nInstallation\nProject Motivation\nFile Descriptions\nSteps /Aprroach /Method\nResults\nLicensing, Authors, and Acknowledgements\n\nInstallation\nI would recommend to use anoconda and jupyter libraries.\nNeed tweeter API access which can be obtained from filling twitter developer account. Note You may have to wait few weeks due to corona virus.\nLink: https://developer.twitter.com/en\nProject Motivation\nThis project will teach basic single linear regression and multiple linear regression and then we will make model and evaluate using metric.\nFile Descriptions\nAdvertisment.csv\nadvertisment data having column ('Tv','Radio','Newspaper', 'Sales')\nURL :http://faculty.marshall.usc.edu/gareth-james/ISL/data.html\nSteps Involved\n\nGathering Data\nExploratory Data Analysis\nCreating Simple Linear Regression\nMultiple Linear Regression Model\nModel metrics\n\nApproach\nwe will be using following methond for linear regression.\n\ncreating the model\nInterepting model coef\nFeature selection\nTrain/Test Split\nModel metrics\n\nLicensing, Authors, Acknowledgements\nMust give credit to Gareth James for providing the data.\nAuthor\nSaphal Adhikari\nBlog post : https://medium.com/@franticarsenal/linear-regression-with-scikit-learn-6139ea602905?sk=3a97bb60d6d328bbd8a530c78821b91c\n""], 'url_profile': 'https://github.com/saphal', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ishtiaqmahmud', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '414 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aitezazakhtar', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'Belo Horizonte', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/diogoeverson', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '161 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ea137', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['localWhiteMatterNoiseRegression\nCode and gear materials for fmriprep white matter noise regression\n'], 'url_profile': 'https://github.com/gkaguirrelab', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RakeshBhugra', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'Windsor,Ontario', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Modelling-Linear-Regression-with-R\n'], 'url_profile': 'https://github.com/jaipranav9957', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'Baltimore', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Applying the following Regression models:\n- Linear Regression\n- Lasso Regression\n- Ridge Regression\n- Random Forest Regression\n\non diamond dataset and compaing the R_scores of the regression models and checking for the performances of the models.\n'], 'url_profile': 'https://github.com/Shreya-Shetty', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}","{'location': 'Philadelphia, PA', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/oshostak', 'info_list': ['HTML', 'Updated Apr 25, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Python', 'Updated Aug 31, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'JavaScript', 'Updated Apr 23, 2020']}"
"{'location': 'Windsor,Ontario', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Modelling-Linear-Regression-with-R\n'], 'url_profile': 'https://github.com/jaipranav9957', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'Baltimore', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Applying the following Regression models:\n- Linear Regression\n- Lasso Regression\n- Ridge Regression\n- Random Forest Regression\n\non diamond dataset and compaing the R_scores of the regression models and checking for the performances of the models.\n'], 'url_profile': 'https://github.com/Shreya-Shetty', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'Houston, Texas', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['LinearRegression_HousePricePrediction\nThis github repository contains my work for the Module 2 Final Project.\n""For this project, you\'ll be working with the King County House Sales dataset. We\'ve modified the dataset to make it a bit more fun and challenging. The dataset can be found in the file ""kc_house_data.csv"", in this repo.\nYou\'ll clean, explore, and model this dataset with a multivariate linear regression to predict the sale price of houses as accurately as possible.""\n01_VMB_FastTrack_poariston.ipynb\n02_FinalWork_poariston.ipynb\nJupyter notebooks containing all coding. The first file prodcues a Minimum Viable Product, and the second one is the final attempt\ncolumn_names.md\nThe description of the column names can be found in the column_names.md file in this repository.\nkc_house_data.csv\nthe King County House Sales dataset\nMod2Project_BusinessPresentation_POAriston.pdf\nMod2Project_Presentation_POAriston.pdf\npresentations of my work in pdf format. Oneis more business oriented, the other more technical.\nOther related files:\nhttps://youtu.be/ktn7rHvYAVs\nhttps://youtu.be/orBP22_Bvws\nVideo of the technical presentation on my youtube channel (2 parts)\nhttps://poariston.github.io/the_one_with_all_the_predictions\nBlog post reflecting on this project\n'], 'url_profile': 'https://github.com/poariston', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['CROSS-VALIDATION-SIMPLE-LINEAR-REGRESSION\n'], 'url_profile': 'https://github.com/Maza455', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Oldkanyewest', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['LineerRegressionWithSenteticData\n'], 'url_profile': 'https://github.com/esoob', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Description of Kaggle Competition\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\nhousing regression1.ipynb\nObjective\nAn intitial regression model for perdicting sale prices of houses, given a dataset of characteristics.\nThe Kaggle Platform\nThis code serves to be an introduction to beginners who may want to participate in a kaggle competition using google colab.\nData Cleaning\n\nCheck datatype of each column\nCheck for missing data\nCombine both train and test set for data preparation\nRemove columns with more than 20% missing data (Set threshold)\nFor columns with missing data (dtype int / float), fill with the median (or mean)\nFor columns with missing data (dtype object), impute with most frequent value\n\nData Preparation\n\nCheck for missing data once more\nDrop any duplicate rows\nPerform one hot encoding for dtype object column values\nSplit train and test set\nWithin the train set, split into training and validation set\n\nRegression\nFor experimentation, run the training set on multiple models from sklearn.\nGet an idea of what are the best performing models. I used Root Mean Squared Log Error and R2 Scores for evaluation\nIn both cases, Random Forest Regressor performs the best.\nafter which, i tried a couple of things.\nFirst, I did minmax and standard scaling. None improved the score.\nSecond, I did feature dimensionality reduction via Principal Component Analysis. Still was not better.\nUltimately, after the first iteration, the socre was 0.15006, where I was ranked 2535 out of 4524 teams, which is not impressive.\nThis is the preliminary model. I will continue to work on the regression model to improve the score.\nhousing regression2.ipynb\nObjective\nTo improve the compeition score in the second iteration.\nMain Changes\n\nHaving prepared the dataset to numerize all object type data and cleaning the rest of the integer type data, there was 1000 estimators involved. Hence, I tuned the parameter n_estimator of the best performing model, the RandomForstRegressor, to 1000. The score improved from 0.15 to 0.14.\nNext, I decided to try XGBoost again despite it not being the best in the first iteration. This time I tuned the n_estimator parameter to 1000 and the score drastically improved. The RMSLE reduced to 0.12980 in the competition which stands as the best score so far.\nI was still skeptical. I tuned and optimised the parameters, colsample_bytree, learning_rate, subsample and max_depth. It generated the best test score, but the competition score was not better than 0.12980\n\nNext Step\nI will implement feature extraction/ feature importance to see what features are most important. The hypothesis is that not all factors affect the prices of houses the same, so identifying which estimators affect the target variable the most will be kept while variables that are not that significant will be removed.\nProbably some neural networks will be implemented as well.\n'], 'url_profile': 'https://github.com/ClarenceToh', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Using-PCA-with-Logistics-Regression\nWe are using PCA for dimension reduction in this code\nWe have a pre processed data for logistics regression, where 0 is for salary less than 50k and 1 for more than 50k\nOur motive here is to understand how PCA and its parameter works\n'], 'url_profile': 'https://github.com/anujjohri', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neeraj2296', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Linear Regression (Marketing Mix Model)\nDeveloping a Marketing Mix Model using Linear Regression in order to determine the impact of each marketing channel on sales and find ideal spending on each of them, including client presentation. Using the following Methods:\n\nMultivariate linear Regression (OLS, Ridge Regression)\nCross Validation and Scaling\n\nand the following libraries:\n\nStatsmodels, SKlearn for the Regression\nMatplotlib, Seaborn for visualization\n\n'], 'url_profile': 'https://github.com/SteveGrimm', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', '1', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'R', 'Updated May 15, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', '1', 'Python', 'Updated May 20, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020']}"
"{'location': 'Cincinnati', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['R--Boston-Housing-Price-prediction-using-Lasso-regression-Regression-Tree-and-Random-Forest \nGoal - The goal of the project is to compare the performance of the machine learning algorithms.  \nProblem Statement- The dataset had 14 variables which are listed in the later part of the report. \nIn this, medv: median price of the house is the target variable. Using rest feature variables and machine learning algorithms, \nwe will predict the medv value.\nRpubs link: https://rpubs.com/soodrk/598667\n'], 'url_profile': 'https://github.com/soodrk', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Sandyford, Dublin', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/atifferoz', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aquilescool', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Madison, WI', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': [""Linear-Regression-with-Stochastic-Gradient-Descent\nProgram uses Mean Squared Error (MSE) in calculating linear regression.  Program utilizes Iterative and Stochastic Gradient Descent in order to reduce the MSE for the data (finds a near-optimal linear regression fit).\nThe data in question is Ice Coverage intervals on Lake Mendota in Madison, Wisconsin from 1855 to 2019 (excluding unavailable years). The data was cleaned before analysis in Python. The cleaned data can be found in the .csv file in this repository titled 'CleanIceData.csv'\n""], 'url_profile': 'https://github.com/ryan-hayward', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '148 contributions\n        in the last year', 'description': [""Perform-Sentiment-Analysis-with-scikit-learn\nBuild a logistic regression model to classify movie reviews as either positive or negative. We will use the popular IMDB data set. Our goal is to use a simple logistic regression estimator from scikit-learn for document classification.\nTask 1: Introduction and Importing the Data\n\nIntroduction to the data set and the problem overview.\nImport essential modules and helper functions from NumPy, Matplotlib, and scikit-learn.\n\nTask 2: Transforming Documents into Feature Vectors\n\nRepresent text data using the bag-of-words model from natural language processing and information retrieval.\nConstruct the vocabulary of the bag-of-words model and transform the provided sample sentences into sparse feature vectors.\n\nTask 3: Term Frequency-Inverse Document Frequency\n\nIn information retrieval and text mining, we often observe words that crop up across our corpus of documents. These words can lead to bad performance during training and test time because they usually don’t contain useful information.\nUnderstand and implement a useful statistical technique, Term frequency-inverse document frequency (tf-idf), to downweight these class of words in the feature vector representation. The tf-idf is the product of the term frequency and the inverse document frequency.\n\nTask 4: Calculate TF-IDF of the Term 'Is'\n\nManually calculate the tf-idf of an example.\nApply scikit-learn’s TfidfTransformer to convert sample text into a vector of tf-idf values and apply the L2-normalization to it.\n\nTask 5: Data Preparation\n\nCleaning and pre-processing text data is a vital process in data analysis and especially in natural language processing tasks.\nStrip the data set of reviews of irrelevant characters including HTML tags, punctuation, and emojis using regular expressions.\n\nTask 6: Tokenization of Documents\n\nEnsures that k-means image compression is performed only on the slider widget's mouse release events.\nRepurpose the data preprocessing and k-means clustering logic from previous tasks to operate on images of your choice.\nVisualize how the image changes as the number of clusters fed to the k-means algorithm is varied.\n\nTask 7: Document Classification Using Logistic Regression\n\nFirst, split the data into training and test sets of equal size.\nThen create a pipeline to build a logistic regression model.\nTo estimate the best parameters and model, we employ cross-validated grid-search over a parameter grid.\n\nTask 8: Load Saved Model from Disk\n\nAlthough the time it takes to train logistic regression models is very little, estimating the best parameters for our model using GridSearchCV can take hours given the size of our training set.\nIn this task, load a pre-trained model that will later be used to find the best parameter settings, cross validation score, and the test accuracy.\n\nTask 9: Model Accuracy\n\nIn this final task, we take a look at the best parameter settings, cross-validation score, and how well our model classifies the sentiments of reviews it has never seen before from the test set\n\n""], 'url_profile': 'https://github.com/yashguptaab99', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '385 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BrunoRossiCarmo', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/hive-jamia', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Buffalo,NY', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['predict-charity-doners-using-regression-models\nIn this project, you will employ several supervised algorithms of your choice to accurately model individuals\' income using data collected from the 1994 U.S. Census. You will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Your goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual\'s income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with. While it can be difficult to determine an individual\'s general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features.  The dataset for this project originates from the UCI Machine Learning Repository. The datset was donated by Ron Kohavi and Barry Becker, after being published in the article ""Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid"". You can find the article by Ron Kohavi online. The data we investigate here consists of small changes to the original dataset, such as removing the \'fnlwgt\' feature and records with missing or ill-formatted entries.\n'], 'url_profile': 'https://github.com/1990huiyuan', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Alexandria', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Logistic-Regression-with-a-Neural-Network-mindset\nAs you keep learning new techniques you will increase it to 80+ % accuracy on cat vs. non-cat datasets. By completing this assignment you will:\n\n\nWork with logistic regression in a way that builds intuition relevant to neural networks.\n\n\nLearn how to minimize the cost function.\n\n\nUnderstand how derivatives of the cost are used to update parameters.\n\n\n'], 'url_profile': 'https://github.com/fahdmekawy', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}","{'location': 'Gujarat, India', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Kaggle-House-Prices-Advanced-Regression-Techniques\nLink of this notebook\nhttps://www.kaggle.com/janvichokshi/advanced-regression-techniques\n'], 'url_profile': 'https://github.com/JANVI2411', 'info_list': ['HTML', 'Updated Apr 21, 2020', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Python', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', '3', 'C', 'MIT license', 'Updated Jul 22, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Logistic-Regression-with-a-Neural-Network-mindset\nBuilding a cat classifier that recognizes cats with 70% accuracy!\n'], 'url_profile': 'https://github.com/HeyJay4625', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/S1WahyuPrasetyo', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': ['Linear-and-Logistic-regression-Algorithm-using-Python\nLinear and Logistic regression algorithm has been implemented from scratch using Python. Polynomial functions of order 1,2,3,4,5 which are equivalent to linear regression were plotting against true cubic relation to demonstrate under fitting and over fitting of the models. For Logistic regression, classifier has been built using Batch gradient descent and Stochastic gradient descent to compare them in terms of accuracy\n'], 'url_profile': 'https://github.com/rposhala', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '613 contributions\n        in the last year', 'description': ['ML-Linear-Regression-Boston-Housing-price-prediction\n#This project was done as part of Online course : MACHINE LEARNING using python\nCertificate of Completion from SKYFI LABS:  My Certificate \n(They checked and verified the output first and are satisfied.)\nHere is the link for the Evaluated Project:\nI have made a video Tutorial: My Project(Youtube Link) \nOverview of the Project:\n    >> This project is mainly about the Prediction of house prices of Boston Dataset which is present in Kaggle Dataset or UCI Dataset or we can directly import it from Scikit learn\n    \n    >> I have given clear instructions by commenting in the code only, On how to use the commands to predict and split the data and how to use the libraries.\n\n'], 'url_profile': 'https://github.com/balajisomasale', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/S1WahyuPrasetyo', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amplefield', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'IIT BHU, India', 'stats_list': [], 'contributions': '609 contributions\n        in the last year', 'description': ['Logistic-Regression-Handling-missing-data-problem\nIn this repository, the Logistic Regression model first fills the missing data in columns with the mean of the column data for the machine to utilize the data and we have also performed standardization on the data to avoid biases and make the model more accurate to predict true results.\nWe are here using the full data as both training and test data but we can also split the data into training and test data with the help of library functions so that we train our model on lets say 70% data and test it on the rest 30% data or you can use some new dataset to test.\nWe are in this problem predicting which coronavirus patients might require intensive care units based on the data available here. But also data has many missing value which we are filling by the median column data. We are also standardizing the data to avoid one feature be dominant and lead to biasing. If you split data into training and testing data, you must standardize both the training and testing data.\n'], 'url_profile': 'https://github.com/shubham1710', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/asepboy', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Africa', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Logistic-regression-algorithm-with-Neural-Network-Mindset\nBuild Logistic Regression from  scratch with a Neural Network Mindset using Python Programming Language\n'], 'url_profile': 'https://github.com/sowole-aims', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jagtapuday', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'HTML', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 25, 2020', 'Updated Apr 21, 2020', 'Python', 'Updated May 2, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['ECOMMERCE-DATA-ANALYSIS-USING-LINEAR-REGRESSION\nThe company is trying to decide whether to focus their efforts on their mobile app experience or their website, Also wants to concentrate on some of the key performance Indicators\n'], 'url_profile': 'https://github.com/jayram181', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'New York, New York ', 'stats_list': [], 'contributions': '599 contributions\n        in the last year', 'description': [""Project Title:\nThe_movie_db_API_Linear_Regression_Analysis\nGoal:\nThe goal of the project is to use the data from api.themoviedb.org and the multiple linear models to predict the movie gross\nand to show more insights of this dataset\nAttribute information\n\nGross profit  =Revenue - budget\nPopularity\nVote_avarage\nVote_count\nGenres\nRuntime\n\nEDA\n\nWe can see the average of total movie popularity slightly increased before 2019, however after 2019 popularity increased dramatically.\nHigher popularity higher profit\n'Runtime' does not affect profit significantly , because 'runtime' has limit. For example, a movie's runtime cannot longger than 5 hours or 24 hours. Mean of runtime is around 2 hours. If a movie runtime is very long, people will feel tidious.\nHigher vote_average higher profit\n\n\n\n\n\n\n\n\nModels:\n\nRegular Linear Regression\nRidge Regression\nLasso Regression\nNeural Network\n\nModel Evaluation:\n\n\nBased on RMSE and (R^2) score to evaluate the best model is Neural Network\n\n""], 'url_profile': 'https://github.com/melanieshi0120', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/asepboy', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Polynomial-Regression-to-Make-Salary-Prediction\n'], 'url_profile': 'https://github.com/S1WahyuPrasetyo', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Animesh-Bagchi', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['Linear-Regression-Project-to-predict-Housing-prices\n'], 'url_profile': 'https://github.com/ssubhash977', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['Logistic-Regression-Model-using-Titanic-data-set\n'], 'url_profile': 'https://github.com/ssubhash977', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': [""EDA-and-ANN-for-Advanced-Regression-Problems\nDataset\nThe kaggle link to download the dataset: \nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\nContext\nThis dataset contains the various attributes required to predict the Saleprice of different housing lots.\nContents\n•\tSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n•\tMSSubClass: The building class\n•\tMSZoning: The general zoning classification\n•\tLotFrontage: Linear feet of street connected to property\n•\tLotArea: Lot size in square feet\\\n•\tStreet: Type of road access\n•\tAlley: Type of alley access\n•\tLotShape: General shape of property\n•\tLandContour: Flatness of the property\n•\tUtilities: Type of utilities available\n•\tLotConfig: Lot configuration\n•\tLandSlope: Slope of property\n•\tNeighborhood: Physical locations within Ames city limits\n•\tCondition1: Proximity to main road or railroad\n•\tCondition2: Proximity to main road or railroad (if a second is present)\n•\tBldgType: Type of dwelling\n•\tHouseStyle: Style of dwelling\n•\tOverallQual: Overall material and finish quality\n•\tOverallCond: Overall condition rating\n•\tYearBuilt: Original construction date\n•\tYearRemodAdd: Remodel date\n•\tRoofStyle: Type of roof\n•\tRoofMatl: Roof material\n•\tExterior1st: Exterior covering on house\n•\tExterior2nd: Exterior covering on house (if more than one material)\n•\tMasVnrType: Masonry veneer type\n•\tMasVnrArea: Masonry veneer area in square feet\n•\tExterQual: Exterior material quality\n•\tExterCond: Present condition of the material on the exterior\n•\tFoundation: Type of foundation\n•\tBsmtQual: Height of the basement\n•\tBsmtCond: General condition of the basement\n•\tBsmtExposure: Walkout or garden level basement walls\n•\tBsmtFinType1: Quality of basement finished area\n•\tBsmtFinSF1: Type 1 finished square feet\n•\tBsmtFinType2: Quality of second finished area (if present)\n•\tBsmtFinSF2: Type 2 finished square feet\n•\tBsmtUnfSF: Unfinished square feet of basement area\n•\tTotalBsmtSF: Total square feet of basement area\n•\tHeating: Type of heating\n•\tHeatingQC: Heating quality and condition\n•\tCentralAir: Central air conditioning\n•\tElectrical: Electrical system\n•\t1stFlrSF: First Floor square feet\n•\t2ndFlrSF: Second floor square feet\n•\tLowQualFinSF: Low quality finished square feet (all floors)\n•\tGrLivArea: Above grade (ground) living area square feet\n•\tBsmtFullBath: Basement full bathrooms\n•\tBsmtHalfBath: Basement half bathrooms\n•\tFullBath: Full bathrooms above grade\n•\tHalfBath: Half baths above grade\n•\tBedroom: Number of bedrooms above basement level\n•\tKitchen: Number of kitchens\n•\tKitchenQual: Kitchen quality\n•\tTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n•\tFunctional: Home functionality rating\n•\tFireplaces: Number of fireplaces\n•\tFireplaceQu: Fireplace quality\n•\tGarageType: Garage location\n•\tGarageYrBlt: Year garage was built\n•\tGarageFinish: Interior finish of the garage\n•\tGarageCars: Size of garage in car capacity\n•\tGarageArea: Size of garage in square feet\n•\tGarageQual: Garage quality\n•\tGarageCond: Garage condition\n•\tPavedDrive: Paved driveway\n•\tWoodDeckSF: Wood deck area in square feet\n•\tOpenPorchSF: Open porch area in square feet\n•\tEnclosedPorch: Enclosed porch area in square feet\n•\t3SsnPorch: Three season porch area in square feet\n•\tScreenPorch: Screen porch area in square feet\n•\tPoolArea: Pool area in square feet\n•\tPoolQC: Pool quality\n•\tFence: Fence quality\n•\tMiscFeature: Miscellaneous feature not covered in other categories\n•\tMiscVal: $Value of miscellaneous feature\n•\tMoSold: Month Sold\n•\tYrSold: Year Sold\n•\tSaleType: Type of sale\n•\tSaleCondition: Condition of sale\nThe 'test.csv' file contain all same attributes as of training set except that of the SalePrice attribute. We have to predict the Saleprice of the test set we are given and store it in a excel sheet.\nAuthors\nThe work on the following project was done by Piyush Singla and Nikhil Sharma.\nLink to Piyush Singla kaggle account -> https://www.kaggle.com/mpiyu20 \nLink to Nikhil Sharma kaggle account -> https://www.kaggle.com/nikhilsharma4\n""], 'url_profile': 'https://github.com/m-piyu20', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'Jamunapur Chauraha Post- Inswardaspur Dist- Raebareli', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': [""Linear-Regression-Project-Usinng-Scikit-Learn-\nAn Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions. Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want. The company is trying to decide whether to focus their efforts on their mobile app experience or their website. This dataset is artificially created and here we will be using Python's populae Machine Learning library i.e. Scikit Learn for getting some interesting results using Linear Regression Model\n""], 'url_profile': 'https://github.com/MrBeast-Anirban', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-on-Salary-Data\nPython Integrated tools, PyCharm is used to implement the project.\n'], 'url_profile': 'https://github.com/Arifur-ratul', 'info_list': ['Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Apr 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques---Kaggle-Competition\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\n'], 'url_profile': 'https://github.com/Aurian-G', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Social Networking Ads Model_Logistic Regression\nThe model is a simple implementation of Logistic Regression . It takes 4 attributes and predicts whether the advertisement will result in user purchasing the particular product or not .\n'], 'url_profile': 'https://github.com/pyshan1999', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'Vitznau, Switzerland', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['CaseStudy-WQU-LinearRegressionSimple\nSimple Linear Regression is used to analyse significance of alpha and beta coefficients (for stock returns time series)\n'], 'url_profile': 'https://github.com/PankoAliaksandr', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['checkpoint-3-\n'], 'url_profile': 'https://github.com/Oumayma-mbarek', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Chronic-Kidney-Disease-Prediction-using-Logistic-Regression\nA forward step-wise logistic regression model built using SPSS to predict the risk of a patient getting Chronic Kidney Disease.\n'], 'url_profile': 'https://github.com/VIgneshGV91', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Fitting-a-Multiple-Log-Linear-Regression-Model\n'], 'url_profile': 'https://github.com/Gichere', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'Windsor, ON Canada', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['End-to-end-bluebook-bulldozer-price-regression\n'], 'url_profile': 'https://github.com/Khushbu-03', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '760 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PajarKharisma', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'Mountain View, CA', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': [""Computational-Skepticism\nMaking a model “\u200bTrustWorthy\u200b”\nAbstract\nProblem statement\nMaking a machine trustworthy and reliable is one of the most important goals of data science today. Models are many times used as black boxes, wherein we give a particular input, know little of what happens inside the model, and get an output. But an important question that often gets overlooked is 'Why?' In some cases, one might not care why a decision was made, it is enough to know that the predictive performance on a test dataset was good. But in other cases, knowing the ‘Why’ can help learn more about the problem,the data and the reason why a model might fail. In cases such as cancer detection, self-driving cars or other critical places that can involve life and death, this becomes extremely crucial.\nProposed solution\nIn this project we aim to build a system that can be used to transform an unassured model in to an assured and dependable one. We will uncover interpretability of a simple model and a complex model (deep learning). For both the cases, the model interpretability will be explained on the basis of Expressive Power, Translucency, Portability and Algorithmic complexity. Supporting code will also be provided to help showcase the theory in real-world applications. Establishing local interpretability for single predictions and group predictions using model-agnostic methods is the ultimate goal of this project.\nProject Members:\nAbhishek Gargha Maheshwarappa\nKartik Kumar\n""], 'url_profile': 'https://github.com/kartikkumar7', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Does higher current account balance make a better place to conduct business?\nRegression analysis and diagnostics of the relationship between current account balance and the Ease of Doing Business Score, using the Doing Business dataset from the World Bank.\nThis is an individual final project for my Quantitative Methods II course at University of California, San Diego. \n\nRead the Stata code here: https://github.com/ahuang95/World-Bank-EDB-and-Current-Account-Balance/blob/master/qm2_individual-analysis-project.do\nRead the analysis report here: https://github.com/ahuang95/World-Bank-EDB-and-Current-Account-Balance/blob/master/qm2-iap-report.pdf\n\n'], 'url_profile': 'https://github.com/ahuang95', 'info_list': ['1', 'HTML', 'Updated May 7, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 24, 2020', 'Stata', 'Updated Jun 28, 2020']}"
"{'location': 'Hinjewadi Phase 3 pune', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Predicting-C02-Emission-Based-on-engine-size\nIn this notebook i used linear regression to predict co2 emission of car based on their engine size.\n'], 'url_profile': 'https://github.com/i-am-akash', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jaredspickard', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DavidAHo', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '495 contributions\n        in the last year', 'description': ['Linear Regression Project\nCreated three models to predict which variables have the most significant impact on base salary for NYC City employees.\nOverview\nNYC Open Data contains a Citywide Payroll Data (Fiscal Year) dataset with 3,333,080 individual salary observations (one listing per employee, per year) for the fiscal years of 2015-2019. Using this data, we evaluate which factors contributed most significantly towards employee base salary.\n\nData Cleaning\nColumns that were irrelevant to predicting base salary, overly correlated with base salary or contained a high level of variability were dropped from the dataset.\n\nLast Name\nFirst Name\nMid Init\nPayroll Number\nLeave Status (kept active entries only)\nTitle Description\nRegular Gross Paid\nTotal Other Pay\nPay Basis (kept FT only)\nBorough: Staten Island (no data)\n\nExploratory Data Analysis\nOver the past 5 years, the average city employee salary has increased by approximately $10,000.\n\nThe borough where an employee works has an impact on mean base salary.\n\n\n\nBorough\nAverage Salary 2015-2019\n\n\n\n\nBronx\n$64,444\n\n\nBrooklyn\n$65,521\n\n\nManhattan\n$72,903\n\n\nQueens\n$68,192\n\n\n\nFeature Engineering & Selection\nThe Years Worked column is an engineered feature which calculates total years worked based on the Agency Start Date through December 31, 2019.\nThe continuous variables were scaled through normalizing. Potential linear relationships between normalized variables and base salary were evaluated. The base salary was then logged so as to make the model more linear. The data was divided into a test and train to create Ridge, Lasso and Linear Regression models. After comparing the Residual Sum Mean Squared Errors (RMSE), the Ridge Model performed sightly better than the Linear Regression Model. Finally we did a K-Fold test to determine the best alpha.\n\nModel Selection\nThe best performing model, or the Ridge Model, predicted the test data within .634 standard deviations. An alpha of .01 was used, which had a slightly lower RMSE.\n\nTools\n\nLinear Regression\nRidge Model\nLasso Model\nK-Fold Test\n\nData Sources:\nNYC Open Data: Citywide Payroll Data (Fiscal Year)\nContributors\nMichael Armistead & Meagan Rossi\n'], 'url_profile': 'https://github.com/meaganrossi', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': [""regime-detection\nDataset:\nBig Macroeconomic dataset from FRED St. Louis desiged by McCracken and Ng (2015).\nInvolves 129 macroeconomic monthly time series data from 1959 to 2018.\n8 Categories: Output and income, labor maket, housing, consumption, orders and inventories, money and credit, interest and exchange rate, prices in the stock market.\nNBER Recession Dates:\n• Labelling based on NBER dataset\n• 8 recession periods during the time period in consideration\n• 628 'normal' periods and '93' recession periods\nData Cleaning:\n• Removal of variables with missing observation/ imputation of some sort\n• Add lags of all variables as additional features\n• Test stationarity of the time series\n• Standardize the dataset\nAdd lags of the variables as additional features:\n• Add 3, 6, 9, 12, 18 month lags for each variable\n• Shift labels for 1 month ahead prediction\n• 699 observation points and 710 features\nStationarity:\n• Augmented Dickey Fuller Test. Null hypothesis of ADFis that the time series is non stationary with the alternative that it is stationary\n• If p value > significance level, we cannot reject null hypothesis. Then take first order difference\n• adfuller function from statsmodels is used\nStandardization:\n• Standardization of feature vectors by removing mean and scaling to unit variance\n• StandardScaler from scikit-learn is used\nMethodology:\n• Perform feature selection to get the most important variables for the forecasts\n• Separate dataset into training and validation datasets. 1960 - 1996: Training, 1996 - 2018: Validation\n• Evaluate the performance of ML Algos on training set with Cross Validation\n• Select the best performing models based on average accuracy and std dev of the CV results. Logistic Regression chosen as benchmark\n• Make predictions on the validation dataset with selection models. Use GridSearchCV to find the best combination of hyperparameters. Evaluate the validation modela nd report accuracy metrics.\nCross Validation:\n• K Fold CV used:\n○ Train the model on (k-1) folds of the training data\n○ The resulting model is validated on the remaining part of the data\n• 'TimeSeriesSplit' is CV technique for time series data. Use first k sets as training, (k+1) as test set\nEvaluation Metric:\n\nROC AUC Score\n\n""], 'url_profile': 'https://github.com/fagan2888', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/melissa-stephen', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['Panel Data Analysis of Traffic Fatalities\nThe purpose of this report is to apply and compare panel data regression techniques to modeling the importance of various numerical and legal factors in driving fatality rates across all 50 states. The procedure will be as follows:\n\n\nWe perform a very thorough exploratory panel data analysis in order to understand every possible caveat of the dataset.\n\n\nWe then move to an OLS model, pooling across states but controlling for year via indicator variables.\n\n\nWe then fit a fixed effects (FE) model, and discuss its advantages to pooled OLS such as  heterogeneity bias, and assess all model assumptions through residual diagnostics of both models.\n\n\nWe compare a fixed effects regression model to a random effects regression model, and interpret the coefficients of the FE model, taking into account statistical significance.\n\n\nWe conclude with a discussion of standard errors associated with heteroskedasticity and serial correlation.\n\n\nThe data sets is provided by the textbook ""Introductory Econometrics: A Modern Approach, 6e"" by Jeffrey M. Wooldridge and cited below:\nhttps://rdrr.io/cran/wooldridge/man/driving.html\n'], 'url_profile': 'https://github.com/siduojiang', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['MushroomEdibility\n'], 'url_profile': 'https://github.com/miguelxngx', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'Vitznau, Switzerland', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['CaseStudy-WQU-InvestingInIndex\nWQU case study which analyses returns on SP500 and Nasdaq Indices from 2015 to 2017. Linear Regression analysis. Normality test\n'], 'url_profile': 'https://github.com/PankoAliaksandr', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}","{'location': 'Vellore, India', 'stats_list': [], 'contributions': '696 contributions\n        in the last year', 'description': ['Student-Grade-Preediction-Model\nA simple Machine Learning model using Linear Regression on a UCI Student Dataset to Predict the final grade of Students.\n'], 'url_profile': 'https://github.com/shaurya-src', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'Python', 'Updated Jan 4, 2021', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Oct 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'MIT license', 'Updated Sep 14, 2020', 'Python', 'Updated Apr 20, 2020', 'R', 'Updated Apr 26, 2020', 'Python', 'Updated Nov 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NadiaBlostein', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'San Francisco ↔️ Prague ', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['fertility-predictive-analysis\nPredicting the chance of being and not being fertile using decision trees and logistic regression with R.\n'], 'url_profile': 'https://github.com/thanhtamluu', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""endpoint-difference-checker\nIf there's an endpoint that needs to be regression tested, this would be the tool to do it.\nThis runs on npm/node. To start this, make sure to have node installed, then run npm i.\nTo boot this, run node . --remoteURL=www.<remoteMaster>.com\n""], 'url_profile': 'https://github.com/JosephmLin', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['house_price_prediction\nThis is a simple machine learning model which uses regression to predict the price of the houses.\n'], 'url_profile': 'https://github.com/prajakta02', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'durgapur, west bengal, india', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': [""Predicting-ages-of-bones-from-X-Rays\nThis notebook explore the dataset used in the Pediatric Bone Age Challenge, 2017, also known as the RSNA(Radiological Society of North America) and attempts to predict the ages of children using X-Ray images of their hands.More information about the dataset and the dataset itself can be found on kaggle here.\nModel description\nThe notebook uses the pre-trained model Xception to predict ages.The ImageDataGenerator class from keras has been used to enable efficient loading, preprocessing and augmentation of the dataset.The model obtained a mean absolute error of approximately 7.6 months.\nRequirements\n\n Tensorflow: To use Xception and related functions for training and preprocessing( keras is included in tensorflow)\n Numpy: For computation on the dataset\n Pandas: To load csv files and plot data\n Matplotlib: For creating visualisations\n Seaborn: For creating visualisations\nSklearn: To split data into training and validation\n\nResults\n\nImprovements possible\nInceptionResNetV2 might perform slightly better but it wasn't possible to train it because of the large size of its weights.There are many combinations of models possible using different pre-trained models, optimizers and learning rates.\nThe same notebook is available on kaggle here.\n""], 'url_profile': 'https://github.com/NitishaS-812k', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Linear-Regression-and-Ensemble-tech-using-Classification-using-hard-voting\nLinear regression - Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.  Voting - You can train your model using diverse algorithms and then ensemble them to predict the final output. Say, you use a Random Forest Classifier, SVM Classifier, Linear Regression etc.; models are pitted against each other and selected upon best performance by voting using the VotingClassifier Class from sklearn.ensemble. Hard voting is where a model is selected from an ensemble to make the final prediction by a simple majority vote for accuracy.\n'], 'url_profile': 'https://github.com/kdmac', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['ELISA-Analyzer-using-Four-Parameter-Logistic_4PL_Regression\nIntroduction\nThe enzyme-linked immunosorbent assay (ELISA) is quantitative method for measuring the concentration of biological soluble factors in liquid samples, commonly used in research, biotechnology and medical laboratories.\nThe method involves loading liquid samples in plate wells that specifically bind a target factor (usually a protein), followed by adding a colorimetric compound that produces a detectable signal, which is proportional to the concentration of the target protein.\nTo render the method quantitative (i.e. allowing determine the exact concentration of proteins in the test samples), a standard curve is used in each assay. The standard curve consist of a serial diluted samples that contain known concentrations of the target protein. The Optical Density (OD; the measured color intensity) and the know concentrations are used for building a standard-curve (std curve) of OD signal as a function of sample concentration. This standard curve is then used to interpolate the concentration of the tested samples.\nDue to the nature of biological reactions and the method used for measuring the OD, which have lower sensitivity at higher protein concentrations, a 4 Parameter Logistic (4PL) model is used for generating the standard curves, to accurately determine the concentration in the test samples.\nELISA Analyzer Description\nELISA results in research laboratories are usually analyzed manually, which is a time consuming and error prone procedure. In the current project, an automatic ELISA analyzer was established to improve the accuracy and reproducibility of ELISA results.\nIn general, ELISA analysis involves the following steps (which are implemented in the attached program):\n\nLoading the OD values of the tested and standard curve samples.\nSubtracting OD values of Blank negative controls (samples without any protein that represent the background signal).\nConstructing the Standard Curve by a 4PL model of OD as a function of protein concentrations.\nCalculating the concentration of tested samples using the standard curve.\nExporting the calculated concentrations of the tested samples into Excel file report.\n\nPlease note, samples with OD values that falls outside the signal range of the standard curve are excluded from the analysis due to inability to accurately determine their concentration. These samples are labeled as below lower-limit-of-quantification (LLOQ) or above upper-limit-of-quantification (ULOQ). Samples with high protein concentration are diluted before the assay, and the dilution factor is being considered in the analysis to calculate the samples\' actual concentrations.\nELISA Analyzer inputs\nThe ELISA analyzer takes as an input an Excel file (""ELISA Tamplet.xlsx""), in which each work sheet represent a different ELISA plate (usually more than one plate is ran in each experiment). In each work sheet, the user need to copy and paste sample OD values (into ""sample ODs"" table), the dilution factor of each sample (into ""Sample dilution factors"" table) and standard curve OD values (into ""Standard curve OD"" table).\nWhen running the program, the user will be asked to enter the following:\n\nEntre the file name (press Enter for using the default ""ELISA Tamplet.xlsx"" name).\nHow many plates would you like to analyze? (press Enter for default value of 3).\nEnter the highest std curve point (press enter for using the default value of 1600 pg/ml)\nEnter the dilution factor of standard curve (press Enter for default value of 2).\n\nELISA Analyzer output\nThe program will create an Excel report with the following sheets:\n\nCalculated concentrations: samples\' calculated concentrations\nErrors (ODs outside std range): indicating samples with OD signal below LLOQ or above ULOQ.\nConcentration without (w_o ) errors: sample concentration, excluding samples below LLOQ or above ULOQ.\nSample OD - raw data\nSample OD minus blank: after subtracting background signal.\nSample dilution factors.\nStd curve details: including model parameter values, goodness of fit (R^2), and standard curve images.\n\nFiles included in the current repository\n\nReadMe file.\nProgram Jupyter notebook.\n""ELISA Tamplet.xlsx "" file (including example OD values for 3 plates).\nExample ""ELISA_4PL_Analysis_Report.xlsx"" output file.\n\n'], 'url_profile': 'https://github.com/Omri-Matalon', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 23, 2020', 'HTML', 'Updated Apr 20, 2020', 'JavaScript', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'hyderabad', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['features-influence-on-car-price----multiple-linear-regression\nYou are required to model the price of cars with the available independent variables.\nIt will be used by the management to understand how exactly the prices vary with the independent variables.\nThey can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels.\nFurther, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/krupabdhi', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""Regularized-Linear-Regression-and-Bias-v.s.-Variance\nThese scrpits implements regularized linear regression to predict the amount of water flowing out of a dam using the change\nof water level in a reservoir.\nThey are Matlab files.\nYou run the main script called main ''main_waterRes.m''\nThis will call all the other necessary files to calculate the necessary steps.\n""], 'url_profile': 'https://github.com/jgmartinez90', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/melvin97n', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '480 contributions\n        in the last year', 'description': ['House-Sales-in-King-County-USA\n'], 'url_profile': 'https://github.com/Haizhuolaojisite', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'Atlanta', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Bike-Sharing-Usage-Forecast\nRegression analysis to forecast the usage of bike-sharing systems in New York City using weather and demography related explanatory variables\n'], 'url_profile': 'https://github.com/nevinthomas', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'MATLAB', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'HTML', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'R', 'Updated May 8, 2020']}"
"{'location': 'new delhi', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Predict-Carbon-dioxide-Emission-of-cars\nUsing scikit-learn to implement simple linear regression. we will make a simple machine learning model to Predict Carbon dioxide Emission of cars.\n\n\nInstall Wget to download dataset from link\n\n\nThere are different model evaluation metrics, lets use MSE here to calculate the accuracy of our model based on the test set:\nMean absolute error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it’s just average error.\nMean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It’s more popular than Mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.\nRoot Mean Squared Error (RMSE): This is the square root of the Mean Square Error.\nR-squared is not error, but is a popular metric for accuracy of your model. It represents how close the data are to the fitted regression line. The higher the R-squared, the better the model fits your data. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\nHi@antiksaini.com\n'], 'url_profile': 'https://github.com/antiksaini', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Emo_detect\nFace recoginition and emotion detection using 2 hidden layer MLP with Xavier initialization, a Support Vector Machine(SVM) and Logstic Regression.\nMultiple classifiers have been used on many different feature spaces to get an idea of which one works good. I have used a 2 hidden layer Multi Layer Perceptron(MLP) with Xavier Initialization, a Support Vector Machine(SVM) and Logistic Regression.\nSeveral feature spaces are used in which some are combinations. For all the datasets, all these datasets work sufficiently good on all feature spaces except PCA and kernel PCA spaces. PCA and kernel PCA do not capture the different classes as in PCA, we remove the components irrespective of the importance as we do not use labels for PCA or KPCA. LDA overcomes this by using the labels to do a better job in dimensionality reduction and further classification. The best combination was found to be SVM on RESNET.\nSimilarly, we use labelled images and a multi layer perceptron to classify the different emotions. This could be used to detect emotions of people in certain areas and allow authorities to monitor suspicious individuals if they consistently show negative emotions.\n'], 'url_profile': 'https://github.com/Rostux', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '233 contributions\n        in the last year', 'description': [""Keep On Closing\nGraduate Course: Quantitative Analysis for Business\nThis was an individual project from a course on Multiple Linear Regression (a supervised machine learning technique for quantitative analysis). Below is the prompt from the client:\n\nHello! I hope this message finds you well! After our last project predicting house prices was so successful, I thought we could work on a follow up project. Since the last time I have collected more data to improve the model to predict house prices. It was clear that I needed at least one other predictor, so I decided to start recording the style of architecture of each property. I specialize in three types of Architectures: Craftsman, Queen Anne, and Victorian. The new data that I collected is in the attached data file.\nIn our last project we verified how much the square footage of the house affects the price of the property, but I don't know how this relationship is affected when we take the architecture style into consideration. I would like to better understand how both the square footage and the style of the house affects the price so that I can better target specific homes to make the most profit on a sale. Can you help me to better understand how both variables affect the Price of these properties?\n\nPlease click here to visit the previous project with this client!\nAnalysis Summary\n1) Understanding the relationships:\nTo understand how the relationship of square footage with house price will change after taking architecture style into consideration, I started with some data exploration to see if I have any evidence to believe that architecture style can be correlated with the House price. I have used Architecture Style and Type interchangeably in the report. Also, numerical summaries of the variables can be found in Table 1 and 2 below.\nTable 1: Descriptive Statistics - Price and Sqft\n\nTable 2: Descriptive Statistics - Type\n\nI produced three box plots of Price variable for each of the architectural type and noticed that all three boxplots are considerably different from each other (see Figure 1). Each box plot gives me a sense of the Price distribution by showing me the minimum, first quartile below which 25% of the data lies, median below which half of the data lies, third quartile below which 75% of the data lies and maximum values of the Price for a particular architecture type. From the box plots, I found that the median price of Victoria type is highest, followed by the median price of Craftsman type and then, the median price of Queen Anne type. Similar sequences were observed for minimum, maximum, first and third quartile values of the Price among the three types. This gave me a strong indication that the type variable can be correlated with the response variable so I decided to include the variable term in the model.\nFigure 1: Box plot – Price vs Architecture Type\n\nMoreover, I also generated a scatter plot with Price on Y-axis, Sqft on X-axis and data points colored by Type variable (see Figure 2). I noticed that Sqft can have a positive linear correlation with the response so I included it in the model. Interestingly, when I added reference line for each of the three type of houses in the scatter plot, I found that all of them suggests a linear correlation with the response. Also, it was visible that reference lines are slanted to each other (different slopes). In other words, it showed different house types may change the relationship between square footage and house prices differently. It motivated me to further investigate this effect by including interaction terms between these two variables in the model.\nFigure 2: Scatter plot – Price vs Sqft (Colored by Type)\n\nAfter estimating the coefficients, I revealed three important correlations of Square Footage and Architecture Type with House Price according to the model (see Table 5). First, a 100 unit increase in the Square footage is associated with 3251.20 dollars increase in the average house price for Queen Anne house types. Second, a 100 unit increase in the Square footage is associated with 12907.5 dollars increase in the average house price for Craftsman house types. Third, a 100 unit increase in the Square footage is associated with 22569.30 dollars increase in the average house price for Victorian house types.\nTable 5: Coefficients\n\nFor the house types, the model estimates that the change in average house prices is roughly 14915.89 dollars higher for Victorian types than Queen Anne types when square footage is zero. In addition, the model estimates that the change in average house prices is 29749.83 dollars higher for Craftsman types than Queen Anne types when square footage is zero. Also, the average house price for Queen Anne house types is roughly 166596.77 dollars when square footage is set to zero. Please note that a house of zero square footage is practically infeasible. These values are theoretical to help the client understand the individual correlation of different house types with the house prices. In practice, I will always have some value of square footage for a particular house.\nOne more thing, only the intercept and interaction terms are found to be statistically significant at 5% significance level. By this I mean that the relationship between the square footage and house price is affected by the type of house which I also expected during the data exploration phase. However, the individual predictor terms do not provide values relative to the variability in the House Price i.e., these terms will not add much to the response value according to the model.\n2) Limitations of the analysis:\nBased on the analysis, it is crucial to keep in mind that the relationships determined are only valid for house area values (Sqft) ranging from roughly 813.80 ft2 to 3284.65 ft2 and architecture types of Queen Anne, Victorian and Craftsman. In addition, 91.3% of the variability in the model is explained by this model after accounting for the model size. This is a great value and I will not actively encourage the client to collect additional features for future analysis considering the costs associated with data collection. However, if the client wishes to further improve the variability explained by the model or to understand the relationship between a new variable and the House Price, he may consider some of the following factors for that purpose: 1) Distance from frequently visited places such as Supermarket, Hospital and Airport, 2) Age of the property 3) Current state of local real estate market (Recession or not) and 4) Condition of the house.\nAppendix\n1) Statistical analysis:\nI summarized the data and found that the (mean, minimum, maximum) values of Price and Sqft are roughly (475218.29, 142585.53, 939780.40) and (1907.50, 813.80, 3284.65), respectively (see Table 1).  I discovered that 42 percent of all the houses in the dataset are Victorian type, 40 percent of all the houses are Craftsman type and rest of the houses are of type Queen Anne (see Table 2). After accounting all information, I hypothesized the following Multiple Linear Regression model for further study:\n Yi = β0 + β1Xi + β2D1i + β3D2i + β4D1iXi + β5D2iXi + εi\n\nwhere Yi is the price in USD of the ith house, Xi is the area in square foot of the ith house, D1i and D2i are two dummy variables where 1 stands for Craftsman and Victorian type, respectively and Queen Anne type is the reference level for the ith house, β4D1iXi and β5D2iXi are the interaction terms between X, D1 and X, D2, respectively, β0, β1, β2 and β3 are the intercept, slope and coefficients of two dummy variables, β4 and β5 are interaction effects, ε i is the error term with the following assumptions: εi ~ indp. Normal(0, σ).\nTable 3: Model Summary\n\nAfter running the regression for this model, I got the estimated co-efficient β0, β1, β3, β4, β5 and β6 as roughly 166596.77, 32.51, 14915.89, 29749.830, 96.56 and 193.181, respectively (see Table 5). From table 3, I can note that the R square for this model came out to be 0.922 which means that 92.2% of the variability in this Price is explained by this model. Also, adjusted R square value is 0.913 which means that 91.3% of the variability is explained by the model after accounting for the model size. Note that the values of R square and adjusted R square are very close indicating that the additional terms do not have a big penalty relative to our sample size of 50. The standard error of the estimate is roughly 58336.03 which is also good relative to the mean house price of 475218.29 USD approximately. Finally, as reported in Table 4, the result of F-test shows that the model is statistically significant with a p-value of nearly 0.000 which is less than 0.05 at 5% significance level i.e., the model is found to be useful as a whole. These different criteria strongly suggest that the model should be considered useful.\nTable 4: ANOVA\n\nTo check the validity of the model, I first looked at the Unstandardized Residual vs Unstandardized Predicted value plot (see Figure3). The plot doesn’t show any clear pattern demonstrating that the model has no heterogeneity issue and it fits well. In addition, in Figure 4 I see that Q-Q plot is aligned with the normal distribution diagonal line suggesting the error terms are almost normal i.e., the model has no normality issue. Lastly, I did not find any evidence of time series structure in the data so no need to perform Durbin-Watson test i.e., no independence issue. Hence, I do not have any evidence to reject the model based on validity issues.\nFigure 4: Normal Q-Q Plot of Unstandardized Residual\n\nFurthermore, I conducted hypothesis testing for the intercept, slope and interaction coefficient at 5% significance level (see Table 5). For β0, the null and alternative hypotheses were as follows: H0: β0 = 0; HA: β0 ≠ 0. Since the P-value (0.010) is less than 0.05 at 5% significance level, I reject the null hypothesis i.e., β0 is statistically significant. For β1, β2 and β3, the null and alternative hypotheses were as follows: H0: β1 = 0; HA: β1 ≠ 0, H0: β2 = 0; HA: β2 ≠ 0 and H0: β3 = 0; HA: β3 ≠ 0, respectively. Since the P-values for β1 (0.265), β2 (0.836) and β3 (0.688) are more than 0.05 at 5% significance level, I fail to reject the null hypotheses i.e., β1, β2 and β3 are not very different from what I would expect if they were zero, according to the model. For β4 and β5, the null and alternative hypotheses were as follows: H0: β4 = 0; HA: β4 ≠ 0 and H0: β5 = 0; HA: β5 ≠ 0, respectively. The P-values came out to be more than 0.05 for both β4 (0.010) and β5 (0.000) i.e., the interaction effects are statistically significant at 5% significance level and there is some interaction between the dummy variables and Sqft that will change the relationship of Sqft with House Price as D1 (Victoria Type) or D2 (Craftsman Type) changes.\n""], 'url_profile': 'https://github.com/SagarBansal7', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Calcutta, India', 'stats_list': [], 'contributions': '215 contributions\n        in the last year', 'description': ['FIFA Young Talent Analysis\nFIFA player ratings are influenced by a big number of factors. Insights into these factors and prediction of the overall rating can be seen as a regression task. This analysis dives head first into this challenge through Jupyter Notebooks, drawing statistical inferences from data in an end-to-end manner, with pipelines for exploratory data analysis, data preparation and cleaning, and regression modeling.\n\nDataset\nObtained through Kaggle, scraped from official servers, which contains over 100 features for around 18K players over 6 years.\n\nPlease use below links as notebooks might not render on Github.\n\n\nExploratory Data Analysis Notebook\n\n\n\nFrameworks used -\n\nPostgreSQL\nPandas\nSeaborn\nStatsmodels\n\nComponents -\n\nUnivariate Analysis\n\nDistribution Plots\nNormality of Distributions\n\n\nMultivariate Analysis\n\nJoint Scatter Plots\nCategorical Plots\n\nViolin Plots\nBox Plots\n\n\nCorrelation Heatmaps\n\n\nRegression Analysis\n\nOrdinary Least Square coefficients\nAdjusted R-2 and tests of Statistical Significance\n\n\n\n\n\n\nData Preparation Notebook\n\n\n\nFrameworks used -\n\nSciPy\nScikit-Learn\n\nComponents -\n\nLinear Relationships\n\nPearson Correlation\n\n\nDistribution Analysis\n\nNormality of Distributions\nData Transformations - Box-Cox\nResidual Plot\n\n\nJoint Scatter Plots\n\nOutlier Detection\n\n\nDimensionality Reduction\n\nPrinciple Component Analysis\n\n\nCategorical Features\n\nFeature Engineering\nInteraction Features\n\n\n\n\n\n\nRegression Modeling Notebook\n\n\n\n\nFrameworks used -\n\nScikit-Learn\nXGBoost\n\nComponents -\n\nLinear Approaches\n\nOrdinary Least Squares\nRegularized Linear Regression\nSupport Vector Regression\nBagging SVR and Lasso\n\n\nTree Based Approaches\n\nDecision Tree\nRandom Forest\nExtremely Randomized Trees\n\n\nBoosting Approaches\n\nAdaboost\nExtreme Gradient Boosting\n\n\nStacking Ensemble\nCorrelation of Predictions\nXGBoost Metalearner\nError Analysis\n\n'], 'url_profile': 'https://github.com/theavicaster', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SanjanaBuchala', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tafannumtahiyat', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'San Ramon, CA', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/satp42', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Hyderabad , India', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Machine-learning-projects\nThis includes projects of Supervised : such as Regression - SLR , MLR ,Classification - SVM , Log Reg , Dec Tree , KNN and Unsupervised : K Means , Heirarchical & DBSCAN & aslo recommendor systems\n'], 'url_profile': 'https://github.com/DheerajPranav', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/oryagol', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Flight-delay-prediction\nPlease use the data from Kaggle: https://www.kaggle.com/usdot/flight-delays/data\nDomain Background - Air Traffic\nAerial commute is increasingly important as the globalization advances and the world population grows. However, the air traffic is also becoming a challenge, especially for the most used regional hubs. While transportation infrastructure is mainly a role for the governments, predicting the flight delays may be accessible for private initiative and it will benefit those passengers running tight on schedule by allowing them to reorganize their tasks on advance.\nThe most common causes of flight delays are varied. While some are not related to accessible data, others are within reach. The inaccessible data will remain as noise caused from security, maintenance and disaster issues. The accessible data are weather and congestion that hopefully will shed some light to predict some of the flight delays.\nData Analysis and Data Processing\nIn order to have a better understanding of the data and have a spatial vision of the quantitative variables Exploratory Data Analysis is performed and PCA, k-means is employed for dimensionality reduction.\nProblem Statement - How much will be the flight delay?\nBasically, the predictive model shall be able to answer the following question:\nGiven the departure information, by how many minutes will be the arrival of the flight be delayed?\nModel validation\nEmployed various supervised machine learninh techniques such as Decision Trees with Minimum Cost Complexity Pruning, Random Forests, Bagging with trees, Artificial Neural Network and KNN and performed hyper-parameter tuninig for optimal results.\n'], 'url_profile': 'https://github.com/GauriGupta19', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Apr 24, 2020', 'Python', 'Updated Jul 24, 2020', '1', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020', 'MIT license', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 25, 2020', '1', 'Python', 'Updated Jul 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['BOX-OFFICE-PREDICTION\nBy using linear regression we are predicting the box office gross collection based on the Production budget Data gathered from thenumbers.com\n'], 'url_profile': 'https://github.com/jayram181', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/krishisharma45', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': ['Wine-Customers-Classification-with-LDA-in-Python\nUsed LDA (an unsupervised technique) to perform dimensionality reduction on datasets. Modeled Logistic Regression on selected features to classify the wine customers.\nModel accuracy- 100.00%\n'], 'url_profile': 'https://github.com/FarheenB', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Vellore. Tamil Nadu', 'stats_list': [], 'contributions': '486 contributions\n        in the last year', 'description': ['Accuracy-Comparision-of-Classifier-Models\nThe accuracy comparision of different Classification models using Confusion Matrix. Models used: 1) Naive Bayes. 2)LinearSVC, 3)Decision Tree, 4)Logistic Regression\n'], 'url_profile': 'https://github.com/Tekraj15', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Durham, NC', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['STA-561-Final\nChenxi Wu (cw401)\nThe final test for the course STA 561: Probablistic Machine Learning, including topics as image classification (Gaussian process regression, CNN), LDA (Gibbs Sampler, EM Algorithm) and essential machine learning concept.\n'], 'url_profile': 'https://github.com/cassie1102', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '451 contributions\n        in the last year', 'description': ['Boston-House-Price-Prediction\nHouse Price Predictions on ""Boston Housing Dataset"" using Linear Regression and Polynomial model..\nIt contains US census data concerning houses in various areas around the city of Boston. Each sample corresponds to a unique area and has about a dozen measures. We should think of samples as rows and measures as columns. The data was first published in 1978 and is quite small, containing only about 500 samples.\nLet us look at a few characteristics of the Dataset\n\nNumber of Instances: 506:\nNumber of Attributes: 13 numeric/categorical\npredictive\nMedian Value (attribute 14) is usually the\ntarget\nIt is based on CRIME per capita crime rate by town\nThe reason we are using this dataset is that it is small in number, well defined and we can perform linear regression based on the Crime\nrate stats.\n\n'], 'url_profile': 'https://github.com/Aakash-Raman', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': [""An Image Classification Algorighm Using Logistic Regression\nClassifies images into CAT pictures and NON-CAT pictures\nBuilt this project while taking a coursera specialization in Deep Learning by Andrew Ng.\nINSTRUCTIONS TO USE MODEL\n\nplace image to classify in the 'images' folder,\nopen main.py and rename 'my_image' to name of image you wish to classify,\nrun the main.py file.\n\n""], 'url_profile': 'https://github.com/abahernest', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': ['Wine-Customers-Classification-with-PCA-in-Python\nUsed PCA (an unsupervised technique) to perform dimensionality reduction on given dataset. Modeled Logistic Regression on selected features to classify the wine customers.\nModel accuracy- 97.22%.\n'], 'url_profile': 'https://github.com/FarheenB', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'Fremont', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Data-Science-Analysis-for-Ecommerce-Product-Reviews\nAnalyzed customer reviews on Women’s Clothing Ecommerce by employing sentiment classification and linear regression to predict whether the customer recommends a reviewed product.\nThree main findings emphasise on the following:\n\nUnivariate and Bivariate Analysis on Age Distribution\nFrequency Distribution of Departments According to the Age Groups\nDistribution of Sentiment Scores Vs Rating Vs Recommended Indicator across Reviews.\nHigher the number recommendation factor provided by a customer, more likely it is to receive higher ratings\n\nAlso, included in this notebook is the following:\nSentiment analysis on the basis of reviewed data.\nDecision Tree\n'], 'url_profile': 'https://github.com/Mytreyi9', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': [""Product-Review-Predictor\nPerformed principal component analysis on 500,000+ lines of product review data and developed a lasso logistic regression model to predict the 5-star rating of Amazon electronics\nTasks To Complete:\n\nData Cleaning\n\n\nCovert all reviewas to lower case\nRemove stop words\n\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) \n\n\nHandle negation\nStem all words using Porter 1979\n\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\n\n\nCreate a bag-of-word vector representation for each review in electronics by creating frequency count table for all word stems in all reviews and using the most frequent 500 words to define the word vector.\n\n\nEstablish a method for measuring the distance between different reviews. Print the mutual distance between the first 100 reviews (review IDs) to screen, sorted from closest to furthest.\n\n\nFor this review data where language structures are complicated, we think Bag-of-Words method might not perform well, so we chose to build a Word2Vec model where we pay attention to the context of words in order to capture more of the semantics.\n\nRun a PCA and graph the first two PCs for the first 100 reviews.\n\nWe ran a two-component PCA on the vocabulary vectors of the Word2Vec model for the first 100 reviews. Then we plot the two Primary components.\nIt reflects our findings that negative reviews like “disappointing”, “not_work”, “not_track” are in one cluster and positive reviews like “like”, “recommend” are in another cluster. So both methods reviews some difference in these reviews.\n\n\nPerform a lasso logistic regression and measure the out-of-sample accuracy of your method of choice.\n\nI used logistic regression to predict the rating of 5 levels of a product. We may use ordinal logistic regression because we have ordinal target variables (ratings from 1-5 means worst to best). Therefore, we can use 0, 0.25, 0.5, 0.75, 1 as our output for the prediction. It is better to use ordinal logistic regression than MNL because MNL has no intrinsic ordering. Since there is an association between the levels of the ratings (5 levels), it is better to use ordinal logistic regression.\n\nImplement a method to aggregate reviews by product. Can you use any of the other columns to help with aggregation? Explain why or why not. Please clearly explain your method.\n\nIn this question, we use helpful start and helpful end, review time as well as bag of word matrix score to help with the aggregation of products. Helpful is a good metric showing whether the review is helpful or not. Review time can see whether recent reviews have more influence. Bag of words show the details of the review text. Then, we use groupby method to create a matrix of showing the average value of overall of different products.\n\nEstablish a method for measuring the distance between different products.\n\nWe use Euclidean distance measure to calculate the distance between different products based on the previous matrix of bag of words and products. By creating a square matrix, the numbers on the diagonal are all zero, because each country is identical to itself, and the numbers above and below are mirror images.\n""], 'url_profile': 'https://github.com/lzhu1111', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Updated Apr 25, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', '1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': [""An Image Classification Algorighm Using Logistic Regression\nClassifies images into CAT pictures and NON-CAT pictures\nBuilt this project while taking a coursera specialization in Deep Learning by Andrew Ng.\nINSTRUCTIONS TO USE MODEL\n\nplace image to classify in the 'images' folder,\nopen main.py and rename 'my_image' to name of image you wish to classify,\nrun the main.py file.\n\n""], 'url_profile': 'https://github.com/abahernest', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'Durgapur, West Bengal , India', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': [""Monroe-USA-Land-price-predicter\nThis is used to predict the price for a given area.\nThe data is being properly visualized and thoroughly Commented.\nI have used simple linear regression to predict the prices of area in sq.ft..\nBut we need to predict the result manually using the formula of y = mx + c, since the predict method isn't included in the code.\n""], 'url_profile': 'https://github.com/imarpan2003', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vidushi-Gautam', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'Orlando, Florida', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Recruit-Restaurant-Visitor-Forecasting\nOverview: https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting\nThe main objective of this project is to make automated future customer prediction by utilizing the restaurant data from Recruit Holdings. The analysis of available reservation and visitation data focuses on developing a reliable machine learning algorithm to help restaurants plan more efficiently and allow them to focus on creating an enjoyable dining experience for their customers.\nTwo different approaches for building the models and predicting the value are followed: restaurant-wise and global modeling. Mostly, restaurant-wise models are applied as the algorithms gave better results when applied to one time series at a time. The test data included 821 restaurants. Therefore, 821 models were built for each algorithm applying restaurant-wise approach. The following machine learning models were trained for the dataset: Linear Regression Model, Auto Regressive (AR) Model, Moving Average (MA) Model, Auto Regressive Moving Average (ARMA) Model, Auto Regressive Integrated Moving Average (ARIMA) Model, Seasonal ARIMA (SARIMA) Model, LightGBM, and Recurrent Neural Network (RNN) or Long Short Term Memory (LSTM) Model.\nRoot Mean Squared Logarithmic Error (RMSLE) is used as the performance metric for evaluating the performance of the algorithms on the dataset. A lower value of RMSLE corresponds to a better model. The performance metric of the above algorithms as measured by the Kaggle Competition can be compared as follows:\n\n\n\nAlgorithm\nPrivate score\nPublic score\n\n\n\n\nRegression Model\n0.60694\n0.56537\n\n\nAR Model\n0.62782\n0.60151\n\n\nMA Model\n0.63095\n0.60998\n\n\nARMA Model\n0.60216\n0.57688\n\n\nARIMA Model\n0.60322\n0.57676\n\n\nSARIMA Model\n0.56575\n0.52322\n\n\nLightGBM Model\n0.71750\n0.69724\n\n\nRNN/LSTM Model\n0.69129\n0.69710\n\n\n\nBased on the experimental results, Seasonal ARIMA model gives the lowest Root Mean Squared Logarithmic Error of 0.56575 for the dataset on private fold (approximately 87% of the test data).\n'], 'url_profile': 'https://github.com/rashijain', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Blast-Furnace-Optimisation\nThis repository contains the files used in my 3rd year dissertation: ""Optimisation of a Blast Furnace Process Using Regression Machine Learning Algorithms and SHAP Value Interpretation"". Please note this was my first ever coding project.\n'], 'url_profile': 'https://github.com/olliroberts', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'Dallas Tx', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Stack_overflow_tags_prediction\nPrediction of the tags for stack overflow post headings using tf-idf vectorization and Ridge Classifier. Bag-of-words model was tried as a substitute for tf-idf and logistic regression as a substitute for Ridge Classifier and the difference in performances were evaluated.\nRidge regression has two main benefits. First, adding a penalty term reduces overfitting. Second, the penalty term guarantees that we can find a solution. Ridge regression uses l2 penalty as a regularization term and the loss function is the linear least squares function. Loss function of cross entropy is used in logistic regression.\nBag-of-words does not take care of the relative position or sequence of words in a text, it just counts frequency of words in a text. Tf-idf takes care of the word-relevancy by counting the frequency of words in the text(document) and in the entire corpus. tf-idf for word k in document j = number of times k appeared in j/(log(number of documents/number of documents having word k))\n'], 'url_profile': 'https://github.com/SurabhiAmit', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Classifying-song-genre-\nIt\'s a project on ""Classifying song genre from audio data"" hosted by Datacamp . Here we used Decision tree and Logistic regression model and compared them.\n'], 'url_profile': 'https://github.com/Tafannumtahiyat', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/andrewronald', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Ionosphere-UCI-ML\nBuild ML models (Logistic Regression and Naive Bayes) from scratch to classify radar returns from ionosphere. Link: http://archive.ics.uci.edu/ml/datasets/Ionosphere\n'], 'url_profile': 'https://github.com/vmk94', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}","{'location': 'Budapest', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Data and code for the paper ""Identifying covariate traits of trance and possession phenomena in the Ethnographic Atlas using multiple imputation and nested sparse regression""\nTOC\n\nmodels: models saved from the SI\nfigures: figures saved from the SI\ndata: data saved by the helper file and the SI\ntrance.bib: bibliography for the paper and the SI\nsi_trance.Rmd: the SI in Rmarkdown\nsi_trance.pdf: the SI in pdf\ntrance_helper.R: the helper file to pull in EA data\n\n'], 'url_profile': 'https://github.com/petyaracz', 'info_list': ['1', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2021', 'Python', 'Updated Jul 23, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Updated Apr 21, 2020', 'TeX', 'Updated May 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Predicting-Data Scientist-Salary\nCreating a model to predict the salary of a data scientist based on the job description found on Glassdoor website.\nPython version: 3.7.0\nPython Packages: pandas, numpy, matplotlib, seaborn, sklearn\nData Collection\nData can be collected from various sources. One of the most popular form of data is in CSV format but it is essential to know that most of the time data are not in CSV form. Beside CSV, data can be extracted from text file, pdf file or through API or web scrapping.\nData Cleaning\nData cleaning is the most important step in the data analysis process. Most of the time, data are in raw format. If not cleaned appropriately, our predictive model will not provide accurate and concise output. In data analysis, Garbage in is Garbage Out. So, it was important to perform data cleaning in our glassdoor data.\nIn our data, we will change the elements of dataframe into more readable format such as changing the salary estimate from $81k - $100K to 81 - 100. We have also derived new columns from the existing column which we believe will help us in making better prediction. For example, we created column with average salary, we created new columns containing how many times job description required knowledge of python, R, spark, etc.\nExploratory Data Analysis\nNext step is to explore the data through visualization. We should always perform exploratory data analysis before moving to model building. EDA not only help us to summarize the main characteristics but also provides intuition for our statistical model. EDA is a simple way to find any discrepancy in our data through visualization.\nSome of the visualization from our EDA are\n\n\nModel Building\nNow we are finally building our prediction model.There are various prediction algorithm such as regression, classification, clustering, etc. Based on the nature and characteristics of the data, needed outcome, we can choose our algorithm. Since, we are predicting quantitative amount, regression would be best choice. Even in regression we would use three model multiple linear regression, lasso regression and random forest. Among three model, we chose the random forest since it performed the best.\n'], 'url_profile': 'https://github.com/Kshitiz14', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['pass-nyc-kaggle\nThis project was my first attempt at machine learning. It was based on the PASSNYC Kaggle competition that ran in late 2018. After assessing the quality several different machine learning regression models, I designed a simple random forest regression model to help the non-profit PASSNYC identify the schools where minority and underserved students would be helped the most.\nMotivation\nI designed this project based on the PASSNYC Kaggle competition from late 2018, which you can read more about here: https://www.kaggle.com/passnyc/data-science-for-good. As a former elementary school teacher with a passion for both coding and social justice, this competition fit nicely into my wheelhouse.\nData Sources\nBoth datasets were taken from the Kaggle ""Data Science for Social Good: PASSNYC"" competition (https://www.kaggle.com/passnyc/data-science-for-good).\nHow to use\nTo run the .ipynb file, download the datasets from the /input folder or by clicking on the Kaggle link provided above.\n'], 'url_profile': 'https://github.com/TGasinski', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Sentiment-Analysis-with-scikit-learn\nApplied the logistic regression classification algorithm using scikit-learn and Python to classify movie reviews as either positive or negative.\nSteps achieved:\n\nClean and pre-process text data.\nPerform feature extraction with nltk\nBuild and employ a logistic regression classifier using scikit-learn.\nTune model hyperparameters and evaluate model accuracy\n\nThe model accuracy acheived was 89.9%.\n'], 'url_profile': 'https://github.com/shubhsaur', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'São Paulo', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Perform Sentiment Analysis with scikit-learn\nIn this project-based course, you will learn the fundamentals of sentiment analysis, and build a logistic regression model to classify movie reviews as either positive or negative. We will use the popular IMDB data set. Our goal is to use a simple logistic regression estimator from scikit-learn for document classification.\nBuild and employ a logistic regression classifier using scikit-learn\nClean and pre-process text data\nPerform feature extraction with The Natural Language Toolkit (NLTK)\nTune model hyperparameters and evaluate model accuracy\nhttps://www.coursera.org/projects/scikit-learn-logistic-regression-sentiment-analysis\n'], 'url_profile': 'https://github.com/rrgmax', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,853 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RakeshBhugra', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kirthy21', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Cincinnati', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/soodrk', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['Linear-Regression-Project-to-choose-Mobile-app-or-website-for-a-Clothing-company\n'], 'url_profile': 'https://github.com/ssubhash977', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Regression-Model-with-Keras-and-Scikit-Learn-to-Predict-Concrete-Strength\nIn this project a dataset of the compressive strength of different samples of concrete will be explored, and concrete strength will be prdicted based on the volumes of different ingredients used for preperation. Ingredients include:\n\nCement\nBlast Furnace Slag\nFly Ash\nWater\nSuperplasticizer\nCoarse Aggregate\nFine Aggregate\n\nA simple Artificial neural networks (ANN) regression model will be built and used for predicting concrete compressive strength based on volumes of said ingredients.\nThereafter, predictions will also be conducted using Scikit-learn Multi-Linear Regression model, and accuracy results will be compare to those of the ANN model.\n'], 'url_profile': 'https://github.com/Omri-Matalon', 'info_list': ['Jupyter Notebook', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Updated Apr 26, 2020', 'Python', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 22, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}"
"{'location': 'Pune', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Case Study on Mercedes-Benz Greener Manufacturing\n1. Check my linkdin for some exciting work : \n\n2. Check my Medium Blog for better understanding of this work : \n\n'], 'url_profile': 'https://github.com/luckyRajputana', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Saint Louis, MO', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Lorsmo', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['-Supervised-Learning\nThis repository contains notebooks of various supervised learning modules i.e. 1. Linear Regression, 2. Perceptron Algorithm, 3.Decision Trees ,4. Naive Bayes. 5. SVM , 6. Ensemble Methods, 7. Model Evaluation 8. Testing model\n'], 'url_profile': 'https://github.com/predictive-ml', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['machine-learning---Happy-life-satisfaction-\nUsing data to know if money makes people happy, the life satisfaction dataset and GDP per capita were used, then the tables by country were joined and a linear regression model was used since the information had that pattern.\n'], 'url_profile': 'https://github.com/jhoanmartinezz', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Building-a-bluffing-detector-\nThis is a bluffing detector for predicting previous salary of a future employee using Random Forest Regression for helping the HR Dept of a company in the recruitment process\n'], 'url_profile': 'https://github.com/swarup0411', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NadiaBlostein', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': ['SUV-Buyers-Classification-with-kPCA-in-Python\nUsed Kernel PCA to extract the principle components of non-linearly separable dataset of SUV Buyers. Modeled Logistic Regression to classify whether a person will buy a SUV or not.\nModel Accuracy- 91.25%\n'], 'url_profile': 'https://github.com/FarheenB', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Predicting-PM2.5-using-ML\nPredicting PM2.5 values from PM10 ,Wind speed,Humidity, Temprature, Pressure, CO, SO2, O3, NOx values.\nLinear regression is implemented using Gradient Descent Algorithm\nData is collected from https://cpcb.nic.in/\nRun app.py to predict the value of PM2.5\n'], 'url_profile': 'https://github.com/rishabh214', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kishorewolfe', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/notahuman-1-0', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jul 4, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020']}"
"{'location': 'Goa', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Simulation-of-Photocatalytic-reduction and Kinetic Reaction Modelling of Photo Catalytic Reactions\nThis study presents the simulation of modelled simultaneous photo chemical reactions using the ode solver (ode15s) of MATLAB and finding the best reaction constant value using nonlinear regression (lsqnonlin, nonlinear least square fit).\nIntroduction: Organic Phenolic pollutant and metal ions in their toxic state pose an acute threat to the environment, so their degradation to nontoxic state is quite important. Studies proved that TiO2 can degrade phenolic compounds as well as metal ions. Reaction constant i.e. k depends on certain factors such as temperature, concentration of reactants and others. We can infer that we can vary k accordingly and find the k (best k) at which degradation is maximum of phenolic compounds and metal ions. In this study we modelled all the simultaneous reactions occurring during the process in photochemical reactor to find the optimum k.\nMethodology: Initial concentration of TiO2, Metal ion, phenol is specified as respectively [0.01252, 250e-6, 5.058e-4]. Using the elementary rate equation, rate is written for each equation. Rate of consumption and generation for each compound is written using material balance.\nTiO2 + Ph \uf0e0 Ph-TiO2\nr4 = k4 [TiO2] [Ph] – k-4[TiO2-Ph]\nTiO2 is not appearing in any other reaction so by material balance\nd [TiO2]/dt = -r4\nThis method applied to all the other 15 compounds specified in the reaction.\nAfter doing this we got a 15*1 matrix of differential equations. These 15 reaction solved using the ode solver of MATLAB. 5 MATLAB files were created. Driver.m is the file from where the implementation of code will start. Some initial guess is given for k matrix (it contains rate constant for all of the simultaneous reactions). Solver.m is the file where the solving of ode will take place using the initial k matrix, time limits for the reaction specified as 0 to 120 min. Each of the ode is defined in the odefun.m file. It will be very tiring process to manually use hit and trial method and find the best k value so we used the nonlinear fit to do it for us. Experimental data is extracted from the 6 points given in the paper using the cubic interpolation (spline function) for points in range 0:120. This data stored in TiO2expdata.m file. Now our aim is to minimise the error i.e. yexp - ytheoretical. Upper bounds and lower bounds for k values are specified under which the odesolver will run and solve for concentration of compounds. yexp - ytheoretical is calculated in the file fun.m using the results by solving ode which is solved by using all the k values within the range specified. yexp - ytheoretical is passed as argument in lsqnonlin function and using this it gave us k_optimized or best k.\n'], 'url_profile': 'https://github.com/mohitvijayv', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['classification-models\nYou load a historical dataset from previous loan applications, clean the data, and apply different classification algorithm on the datalgorithms to build your models:  k-Nearest Neighbour Decision Tree Support Vector Machine Logistic Regression\n'], 'url_profile': 'https://github.com/rukmini-15', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Advertisement-dataset\nUsing a fake advertising data set, and performing Logistic Regression to predict whether or not a particular user will click on an ad based off the features of that user.\n'], 'url_profile': 'https://github.com/Vyshnave', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'Pune, Maharashtra', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anushka1096', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'Hamburg, Germany', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['soep-unemployment-health-spillovers\nReplication code for Everding and Marcus (2020, Health Economics).\nSpecifically, this study examines the causal effect of unemployment on spousal smoking behavior.\nFor this purpose, we focus on involuntary entries into unemployment, combining difference-in-differences (DiD) estimation with a matching strategy based on entropy balancing (EB, Hainmueller 2012).\nFor selecting control variables, we complement our econometric approach with two different procedures for control variable selection; a conventional approach based on previous studies and economic intuition as well as a machine learning approach based on Lasso regressions, the post-double-selection method (PDS, Belloni et al. 2014).\nMain do-file\nmaster_spousal_ue.do defines all relevant macros, the folder structure, and executes all files sequentially\nAdditional do-files\nThe actual steps of data pre-processing and analyses are divided into several additional do-files (""script"" files in Stata), as described in the following:\ngen_spousal_ue.do pulls the data\ntrans_spousal_ue.do transforms the data and generates the relevant variables\ncandidatevar_spousal_ue.do constructs and adds leads, lags, transformed variables (polynomials and log. trans.) and imputation flags\npds_spousal_ue.do fits Lasso regressions (PDS, see Belloni et al. 2014) on candidate variables\npdsmech_spousal_ue.do fits Lasso regressions and selects controls for analysis of mechanisms (i.e. alternative outcomes)\nmain-analysis1_spousal_ue.do runs main analysis (with EB, see Hainmueller 2012), part 1 (without post-double selection)\nmain-analysis2_spousal_ue.do runs main analysis (with EB), part 2 (only post-double selection)\ndesc-stats1_spousal_ue.do generates table for descriptive statistics and matching quality, part 1\nrob-analysis-pds_spousal_ue.do performs all post-double selection robustness analyses\nhet-analysis1_spousal_ue.do investigates treatment effect heterogeneity by smoking status at baseline, part 1\nhet-analysis2_spousal_ue.do investigates treatment effect heterogeneity by smoking status at baseline, part 2\nmech_spousal_ue.do runs analysis of mechanisms (with PDS), part 1\nmech2_spousal_ue.do runs analysis of mechanisms (without PDS), part 2\ndesc-stats2_spousal_ue.do generates table for descriptive statistics and matching quality, part 2\nData\nThe main data source is the German Socio-Economic Panel (SOEP, version 33).\nSee https://www.diw.de/soep for detailed information on data access options.\nReferences\nBelloni, A., V. Chernozhukov, and C. Hansen. 2014. Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2), 608-650.\nEverding, J. and J. Marcus. 2020. The effect of unemployment on the smoking behavior of couples. Health Economics, 29(2), 154-170.\nHainmueller, J. 2012. Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political Analysis, 20(1), 25-46.\n'], 'url_profile': 'https://github.com/jeverding', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'Ahmedabad, India', 'stats_list': [], 'contributions': '372 contributions\n        in the last year', 'description': [""Kaggle_Competitions\nIt's about my analysis on large and real life problem based competitions @kaggle  and Applied Data analysis, machine learning and Deep learning techniques to build necessary model. Follow me on @Kaggle : https://www.kaggle.com/harshkothari21\n1. House Price\nPredict House-Price on DataSet that conatins 81 features and 3000 Rows(1460 for Train and 1459 for Test).\nSkills Applied:\n\nData Cleaning\nEDA\nFeature Selection\nFeature Engineering\nRandom Forest Model and XgBoost\nHyperparameter tuning\nDeep Learning using Keras\n\n2. Titanic\nClassification problem to predict weather the person survived or not during the famous titanic Accident.\nSkills Applied:\n\nData Cleaning\nEDA\nFeature Engineering\nScaling\nCross Validation\nHyperparameter tuning\nLogistic Regression | SVM | SVC | KNN | Decision Tree |Random Forest Model\n\n3.Career Village\nThis competition contains dataset from https://www.careervillage.org/ , We need to be able to send the right questions to the right volunteers and get the insights of the data.\nSkills Appied:\n\nData Cleaning\nEDA\nData Visualization\nBuild WordCloud\n\n4.Don't Overfit the Model\nA binary classification task to train a model with 300 features and only 250 training examples and 79times more samples on test data.\n""], 'url_profile': 'https://github.com/HarshKothari21', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'Lugano, Switzerland', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': ['Robotics Final Project\nNoli Manzoni, Micheal Denzler\nIf you want to have more information about our implementaiton please look at project description in the pdf file.\nHow to use\nFirst add the cnn-angular-velocity package to your your_catkin_workspace\nGet training data\nTo get the training data please launch Gazebo with the wanted world (simple or pitfalls)\nroslaunch cnn-angular-velocity thymio_gazebo_bringup.launch name:=thymio10 world:=simple\n\nand then execute the random_walk.launch file as follow:\nroslaunch cnn-angular-velocity random_walk.launch\n\nThis script will populate the folder data/imgs with images from the Thymio (one per second) and once the program is closed with ctrl+c (better to do this when the Thymio is moving forward) it also saves the target data in sensor_data.npy (when using the save & flag system it will save also  pitfall_flags.npy and object_flags.npy ).\nPlease record one big dataset for training and one small one for validation (model selection via wandb). In our presentation the training set had around 2000 images and 250 for the validation set.\nCollection systems\nLonger ranges: random_walk.launch\nSave & flag: random_walk_pitfalls.launch\nTeleoperation: random_walk_teleop.launch\nTrain model\nTo train the model execute the file train_model.py and pass as an argument the directory containing the train and validation set.\nThe directory should be designed as follow:\n\ndata/\n\ntrain/\n\nimgs/\nsensor_data.npy\n\n\nval/\n\nimgs/\nsensor_data.npy\n\n\n\n\n\nWe trained our model on a GPU node in USI HPC cluster.\nIf you use the save and flag system, please change the dataset import in the top of train_model.pyand add the additional file to the data directory.\nAlready trained model (and corresponding datasets) can be found at the following link where  pitfalls.tar is a model trained with the save and flag system on the pitfalls map and obstacles.tar is the model trained with long ranges on the simple map.\nTest model\nTo test the model put the .tar file in the model directory and then  launch Gazebo with the wanted world (simple or pitfalls)\nroslaunch cnn-angular-velocity thymio_gazebo_bringup.launch name:=thymio10 world:=simple\nor the test world\nroslaunch cnn-angular-velocity thymio_test_gazebo_bringup.launch\n\nand then execute the avoid_obstacle.launch file with the chosen model (pitfalls or obstacles) as follow (X is 10 for normal world and 11 to 16 for the test world):\nroslaunch cnn-angular-velocity avoid_obstacles.launch robot_name:=thymioX model:=pitfalls\n\nTeleoperation\nTo test the model please launch Gazebo with the wanted world\nroslaunch cnn-angular-velocity thymio_gazebo_bringup.launch name:=thymio10 world:=pitfalls\n\nand then execute the avoid_obstacle.launch file as follow:\nroslaunch cnn-angular-velocity teleoperate.launch\n\nResults\n\nContacts\nIf you have any doubts please contact us at noli.manzoni@usi.ch or michael.denzler@usi.ch\n'], 'url_profile': 'https://github.com/raikilon', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'United States ', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': [""Python\nData:\nThe data consists of aggregated trip logging metrics from commercial vehicles,\nsuch as semi-trucks. The data have been grouped by intersection, month, hour of day,\ndirection driven through the intersection, and whether the day was on a weekend or not.\nThe data consists of the 20th, 50th, and 80th percentiles for the total time stopped at an intersection\nand the distance between the intersection and the first place a vehicle stopped while waiting\n\nWhat is the question you want to ask?\n\n\nPredict congestion time from the dataset that includes aggregate stopped vehicle information and intersection wait times\nHow does this differ from a weekday to weekend - Done\nAlso, how does the wait time differ in Peak and lean hours? - Done\nTop 50/100 traffic junctions/intersections - Pending\n\n\nHow do you plan on using data to answer that question?\n\n\nExplore data that I have by typical EDA( Exploratory data Analysis Steps)\nThe data is grouped at month,intersection, hour of the day, direction driven through the intersection, a weekend or not\nPlanning to cluster the data to see if there is a way to summarize the distribution of wait times and stop distances at each intersection\n'Total time stopped' and 'Distance to first stop' are the vital metrics which would help me understand the above question\n\n\nHow are you going to get that data?\n\n\nKaggle Data Set\n\n\nWhat analysis are you planning to perform on it?\n\n\nExploratory data Analysis\nK-Means Clustering\nUnderstanding Wait times via heat Maps\nCity wise Top Wait time junctions/intersections at Morning Peak and Evening Peak hours (To make sure that commuters in that city know how long it takes to wait at a juction)\n\n\nHow are you going to present your results?\n\n\nJupyter Notebook\nHeatmaps\nMatplotlib and Seaborn utilization\nGraphs showcasing the patterns - clusters\npredict and minimize the RMSE from the model\n\n""], 'url_profile': 'https://github.com/vamsimessi10', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'San Francisco ↔️ Prague ', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['apprentice-chef-classification-modeling\nBuilding a classification-based predictive model to predict a cross-sell promotion success and providing insights on what target groups to focus on. The analysis consists of initial exploratory data analysis, feature treatment and engineering, and utilizing appropriate machine learning modeling techniques such as decision trees, random forests, logistic regression, kNN, and K-Means.\n'], 'url_profile': 'https://github.com/thanhtamluu', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/venkatesh2395', 'info_list': ['MATLAB', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', '1', 'Stata', 'MIT license', 'Updated Mar 2, 2021', '1', 'Jupyter Notebook', 'Updated Aug 16, 2020', 'Python', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 16, 2020']}"
"{'location': 'Boston', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/valentinastoma', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Pima-Indians-Diabetes\nMachine Learning algorithms are well known in the medical field for predicting diseases. This study was conducted using three different ML algorithms including Logistic  regression, Decision Tree, and the Naïve Bayes to predict if a particular observation is at a risk of developing diabetes. Moreover, K-means clustering and data mining is used to discover the grouping structures inherent in data.\nThe following steps was performed on the dataset:\n\nData Preparation\nPredictive Modeling/Classification:\na. Classification using Decision Tree\nb. Classification using Naive Bayes\nc. Classification using Logistic Regression\nd. Comparing the results of the 3 techniques\nPost-prediction Analysis\na. Applying cluster analysis and association rule mining on the results of the classification to provide customized   recommendations to the organization for its customers.\nConclusions and Recommendations\n\nSoftware used: R and Weka\n'], 'url_profile': 'https://github.com/psoleymani', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '237 contributions\n        in the last year', 'description': ['Steam_user_suggestion_pred\nObjective:\nThis project used steam game review data to predict user suggestions (binary variable: 0 or 1). The natural language processing, word embedding techniques such as word2vec and sentiment scores are applied to do feature engineering and extraction. The SVM and logistic regression are used to predict whether the user will recommend this game to other users or not.\nDataset description:\n\ntrain data features:\n\nreview_id: unique user id\ntitle: game title\nyear: release year\nuser_review: user comments\nuser_suggestion: binary data with 0 (not recommend) or 1 (recommend)\n\n\ntest data\ngame overview features:\n\ntitle: game title\ndeveloper: game developer\npublisher: game publisher\ntags: game tags to represent its type or features\noverview: game introduction\n\n\n\nMachine learning techniques:\n\nnatural language processing: word tokenization (nltk)\nsentiment analysis: review scoring as negative, netural, and positive\nword embedding: word2vec (considering the word context)\npredictive / classification models: SVM and logistic regression\n\nResults:\nEDA:\n1) Most games had title length of 17.\n\n2) Most games in the dataset were released in 2018.\n\n3) Valve had most published or developed games in the dataset.\n\nPredictive model:\n1) The logistic regression (84%) overperformed SVM (57%) when evaluating in-sample accuracy. \n\n'], 'url_profile': 'https://github.com/Freiheit77', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['ML_SMSspamClassifier\nI have used NLTK library in python to distinguish between spam and ham SMS\'s. I have trained different models: K Nearest Neighbors, Decision Tree,Random Forest"" Logistic Regression, SGD Classifier,Naive Bayes, SVM Linear and obserrved by that best results are given by Naive Bayes model followed by SVM Classifier.\n'], 'url_profile': 'https://github.com/vash-ashutosh', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aparajitakar', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['survivalMLPUsingcasebase-AI-Genomics\nMAIN analysis done in R. See the .Rmd\nCasebase sampling has been proven to work with logistic regression, but this has yet to be proven with a neural network. Here, we attempt to use a multilayer perceptron to estimate the cumulative incidence curve of individuals within a sample. slides can be found here: https://docs.google.com/presentation/d/1CIknocOOVq8cWEws7pi9ublf7FVGswr7vey1o2NUrrM/edit?usp=sharing\n'], 'url_profile': 'https://github.com/Jesse-Islam', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Big-City-Health-Data\nThis project aims to utilize statistical methods such as Linear Regression and Decision Trees, to analyze the major factors that influence smoking and drinking issues among high school students as well as adults in the most populated urban cities of the United States.\nThe dataset being used is Big Cities Health data [1] which contains health status of twentyeight of the nation’s largest and most urban cities, as captured by 34 health (and six demographics-related) indicators. Each city is rich with its own culture and history and we are considering the demographics of the subjects that are disproportionately scattered among the cities.\nQuestions of Interest The main objective of the project is to address the following questions:\ni) What are the major factors causing smoking and drinking problems among High School students in the most urban cities of the United States? How much are these conditions influenced by the place, ethnicity, and gender of the students?\nii) Similarly, how much effect do predictors like place, gender and ethnicity have on smoking and drinking problems among adults in US’s biggest cities?\nThese questions have been chosen as the basis of research because smoking and binge drinking are major issues of concern especially among young students, and lead to dropouts, termination and such outcomes. Therefore, if analysis can be performed to identify influencing factors, the result can be utilized to curb such problems to an extent. The intention is to conduct statistical analysis and inference using appropriate methods to isolate any underlying factors and help regulate these societal problems from the core.\n'], 'url_profile': 'https://github.com/arch456', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['ML_SMSspamsClassifier\nI have used NLTK library in python to distinguish between spam and ham SMS\'s. I have trained different models: K Nearest Neighbors, Decision Tree,Random Forest"" Logistic Regression, SGD Classifier,Naive Bayes, SVM Linear and obserrved by that best results are given by Naive Bayes model followed by SVM Classifier.\n'], 'url_profile': 'https://github.com/vash-ashutosh', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RitvikKapila', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}","{'location': 'Scotland, United Kingdom', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/stella-spyrou', 'info_list': ['HTML', 'Updated Apr 20, 2020', 'LGPL-3.0 license', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 22, 2020', 'Updated Apr 20, 2020', 'R', 'MIT license', 'Updated Apr 24, 2020', 'R', 'Updated Apr 25, 2020', 'Updated Apr 22, 2020', 'Python', 'Updated Apr 25, 2020', 'R', 'Updated Apr 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yasminelsawy', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'Belo Horizonte', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/diogoeverson', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'Belo Horizonte', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/diogoeverson', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['Stats Repo\nNote: data is not provided in the repository. Links are available in the README.\nThis repository houses an ongoing series of small statistics projects. There will typically be three files\nfor each R project; one will contain pure code (.R), another will be\nmarkdown (.Rmd), and the last (.pdf) can be viewed directly in\ngithub. The pdf file description includes the associated project\ndescription. There will be one file (.ipynb) for python projects as notebooks can be viewed directly in github.\nFiles\nOLS_ElasticNet_Ridge_Lasso_RandomForest.pdf\nThis project explores and applies four methods of linear regression, in addition to a\nrandom forest model, to predict the sale price of homes based on 40\nquantitative attributes (kaggle dataset link provided). The random\nforest model is included to evaluate the trade-off in time vs. potential\nincrease in accuracy. The four linear regression methods applied within\nare Ordinary Least Squares (OLS), Ridge, Lasso, and\nElastic-Net. Seeing as Ridge and Lasso are special cases of the\nElastic-Net, an arbitrary alpha (.5) is used in the Elastic-Net model.\nThis analysis also includes bootstrapped error bars of feature significance for\neach model. Data link is in the report.\none_factor_anova.pdf\nViewable output of one_factor.rmd file. This project explores (i) the\nassumptions for the one factor (one-way) ANOVA test using Citibike data, and\n(ii) various tools available in R to verify these assumptions prior to\nconducting the analysis. Some tools explored in this analysis are as\nfollows: Shapiro-Wilk’s Normality Test, Levene’s Test for Equality of\nVariance, Tukey’s Honest Significant Difference Test, distribution\nvisualization (histograms, boxplots), and further EDA. Citibike data was pulled and aggregated from here.\n'], 'url_profile': 'https://github.com/cordero-c-perez', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'Riverside, California', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['AroundTheGlobe\nThrough this project we aim to devise a mechanism to enhance an individual’s experience of the world by helping them examine the diversity around the continents, countries, provinces, cities and regions with the help of visualizations of representative images on the world map using Open Layers. We have crawled geo-tagged images from Flickr using Python, downloading images and storing their details in Mongo DB database using JSON, training a Linear Regression model to rank images for each location based on derived parameters and showing visualizations of the top ranked images at different zoom levels on world map using Open Layers\n'], 'url_profile': 'https://github.com/zhass001', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'Bandung, Indonesia', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Student-Performance-Prediction-for-3rd-year-period\nThis data approach student achievement in secondary education of two Portuguese schools.\nThe data attributes include student grades, demographic, social and school related features) and it was collected by using school reports and questionnaires.\nDatasets are provided regarding the performance in two distinct subjects: Mathematics (mat) In [Cortez and Silva, 2008],\ndatasets were modeled under binary/five-level classification and regression tasks. Important note: the target attribute G3 has a strong correlation with attributes G2 and G1.\nThis occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades.\nIt is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details).\n'], 'url_profile': 'https://github.com/itsnurdin', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/weijiaru', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': [""Estimating-the-effect-of-climate-change-on-sea-level-using-ML\nSea-level rise affects the lives of people in coastal areas and it is causing the sea to inundate villages during high tide season which has changed people's lives because houses are destroyed and areas, where families gather, are being washed away. By predicting sea-level we can save their lives. Initially, for the local prediction, I have considered the following features which are affecting the most to rise of sea level: Local precipitation, Local temperature, Local population, Local CO2 concentration and for the global prediction, I have applied a different set of features: Global precipitation, Global temperature index, Global population, Global CO2 concentration. If we increased features than prediction will get even worse. So we have to choose features that affect the most to the prediction. The performance of linear regression algorithms under various conditions is derived using cross-validation on the latest 20% of the dataset, which was left out from the data-set. Other machine learning concept like PCA, GMM and SVM are also applied to understand their behavior.\n""], 'url_profile': 'https://github.com/rajvipatel-223', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/surajkathik', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 24, 2020', 'R', 'Updated Feb 21, 2021', 'Python', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Updated Apr 23, 2020']}",
