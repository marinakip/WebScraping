"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Event-Based Angular Velocity Regression with Spiking Networks\n\n\n\n\n\nThis is the code for the paper Event-Based Angular Velocity Regression with Spiking Networks by Mathias Gehrig, Sumit Bam Shrestha, Daniel Mouritzen and Davide Scaramuzza.\nYou can find a pdf of the paper here.\nIf you use any of this code, please cite the following publication:\n@Article{Gehrig20icra,\n  author        = {Mathias Gehrig and Sumit Bam Shrestha and Daniel Mouritzen and Davide Scaramuzza},\n  title         = {Event-Based Angular Velocity Regression with Spiking Networks},\n  journal       = {{IEEE} International Conference on Robotics and Automation (ICRA)},\n  url           = {http://rpg.ifi.uzh.ch/docs/ICRA20_Gehrig.pdf},\n  year          = 2020\n}\nSetup\nTested with:\n\nPyTorch 1.4.0\ntorchvision 0.5.0\nstrictyaml 1.0.6\ntqdm 4.43.0\n\nusing:\n\nUbuntu 18.04.4 LTS\ngcc 7.4.0\nCUDA Tookit 10.0\nNvidia Turing GPUs\n\nData and Log Directory\ngit clone git@github.com:uzh-rpg/snn_angular_velocity.git\ncd snn_angular_velocity\nEither set up data and logs directory within the cloned repository with:\nexport data_dir=$(pwd)/data\nexport log_dir=$(pwd)/logs\nmkdir -p $data_dir $log_dir/train $log_dir/test\nor set up an external data and logs directory with symbolic links:\nexport data_dir=""YOUR_DATA_PATH""\nexport log_dir=""YOUR_LOG_PATH""\n\nmkdir -p $data_dir $log_dir/train $log_dir/test\nln -s $data_dir data\nln -s $log_dir logs\nor pass the data and logging directories with arguments. For more info:\npython test.py -h\nInstallation with Anaconda\nAdapt the CUDA toolkit version according to your setup. We also recommend to build PyTorch from source to avoid conflicts while compiling SLAYER. However the following option might work for you as well:\nCreate conda environment from scratch with precompiled PyTorch\ncuda_version=10.0\n\nconda create -y -n snn python=3.7\nconda activate snn\nconda install -y pytorch torchvision cudatoolkit=$cuda_version -c pytorch\nconda install -y strictyaml tqdm -c conda-forge\nconda install -y h5py pyyaml -c anaconda\n\n# Setup for SLAYER\ncd slayerpytorch\npython setup.py install\ncd ..\nDownload Dataset and Model\nDownload pretrained model.\nwget ""http://rpg.ifi.uzh.ch/data/snn_angular_velocity/models/pretrained.pt"" -O pretrained/cnn5-avgp-fc1.pt\n\nDownload and extract test dataset. This requires zstd which you can get with sudo apt install zstd on Ubuntu.\nwget ""http://rpg.ifi.uzh.ch/data/snn_angular_velocity/dataset/test.tar.zst"" -O $data_dir/test.tar.zst\ncd $data_dir\nzstd -vd test.tar.zst\ntar -xvf test.tar\nrm test.*\nSimilarly, you can get the training dataset\nwget ""http://rpg.ifi.uzh.ch/data/snn_angular_velocity/dataset/train.tar.zst"" -O $data_dir/train.tar.zst\nvalidation dataset\nwget ""http://rpg.ifi.uzh.ch/data/snn_angular_velocity/dataset/val.tar.zst"" -O $data_dir/val.tar.zst\nand the original panorama images to recreate the dataset from scratch\nwget ""http://rpg.ifi.uzh.ch/data/snn_angular_velocity/dataset/imgs.tar"" -O $data_dir/imgs.tar\nTest\nTo reproduce the numbers in the paper run:\npython test.py\nConfig file\nThis uses by default the configuration file in test_config.yaml.\nModify the test config if you want to change one of the following parameters:\n\nBatch size\nNumber of reader threads\nGPU device number\n\nWriting predictions to disk\nIf you would like to write the predictions of the network to disk:\npython test.py --write\nThis will generate the following three files in $log_dir/test/*/out/:\n\nindices.npy (sample): for each sample the index of the h5 filename that has been used.\ngroundtruth.npy (sample, angle, time): groundtruth angular velocity\npredictions.npy (sample, angle, time): predicted angular velocity\n\nTrain\nCode in progress\n'], 'url_profile': 'https://github.com/uzh-rpg', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.diagnostic import linear_rainbow, het_breuschpagan\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.preprocessing import LabelEncoder\nInferential Modeling Workflow\nThis dataset was downloaded from Kaggle and reflects data collected by the WHO about life expectancy and potentially-related factors.  The information is aggregated on a per-country per-year basis.\nThe following questions have been posed:\n\nDoes various predicting factors which has been chosen initially really affect the Life expectancy? What are the predicting variables actually affecting the life expectancy?\nShould a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan?\nHow does Infant and Adult mortality rates affect life expectancy?\nDoes Life Expectancy has positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol etc.\nWhat is the impact of schooling on the lifespan of humans?\nDoes Life Expectancy have positive or negative relationship with drinking alcohol?\nDo densely populated countries tend to have lower life expectancy?\nWhat is the impact of Immunization coverage on life Expectancy?\n\nImporting the Data\ndf = pd.read_csv(""data/life_expectancy.csv"")\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCountry\nYear\nStatus\nLife expectancy\nAdult Mortality\ninfant deaths\nAlcohol\npercentage expenditure\nHepatitis B\nMeasles\n...\nPolio\nTotal expenditure\nDiphtheria\nHIV/AIDS\nGDP\nPopulation\nthinness  1-19 years\nthinness 5-9 years\nIncome composition of resources\nSchooling\n\n\n\n\n0\nAfghanistan\n2015\nDeveloping\n65.0\n263.0\n62\n0.01\n71.279624\n65.0\n1154\n...\n6.0\n8.16\n65.0\n0.1\n584.259210\n33736494.0\n17.2\n17.3\n0.479\n10.1\n\n\n1\nAfghanistan\n2014\nDeveloping\n59.9\n271.0\n64\n0.01\n73.523582\n62.0\n492\n...\n58.0\n8.18\n62.0\n0.1\n612.696514\n327582.0\n17.5\n17.5\n0.476\n10.0\n\n\n2\nAfghanistan\n2013\nDeveloping\n59.9\n268.0\n66\n0.01\n73.219243\n64.0\n430\n...\n62.0\n8.13\n64.0\n0.1\n631.744976\n31731688.0\n17.7\n17.7\n0.470\n9.9\n\n\n3\nAfghanistan\n2012\nDeveloping\n59.5\n272.0\n69\n0.01\n78.184215\n67.0\n2787\n...\n67.0\n8.52\n67.0\n0.1\n669.959000\n3696958.0\n17.9\n18.0\n0.463\n9.8\n\n\n4\nAfghanistan\n2011\nDeveloping\n59.2\n275.0\n71\n0.01\n7.097109\n68.0\n3013\n...\n68.0\n7.87\n68.0\n0.1\n63.537231\n2978599.0\n18.2\n18.2\n0.454\n9.5\n\n\n\n5 rows × 22 columns\n\ndf.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nYear\nLife expectancy\nAdult Mortality\ninfant deaths\nAlcohol\npercentage expenditure\nHepatitis B\nMeasles\nBMI\nunder-five deaths\nPolio\nTotal expenditure\nDiphtheria\nHIV/AIDS\nGDP\nPopulation\nthinness  1-19 years\nthinness 5-9 years\nIncome composition of resources\nSchooling\n\n\n\n\ncount\n2938.000000\n2928.000000\n2928.000000\n2938.000000\n2744.000000\n2938.000000\n2385.000000\n2938.000000\n2904.000000\n2938.000000\n2919.000000\n2712.00000\n2919.000000\n2938.000000\n2490.000000\n2.286000e+03\n2904.000000\n2904.000000\n2771.000000\n2775.000000\n\n\nmean\n2007.518720\n69.224932\n164.796448\n30.303948\n4.602861\n738.251295\n80.940461\n2419.592240\n38.321247\n42.035739\n82.550188\n5.93819\n82.324084\n1.742103\n7483.158469\n1.275338e+07\n4.839704\n4.870317\n0.627551\n11.992793\n\n\nstd\n4.613841\n9.523867\n124.292079\n117.926501\n4.052413\n1987.914858\n25.070016\n11467.272489\n20.044034\n160.445548\n23.428046\n2.49832\n23.716912\n5.077785\n14270.169342\n6.101210e+07\n4.420195\n4.508882\n0.210904\n3.358920\n\n\nmin\n2000.000000\n36.300000\n1.000000\n0.000000\n0.010000\n0.000000\n1.000000\n0.000000\n1.000000\n0.000000\n3.000000\n0.37000\n2.000000\n0.100000\n1.681350\n3.400000e+01\n0.100000\n0.100000\n0.000000\n0.000000\n\n\n25%\n2004.000000\n63.100000\n74.000000\n0.000000\n0.877500\n4.685343\n77.000000\n0.000000\n19.300000\n0.000000\n78.000000\n4.26000\n78.000000\n0.100000\n463.935626\n1.957932e+05\n1.600000\n1.500000\n0.493000\n10.100000\n\n\n50%\n2008.000000\n72.100000\n144.000000\n3.000000\n3.755000\n64.912906\n92.000000\n17.000000\n43.500000\n4.000000\n93.000000\n5.75500\n93.000000\n0.100000\n1766.947595\n1.386542e+06\n3.300000\n3.300000\n0.677000\n12.300000\n\n\n75%\n2012.000000\n75.700000\n228.000000\n22.000000\n7.702500\n441.534144\n97.000000\n360.250000\n56.200000\n28.000000\n97.000000\n7.49250\n97.000000\n0.800000\n5910.806335\n7.420359e+06\n7.200000\n7.200000\n0.779000\n14.300000\n\n\nmax\n2015.000000\n89.000000\n723.000000\n1800.000000\n17.870000\n19479.911610\n99.000000\n212183.000000\n87.300000\n2500.000000\n99.000000\n17.60000\n99.000000\n50.600000\n119172.741800\n1.293859e+09\n27.700000\n28.600000\n0.948000\n20.700000\n\n\n\n\ndf.columns\nIndex([\'Country\', \'Year\', \'Status\', \'Life expectancy \', \'Adult Mortality\',\n       \'infant deaths\', \'Alcohol\', \'percentage expenditure\', \'Hepatitis B\',\n       \'Measles \', \' BMI \', \'under-five deaths \', \'Polio\', \'Total expenditure\',\n       \'Diphtheria \', \' HIV/AIDS\', \'GDP\', \'Population\',\n       \' thinness  1-19 years\', \' thinness 5-9 years\',\n       \'Income composition of resources\', \'Schooling\'],\n      dtype=\'object\')\n\nInitial Data Preparation\nThe original column names have extra spaces and other irregularities.  Let\'s clean those up, and also move the target variable to be the first column, for readability\n# rename so everything is snake_case\ndf = df.rename(columns={\n    \'Life expectancy \': \'Life_Expectancy\',\n    \'Adult Mortality\': \'Adult_Mortality\',\n    \'infant deaths\': \'Infant_Deaths\',\n    \'percentage expenditure\': \'Percentage_Expenditure\',\n    \'Hepatitis B\': \'Hepatitis_B\',\n    \'Measles \': \'Measles\',\n    \' BMI \': \'BMI\',\n    \'under-five deaths \': \'Under_five_Deaths\',\n    \'Total expenditure\': \'Total_Expenditure\',\n    \'Diphtheria \': \'Diptheria\',\n    \' HIV/AIDS\': \'HIV_AIDS\',\n    \' thinness  1-19 years\': \'Thinness_1_19_years\',\n    \' thinness 5-9 years\': \'Thinness_5_9_years\',\n    \'Income composition of resources\': \'Income_Composition_of_Resources\'\n})\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCountry\nYear\nStatus\nLife_Expectancy\nAdult_Mortality\nInfant_Deaths\nAlcohol\nPercentage_Expenditure\nHepatitis_B\nMeasles\n...\nPolio\nTotal_Expenditure\nDiptheria\nHIV_AIDS\nGDP\nPopulation\nThinness_1_19_years\nThinness_5_9_years\nIncome_Composition_of_Resources\nSchooling\n\n\n\n\n0\nAfghanistan\n2015\nDeveloping\n65.0\n263.0\n62\n0.01\n71.279624\n65.0\n1154\n...\n6.0\n8.16\n65.0\n0.1\n584.259210\n33736494.0\n17.2\n17.3\n0.479\n10.1\n\n\n1\nAfghanistan\n2014\nDeveloping\n59.9\n271.0\n64\n0.01\n73.523582\n62.0\n492\n...\n58.0\n8.18\n62.0\n0.1\n612.696514\n327582.0\n17.5\n17.5\n0.476\n10.0\n\n\n2\nAfghanistan\n2013\nDeveloping\n59.9\n268.0\n66\n0.01\n73.219243\n64.0\n430\n...\n62.0\n8.13\n64.0\n0.1\n631.744976\n31731688.0\n17.7\n17.7\n0.470\n9.9\n\n\n3\nAfghanistan\n2012\nDeveloping\n59.5\n272.0\n69\n0.01\n78.184215\n67.0\n2787\n...\n67.0\n8.52\n67.0\n0.1\n669.959000\n3696958.0\n17.9\n18.0\n0.463\n9.8\n\n\n4\nAfghanistan\n2011\nDeveloping\n59.2\n275.0\n71\n0.01\n7.097109\n68.0\n3013\n...\n68.0\n7.87\n68.0\n0.1\n63.537231\n2978599.0\n18.2\n18.2\n0.454\n9.5\n\n\n\n5 rows × 22 columns\n\ndf.columns\nIndex([\'Country\', \'Year\', \'Status\', \'Life_Expectancy\', \'Adult_Mortality\',\n       \'Infant_Deaths\', \'Alcohol\', \'Percentage_Expenditure\', \'Hepatitis_B\',\n       \'Measles\', \'BMI\', \'Under_five_Deaths\', \'Polio\', \'Total_Expenditure\',\n       \'Diptheria\', \'HIV_AIDS\', \'GDP\', \'Population\', \'Thinness_1_19_years\',\n       \'Thinness_5_9_years\', \'Income_Composition_of_Resources\', \'Schooling\'],\n      dtype=\'object\')\n\n# reorder so life expectancy is the first column\ncols = list(df.columns)\ncols = [cols[3]] + cols[:3] + cols[4:]\ndf = df[cols]\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nLife_Expectancy\nCountry\nYear\nStatus\nAdult_Mortality\nInfant_Deaths\nAlcohol\nPercentage_Expenditure\nHepatitis_B\nMeasles\n...\nPolio\nTotal_Expenditure\nDiptheria\nHIV_AIDS\nGDP\nPopulation\nThinness_1_19_years\nThinness_5_9_years\nIncome_Composition_of_Resources\nSchooling\n\n\n\n\n0\n65.0\nAfghanistan\n2015\nDeveloping\n263.0\n62\n0.01\n71.279624\n65.0\n1154\n...\n6.0\n8.16\n65.0\n0.1\n584.259210\n33736494.0\n17.2\n17.3\n0.479\n10.1\n\n\n1\n59.9\nAfghanistan\n2014\nDeveloping\n271.0\n64\n0.01\n73.523582\n62.0\n492\n...\n58.0\n8.18\n62.0\n0.1\n612.696514\n327582.0\n17.5\n17.5\n0.476\n10.0\n\n\n2\n59.9\nAfghanistan\n2013\nDeveloping\n268.0\n66\n0.01\n73.219243\n64.0\n430\n...\n62.0\n8.13\n64.0\n0.1\n631.744976\n31731688.0\n17.7\n17.7\n0.470\n9.9\n\n\n3\n59.5\nAfghanistan\n2012\nDeveloping\n272.0\n69\n0.01\n78.184215\n67.0\n2787\n...\n67.0\n8.52\n67.0\n0.1\n669.959000\n3696958.0\n17.9\n18.0\n0.463\n9.8\n\n\n4\n59.2\nAfghanistan\n2011\nDeveloping\n275.0\n71\n0.01\n7.097109\n68.0\n3013\n...\n68.0\n7.87\n68.0\n0.1\n63.537231\n2978599.0\n18.2\n18.2\n0.454\n9.5\n\n\n\n5 rows × 22 columns\n\nData Understanding\nThere are a lot of variables here!  Let\'s look at a correlation matrix to see which ones might be the most useful.  (Here we are looking for variables that are highly correlated with the target variable, but not highly correlated with other input variables)\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig1, ax1 = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr, mask=mask, ax=ax1)\n<matplotlib.axes._subplots.AxesSubplot at 0x12631bac8>\n\n\nOk, it looks like there are only a few that are highly positively correlated with life expectancy.  Let\'s make a pair plot of those.\n(Note: we don\'t want to do a pair plot right from the outset because it would be too slow)\npositively_correlated_cols = [\'Life_Expectancy\',\'BMI\', \'Income_Composition_of_Resources\', \'Schooling\']\npositively_correlated_df = df[positively_correlated_cols]\nsns.pairplot(positively_correlated_df);\n\nFirst Simple Model\nOk, it looks like the correlation with BMI is a little fuzzier than the others, so let\'s exclude it for now.  Schooling and Income_Composition_of_Resources are highly correlated with both life expectancy and each other, so let\'s only include one of them.  Schooling seems like a good choice because it would allow us to answer Question 5.\nfsm_df = df[[""Schooling"", ""Life_Expectancy""]].copy()\nfsm_df.dropna(inplace=True)\nfsm = ols(formula=""Life_Expectancy ~ Schooling"", data=fsm_df).fit()\nfsm.summary()\n\nOLS Regression Results\n\nDep. Variable: Life_Expectancy   R-squared:             0.565\n\n\nModel: OLS   Adj. R-squared:        0.565\n\n\nMethod: Least Squares   F-statistic:           3599.\n\n\nDate: Mon, 02 Mar 2020   Prob (F-statistic):   0.00\n\n\nTime: 09:50:09   Log-Likelihood:      -8964.3\n\n\nNo. Observations:   2768   AIC:                1.793e+04\n\n\nDf Residuals:   2766   BIC:                1.794e+04\n\n\nDf Model:      1    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    44.1089     0.437   100.992  0.000    43.252    44.965\n\n\nSchooling     2.1035     0.035    59.995  0.000     2.035     2.172\n\n\n\n\nOmnibus: 283.391   Durbin-Watson:         0.267\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):   1122.013\n\n\nSkew: -0.445   Prob(JB):           2.28e-244\n\n\nKurtosis:  5.989   Cond. No.               46.7\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nModel Evaluation\nNot too bad.  We are only explaining about 57% of the variance in life expectancy, but we only have one feature so far and it\'s statistically significant at an alpha of 0.05.\nWe could stop right now and say that according to our model:\n\nA country with zero years of schooling on average is expected to have a life expectancy of 44.1 years\nFor each additional average year of schooling, we expect life expectancy to increase by 2.1 years\n\nBut before we move forward, let\'s also check for the assumptions of linear regression\nLinearity\nLinear regression assumes that the input variable linearly predicts the output variable.  We already qualitatively checked that with a scatter plot.  But I also think it\'s a good idea to use a statistical test.  This one is the Rainbow test which is available from the diagnostic submodule of StatsModels\nrainbow_statistic, rainbow_p_value = linear_rainbow(fsm)\nprint(""Rainbow statistic:"", rainbow_statistic)\nprint(""Rainbow p-value:"", rainbow_p_value)\nRainbow statistic: 1.291015978641167\nRainbow p-value: 1.0575796565077053e-06\n\nThe null hypothesis is that the model is linearly predicted by the features, alternative hypothesis is that it is not.  Thus returning a low p-value means that the current model violates the linearity assumption.\nNormality\nLinear regression assumes that the residuals are normally distributed.  It is possible to check this qualitatively with a Q-Q plot, but this example shows how to assess it statistically.\nThe Jarque-Bera test is performed automatically as part of the model summary output, labeled Jarque-Bera (JB) and Prob(JB).\nThe null hypothesis is that the residuals are normally distributed, alternative hypothesis is that they are not.  Thus returning a low p-value means that the current model violates the normality assumption.\nHomoscadasticity\nLinear regression assumes that the variance of the dependent variable is homogeneous across different value of the independent variable(s).  We can visualize this by looking at the predicted life expectancy vs. the residuals.\ny = fsm_df[""Life_Expectancy""]\ny_hat = fsm.predict()\nfig2, ax2 = plt.subplots()\nax2.set(xlabel=""Predicted Life Expectancy"",\n        ylabel=""Residuals (Actual - Predicted Life Expectancy)"")\nax2.scatter(x=y_hat, y=y-y_hat, color=""blue"", alpha=0.2)\n<matplotlib.collections.PathCollection at 0x1277dfd68>\n\n\nJust visually inspecting this, it seems like our model over-predicts life expectancy between 60 and 70 years old in a way that doesn\'t happen for other age groups.  Plus we have some weird-looking data in the lower end that we might want to inspect.  Maybe there was something wrong with recording those values, or maybe there is something we can feature engineer once we have more independent variables.\nLet\'s also run a statistical test.  The Breusch-Pagan test is available from the diagnostic submodule of StatsModels\nlm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, fsm_df[[""Schooling""]])\nprint(""Lagrange Multiplier p-value:"", lm_p_value)\nprint(""F-statistic p-value:"", f_p_value)\nLagrange Multiplier p-value: nan\nF-statistic p-value: 2.2825932549998897e-67\n\nThe null hypothesis is homoscedasticity, alternative hypothesis is heteroscedasticity.  Thus returning a low p-value means that the current model violates the homoscedasticity assumption\nIndependence\nThe independence assumption means that the independent variables must not be too collinear.  Right now we have only one independent variable, so we don\'t need to check this yet.\nAdding Features to the Model\nSo far, all we have is a simple linear regression.  Let\'s start adding features to make it a multiple regression.\nFirst, I\'ll repeat the process of the highly positively correlated variables, but this time with the highly negatively correlated variables (based on looking at the correlation matrix)\nnegatively_correlated_cols = [\n    \'Life_Expectancy\',\n    \'Adult_Mortality\',\n    \'HIV_AIDS\',\n    \'Thinness_1_19_years\',\n    \'Thinness_5_9_years\'\n]\nnegatively_correlated_df = df[negatively_correlated_cols]\nsns.pairplot(negatively_correlated_df);\n\nAdult_Mortality seems most like a linear relationship to me.  Also, the two thinness metrics seem to be providing very similar information, so we almost certainly should not include both\nA quick experiment to try to flatten out that curve:\nfig3, ax3 = plt.subplots()\n\nax3.set(xlabel=""Adult_Mortality"", ylabel=""Life_Expectancy"")\nax3.scatter(x=np.sqrt(df[""Adult_Mortality""]), y=df[""Life_Expectancy""])\n<matplotlib.collections.PathCollection at 0x127ffe390>\n\n\nThis gives me straighter lines, but seems to indicate that we probably need at least two separate models to represent this data correctly.  However in the interest of time, I\'m just going to assume the relationship is linear.\nmodel_2_df = df[[""Life_Expectancy"", ""Schooling"", ""Adult_Mortality""]].copy()\nmodel_2_df.dropna(inplace=True)\nmodel_2 = ols(formula=""Life_Expectancy ~ Schooling + Adult_Mortality"", data=model_2_df).fit()\nmodel_2.summary()\n\nOLS Regression Results\n\nDep. Variable: Life_Expectancy   R-squared:             0.714\n\n\nModel: OLS   Adj. R-squared:        0.713\n\n\nMethod: Least Squares   F-statistic:           3443.\n\n\nDate: Mon, 02 Mar 2020   Prob (F-statistic):   0.00\n\n\nTime: 10:05:02   Log-Likelihood:      -8387.7\n\n\nNo. Observations:   2768   AIC:                1.678e+04\n\n\nDf Residuals:   2765   BIC:                1.680e+04\n\n\nDf Model:      2    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    56.0636     0.475   117.981  0.000    55.132    56.995\n\n\nSchooling     1.5541     0.032    48.616  0.000     1.491     1.617\n\n\nAdult_Mortality    -0.0329     0.001   -37.803  0.000    -0.035    -0.031\n\n\n\n\nOmnibus: 537.142   Durbin-Watson:         0.685\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):   2901.045\n\n\nSkew: -0.813   Prob(JB):               0.00\n\n\nKurtosis:  7.745   Cond. No.           1.02e+03\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.02e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\nSecond Model Evaluation\nAdding another feature improved the r-squared from 0.565 to 0.714\nBut let\'s also look at the model assumptions\nLinearity\nrainbow_statistic, rainbow_p_value = linear_rainbow(model_2)\nprint(""Rainbow statistic:"", rainbow_statistic)\nprint(""Rainbow p-value:"", rainbow_p_value)\nRainbow statistic: 1.0919639546889188\nRainbow p-value: 0.05102555171520467\n\nAssuming an alpha of 0.05, we are no longer violating the linearity assumption (just barely)\nNormality\nThe Jarque-Bera (JB) output has gotten worse.  We are still violating the normality assumption.\nHomoscadasticity\ny = model_2_df[""Life_Expectancy""]\ny_hat = model_2.predict()\nfig4, ax4 = plt.subplots()\nax4.set(xlabel=""Predicted Life Expectancy"",\n        ylabel=""Residuals (Actual - Predicted Life Expectancy)"")\nax4.scatter(x=y_hat, y=y-y_hat, color=""blue"", alpha=0.2)\n<matplotlib.collections.PathCollection at 0x12840d6d8>\n\n\nlm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, model_2_df[[""Schooling"", ""Adult_Mortality""]])\nprint(""Lagrange Multiplier p-value:"", lm_p_value)\nprint(""F-statistic p-value:"", f_p_value)\nLagrange Multiplier p-value: 3.894654511131417e-47\nF-statistic p-value: 1.2521305604465352e-47\n\nBoth visually and numerically, we can see some improvement.  But we are still violating this assumption to a statistically significant degree.\nIndependence\nYou might have noticed in the regression output that there was a warning about the condition number being high.  The condition number is a measure of stability of the matrix used for computing the regression (we\'ll discuss this more in the next module), and a number above 30 can indicate strong multicollinearity.  Our output is way higher than that.\nA different (more generous) measure of multicollinearity is the variance inflation factor.  It is available from the outlier influence submodule of StatsModels.\nrows = model_2_df[[""Schooling"", ""Adult_Mortality""]].values\n\nvif_df = pd.DataFrame()\nvif_df[""VIF""] = [variance_inflation_factor(rows, i) for i in range(2)]\nvif_df[""feature""] = [""Schooling"", ""Adult_Mortality""]\n\nvif_df\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nVIF\nfeature\n\n\n\n\n0\n1.937556\nSchooling\n\n\n1\n1.937556\nAdult_Mortality\n\n\n\n\nA ""rule of thumb"" for VIF is that 5 is too high, so I think it\'s reasonable to say that we are not violating the independence assumption, despite the high condition number.\nAdding a Categorical Value\nThis is less realistic than the previous steps but I wanted to give an example\nIn this dataset, we have a lot of numeric values (everything in that correlation matrix), but there are a few that aren\'t.  One example is Status\nmodel_3_df = df[[""Life_Expectancy"", ""Schooling"", ""Adult_Mortality"", ""Status""]].copy()\nmodel_3_df.dropna(inplace=True)\nmodel_3_df[""Status""].value_counts()\nDeveloping    2304\nDeveloped      464\nName: Status, dtype: int64\n\nsns.catplot(x=""Status"", y=""Life_Expectancy"", data=model_3_df, kind=""box"")\n<seaborn.axisgrid.FacetGrid at 0x128360d68>\n\n\nIt looks like there is a difference between the two groups that might be useful to include\nThere are only two categories, so we only need a LabelEncoder that will convert the labels into 1s and 0s.  If there were more than two categories, we would use a OneHotEncoder, which would create multiple columns out of a single column.\nlabel_encoder = LabelEncoder()\nstatus_labels = label_encoder.fit_transform(model_3_df[""Status""])\nstatus_labels\narray([1, 1, 1, ..., 1, 1, 1])\n\nlabel_encoder.classes_\narray([\'Developed\', \'Developing\'], dtype=object)\n\nThis is telling us that ""Developed"" is encoded as 0 and ""Developing"" is encoded as 1.  This means that ""Developed"" is assumed at the intercept.\nmodel_3_df[""Status_Encoded""] = status_labels\nmodel_3_df.drop(""Status"", axis=1)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nLife_Expectancy\nSchooling\nAdult_Mortality\nStatus_Encoded\n\n\n\n\n0\n65.0\n10.1\n263.0\n1\n\n\n1\n59.9\n10.0\n271.0\n1\n\n\n2\n59.9\n9.9\n268.0\n1\n\n\n3\n59.5\n9.8\n272.0\n1\n\n\n4\n59.2\n9.5\n275.0\n1\n\n\n...\n...\n...\n...\n...\n\n\n2933\n44.3\n9.2\n723.0\n1\n\n\n2934\n44.5\n9.5\n715.0\n1\n\n\n2935\n44.8\n10.0\n73.0\n1\n\n\n2936\n45.3\n9.8\n686.0\n1\n\n\n2937\n46.0\n9.8\n665.0\n1\n\n\n\n2768 rows × 4 columns\n\nmodel_3 = ols(formula=""Life_Expectancy ~ Schooling + Adult_Mortality + Status_Encoded"", data=model_3_df).fit()\nmodel_3.summary()\n\nOLS Regression Results\n\nDep. Variable: Life_Expectancy   R-squared:             0.718\n\n\nModel: OLS   Adj. R-squared:        0.718\n\n\nMethod: Least Squares   F-statistic:           2350.\n\n\nDate: Mon, 02 Mar 2020   Prob (F-statistic):   0.00\n\n\nTime: 10:17:00   Log-Likelihood:      -8364.0\n\n\nNo. Observations:   2768   AIC:                1.674e+04\n\n\nDf Residuals:   2764   BIC:                1.676e+04\n\n\nDf Model:      3    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    58.9976     0.634    93.014  0.000    57.754    60.241\n\n\nSchooling     1.4447     0.035    40.772  0.000     1.375     1.514\n\n\nAdult_Mortality    -0.0324     0.001   -37.395  0.000    -0.034    -0.031\n\n\nStatus_Encoded    -2.0474     0.296    -6.910  0.000    -2.628    -1.466\n\n\n\n\nOmnibus: 570.672   Durbin-Watson:         0.678\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):   2798.757\n\n\nSkew: -0.899   Prob(JB):               0.00\n\n\nKurtosis:  7.586   Cond. No.           1.45e+03\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.45e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\nThird Model Evaluation\nAdding another feature improved the r-squared a tiny bit from 0.714 to 0.718\nLet\'s look at the model assumptions again\nLinearity\nrainbow_statistic, rainbow_p_value = linear_rainbow(model_3)\nprint(""Rainbow statistic:"", rainbow_statistic)\nprint(""Rainbow p-value:"", rainbow_p_value)\nRainbow statistic: 1.0769559317010546\nRainbow p-value: 0.08416346182745871\n\nAnother small improvement\nNormality\nThe Jarque-Bera (JB) output has gotten slightly better.  But we are still violating the normality assumption.\nHomoscadasticity\ny = model_3_df[""Life_Expectancy""]\ny_hat = model_3.predict()\nfig5, ax5 = plt.subplots()\nax5.set(xlabel=""Predicted Life Expectancy"",\n        ylabel=""Residuals (Actual - Predicted Life Expectancy)"")\nax5.scatter(x=y_hat, y=y-y_hat, color=""blue"", alpha=0.2)\n<matplotlib.collections.PathCollection at 0x12870b048>\n\n\nlm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, model_3_df[[""Schooling"", ""Adult_Mortality"", ""Status_Encoded""]])\nprint(""Lagrange Multiplier p-value:"", lm_p_value)\nprint(""F-statistic p-value:"", f_p_value)\nLagrange Multiplier p-value: 6.828322778473366e-89\nF-statistic p-value: 9.274093378149953e-95\n\nThis metric got worse, although the plot looks fairly similar\nIndependence\nrows = model_3_df[[""Schooling"", ""Adult_Mortality"", ""Status_Encoded""]].values\n\nvif_df = pd.DataFrame()\nvif_df[""VIF""] = [variance_inflation_factor(rows, i) for i in range(3)]\nvif_df[""feature""] = [""Schooling"", ""Adult_Mortality"", ""Status_Encoded""]\n\nvif_df\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nVIF\nfeature\n\n\n\n\n0\n3.120074\nSchooling\n\n\n1\n2.838221\nAdult_Mortality\n\n\n2\n4.518962\nStatus_Encoded\n\n\n\n\nThe VIF metrics are getting higher, which means that there is stronger multicollinearity.  But we have still not exceeded the threshold of 5.\nSummary\nWe started with a baseline model where the only input feature was Schooling.  Our baseline model had an r-squared of 0.565.  This model violated the linearity (p < 0.001), normality (p < 0.001), and homoscadasticity (p < 0.001) assumptions of linear regression.  The independence assumption was met by default because there was only one input feature.\nThe final model for this lesson had three input features: Schooling, Adult_Mortality, and Status_Encoded.  It had an r-squared of 0.718.  This model did not violate the linearity assumption (p = 0.084), but it did violate the normality (p < 0.001) and homoscedasticity (p < 0.001) assumptions.  Based on the variance inflaction factor metric, it did not violate the independence assumption.\nWe are able to address the following questions from above:\n1. Does various predicting factors which has been chosen initially really affect the Life expectancy? What are the predicting variables actually affecting the life expectancy?\nWith only 3 features we are able to explain about 71% of the variance in life expectancy.  This indicates that these factors truly are explanatory.  More analysis is required to understand how much additional explanatory power would be provided by incorporating additional features into the model.\n3. How does Infant and Adult mortality rates affect life expectancy?\nSo far we have only investigated adult mortality.  The adult mortality rate (""probability of dying between 15 and 60 years per 1000 population"") has a negative correlation with life expectancy.  For each increase of 1 in the adult mortality rate, life expectancy decreases by about .03 years.\n5. What is the impact of schooling on the lifespan of humans?\nIn our latest model, we find that each additional year of average schooling is associated with 1.4 years of added life expectancy.  However it is challenging to interpret whether it is schooling that is actually having the impact.  Schooling is highly correlated with Income_Composition_of_Resources (""Human Development Index in terms of income composition of resources"") so it is very possible that schooling is the result of some underlying factor that also impacts life expectancy, rather than schooling impacting life expectancy directly.\nAppendix\nThings I have not done in this lesson, but that you should consider in your project:\n\nMore robust cleaning (possible imputation of missing values, principled exclusion of some data)\nFeature scaling\nNearest-neighbors approach (requires more complex feature engineering)\nPulling information from external resources\nRemoving independent variables if you determine that they are causing too high of multicollinearity\nSetting up functions so the code is not so repetitive\n\nAlso, I\'ve included a dataset called cars.csv if you are interested in additional practice that does not use the King County Housing Data\nfig3, ax3 = plt.subplots()\n\nax3.set(xlabel=""Status_Encoded"", ylabel=""Life_Expectancy"")\nax3.scatter(x=model_3_df[""Status_Encoded""], y=model_3_df[""Life_Expectancy""])\n<matplotlib.collections.PathCollection at 0x1287d87b8>\n\n\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'GuangZhou/Warsaw', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Bounding-Box-Regression-GUI\nThis program shows how Bounding-Box-Regression works in a visual form.\nUpdating soon for Digit-Recognition-CNN-and-ANN-using-Mnist-with-GUI\nhttps://github.com/timmmGZ/Digit-Recognition-CNN-and-ANN-using-Mnist-with-GUI\nI am going to modify the above program in my free time, add this Bounding Box Regression along with ROI Align layer(https://github.com/timmmGZ/ROIAlign-Bounding-Box-ROI-Align-of-Mask-RCNN-GUI) in it, make it become a MNIST object detection.\nFirst of all, Let\'s see how it works\n\nPredefine\nDownload the MNIST digit 60000 train set and 10000 test set here:\nhttps://drive.google.com/open?id=1VwABcxX0DaQakPpHbMaRQJHlJf3mVONf\n\n\nPut both files to ../dataset/, and then go to ""tool"" package.\n\n\nRun the ""CreatePictureForObjectDetectionFromMNIST.java"", it will create random pictures base on MNIST datasets in ../dataset/pictures.\n\n\nAfter step 2, run the ""CreateForeOrBackgroundSample.java"", it will create random foreground(in ../dataset/foreground) and background(in ../dataset/background) datasets based on output pictures of step 2, you could see it more clearly in ../dataset/groundTruthExamples.\nas below:\n\n\n\nBoth step 2 and 3 will create Label-files in ../dataset/standardOutput, you could see the column names in first line of each Label-files\n\n\nMake sure you have big enough RAM if you want to store more datasets in RAM, watch below picture:\n\n\n\nStart\nRun the MainFrame.java, if you don\'t want to train the model, click ""Menu"" then ""Read Weight"", that is my trained weights around 94% accuracy on both train and test set(70000 datasets in total, I define it is true prediction if Predicted-Bounding-Box has higher IOU with Ground-Truth-Bounding-Box than it has with input Bounding-Box).\nWarning\nActually the number of predicted boxes should = the number of classes(e.g. one Bounding-Box can have both Apple and Bird inside it), but this program is just a Digit-Detection, for convenience and higher FPS in real-time detection, I make it have only one predicted box, see the advantage of having normal number of predicted boxes as below picture:\n\nBounding-Box-Regression is used after NMS(Non Maximum Suppression)\nFor each picture, we will get so many Bounding-Box(Region-Propasal), NMS is used for filtering out the best Bounding-Boxes, below gif shows what will look like if we only use ROI-Align-layer(not necessary but better use it) and NMS but not use Bounding-Box-Regression:\nPredefine about ROI-Align-layer: sample size=1, output size=7, feature maps=16(https://github.com/timmmGZ/ROIAlign-Bounding-Box-ROI-Align-of-Mask-RCNN-GUI)\n\nLet\'s cut one picture from the gif, when the object is small like below picture, and the number of objects is big, obviously we need to do Bounding-Box-Regression, or it will be a mess.\n\n'], 'url_profile': 'https://github.com/timmmGZ', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'California, United States', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': [""DORN_depth_estimation_Pytorch\nThis is an unoficial Pytorch implementation of Deep Ordinal Regression Network for Monocular Depth Estimation paper by Fu et. al.\nTable. Performance on NYU V2.\n\n\n\nSource\nδ1\nδ2\nδ3\nrel\nlog10\nrms\n\n\n\n\nOriginal paper*\n0.828\n0.965\n0.992\n0.115\n0.051\n0.509\n\n\nThis repo*\n0.806\n0.957\n0.989\n0.151\n0.062\n0.586\n\n\n\n*Note, that the data splits are different (see Known Differences below for details). The worse performance might be due to the smaller training set (795 vs about 120K images).\nHow to use\nThese steps show how to run the code on the official split of the NYU V2 depth dataset.\nTo prepare data:\n\nDownload nyu_depth_v2_labeled.mat (data) and splits.mat.\nEdit create_nyu_h5.py to add data_path (folder with the .mat files from previous step) and output_path.\nRun:\n\npython create_nyu_h5.py\nFor start training on NYU V2 run:\ntrain.py [-h] [--dataset DATASET] [--data-path DATA_PATH]\n              [--pretrained] [--epochs EPOCHS] [--bs BS] [--bs-test BS_TEST]\n              [--lr LR] [--gpu GPU]\nOr simply:\npython train.py --data-path DATA_PATH --pretrained\n(where DATA_PATH is same as output_path used during preparing data).\nFor more info on arguments run:\npython train.py --help\nTo train on a different dataset, implementation of the DataLoader is required.\nTo monitor training, use Tensorboard:\ntensorboard --logdir ./logs/\nKnown Differences\nThe implementation closely follows the paper and the official repo with some exceptions. The list of known differences:\n\nOnly training on the labeled part of NYU V2 is currently implemented (not on all the raw data).\nColorJitter is used instead of the color transformation from the Eigen's paper.\nFeature extractor is pretrained on a different dataset.\n\nPretrained feature extractor\nDORN uses a modified version of ResNet-101 as a feature extractor (with dilations and three 3x3 convolutional layers in the begining instead of one 7x7 layer). If you select pretrained=True, weights pretrained on MIT ADE20K dataset will be loaded from this project. This is different from the paper (the authors suggest pretraining on ImageNet). That is the only suitable pretrained model on the Web that I am aware of.\nRequirements\n\nPython 3\nPytorch (version 1.3 tested)\nTorchvision\nTensorboard\n\nAcknowledgements\nThe code is based on this implementation.\n""], 'url_profile': 'https://github.com/liviniuk', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'Atlanta', 'stats_list': [], 'contributions': '244 contributions\n        in the last year', 'description': ['\n\n\n\n\nGeorgia Tech Item Response Theory\n\nGirth is a python package for estimating item response theory (IRT) parameters.  In addition, synthetic IRT data generation is supported. Below is a list of available functions, for more information visit the GIRTH homepage.\nInterested in Bayesian Models? Check out girth_mcmc. It provides markov chain and variational inference estimation methods.\nDichotomous Models\n\nRasch Model\n\nJoint Maximum Likelihood\nConditional Likelihood\nMarginal Maximum Likelihood\n\n\nOne Parameter Logistic Models\n\nJoint Maximum Likelihood\nMarginal Maximum Likelihood\n\n\nTwo Parameter Logistic Models\n\nJoint Maximum Likelihood\nMarginal Maximum Likelihood\nMixed Expected A Prior / Marginal Maximum Likelihood\n\n\nThree Parameter Logistic Models\n\nMarginal Maximum Likelihood (No Optimization and Minimal Support)\n\n\n\nPolytomous Models\n\nGraded Response Model\n\nJoint Maximum Likelihood\nMarginal Maximum Likelihood\nMixed Expected A Prior / Marginal Maximum Likelihood\n\n\nPartial Credit Model\n\nJoint Maximum Likelihood\nMarginal Maximum Likelihood\n\n\nGraded Unfolding Model\n\nMarginal Maximum Likelihood\n\n\n\nAblity Estimation\n\nDichotomous\n\nMarginal Likelihood Estimation\nMaximum a Posteriori Estimation\nExpected a Posteriori Estimation\n\n\nPolytomous\n\nExpected a Posteriori Estimation\n\n\n\nSupported Synthetic Data Generation\n\nRasch / 1PL Models Dichotomous Models\n2 PL Dichotomous Models\n3 PL Dichotomous Models\nGraded Response Model Polytomous\nPartial Credit Model Polytomous\nGraded Unfolding Model Polytomous\nMultidimensional Dichotomous Models\n\nInstallation\nVia pip\npip install girth --upgrade\n\nFrom Source\npython setup.py install --prefix=path/to/your/installation\n\nDependencies\nWe recommend the anaconda environment which can be installed\nhere\n\nPython 3.7\nNumpy\nScipy\nNumba\n\nUsage\nimport numpy as np\n\nfrom girth import create_synthetic_irt_dichotomous\nfrom girth import twopl_mml\n\n# Create Synthetic Data\ndifficulty = np.linspace(-2.5, 2.5, 10)\ndiscrimination = np.random.rand(10) + 0.5\ntheta = np.random.randn(500)\n\nsyn_data = create_synthetic_irt_dichotomous(difficulty, discrimination, theta)\n\n# Solve for parameters\nestimates = twopl_mml(syn_data)\n\n# Unpack estimates\ndiscrimination_estimates = estimates[\'Discrimination\']\ndifficulty_estimates = estimates[\'Difficulty\']\nMissing Data\nMissing data is supported with the tag_missing_data function.\nfrom girth import tag_missing_data\nfrom girth import twopl_mml\n\n# import data (you supply this function)\nmy_data = import_data(filename)\n\n# Assume its dichotomous data with True -> 1 and False -> 0\ntagged_data = tag_missing_data(my_data, [0, 1])\n\n# Run Estimation\nresults = twopl_mml(tagged_data)\nUnittests\nWithout coverage.py module\nnosetests testing/\n\nWith coverage.py module\nnosetests --with-coverage --cover-package=girth testing/\n\nContact\nRyan Sanchez\nrsanchez44@gatech.edu\nLicense\nMIT License\nCopyright (c) 2020 Ryan Sanchez\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n'], 'url_profile': 'https://github.com/eribean', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'Seoul, Korea', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['Naver Best webtoon prediction\n\n\nURL: https://comic.naver.com/genre/challenge.nhn?m=list\n\n\nDescription\nNaver webtoon is a 3 stage league.\nThe best webtoons are exposed to main page. To be one of the best they need to be the winenrs of 3rd and 2nd league. This is about prdicting which is likely to be selected using linear regression model.\n\nRequirements\nDataset\n\n\n\n'], 'url_profile': 'https://github.com/h33sun', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '550 contributions\n        in the last year', 'description': ['Ways to close backdoors in DAGs\n\nSee the actual blog\npost.\n\n\nI’ve been teaching program\nevaluation in the MPA/MPP\nprogram at GSU for the past semester and a half, and since I was given\nfree rein over how to teach it, I decided to make it as modern and\ncutting edge as possible. To do this, I’ve infused the class with modern\ncausal inference techniques (commonly known as the “causal\nrevolution”),\nfocused on the language of directed acyclic graphs\n(DAGs),\ndo-calculus, confounding,\ncolliding, and the rest of Judea Pearl’s world of finding causation in\nobservational data.\nHowever, I was never taught this approach in any of my doctoral classes,\nand I’m entirely self-taught. I read The Book of\nWhy\nin March 2019, and I’ve thrown myself into every possible journal\narticle, blog post, and online course I’ve come across in order to\nunderstand how to isolate and identify causal effects. It’s been hard,\nbut thanks to an incredibly welcoming community of economists,\nepidemiologists, and political scientists on Twitter, I’m getting it!\n(And so are my students!)\nThe purpose of this post isn’t to introduce causal models and DAGs and\nconfounders vs.\xa0colliders and all that. For that kind of introduction,\nconsult any (or all!) of these resources:\n\nNick Huntington-Klein’s ECON 305: Economics, Causality, and\nAnalytics course (especially\nlectures 13–18)\nJulia M. Rohrer, “Thinking Clearly About Correlations and Causation:\nGraphical Causal Models for Observational Data,” Advances in Methods\nand Practices in Psychological Science 1, no. 1 (March 2018):\n27–42, doi:\n10.1177/2515245917745629\nMiguel A. Hernán and James M. Robbins, Causal Inference: What If\n(CRC Press, 2020),\nhttps://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/\nPaul Hünermund and Elias Bareinboim, “Causal Inference and\nData-Fusion in Econometrics,” December 19, 2019, arXiv: 1912.09104\n[econ.EM], https://arxiv.org/abs/1912.09104\nFelix Elwert, “Graphical Causal Models,” chap.\xa013 in Handbook of\nCausal Analysis for Social Research, ed.\xa0Stephen L. Morgan (New\nYork: Springer, 2013), 245–273, doi:\n10.1007/978-94-007-6094-3_13\nJulian Schuessler and Peter Selb, “Graphical Causal Models for\nSurvey Inference” (December 17, 2019), doi:\n10.31235/osf.io/hbg3m,\nhttps://osf.io/preprints/socarxiv/hbg3m/\nScott Cunningham, “Directed acyclical graphs” in Causal Inference:\nThe Mixtape (2018), https://www.scunning.com/mixtape.html\nPaul Hünermund, “Causal Inference with Directed Acyclic Graphs,”\nUdemy\nJason A. Roy, “A Crash Course in Causality: Inferring Causal Effects\nfrom Observational Data,”\nCoursera\n\nInstead, this post is a practical example of how exactly you can isolate\ncausal effects by closing backdoor paths and adjusting for confounders\nin observational data. At its core, DAG-based causal inference involves\nisolating relationships—if some variable causes both your treatment and\nyour outcome (thus confounding it), you can deal with that common cause\nin some statistical way and isolate the treatment–outcome effect.\nThere’s no one right way to statistically deal with confounding, so\nhere I show a few different ways to do it.\nTo make this more concrete and practical, I use simulated data from a\nhypothetical program, but I use actual variable names rather than the\n(x), (y), and (z) variables that are common in tutorials and\narticles about DAGs. Here, our outcome (y) is a final grade, (x) is\na special math camp, and (z) is all the possible confounders of the\neffect of the math camp on the grade.\nLike my post showing a bunch of different ways to test differences in\nmeans,\nthis is mostly a resource to my students and to future me. Please feel\nfree to comment and make corrections or additions at\nGitHub!\nHere’s a tl;dr table of contents since this is a little long:\n\nExample program\nSimulated data\nIncorrect “correlation is not causation”\nestimate\nAdjustment using forbidden unmeasured\nvariable\nAdjustment using educated-guess-based naive\nmatching\nBrief interlude: Matching + slightly simpler\nDAG\nAdjustment using inverse probability weighting\n(IPW)\nAdjustment using matching (with Mahalanobis\ndistance)\nSomeday when I’m smarter:\ndo-calculus\nComparison of all methods\n\nExample program\nWe’ll refer to a hypothetical math camp program throughout all these\nexamples. Many policy schools offer a brief math camp in the weeks\nbefore students begin their graduate degrees, with the hope that it will\nhelp students be more prepared in math-heavy classes like statistics and\nmicroeconomics. For these examples, we’re interested in answering one\nquestion: what is the causal effect of attending math camp on final\nstudent outcomes?\nWe can use\nggdag\nto draw a simplified causal model that explains what causes final\nstudent outcomes (it’s probably wrong, but whatever). I added all the\nfancy bells and whistles to the graph object here just for the sake of\nreference. In reality, you don’t need labels or coordinates or the\nindividual geom_dag_*() layers and you can just do\nggdag(math_camp_dag) to get a basic graph, but for fun I’ve included\neverything you need for a publication-worthy graph.\nlibrary(tidyverse)  # ggplot, dplyr, %>%, and friends\nlibrary(ggdag)  # Make DAGs with ggplot\nlibrary(dagitty)  # Do basic DAG math\nlibrary(broom)  # For converting model output to data frames\n\nnode_details <- tribble(\n  ~name, ~label, ~x, ~y,\n  ""math_camp"", ""Math camp"", 2, 1,\n  ""final_grade"", ""Final grade"", 4, 1,\n  ""needs_camp"", ""Needs camp"", 1, 2,\n  ""gre_quant"", ""GRE quantitative"", 2.5, 2,\n  ""gre_verbal"", ""GRE verbal"", 5, 2,\n  ""background"", ""Background"", 2, 3,\n  ""undergraduate_gpa"", ""Undergraduate GPA"", 4, 3\n)\n\nnode_labels <- node_details$label\nnames(node_labels) <- node_details$name\n\nmath_camp_dag <- dagify(final_grade ~ math_camp + gre_quant + gre_verbal + \n                          undergraduate_gpa + background,\n                        math_camp ~ needs_camp, \n                        needs_camp ~ background + undergraduate_gpa + gre_quant,\n                        gre_quant ~ background + undergraduate_gpa,\n                        gre_verbal ~ background + undergraduate_gpa,\n                        undergraduate_gpa ~ background,\n                        exposure = ""math_camp"",\n                        outcome = ""final_grade"",\n                        latent = ""background"",\n                        coords = node_details,\n                        labels = node_labels)\n\n# Turn DAG into a tidy data frame for plotting\nmath_camp_dag_tidy <- math_camp_dag %>% \n  tidy_dagitty() %>%\n  node_status()   # Add column for exposure/outcome/latent\n\nstatus_colors <- c(exposure = ""#0074D9"", outcome = ""#FF4136"", latent = ""grey50"")\n\n# Fancier graph\nggplot(math_camp_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_label_repel(aes(label = label, fill = status), seed = 1234,\n                       color = ""white"", fontface = ""bold"") +\n  scale_color_manual(values = status_colors, na.value = ""grey20"") +\n  scale_fill_manual(values = status_colors, na.value = ""grey20"") +\n  guides(color = FALSE, fill = FALSE) + \n  theme_dag()\n\nWe can tell a fairly complex story using this graph. Your final grade in\nthe program is caused by a host of things, including your quantitative\nand verbal GRE scores (PROBABLY DEFINITELY NOT in real\nlife,\nbut go with it), your undergraduate GPA, and your unmeasured background\nfactors (age, parental income, math anxiety, level of interest in the\nprogram, etc.). Your undergraduate GPA is determined by your background,\nand your GRE scores are determined by both your undergraduate GPA and\nyour background. Because this math camp program is open to anyone, there\nis self-selection in who chooses to do it. We can pretend that this is\ndecided by your undergraduate GPA, your quantitative GRE score, and your\nbackground. If the program was need-based and only offered to people\nwith low GRE scores, we could draw an arrow from GRE quantitative to\nmath camp, but we don’t. Finally, needing the math camp causes people to\ndo it.\nThere is a direct path between our treatment and outcome (math camp →\nfinal grade), but there is also some possible backdoor confounding. Both\nGRE quantitative and undergraduate GPA have arrows pointing to final\ngrade and math camp (through “needs camp”), which means they’re a common\ncause, and background is both a confounder and unmeasurable. But we\ndon’t need to give up! If we adjust or control for “needs camp,” we can\nblock the association between background, GRE quantitative, and\nundergraduate GPA. With this backdoor closed, we’ve isolated the math\ncamp → final grade relationship and can find the causal effect.\nHowever, we don’t really have a measure for needing math camp—we can’t\nread peoples’ minds and see if they need the program—so while it’d be\ngreat to just include a needs_camp variable in a regression model,\nwe’ll have to use other techniques to close the backdoor.\nYou can find the backdoors automatically with Dagitty (draw the DAG\nthere and notice red arrows between the unblocked confounders), or with\nfunctions in the dagitty R\npackage. If you run\npaths(math_camp_dag), you can see that the only node pointing back\ninto math_camp is needs_camp, and if you run\nadjustmentSets(math_camp_dag), you’ll see that needs_camp is the\nonly required adjustment set:\npaths(math_camp_dag)\n## $paths\n##  [1] ""math_camp -> final_grade""                                                                            \n##  [2] ""math_camp <- needs_camp <- background -> final_grade""                                                \n##  [3] ""math_camp <- needs_camp <- background -> gre_quant -> final_grade""                                   \n##  [4] ""math_camp <- needs_camp <- background -> gre_quant <- undergraduate_gpa -> final_grade""              \n##  [5] ""math_camp <- needs_camp <- background -> gre_quant <- undergraduate_gpa -> gre_verbal -> final_grade""\n##  [6] ""math_camp <- needs_camp <- background -> gre_verbal -> final_grade""                                  \n##  [7] ""math_camp <- needs_camp <- background -> gre_verbal <- undergraduate_gpa -> final_grade""             \n##  [8] ""math_camp <- needs_camp <- background -> gre_verbal <- undergraduate_gpa -> gre_quant -> final_grade""\n##  [9] ""math_camp <- needs_camp <- background -> undergraduate_gpa -> final_grade""                           \n## [10] ""math_camp <- needs_camp <- background -> undergraduate_gpa -> gre_quant -> final_grade""              \n## [11] ""math_camp <- needs_camp <- background -> undergraduate_gpa -> gre_verbal -> final_grade""             \n## [12] ""math_camp <- needs_camp <- gre_quant -> final_grade""                                                 \n## [13] ""math_camp <- needs_camp <- gre_quant <- background -> final_grade""                                   \n## [14] ""math_camp <- needs_camp <- gre_quant <- background -> gre_verbal -> final_grade""                     \n## [15] ""math_camp <- needs_camp <- gre_quant <- background -> gre_verbal <- undergraduate_gpa -> final_grade""\n## [16] ""math_camp <- needs_camp <- gre_quant <- background -> undergraduate_gpa -> final_grade""              \n## [17] ""math_camp <- needs_camp <- gre_quant <- background -> undergraduate_gpa -> gre_verbal -> final_grade""\n## [18] ""math_camp <- needs_camp <- gre_quant <- undergraduate_gpa -> final_grade""                            \n## [19] ""math_camp <- needs_camp <- gre_quant <- undergraduate_gpa -> gre_verbal -> final_grade""              \n## [20] ""math_camp <- needs_camp <- gre_quant <- undergraduate_gpa -> gre_verbal <- background -> final_grade""\n## [21] ""math_camp <- needs_camp <- gre_quant <- undergraduate_gpa <- background -> final_grade""              \n## [22] ""math_camp <- needs_camp <- gre_quant <- undergraduate_gpa <- background -> gre_verbal -> final_grade""\n## [23] ""math_camp <- needs_camp <- undergraduate_gpa -> final_grade""                                         \n## [24] ""math_camp <- needs_camp <- undergraduate_gpa -> gre_quant -> final_grade""                            \n## [25] ""math_camp <- needs_camp <- undergraduate_gpa -> gre_quant <- background -> final_grade""              \n## [26] ""math_camp <- needs_camp <- undergraduate_gpa -> gre_quant <- background -> gre_verbal -> final_grade""\n## [27] ""math_camp <- needs_camp <- undergraduate_gpa -> gre_verbal -> final_grade""                           \n## [28] ""math_camp <- needs_camp <- undergraduate_gpa -> gre_verbal <- background -> final_grade""             \n## [29] ""math_camp <- needs_camp <- undergraduate_gpa -> gre_verbal <- background -> gre_quant -> final_grade""\n## [30] ""math_camp <- needs_camp <- undergraduate_gpa <- background -> final_grade""                           \n## [31] ""math_camp <- needs_camp <- undergraduate_gpa <- background -> gre_quant -> final_grade""              \n## [32] ""math_camp <- needs_camp <- undergraduate_gpa <- background -> gre_verbal -> final_grade""             \n## \n## $open\n##  [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n## [12]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n## [23]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\nadjustmentSets(math_camp_dag)\n##  { needs_camp }\n\nSimulated data\nAssuming this causal graph is correct (it’s probably not, but again, go\nwith it), we can simulate data that reflects these causal relationships.\nThere are a host of R packages for simulating data (like\nwakefield,\nsimstudy, and\nfabricatr), but here I do\nit a little more manually using MASS::mvrnorm() to use a multivariate\nnormal distribution to generate continuous variables that have a\nspecific mean, standard deviation, and relationship with other\nvariables.\nBecause data generation is beyond the scope of this post, the code below\nis heavily annotated. Importantly, the various mutate() commands that\ncreate the math_camp data below create relationships between the\nconfounders, treatment, and outcome. We also create a needs_camp\nvariable that is true if both a student’s quantitative GRE score is\nless than the average and if their undergraduate GPA is less than the\naverage. We also build in some noncompliance: 80% of those who need math\ncamp do it; 20% of those who don’t need it do it.\nFor the sake of simplicity, the outcome here (final_grade) isn’t GPA\nor anything—it’s an arbitrary number between 120 and 160 (though we\ncould rescale it to something else).\nMost importantly, we force the math camp program to cause a 10\npoint increase in students’ final grades. This is our baseline average\ntreatment effect that we want to be able to find using different\nbackdoor adjustment techniques.\n# Make these random draws the same every time\nset.seed(1234)\n\n# Create 2,000 rows\nnum <- 2000\n\n# Create confounder variables that are related to each other\nmu <- c(undergrad_gpa = 3, gre_verbal = 160, gre_quant = 145)\nstddev <- c(undergrad_gpa = 0.5, gre_verbal = 10, gre_quant = 5)\n\n# Correlation matrix: undergrad GPA and verbal GRE have a correlation of 0.8;\n# undergrad GPA and quantitative GRE have a correlation of 0.6, and verbal GRE\n# and quantitative GRE have a correlation of 0.4\ncor_matrix <- matrix(c(1.0, 0.8, 0.6,\n                       0.8, 1.0, 0.4,\n                       0.6, 0.4, 1.0),\n                     ncol = 3)\n\n# Convert correlation matrix to covariance matrix using fancy math\ncov_matrix <- stddev %*% t(stddev) * cor_matrix\n\n# Draw random numbers\nconfounders <- MASS::mvrnorm(n = num, mu = mu, Sigma = cov_matrix, empirical = TRUE) %>%\n  as_tibble() %>%\n  # Truncate values so they\'re within 130-170 range for GRE and less than 4.0 for GPA\n  mutate_at(vars(gre_verbal, gre_quant),\n            ~case_when(\n              . > 170 ~ 170,\n              . < 130 ~ 130,\n              TRUE ~ .\n            )) %>%\n  mutate(undergrad_gpa = ifelse(undergrad_gpa > 4, 4, undergrad_gpa))\n\n# Make official dataset of simulated values\nmath_camp <- tibble(id = 1:num) %>%\n  bind_cols(confounders) %>%  # Bring in confounders\n  # People need math camp if their GRE and GPA is lower than the average\n  mutate(needs_camp = gre_quant < mean(gre_quant) & \n           undergrad_gpa < mean(undergrad_gpa)) %>%\n  # Build in some noncompliance: 80% of those who need math camp do it; 20% of\n  # those who don\'t need it do it.\n  mutate(math_camp = case_when(\n    needs_camp == TRUE ~ rbinom(n(), 1, 0.8),\n    needs_camp == FALSE ~ rbinom(n(), 1, 0.2)\n  )) %>%\n  # Create random error in grades\n  mutate(grade_noise = rnorm(num, 15, 5)) %>%\n  # Create final grade based on all the arrows going into the node in the DAG.\n  # There\'s a 10 point causal effect\n  mutate(final_grade = (0.3 * gre_verbal) + (0.5 * gre_quant) + \n           (0.4 * undergrad_gpa) + (10 * math_camp) + grade_noise) %>%\n  mutate(math_camp = as.logical(math_camp))  # Make true/false\nPhew. That’s a lot of code to make fake data, but it worked! We can\nlook at the first few rows:\nhead(math_camp)\n## # A tibble: 6 x 8\n##      id undergrad_gpa gre_verbal gre_quant needs_camp math_camp grade_noise\n##   <int>         <dbl>      <dbl>     <dbl> <lgl>      <lgl>           <dbl>\n## 1     1          3.90       170       151. FALSE      FALSE            13.5\n## 2     2          3.20       163.      143. FALSE      FALSE            13.2\n## 3     3          2.83       162.      140. TRUE       TRUE             10.4\n## 4     4          2.63       144.      154. FALSE      FALSE            17.0\n## 5     5          3.24       170       148. FALSE      FALSE            16.4\n## 6     6          2.95       167.      146. FALSE      FALSE            19.0\n## # … with 1 more variable: final_grade <dbl>\n\nAbout 40% of the students participated in math camp:\nmath_camp %>% \n  count(math_camp) %>% \n  mutate(prop = n / sum(n))\n## # A tibble: 2 x 3\n##   math_camp     n  prop\n##   <lgl>     <int> <dbl>\n## 1 FALSE      1182 0.591\n## 2 TRUE        818 0.409\n\nIncorrect “correlation is not causation” estimate\nWe can take an initial stab at finding the causal effect of the program\nby comparing the average final grades for those who did math camp and\nthose who didn’t. At first glance, it looks like math camp participants\nhave a higher grade!\nmath_camp %>% \n  group_by(math_camp) %>% \n  summarize(avg = mean(final_grade))\n## # A tibble: 2 x 2\n##   math_camp   avg\n##   <lgl>     <dbl>\n## 1 FALSE      138.\n## 2 TRUE       144.\n\nThe distribution of scores is higher for those who did the program:\nggplot(math_camp, aes(x = final_grade, fill = math_camp)) +\n  geom_histogram(binwidth = 2, color = ""white"") + \n  guides(fill = FALSE) +\n  facet_wrap(vars(math_camp), ncol = 1) +\n  theme_light()\n\nWe can run a simple linear regression model to estimate the exact\neffect:\nmodel_wrong <- lm(final_grade ~ math_camp, data = math_camp)\ntidy(model_wrong)\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)     138.       0.185     744.  0.       \n## 2 math_campTRUE     6.54     0.290      22.6 8.48e-101\n\nBased on this model, participating in the program is associated with 6.5\nmore points in your final grade. Neat.\nThis is wrong, though, since there are confounders at play that cause\nboth attendance at math camp and final grade. We need to adjust for\nthose to get the actual causal effect.\nAdjustment using forbidden unmeasured variable\nThe backdoor confounder we have to worry about is needs_camp. We\ncreated this variable when we generated the data, so for fun, we can\ninclude it in the regression model as a control variable to adjust for\nit:\nmodel_adj_needs_camp <- lm(final_grade ~ math_camp + needs_camp, data = math_camp)\ntidy(model_adj_needs_camp)\n## # A tibble: 3 x 5\n##   term           estimate std.error statistic   p.value\n##   <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)      139.       0.174     798.  0.       \n## 2 math_campTRUE     10.2      0.320      31.9 2.79e-181\n## 3 needs_campTRUE    -6.69     0.329     -20.3 1.28e- 83\n\nThe coefficient for math_campTRUE is now ≈10, which is what it should\nbe! Adjusting for needing math camp isolated the causal effect.\nBut we can’t do that in real life. We don’t know what needing math camp\nlooks like in actual data, so we need to use other techniques.\nAdjustment using educated-guess-based naive matching\nOne possible approach to guessing the need for math camp is to create\ngroups of students based on what we think might be driving the need for\ncamp. We know from the causal model that quantitative GRE scores and\nundergraduate GPAs partially cause needing the program. We can assume\nthat people with lower test scores or lower GPAs need the camp and\ncreate our own guess about what the threshold might be.\nLet’s look at the distribution of GRE scores and see if there’s any\npattern about why people may have chosen to do the program:\nggplot(math_camp, aes(x = gre_quant, fill = math_camp)) +\n  geom_histogram(binwidth = 2, color = ""white"") + \n  guides(fill = FALSE) +\n  facet_wrap(vars(math_camp), ncol = 1) +\n  theme_light()\n\nThere’s a pretty noticable break in the distribution of GRE scores for\nthose who did the program: lots of people who scored under 145ish did\nthe program, while not a lot of people who scored over 145 did. Without\nknowing anything about what completely causes math camp need, we can\npretend that 145 is some sort of threshold of need and use that as our\nconfounder:\nmath_camp_guessed_need <- math_camp %>% \n  mutate(maybe_needs_camp = gre_quant < 145)\n\nmodel_adj_needs_camp_guess <- lm(final_grade ~ math_camp + maybe_needs_camp, \n                                 data = math_camp_guessed_need)\ntidy(model_adj_needs_camp_guess)\n## # A tibble: 3 x 5\n##   term                 estimate std.error statistic   p.value\n##   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)            140.       0.197     708.  0.       \n## 2 math_campTRUE            8.70     0.292      29.8 4.41e-162\n## 3 maybe_needs_campTRUE    -5.33     0.287     -18.6 2.36e- 71\n\nAfter adjusting for our possible needing camp confounder, the causal\neffect is now 8.7, which is closer to 10! It’s still not entirely\ncorrect, but we’re getting closer.\nBrief interlude: Matching + slightly simpler DAG\nWe just attempted to guess what the “needs camp” node might be based on\nthe nodes flowing into it. If you notice, though, the “needs camp” node\nis an intermediate node on the path between GPA and GRE scores and\nactually participating in the math camp program. If we can guess what\ncauses people to enroll in the program, that’s roughly the same as\npredicting their need for the camp.\nAdditionally, predicting enrollment in the program directly (rather than\nthe desire to enroll) lets us use better matching techniques. Our guess\nof needing camp was pretty naive—it’d be more accurate if we\nincorporated other variables (like GPA and background) into our manual\ngrouping. But the intuition of trying to group manually was correct—we\nlooked at the factors that caused needing math camp and guessed that\nsome things make it more likely. We can make this process more formal by\nbuilding an actual model that predicts the likelihood of treatment.\nTo do this, we can remove the “needs camp” node, since it wasn’t doing\nmuch in the model and since we can use confounders like GPA and\nquantitative GRE to estimate the probability of enrollment in math camp\ndirectly. Here’s a slightly simpler version without “needs camp” and\n“background”:\nnode_details_simpler <- tribble(\n  ~name, ~label, ~x, ~y,\n  ""math_camp"", ""Math camp"", 2, 1,\n  ""final_grade"", ""Final grade"", 4, 1,\n  ""gre_quant"", ""GRE quantitative"", 2.5, 2,\n  ""gre_verbal"", ""GRE verbal"", 5, 2,\n  ""undergraduate_gpa"", ""Undergraduate GPA"", 4, 3\n)\n\nnode_labels_simpler <- node_details_simpler$label\nnames(node_labels_simpler) <- node_details_simpler$name\n\nmath_camp_dag_simpler <- dagify(final_grade ~ math_camp + gre_quant + gre_verbal + \n                                  undergraduate_gpa,\n                                math_camp ~ undergraduate_gpa + gre_quant,\n                                gre_quant ~ undergraduate_gpa,\n                                gre_verbal ~ undergraduate_gpa,\n                                exposure = ""math_camp"",\n                                outcome = ""final_grade"",\n                                coords = node_details,\n                                labels = node_labels)\n\n# Turn DAG into a tidy data frame for plotting\nmath_camp_dag_simpler_tidy <- math_camp_dag_simpler %>% \n  tidy_dagitty() %>%\n  node_status()   # Add column for exposure/outcome\n\nstatus_colors <- c(exposure = ""#0074D9"", outcome = ""#FF4136"", latent = ""grey50"")\n\n# Fancier graph\nggplot(math_camp_dag_simpler_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_label_repel(aes(label = label, fill = status), seed = 1234,\n                       color = ""white"", fontface = ""bold"") +\n  scale_color_manual(values = status_colors, na.value = ""grey20"") +\n  scale_fill_manual(values = status_colors, na.value = ""grey20"") +\n  guides(color = FALSE, fill = FALSE) + \n  theme_dag()\n\nAdjustment using inverse probability weighting (IPW)\nOne common method for matching the assignment to treatment based on\nconfounders that is quite popular in epidemiology is to use inverse\nprobability weighting (IPW). To estimate causal effects with IPW, we\nfollow a two-step process. In the first step, we use the confounders to\nestimate propensity scores for each observation, or the probability of\nthat observation going to math camp given other characteristics.\nA common way to generate propensity scores is to use logistic\nregression. Here we build a model estimating participation in math camp\nbased on undergraduate GPA and quantitative GRE scores. We then use\naugment() to plug the GPA and GRE values for each observation into the\nmodel and generate a predicted probability:\nneeds_camp_model <- glm(math_camp ~ undergrad_gpa + gre_quant, data = math_camp, \n                        family = binomial(link = ""logit""))\n\nmath_camp_propensities <- augment(needs_camp_model, math_camp, type.predict = ""response"") %>%\n  mutate(p_camp = .fitted)  # Rename column\n\nmath_camp_propensities %>% \n  select(id, undergrad_gpa, gre_quant, math_camp, p_camp) %>% \n  head()\n## # A tibble: 6 x 5\n##      id undergrad_gpa gre_quant math_camp p_camp\n##   <int>         <dbl>     <dbl> <lgl>      <dbl>\n## 1     1          3.90      151. FALSE     0.0969\n## 2     2          3.20      143. FALSE     0.371 \n## 3     3          2.83      140. TRUE      0.554 \n## 4     4          2.63      154. FALSE     0.291 \n## 5     5          3.24      148. FALSE     0.258 \n## 6     6          2.95      146. FALSE     0.371\n\nWe have a new column p_camp that shows the probability of going to\ncamp based on grades and test scores. Our first person has a high GPA\nand high GRE score, so they have a 9% chance of going to math camp,\nwhile person 3 has a low GPA and low test score, so they’re more likely\nto need it.\nIn the second step, we incorporate these predicted probabilities into\nour causal effect estimation by converting them into weights, which\ncreates a pseduo-population of observations (i.e.\xa0some student\nobservations are more important than others for estimating the causal\neffect). What this means practically is that people with a low\nlikelihood of attending math camp who do attend it anyway should be\ntreated as more important than those who follow expectations, since\nthose with higher weights are more likely to influence the overall\ncausal effect.\nThere are a whole bunch of different weighting techniques, and Lucy\nD’Agostino McGowan covers them in\ndepth in her excellent blog post\nhere\n(see also\nthis).\nFor the sake of this example, we’ll calculate weights that are\nappropriate for estimating two different causal quantities. Here, (i)\nrepresents an individual student, (e_i) represents the propensity\nscore for an individual needing/participating in math camp, or the\ntreatment (Z_i):\n\nAverage treatment effect (ATE): overall average effect, or the\ndifference in potential outcomes when the Z = 1 and Z = 0. Formula\nfor weights: (\\frac{Z_i}{e_i} + \\frac{1 - Z_i}{1 - e_i})\nAverage treatment effect among the overlap population (ATO): effect\nof math camp across observations that overlap (i.e.\xa0those who are\nboth likely and unlikely to need math camp). Formula for weights:\n((1-e_i)Z_i + e_i(1-Z_i))\n\nmath_camp_propensities <- math_camp_propensities %>% \n  mutate(w_ate = (math_camp / p_camp) + ((1 - math_camp) / (1 - p_camp)),\n         w_ato = (1 - p_camp) * math_camp + p_camp * (1 - math_camp))\n\nmath_camp_propensities %>% \n  select(id, p_camp, w_ate, w_ato) %>% \n  head()\n## # A tibble: 6 x 4\n##      id p_camp w_ate  w_ato\n##   <int>  <dbl> <dbl>  <dbl>\n## 1     1 0.0969  1.11 0.0969\n## 2     2 0.371   1.59 0.371 \n## 3     3 0.554   1.80 0.446 \n## 4     4 0.291   1.41 0.291 \n## 5     5 0.258   1.35 0.258 \n## 6     6 0.371   1.59 0.371\n\nFinally, we can incorporate these weights into a regression model. Note\nhow we’re using math_camp as the only explanatory variable. Because we\nused undergraduate GPA and quantitative GRE scores to estimate the\npropensity scores for needing camp (and receiving the program),\nincluding the weights should be enough to close the “needs camp” back\ndoor:\nmodel_ipw_ate <- lm(final_grade ~ math_camp, \n                    data = math_camp_propensities, weights = w_ate)\ntidy(model_ipw_ate)\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)      137.      0.222     614.  0.       \n## 2 math_campTRUE     10.9     0.308      35.3 8.75e-213\n\nmodel_ipw_ato <- lm(final_grade ~ math_camp, \n                    data = math_camp_propensities, weights = w_ato)\ntidy(model_ipw_ato)\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)      136.      0.203     672.  0.       \n## 2 math_campTRUE     10.0     0.286      35.0 1.02e-209\n\nBoth of these causal estimates are pretty spot on, with the ATO\nproviding an answer incredibly close to the true value of 10. Neat!\nIf we had other backdoors to adjust for, we could include them in the\npropensity score model as well. We can do all our backdoor adjustment in\nthe logit model, generate propensity scores, generate inverse\nprobability weights, and then use the weights in a simple regression\nmodel to find the actual causal effect.\nAdjustment using matching (with Mahalanobis distance)\nWe can use other methods to find matches in the data to estimate the\nprobability of attending math camp. There’s a whole world of other\nstatistical methods for creating matches; inverse probability weights\nare just one method.\nWhile matching based on propensity scores (i.e.\xa0building some model to\ngenerate predicted probabilities for the likelihood of treatment and\nmatching observations that have similar propensities) is popular, it can\ncause problems when you use it for causal identification. Following\nGary King’s suggestions,\nwe can match with other techniques. (Again, covering what all these do\ngoes beyond the scope of this post, but there are some excellent\nresources out there, like this highly accessible video by Gary\nKing.)\nOne popular technique in political science is to match based on\nMahalanobis (or\nEuclidean) distance.\nWe can use matchit() from the MatchIt library to group students\nwith similar needs. Instead of creating a new grouping variable like we\ndid before for needs_camp, because we know that undergrad GPA and\nquantitative GRE scores cause needing math camp, and that needing math\ncamp is the only path into actually doing the program, we can model the\nprobability of treatment by using undergrad GPA and quantitative GRE.\nlibrary(MatchIt)  # For matching stuff\n\nmatched <- matchit(math_camp ~ undergrad_gpa + gre_quant, data = math_camp,\n                   method = ""nearest"", distance = ""mahalanobis"", replace = TRUE)\nmatched\n## \n## Call: \n## matchit(formula = math_camp ~ undergrad_gpa + gre_quant, data = math_camp, \n##     method = ""nearest"", distance = ""mahalanobis"", replace = TRUE)\n## \n## Sample sizes:\n##           Control Treated\n## All          1182     818\n## Matched       366     818\n## Unmatched     816       0\n## Discarded       0       0\n\nBy matching this way, we found 366 people who didn’t do math camp who\nlook similar to those who did, which means we can arguably say that\nthose who didn’t do it didn’t need to. If we want, we can see which\nobservations were matched:\nhead(matched$match.matrix)\n##    1     \n## 3  ""1864""\n## 7  ""646"" \n## 9  ""586"" \n## 12 ""83""  \n## 15 ""244"" \n## 20 ""1815""\n\nAnd we can extract the details from the match:\nmath_camp_matched <- match.data(matched)\nhead(math_camp_matched)\n##    id undergrad_gpa gre_verbal gre_quant needs_camp math_camp grade_noise\n## 3   3      2.828828   161.6843  140.4169       TRUE      TRUE    10.37564\n## 5   5      3.244684   170.0000  147.9607      FALSE     FALSE    16.40707\n## 7   7      3.757950   170.0000  151.4425      FALSE      TRUE    24.01470\n## 9   9      3.085583   162.4533  149.3013      FALSE      TRUE    21.00359\n## 12 12      3.300517   167.0163  152.7640      FALSE      TRUE    15.10616\n## 15 15      3.809366   170.0000  141.3918      FALSE      TRUE    12.68628\n##    final_grade distance   weights\n## 3     140.2209       NA 1.0000000\n## 5     142.6853       NA 0.4474328\n## 7     162.2392       NA 1.0000000\n## 9     155.6245       NA 1.0000000\n## 12    152.9133       NA 1.0000000\n## 15    145.9059       NA 1.0000000\n\nWe have one new column in this data: weights. Observations are now\nweighted differently based on how distant they are from their\nmatches—observations who attended math camp that are similar to those\nwho didn’t have a higher weight. This weighting creates a\npseudo-population of students (i.e.\xa0some student observations are more\nimportant than others for estimating the causal effect), just like we\ndid with IPW.\nWe can incorporate these weights into a regression model. Note how we’re\nusing math_camp as the only explanatory variable. Because we used\nmatching to guess the likelihood of needing camp (and receiving the\nprogram), including the weights should be enough to close the “needs\ncamp” back door:\nmodel_matched <- lm(final_grade ~ math_camp, data = math_camp_matched, weights = weights)\ntidy(model_matched)\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)      134.      0.338     398.  0.       \n## 2 math_campTRUE     10.0     0.407      24.7 7.68e-109\n\nAnd look at that! The coefficient for math_campTRUE is 10, which is\nwhat the true causal effect is.\nMatching with Mahalanobis distance isn’t the only technique\navailable—depending on the context of your actual data (and how\ncomplicated you want to get), you could use other algorithms like\ncoarsened exact matching (CEM), optimal matching, or other techniques\nincluded in the MatchIt package.\nOne potential downside to matching is that it throws away data. Notice\nhow there are only 1,184 rows in the math_camp_matched dataset, while\nusing inverse probability weights let us use the full 2,000 rows in the\noriginal data.\nSomeday when I’m smarter: do-calculus\nFinally, Judea Pearl’s most important contribution to the world of\ncausal inference is a complete set of three\naxioms\n(or rules) that allow us to convert equations with a (do(\\cdot))\noperator into something estimatable with observational data.\nVery briefly, the do operator lets us define interventions (like\nprograms and policies) in mathematical formulas. For instance, in our\nongoing example, we’re interested in\n(Pr(\\text{Grade} | do(\\text{Math camp}))), or the probability\ndistribution of final grades given that someone does math camp. Math\ncamp is an intervention, and in a randomized controlled trial, we’d be\nable to control who got to do it, and thereby estimate the causal\neffect.\nGiven observational data, though, we’re left only with the ability to\ncalculate (Pr(\\text{Grade} | \\text{Math camp})), or the probability\ndistribution of final grades given math camp. Because there’s no\n(do(\\cdot)) here, we can’t isolate the effect entirely if there are\nconfounders like GRE and GPA. We tried that earlier in the “correlation\nisn’t causation” section and found an incorrect program effect.\nThe three rules of Pearl’s do-calculus allows us to chop up causal\ndiagrams in systematic ways that can remove the (do(\\cdot)). The\nreason closing backdoors by adjusting for confounders works is because\nthe approach follows one of the do-calculus rules that removes\n(do(\\cdot)) from (Pr(\\text{Grade} | do(\\text{Math camp}))). For\ninstance (and apologies for using X, Y, and Z instead of actual\nvariables!), in a DAG with one confounder (Z)…\nbackdoor_dag <- dagify(Y ~ X + Z,\n                       X ~ Z,\n                       exposure = ""X"",\n                       outcome = ""Y"",\n                       coords = list(x = c(X = 1, Y = 3, Z = 2),\n                                     y = c(X = 1, Y = 1, Z = 2)))\n\nggdag(backdoor_dag) + \n  theme_dag()\n\n…the do-calculus version of backdoor adjustment is:\n[\nP(Y | do(X)) = \\sum_Z P(Y | X, Z) \\times P(Z)\n]\nIn other words, we can remove the (do(\\cdot)) if we multiply the\nprobability distribution of Y given both X and Z by the probability\ndistribution of Z, and then add all those values up for every value of\nZ. If X, Y, and Z were all binary, we’d be able to write the (do)-free\nversion of the causal effect like this:\n# P(y|do(x=1)) = P(y|x=1, z=1)*P(z=1) + P(y|x=1, z=0)*P(z=0)\nmean(y[x == 1 & z == 1]) * mean(z == 1) + mean(y[x == 1 & z == 0]) * mean(z == 0)\nThere are fancy algorithms that can determine the exact adjustment\nformula for a given DAG, and the causaleffect package lets you use\nthese algorithms in R. Here’s the do-calculus version of our math camp\nexample. Unfortunately we have to rewrite the DAG with a different\nsyntax for it to work:\nlibrary(causaleffect)\nlibrary(igraph)\n\nmath_dag <- graph.formula(grade +- undergradGPA, \n                          grade +- GREquant, \n                          grade +- GREverbal,\n                          grade +- mathcamp,\n                          mathcamp +- GREquant,\n                          mathcamp +- undergradGPA,\n                          GREquant +- undergradGPA,\n                          GREverbal +- undergradGPA,\n                      simplify = FALSE)\n# plot(math_dag)  # Plot this\n\n# expr returns a LaTeX formula; simp simplifies the formula through repeated\n# application of the rules of do-calculus\ndo_calc_formula <- causal.effect(y = ""grade"", x = ""mathcamp"",\n                                 G = math_dag, expr = TRUE, simp = TRUE)\nThis produces the following do-calculus-based adjustment formula:\n[\n\\sum_{GREquant,undergradGPA}P(grade|undergradGPA,GREquant,mathcamp)P(GREquant|undergradGPA)P(undergradGPA)\n]\nNeat! We still need to adjust for GRE quantitative scores and undergrad\nGPA, and if we somehow multiply the three probability distributions\n(final grade given GPA, GRE, and mathcamp, GRE given GPA, and GPA),\nwe’ll have a (do(\\cdot))-free version of the causal effect.\nUnfortunately I have absolutely no idea how to do this with R.\nContinuous variables blow up the math in do-calculus equations and I\ndon’t know how to deal with that.\nComparison of all methods\nPhew. We just used naive matching, inverse probability weighting, and\nMahalanobis matching to estimate the causal effect of a hypothetical\nmath camp program on final grades using only observational data. How’d\nwe do?!\nHere are all the estimates we have, along with a blue dotted line for\nthe truth. Adjusting for backdoor confounders allows us to get far more\naccurate and identified causal results than when we leave things\nunadjusted, and we get the most accurate results when we explicitly\nattempt to match treated and untreated observations (as with do with the\nIPW ATO and with Mahalanobis matching). That’s likely not always the\ncase—lots of these methods depend on the relationships between the\ndifferent nodes in the graph, and it’s possible that they don’t work\nwhen there are non-linear relationships.\nBut in this case, at least, we can prove causation with observational\ndata, which is really neat!\nlibrary(ggstance)\n\nall_models <- tribble(\n  ~method, ~model,\n  ""Wrong correlation-not-causation"", model_wrong,\n  ""Forbidden needs camp"", model_adj_needs_camp,\n  ""Educated guess needs camp"", model_adj_needs_camp_guess,\n  ""Inverse probability weighting ATE"", model_ipw_ate,\n  ""Inverse probability weighting ATO"", model_ipw_ato,\n  ""Mahalanobis matching"", model_matched\n) %>% \n  # Extract coefficient for math_camp from each model\n  mutate(tidied = map(model, ~tidy(., conf.int = TRUE)),\n         effect = map(tidied, ~filter(., term == ""math_campTRUE""))) %>% \n  unnest(effect) %>% \n  select(-model, -tidied) %>% \n  mutate(method = fct_inorder(method))\n\nggplot(all_models, aes(x = estimate, y = fct_rev(method), color = method)) +\n  geom_vline(xintercept = 10, size = 1, linetype = ""dashed"", color = ""#0074D9"") +\n  geom_pointrangeh(aes(xmin = conf.low, xmax = conf.high), size = 1) +\n  scale_color_viridis_d(option = ""plasma"", end = 0.9, guide = FALSE) +\n  labs(x = ""Causal effect"", y = NULL, caption = ""Dotted line shows true effect"") +\n  theme_light()\n\n'], 'url_profile': 'https://github.com/andrewheiss', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '366 contributions\n        in the last year', 'description': ['MacTahminBotu\nA bot that provides soccer predictions. Currently on Telegram, still in development\nTHE BOT WILL BE DEPLOYED AFTER LEAGUES OFFICIALLY START\nTutorial(in Turkish): https://medium.com/zeitgeist-dergi/telegram-mac-tahmin-botu-e9121851f243\n\n\n'], 'url_profile': 'https://github.com/umitkaanusta', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'La Jolla, CA', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['conquer\nConvolution-type smoothed quantile regression\nDescription\nThe conquer library performs fast and accurate convolution-type smoothed quantile regression (Fernandes, Guerre and Horta, 2019) implemented via Barzilai-Borwein gradient descent (Barzilai and Borwein, 1988) with a Huber regression warm start. The package can also construct confidence intervals for regression coefficients using multiplier bootstrap.\nInstallation\nconquer is available on CRAN, and it can be installed into R environment:\ninstall.packages(""conquer"")\nCommon errors or warnings\nA collection of error / warning messages we received from issues or e-mails and their solutions:\n\n\nError: smqr.cpp: \'quantile\' is not a member of \'arma’. Solution: \'quantile\' function was added into RcppArmadillo version 0.9.850.1.0 (2020-02-09), so reinstalling / updating the library RcppArmadillo will fix this issue.\n\n\nError: unable to load shared object.. Symbol not found: _EXTPTR_PTR. Solution: This issue is common in some specific versions of R when we load Rcpp-based libraries. It is an error in R caused by a minor change about EXTPTR_PTR. Upgrading R to 4.0.2 will solve the problem.\n\n\nExamples\nLet us illustrate conquer by a simple example. For sample size n = 5000 and dimension p = 70, we generate data from a linear model yi = β0 + <xi, β> + εi, for i = 1, 2, ... n. Here we set β0 = 1, β is a p-dimensional vector with every entry being 1, xi follows p-dimensional standard multivariate normal distribution (available in the library MASS), and εi is from t2 distribution.\nlibrary(MASS)\nlibrary(quantreg)\nlibrary(conquer)\nn = 5000\np = 70\nbeta = rep(1, p + 1)\nset.seed(2020)\nX = mvrnorm(n, rep(0, p), diag(p))\nerr = rt(n, 2)\nY = cbind(1, X) %*% beta + err\nThen we run both quantile regression using package quantreg, with a Frisch-Newton approach after preprocessing (Portnoy and Koenker, 1997), and conquer (with Gaussian kernel) on the generated data. The quantile level τ is fixed to be 0.5.\ntau = 0.5\nstart = Sys.time()\nfit.qr = rq(Y ~ X, tau = tau, method = ""pfn"")\nend = Sys.time()\ntime.qr = as.numeric(difftime(end, start, units = ""secs""))\nest.qr = norm(as.numeric(fit.qr$coefficients) - beta, ""2"")\n\nstart = Sys.time()\nfit.conquer = conquer(X, Y, tau = tau)\nend = Sys.time()\ntime.conquer = as.numeric(difftime(end, start, units = ""secs""))\nest.conquer = norm(fit.conquer$coeff - beta, ""2"")\nIt takes 0.1955 seconds to run the standard quantile regression but only 0.0255 seconds to run conquer. In the meanwhile, the estimation error is 0.1799 for quantile regression and 0.1685 for conquer. For readers’ reference, these runtimes are recorded on a Macbook Pro with 2.3 GHz 8-Core Intel Core i9 processor, and 16 GB 2667 MHz DDR4 memory.\nGetting help\nHelp on the functions can be accessed by typing ?, followed by function name at the R command prompt.\nFor example, ?conquer will present a detailed documentation with inputs, outputs and examples of the function conquer.\nLicense\nGPL-3.0\nSystem requirements\nC++11\nAuthors\nXuming He xmhe@umich.edu, Xiaoou Pan xip024@ucsd.edu, Kean Ming Tan keanming@umich.edu and Wen-Xin Zhou wez243@ucsd.edu\nMaintainer\nXiaoou Pan xip024@ucsd.edu\nReferences\nBarzilai, J. and Borwein, J. M. (1988). Two-point step size gradient methods. IMA J. Numer. Anal. 8 141–148. Paper\nFernandes, M., Guerre, E. and Horta, E. (2019). Smoothing quantile regressions. J. Bus. Econ. Statist., in press. Paper\nHe, X., Pan, X., Tan, K. M., and Zhou, W.-X. (2020). Smoothed quantile regression with large-scale inference. Preprint.\nHorowitz, J. L. (1998). Bootstrap methods for median regression models. Econometrica 66 1327–1351. Paper\nKoenker, R. (2005). Quantile Regression. Cambridge Univ. Press, Cambridge. Book\nKoenker, R. (2019). Package ""quantreg"", version 5.54. CRAN\nKoenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica 46 33-50. Paper\nPortnoy, S. and Koenker, R. (1997). The Gaussian hare and the Laplacian tortoise: Computability of squared-error versus absolute-error estimators. Statist. Sci. 12 279–300. Paper\nSanderson, C. and Curtin, R. (2016). Armadillo: A template-based C++ library for linear algebra. J. Open Source Softw. 1 26. Paper\n'], 'url_profile': 'https://github.com/XiaoouPan', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['active_learning_cfd\n\nActive learning based regression for CFD cases\n\nRequirements\nThis requires the following packages and their dependencies:\n\nnumpy\nmatplotlib\nmodAL\nPyFoam\n\nTo run the examples, it is also needed:\n\nsklearn\n\nInstallation\nWe recommend that the package be installed in development mode:\npip3 install -e .\n\nUsage\nExamples cases are provided in the example folder.\nThe test cases presented on the article are available on the cases folder:\n\nstatic_mixer\norifice\nmixer\nmixer3D\n\nEach case is composed by a folder with the OpenFOAM template and a runner\npython script for running the case and extracting outputs.\nThe regression_batch scripts run a set of different strategies, with the\npossibility of repeating  each one several times for statistics.\nThe reference scripts generate reference results for estimation of\ninterpolation error.\nAbout\nG. F. N. Gonçalves, A. Batchvarov, Y. Liu, Y. Liu, L. Mason, I. Pan,\nO. K. Matar (2020). Data-driven surrogate modelling and benchmarking\nfor process equipment. Data-Centric Engineering. DOI: 10.1017/dce.2020.8\n'], 'url_profile': 'https://github.com/ImperialCollegeLondon', 'info_list': ['63', 'Python', 'GPL-3.0 license', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '15', 'Java', 'Updated Feb 27, 2020', '16', 'Python', 'Updated Apr 6, 2020', '12', 'Python', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '8', 'Updated Feb 27, 2020', '10', 'Python', 'MIT license', 'Updated Apr 7, 2020', '4', 'C++', 'Updated Sep 3, 2020', '4', 'C++', 'MIT license', 'Updated Aug 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['FALL\nFast anchor regularized local linear model\nAuthors\nThis work has been done at RIKEN AIP by Mathis Petrovich and Makoto Yamada.\nInstallation\ngit clone https://github.com/Mathux/Fall.git\ncd fall\nPython requirements\nThis code needs Python 3 and the following packages:\n\nclick\nnumpy\npandas\noptuna\nsklearn\n\nOptional packages:\n\nmatplotlib (for the figures)\nxlrd (for loading Excel format datasets)\n\nAll the packages can be installed with:\npip install --user -r requirements.txt\nUsage\nThe implementation of our method can be found in src/models/fall.py. Please import Fall from this file and use it as a sklearn model (model.fit and model.predict)\n# Import the model\nfrom src.models.fall import Fall\n\n# Import some dataset\nfrom src.data.concrete import Concrete\n\n# Create the model\nmodel = Fall(k=40, K_anchors=20, K_prediction=5, lam=10)\n\n# Load the data\ndata = Concrete().datasets()\n\n# Fit the model\nmodel.fit(data[""train""][""X""], data[""train""][""Y""])\n\n# Predict with the test data\ny_pred = model.predict(data[""test""][""X""])\n\n# Compute the mean squared error\nfrom sklearn.metrics import mean_squared_error\nerror = mean_squared_error(y_pred, data[""test""][""Y""])\nExperiments\nUsage: python main.py [OPTIONS]\n\nOptions:\n  --dataset [fishtoxicity|aquatictoxicity|concrete|superconduct|parkinson_updrs_motor|parkinson_updrs_total]\n  --model [all|fall|netlasso|ridge|lasso|knn|krr]\n  --cv INTEGER                    Number of splits for the cross-validation.\n  --nepochs INTEGER               Number of times to compute everything.\n  --timeout INTEGER               Timeout for computing the best hyper\n                                  parameters.\n  --output TEXT                   Name of the latex output file to store the\n                                  results.\n  --verbose / --no-verbose        Verbosity for optuna.\n  --help                          Show this message and exit.\n\nFor exemple to test our method on the fishtoxicity dataset for 1 epochs, by doing a 3-cross-validation with 3 seconds to find the hyper parameters:\npython main.py --dataset fishtoxicity --model fall --cv 3 --nepochs 1 --timeout 3\nFigures\nYou can recreate the figures of the paper by running this.\nUsage: python figures.py [OPTIONS]\n\nOptions:\n  --folder TEXT       Folder to store the figures.\n  --show / --no-show  Show the plots during the generation.\n  --help              Show this message and exit.\n\nReference\nIf you find this implementation useful in your work, please acknowledge it appropriately and cite the paper:\n@techreport{PetM:2020,\n  author = ""Petrovich, Mathis and Yamada, Makoto"",\n  title = ""Fast local linear regression with anchor regularization"",\n  year = ""2020"",\n  month = ""2"",\n  url = ""http://hdl.handle.net/2433/245860"",\n  abstract = ""Regression is an important task in machine learning and data mining. It has several applications in various domains, including finance, biomedical, and computer vision. Recently, network Lasso, which estimates local models by making clusters using the network information, was proposed and its superior performance was demonstrated. In this study, we propose a simple yet effective local model training algorithm called the fast anchor regularized local linear method (FALL). More specifically, we train a local model for each sample by regularizing it with precomputed anchor models. The key advantage of the proposed algorithm is that we can obtain a closed-form solution with only matrix multiplication; additionally, the proposed algorithm is easily interpretable, fast to compute and parallelizable. Through experiments on synthetic and real-world datasets, we demonstrate that FALL compares favorably in terms of accuracy with the state-ofthe- art network Lasso algorithm with significantly smaller training time (two orders of magnitude).""\n}\n\n'], 'url_profile': 'https://github.com/Mathux', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['Non-Standard Regression\nNotes and code related to a couple of presentations on some less-commonly-taught types of regression:\n\nCensored regression\nLogit-normal regression\nBeta regression\nOrdered logistic regression\nMultinomial logistic regression\n\nAn RMarkdown summary, with sample R and Stan code, is available here.\nThe HEDW 2020 presentation is available here.\nThe R Users Group presentation from May 2020 is available here.\n'], 'url_profile': 'https://github.com/kaplanas', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Linear-regression-From-Scratch\nSimple Linear Regression in implemented from scratch using the Housing_data.txt\n'], 'url_profile': 'https://github.com/naveen-770', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'Casablanca', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['Linear Regression Implementation\nIn this assignment,I will implement linear regression and get to see it work on data. Before starting on this programming exercise, we strongly recommend watching the video lectures and completing the review questions for the associated topics here it\'s for free and it will help you a lot.\n\nThis assignment implemented using Octave and python (from scratch) and also using python packages to make it easy to implement it in new and feature projects.\n\nUsing Octave\nRequirement\nFirstival for Octave implementation is a programing assignment to complet the secound week in maching learning coursera course by Andew Ng here.\n\nThe firts think you need to install Octave in your computer here.\nIf you are a beginner in Octave programing it\'s better to check file ex1.pdf because this file has all step that you need to successful we that.\nYou can also use Matlab programing language\n\nRunning the tests\nDataset\nOur dataset that we use\n\nex1data1.txt - Dataset for linear regression with one variable\nex1data2.txt - Dataset for linear regression with multiple variables\n\nFunctions\nThose files content the function that we will need.\n\nwarmUpExercise.m - Simple example function in Octave/MATLAB\nplotData.m - Function to display the dataset\ncomputeCost.m - Function to compute the cost of linear regression\ngradientDescent.m - Function to run gradient descent\ncomputeCostMulti.m - Cost function for multiple variables\ngradientDescentMulti.m - Gradient descent for multiple variables\nfeatureNormalize.m - Function to normalize features\nnormalEqn.m - Function to compute the normal equations\n\nTest\nThose files content our test for linear regression for one variable and for multi variable call all functions that we had completed in the previous step.\n\nex1.m - Octave/MATLAB script that steps you through the exercise\nex1 multi.m - Octave/MATLAB script for the later parts of the exercise\nFirst run the ex1.m to see all iterations for one variable than you can go to ex1_multi.m for multi variables.\n\nUsing Python (from Scratch)\nRequirement (Tools)\nyou need to install python in you computer and Jupyter notebook or jupyterLab.\nGetting started with JupyterLab\n\nInstallation\nJupyterLab can be installed using conda or pip. For more detailed instructions, consult the installation guide.\nconda\nIf you use conda, you can install it with:\n\nconda install -c conda-forge jupyterlab\n\npip\n-If you use pip, you can install it with:\npip install jupyterlab\n\n\nIf installing using pip install --user, you must add the user-level bin directory to your PATH environment variable in order to launch jupyter lab.\n\nGetting started with the classic Jupyter Notebook\n\n\nPrerequisite: Python\n\n\nWhile Jupyter runs code in many programming languages, Python is a requirement (Python 3.3 or greater, or Python 2.7) for installing the JupyterLab or the classic Jupyter Notebook.\n\n\nInstalling Jupyter Notebook using Conda\nconda\n\n\nWe recommend installing Python and Jupyter using the conda package manager. The miniconda distribution includes a minimal Python and conda installation.\n\n\nThen you can install the notebook with:\n\n\nconda install -c conda-forge notebook\n\npip\n\nIf you use pip, you can install it with:\n\npip install notebook\n\n\nTo install the requirement packages you need to run this command.\nOpen this folder in your Terminal or Command Prompt (Windows) and run this command.\n\npip install -r requirement.txt\n\nTest\nCongratulation.\n\nnow you can open the main.ipynb and edit it as you want\n\n\nUsing Python Sklearn Packages\nInstall Requirement\n\nInstallation\n\nconda\n\nIf you use conda, you can install it with:\n\n1 - Install scipy:\nconda install -c anaconda scikit-learn\n\n2 - Install sklearn:\nconda install -c anaconda scipy\n\npip\n\nIf you use pip, you can install it with:\n\n1 - Install scipy :\npip install scipy\n\n2 - Install sklearn :\npip install sklearn\n\nIn order to check your installation you can use\n\nTo see which version and where scikit-learn is installed\n\npython -m pip show scikit-learn\n\n\nTo see all packages installed in the active virtualenv (if you are using Virtual Environment)\n\npython -m pip freeze\n\npython -c ""import sklearn; sklearn.show_versions()""\n\n\nTo install the requirement packages you need to run this command.\nOpen this folder in your Terminal or Command Prompt (Windows) and run this command.\n\npip install -r requirement.txt\n\nTest\nCongratulation.\n\nnow you can open the main.ipynb and edit it as you want.\n\nAuthors\n\nEl Houcine ES SANHAJI - Initial work - essanhaji\n\nThank you.\n'], 'url_profile': 'https://github.com/essanhaji', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashoksubramaniam91', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regression\nMATLAB - Linear, Ridge, Logistic and L2-Regularized Logistic Regression\n'], 'url_profile': 'https://github.com/nipunbhanot', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/Bharathnagiri', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harirajgovi', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Imagetovideoconverter\nThis code is used to convert your images into video depening on number of frame per second that you determine. Opencv, numpy, and glob libraries were used. First of all, you determine your frame per second number as fps after that null array was created after that you copy information of your images into this array by using for loop.Depending of your image extension you can change for loop. And then  by using cv2.VideoWriter and .write function you can cretae your video.\n'], 'url_profile': 'https://github.com/afeyzadogan', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Python', 'Updated Mar 2, 2020', '4', 'R', 'Updated May 6, 2020', '4', 'Python', 'Updated Feb 29, 2020', '3', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020']}"
"{'location': 'Nagpur', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/surkshabageshwar', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rakshit888', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Brazil, Minas Gerais, Itabira', 'stats_list': [], 'contributions': '965 contributions\n        in the last year', 'description': ['Regression\nPlaying with regression,\nLearning more about regression\n'], 'url_profile': 'https://github.com/MaksonViini', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['TBR Regression\nThis repository contains implementation of various regression approaches to approximate TBR without having to run TBR simulation. It is assumed that a sufficiently large data set of sampled TBR is provided.\nUsage\nThe repository provides the tbr_reg Python package. Use pip to install it on your system.\nTraining & evaluating models\nThe package exposes the tbr_train command line endpoint to enable quick model training and evaluation.\nSee the implementation for details. Common usage is as follows:\ntbr_train <model> <data_dir> <batch_start> <batch_end> <test_fraction> <plot_perf> <feature_def> <prev_model> <optional arguments...>\nwhere:\n\nmodel is one of the supported models, see model_loader.py for details,\ndata_dir is path to directory containing CSV batch files,\nbatch_start and batch_end is range of batch file indices to use,\ntest_fraction can be either:\n\nfloating-point in the interval (0;1] determining the fractional size of the test set,\n0 to disable testing (in that case entire input is used to train the model),\na negative integer determining number of folds for cross-validation (e.g. -5 yields 5-fold c.v.)\n\n\nplot_perf can be either:\n\nint to generate interactive regression performance plot,\n0 to disable performance plotting,\na file name where to save the plot (to avoid overwriting files in cross-validation, the filename can include %d which will be replaced for the fold index)\n\n\nfeature_def can be either:\n\n0 to include all features,\na name of a file containing a line-separated list of features to include,\n\n\nprev_model can be either:\n\n0 to use only the input features for training,\na name of a model file that is evaluated on the input features to produce additional feature set,\n\n\nand optional arguments depend on the chosen model, see the contents of the models/ directory for details.\n\nVisualizing models\nThe package exposes the tbr_visualizer GUI endpoint to enable model inspection and visualization.\nSee the implementation for details. Common usage is as follows:\ntbr_visualizer\n\nEvaluating models\nThe package exposes the tbr_eval command line endpoint to enable visual model evaluation.\nSee the implementation for details. Common usage is as follows:\ntbr_eval <data_dir> <batch_start> <batch_end> <model_file>\nwhere:\n\ndata_dir is path to directory containing CSV batch files,\nbatch_start and batch_end is range of batch file indices to use,\nmodel_file is a path to previously saved model file.\n\n\nCompressing dimensions with autoencoders\nThe package exposes the tbr_ae command line endpoint to enable lossy data compression with autoencoders.\nSee the implementation for details. Common usage is as follows:\ntbr_ae <data_dir> <batch_start> <batch_end> <optional arguments...>\nFixing discrete parameters\nThe package exposes the tbr_split command line endpoint to separate mixed data sets into groups selected\nby fixing discrete parameters to constant values. See the implementation\nfor details. Common usage is as follows:\ntbr_split <data_dir> <output_dir> <batch_start> <batch_end> <optional arguments...>\nSearching hyperparameter space\nThe package exposes the tbr_search command line endpoint to facilitate search of arbitrary subspace of the model hyperparameter domain.\nSee the implementation for details. Common usage is as follows:\ntbr_search <data_dir> <batch_start> <batch_end> <out_dir> <search_space_config> [--feature-def=path] [--k-folds=int] [--score=str] [--strategy=str] [--keep-unimproved] [--save-trained-models] [--save-plots] [--save-interval=int] [--n-jobs=int]\nwhere:\n\ndata_dir is path to directory containing CSV batch files,\nbatch_start and batch_end is range of batch file indices to use,\nout_dir is path to output (writable) directory (warning: may be overwritten if not empty!)\nsearch_space_config is path a YAML file that determines the model and the searched subspace of its hyperparameter domain (see the ./search_space directory for examples corresponding to various models)\n--feature-def, if provided, is a name of a file containing a line-separated list of features to include,\n--k-folds, if provided, is number of folds used for cross-validation (defaults to 5),\n--score, if provided, is choice of metric to optimize against (defaults to ""r2"", can be either ""mae"", ""r2"", ""adjusted_r2"" or ""std_error""),\n--strategy, if provided, is sampling strategy for hyperparamter optimization (defaults to ""grid"", can be either ""grid"" or ""bayesian""),\n--keep-unimproved enables saving model directories corresponding to models that are worse than the last best model encountered (off to save space by default),\n--save-trained-models, enables saving pickled models in addition to performance plots (off to save space by default),\n--save-plots enables saving generation of PDF and PNG performance plots for each fold (off to save space by default).\n--save-interval determines how often is CSV output produced (low values increase reliability but also overhead, defaults to 5),\n--n-jobs can be used to enable parallel processing, optimal setting depends on the hardware used (1 means no parallelization),\n\nBenchmarking models\nThe package exposes the tbr_search_benchmark command line endpoint that enables benchmarking a given set of hyperparameters on sample sets of varying sizes.\nSee the implementation for details. Common usage is as follows:\ntbr_search_benchmark <data_dir> <batch_start> <batch_end> <search_file> <out_dir> <model_type> [--feature-def=path] [--k-folds=int] [--score=str] [--score-ascending] [--n-best-models=int] [--n-jobs=int] [--save-plots] [--save-trained-models] [--set-size=int...]\nwhere:\n\ndata_dir is path to directory containing CSV batch files,\nbatch_start and batch_end is range of batch file indices to use,\nsearch_file is path to CSV file previously generated by tbr_search that contains hyperparameter values and measured performance,\nout_dir is path to output (writable) directory (warning: may be overwritten if not empty!)\nmodel_type is a string identifying the model class used, see model_loader.py for details,\n--feature-def, if provided, is a name of a file containing a line-separated list of features to include,\n--k-folds, if provided, is number of folds used for cross-validation (defaults to 5),\n--score, if provided, is the choice of metric that is used to sort the search file and determine the best models (defaults to ""r2"", can be either ""mae"", ""r2"", ""adjusted_r2"" or ""std_error""),\n--score-ascending, if provided, reverses the sorting direction (useful for ""mae"" and ""std_error""),\n--n-best-models, if provided, overrides the number of best models considered (defaults to 10),\n--n-jobs can be used to enable parallel processing, optimal setting depends on the hardware used (1 means no parallelization),\n--save-plots enables saving generation of PDF and PNG performance plots for each fold (off to save space by default),\n--save-trained-models, enables saving pickled models in addition to performance plots (off to save space by default),\n--set-size is used to provide desired set sizes to benchmark against (this size includes the training as well as testing set), this option can be repeated multiple times.\n\nQuality-adaptive surrogate sampling\nThe package exposes the tbr_qass command line endpoint to provide adaptive-sampling based training of generic surrogates based on MCMC-style filling of high-need parameter regions.\nSee the implementation for details. Common usage is as follows:\ntbr_qass [***]\nwhere:\n\n\n\nEvaluating model hyperparameters\nThe package exposes the tbr_eval_ho command line endpoint to enable evaluation and comparison of arbitrary number of model hyperparameters.\nSee the implementation for details. Common usage is as follows:\ntbr_eval_ho <model_name1> <model_dir1> <model_name2> <model_dir2> ... [--performance-metric=str] [--n-top-models=int]\nwhere:\n\nmodel_nameN is human-readable model name (can use LaTeX dollar notation),\nmodel_dirN is path to corresponding model directory generated by tbr_search,\n--performance-metric, if provided, is metric for measuring regression performance (defaults to ""r2"", can be either ""mae"", ""r2"", ""adjusted_r2"" or ""std_error""),\n--n-top-models, if provided, is the maximum number of top-performing hyperparameter configurations to display per models.\n\nLicense\nThis work was realised in 2020 as a group project at University College London with the support from UKAEA. The authors of the implementation are Petr Mánek and Graham Van Goffrier.\nPermission to use, distribute and modify is hereby granted in accordance with the MIT License. See the LICENSE file for details.\n'], 'url_profile': 'https://github.com/ucl-tbr-group-project', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashoksubramaniam91', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Suzyhou', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['Machine-Learning-Regression\nComparison of Ordinary Least Squares regression, Ridge regression, LASSO regression and polynomial regression for Compressive Strength of Concrete dataset.\nYou can find the dataset here\n'], 'url_profile': 'https://github.com/ArmandoDomi', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/w1449550206', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Portland, Oregon', 'stats_list': [], 'contributions': '447 contributions\n        in the last year', 'description': ['boston housing\n'], 'url_profile': 'https://github.com/laurenalexandra999', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Machine_Learning\n'], 'url_profile': 'https://github.com/KunalPandey36', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'Python', 'MIT license', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '2', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 29, 2020', '4', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['GaussianRegression\nGaussian Regression.\n'], 'url_profile': 'https://github.com/nagoyan777', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'Gandhinagar', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression\n'], 'url_profile': 'https://github.com/HarshadNSuthar', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'Gandhinagar', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression\n'], 'url_profile': 'https://github.com/HarshadNSuthar', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Logistic_Regression\nPA Assignment\n'], 'url_profile': 'https://github.com/snehaa2632000', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Implementation of Linear-Ridge regression and regularized logistic regression\nLinear Ridge Regression\nIn this experiment we studied Linear Regression, which is linear model for modelling\ncontinuous scalar output. Linear regression can be solved using two approaches namely\nGradient descent and Closed form solution. We used Closed form solution to predict\nweights based on Training data and analyse its performance on Testing data. Apart\nfrom these various experiments based on Average Testing MSE, λ, Fraction values are\ndone to understand impact of these parameters on getting a better fitted model.\nRequired Tools\n• Numpy\n• Matplotlib\n• Python3\nTo execute code in Linear ridge regression/code directory run following code :\npython3 answer.py\nOr\npython3 answer.py > result.txt\n\nIn case of first command results will be displayed on terminal. In case of\nsecond command results will be stored in result.txt file.\nPlots generated are stored in figures folder.\nRegularized logistic regression\nIn this experiment, we implement regularised logistic regression using Gradient Descent as\nwell as Newton Raphson method. We then implement feature transformation to convert\ndata into higher dimension space for different degree and implement logistic regression on\nit. We analyse performance of Logistic Regression by varying Regularisation parameter.\nRequired Tools\n• Numpy\n• Matplotlib\n• Python3\n• Scipy\nTo execute code in Regularized logistic regression/code directory run following code :\npython3 answer.py\nOr\npython3 answer.py > result.txt\n\nIn case of first command results will be displayed on terminal. In case of\nsecond command results will be stored in result.txt file.\nPlots generated are stored in figures folder.\n'], 'url_profile': 'https://github.com/jnikhilreddy', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Kaggle-House-Prices\n'], 'url_profile': 'https://github.com/M-R-S-97', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Polynomail-Regression\nPolynomial Regression from scratch\n'], 'url_profile': 'https://github.com/nipunbhanot', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/M-A-A-B', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'Lubbock', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Linear-Regression-Machine-Learning\nLinear Regression Machine Learning\n'], 'url_profile': 'https://github.com/nikhoj', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}","{'location': 'State College, PA', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kentedegrees', 'info_list': ['Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '2', 'R', 'Updated Feb 29, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'R', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Jun 12, 2020']}"
"{'location': 'United States', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nipunbhanot', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nipunbhanot', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/supsub01', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/emreozb', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['CS559-Machine-Learning-Assignment-2\nRegression and SVM\n'], 'url_profile': 'https://github.com/rpatel1291', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'Lubbock', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Gaussian-Process-Regression\nGaussian Process Regression\n'], 'url_profile': 'https://github.com/nikhoj', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Regression\nStatistical Models (OLS,Logistic and Panel data modelling)\n'], 'url_profile': 'https://github.com/ajchandy', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhiisinghh', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': [""Pixel perfect\nVisual regression testing tool\nCLI usage\nnpm install -D pixel-perfect-cli\npp init\nConfigure pp.config.js\npp test\npp approve\nRun service\nexample docker-compose.yaml\nversion: '3.7'\n\nservices:\n  pixel-perfect:\n    image: kaugnius/pixel-perfect-service\n    ports:\n      - 8010:8010\n    environment:\n      - MONGO_CONNECTION_STRING=mongodb://mongo/pixel-perfect\n      - SELENIUM_SERVER=http://selenium-chrome:4444/wd/hub\n    depends_on:\n      - mongo\n      - selenium-chrome\n  mongo:\n    image: mongo:4.0\n    volumes:\n      - pp-data:/data/db\n  selenium-chrome:\n    image: selenium/standalone-chrome:3.141\n    volumes:\n      - /dev/shm:/dev/shm\n    environment:\n      - NODE_MAX_INSTANCES=5\nvolumes:\n  pp-data:\n""], 'url_profile': 'https://github.com/ugnius', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,208 contributions\n        in the last year', 'description': ['Machine_learning_slr\nsimple linear regression model\n'], 'url_profile': 'https://github.com/N0v0cain3', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 26, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'MIT license', 'Updated Feb 26, 2020', 'Updated Feb 26, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated May 18, 2020', 'Python', 'Updated Feb 23, 2020']}"
"{'location': 'Delhi', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['linear-regression\nI have worked with the Ecommerce Customers csv file from the company. It has Customer info, suchas Email, Address, and their color Avatar. Then it also has numerical value columns:\n\nAvg. Session Length: Average session of in-store style advice sessions.\nTime on App: Average time spent on App in minutes\nTime on Website: Average time spent on Website in minutes\nLength of Membership: How many years the customer has been a member.\n\n'], 'url_profile': 'https://github.com/SimpsonStark', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': [""Support Vector Regression Machine Learning\nThis example explains working with polynomail regression and concrete dataset.\nLink to the dataset - https://www.kaggle.com/maajdl/yeh-concret-data\nSupport Vector regression is used for non-linear regression problems.\nSteps:\n1) Load the Data:\n\nLoad the data in pandas.\nThis dataset will not get coloumns here but we can add that easily\nCheck the notebook for the code.\n\n2) Clean The Data:\n\nAs its the easiest problem the dataset doesn't have any Missing or duplicate values.\nAlthouth there's a scope of work everywhere so we can check for understanding.\n\n3) Feature Selections:\n\nWith basic method of correlection we will first try to judge which features we can drop.\nWe can also play with different method and get different predication scores.\nI did try the univariat feature selection.\nUltimately the score was same so, revert back....\n\n4) Split the data:\n\nWe need to split the data for training and testing.\nThe data is split into 70% training and 30% testing.\n\n5) Fit the model:\n\nwe use the liner regression model to get predicsitons.\nafter the fit, use predict method for prediction\nTo create a regressor object in SVR there are few parameters need to be know.\nThe first is type of kernal.... linear, rbf, polynomial\nDegree is for polynomail kernel\nC is the type of regularization.\n\n6) Prediction score:\n\nthere are many ways to check the goodness of model\nI have used r2_score to check and the model worked fine.\n\nThe kerner didnt work well on this data set as it is kind of linear. You can try it with different datasets and see how it works. Try SVR for all the kernals and see how the scores changes.\nCheck the code and use for more improvement and share...!!!!!\n""], 'url_profile': 'https://github.com/futureautomate', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Boston_Dataset\nUsing Multiple Linear Regression\n'], 'url_profile': 'https://github.com/Geetansh07', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sowndarya24', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'United Kingdom, London', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/breadboykid', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/baqarali7', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['ML-INTERNSHIP-PROJECT\nInternship project conducted at Techno Triumph\nDiabetes Detection using Logistic Regression\n'], 'url_profile': 'https://github.com/aneez007', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'Bengaluru, Karnataka, India', 'stats_list': [], 'contributions': '285 contributions\n        in the last year', 'description': [""Machine-Learning\nIn basic terms, ML is the process of training a piece of software, called a model, to make useful predictions using a data set.\nThis predictive model can then serve up predictions about previously unseen data.\nContents of the repository\nRegression:\n- Simple linear regression\n- Multiple linear regression\n- Polynomial regression\n- Decision tree regression\n- Support vector regression\n- Random forest regression\n\nClassification:\n- Logistic regression\n- Kernel SVM\n- K-Nearest Neighbors\n- Naive Bayes\n- Decision tree classification\n- Support vector machine\n- Random forest classification\n\nClustering:\n- K-Means clustering\n- Hierarchical clustering\n\nReducing loss\nReducing loss can be done by computing gradient . Gradient is the derivative of the loss function\nwith respect to the weight of the parameter.\nBy computing the derivative, we get to know how much the loss changes for a give example, so we take small steps repeatedly in the direction that minimizes the loss. This is called gradient decent.\nLearning rate tells us how large of a step we should take towards the negative gradient.\nIf the learning rate is small, then it takes small steps and requires a lot of time to reach local minimum.\nIf the learning rate is large, the steps towards the local minimum is very large , and chances of over shooting the local minimum is high.\nWeight initialization :\nWhen there is only one local minimum, the initialization of the weights doesnt matter as eventually it\nreaches the local minimum.\nWhere as , when the function has more than one local minima , the initilization matters.\nstochastic gradient decent : one example at a time\nmini-batch gradinet decent : batches of 10 to 1000 at a time\nGeneralization\nThe larger the training set, the better model we will be able to compute.\nThe larger the testing set, the better confidence we will be able to have in the trained model.\nValidation set: this is the third set of data we get from the sample data set.\nWe tweak the model trained on training data, to make changes to the patterns learnt by the model, before we test it on the test set.\nHandling non-linear data\nThe non-linear data cant be separated on the number of features available, cause the features are limited.\nTherefore we add one more feature which is the cross product of all the other features and increase the number of features. This way we increase the dimentionality and we will be able to separate the data points by a plane linearly.\nThese new features are called feature crosses\nexample: x, y are the features given ; x*y is the feature cross\nSometimes adding these feature crosses won't add any value, so we need to consider only the meaningful feature crosses.\nRegularization\nRegularization is done to avoid overfitting in a model.\nThis can be done by reducing the model's complexity.\nLinear models :\n\n\nL1 Regularization:\n\nPenalize sum of abs(weights)\nEncourage sparsity\nReduces model's size\n\n\n\nL2 Regularization:\n\nnew function to minimize is => minimize(loss(Data|model) + complexity(model))\nIncreases model stability\n\n\n\nNon-linear models :\n\nTo regularize a deep neural network model, we use Dropout regularization.\nDropout removes a random selection of a fixed percentage of the neurons in a network layer for a single gradient step.\n\n""], 'url_profile': 'https://github.com/Pooja-Lokesh', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'Istanbul/ Turkey', 'stats_list': [], 'contributions': '372 contributions\n        in the last year', 'description': ['Prediction of house prices - Kaggle\nIn this repository, I try to predict the price of houses. I used Kaggle dataset for training and testing my project. With different regression techniques, we can estimate the price of a house and use it in several fields. I used Decision Tree Regressor, and Gradient Boosting Regressor in my project so that we can see the performance of these two regressors.\nI have also kernel of this project in Kaggle, you can find here.\n\nThe procedure I follow:\n\nVisualization of data and correlation between features\nData cleaning\n\nDrop missing data\n\n\nFeature engineering\n\nDrop outliers\nDecreasing skewness\n\n\nModeling & Evaluation\nSubmission\n\n'], 'url_profile': 'https://github.com/afraarslan', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/luxludem', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Mar 1, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', '2', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Roff', 'Updated Nov 25, 2020']}"
"{'location': 'Lubbock', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Bayesian-MAP-with-Logistics-Regression\nBayesian MAP with Logistics Regression\n'], 'url_profile': 'https://github.com/nikhoj', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'New York, New York ', 'stats_list': [], 'contributions': '599 contributions\n        in the last year', 'description': [""Agriculture_Project\nProject Managers\n\nHua Shi\nTrevor O'Hearn\n\nGoal\n\nTo research the relationship between income and other variables and to determine which one affects the income the most; as well as, do a Regression on the different Regions of the United States of America\n\nData Process\n\nClean five tables of Census Data from 2017.\nRemoved columns with more than 20% missing data\nRemoved data outside the 95th percent quantile\nCreated categorical dummy variables for region and land size\nTook the square root of the target variable\n\nStatistical Tests\n\nChi squared test on 4 regions of the US\n\nReject the null hypothesis that the means of the region are the same\n\n\nTwo sample mean test\n\nRejected null hypothesis that the means of workers working more or less than 150 days are the same\n\n\nChi squared test on the categorical size of the Land\n\nReject the null hypothesis that the means of the size of land are the same\n\n\n\nLinear Models used\n\nScaled the data using the standard scalar\nTrain-test-split the data\nDetermined the proper feature selection based on 5000 iterations of train-test-splitting the data\nPerformed three regression models on the overall data as well as the the four separate regions of the U.S\nFound the best model to be the Ridge Regression\n\nEDA\n\n\nFeature Explanation\n\nThe top, and most obscure feature, overall, is the inventory of the farm's horses and ponies\nThe other four top features dealt with either money or land\nThe amount of government funding received or\nThe efficiency of the land used\nMarket value of equipment on site such as machinery\n\nConclusion\n\nThe horse business is large and has a massive impact on the farming business\nThere is a target amound of land to have and efficient use of that land is important\nMajor assets help determine the farm's ability to thrive in the market\n\nSources\n\nhttps://www.nass.usda.gov/Publications/AgCensus/2017/index.php#highlights\n\nPowerpoint\n\nhttps://docs.google.com/presentation/d/1VkiJMo7tENhxQyWEJM6R-SV0TAAuppFtSI037s9Xv_s/edit#slide=id.g7e392c2e99_0_119\n\n""], 'url_profile': 'https://github.com/melanieshi0120', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['ExampleLogistic\nTuning hyperparameters for Logisitic regression\nI researched how Binning, GridSearchCV, PipleLine , PowerTransform are working. And how it is\nhelpful for tuning hyperparameters for algorithm. Here I used dataset from kaggle.\n'], 'url_profile': 'https://github.com/plainvan', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'East Lansing, MI, USA', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PriyankaGoenka', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '586 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnthonyHewins', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Stat_4010_hwrk4\nPrediction for linear regression models\n'], 'url_profile': 'https://github.com/jmox0351', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nit1n', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/padmaraouppuluri123', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'Shirdi, India', 'stats_list': [], 'contributions': '198 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhushanyadav07', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Practice-Materials-for-29th-01-March-2020\nMaterial contain Descriptive statitics, Simple and Multi linear Regression and Logistic Regression\n'], 'url_profile': 'https://github.com/pjayeshkanayi', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'MATLAB', 'Updated Feb 26, 2020', 'Go', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', '2', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nmishra1708', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jayvachhani77', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['LINEAR REGRESSION\nLinear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables).\n\nSIMPLE LINEAR REGRESSION\nIf we use only 1 independent variable to build our Machine Learning Model, then it is a Simple Linear Regression.\n'], 'url_profile': 'https://github.com/iamsrilakshmi', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['StockX Regression Project\nPurpose\nStockX.com is a website where sellers can resell exotic/limited quantity shoes. Many of these shoes sell for much higher than their initial retail price. StockX operates much like a stock market exchange with bid and ask prices displayed. Identifying key features relevant to pricing the shoes is conceptually interesting and applicable if one were to look for undervalued shoes.\nTechnologies Used:\n\nBeautifulSoup and Requests (Webscraping)\nSeaborn (Visualization)\nscikit-learn (Modeling)\n\nData Sources:\nStockX.com\nMotivation\nIn this investigation, I collect StockX.com shoe data such as initial retail price, release date of shoe, brand, shoe size, number of sales, etc (independent variables) to predict the last sale price (dependent variable). I chose the last sale price rather than the bid or ask prices because it is a better replication of the true value of the shoe. For instance, some shoes with large bid/ask spreads have asks that are too high to actually be sold. Some independent variables such as shoe size are critical to collect due to the price varying based on the supply/demand of that shoe size. More information can be found: https://stockx.com/news/the-stockx-data-guide-to-resale/\nWebscraping\nUtilizing the requests and BeautifulSoup library, I was able to scrape and parse 2863 StockX.com listings. I was able to collect key features such as size, release date, retail price, number of sales, brand, color, etc. One must be cognizant of the rate limit on this page and should implement a try-except with a time.sleep() on except.\n\nData Cleaning and Feature Engineering\nI removed certain shoe sizes that were too sparse in my data and considered children shoe sizes to be one combined size. Then I used each shoe size as a dummy variable (meaning a variable that is coded as 0 or 1 depending if the observation is that category). I also was able to extract whether the shoe was celebrity enddorsed from the name of the shoe. Release date was changed to days since release to work with the regression models. I removed shoes with too high of a premium as they could potentially skew the model.\n\nModeling\nAfter splitting the data into training and test sets, I initially attempted to model with a simple linear regression, which was not capturing the higher last sale prices. There were potentially complex interactions between the variables that a simple, non-polynomial linear model would not be able to capture.\nLinear Model Residuals\n\nIn order to counteract this, I generated polynomial features with the independent variables by a degree of 2 and ran a LASSO model on these variables with an alpha of 0.01. This is the regularization strength that allows the model to select relevant features based on a cost function and prevent overfitting. Using the polynomial features with a LASSO model, I was able to improve the explained variance in my model without overfitting.\nLASSO Model Residuals\n\nJust to build a comparison to the regression models, I built a Random Forest, which unsurprisingly had the best performance of the models at the cost of losing some interpretability. This increased the performance on the test set drastically. In a business case where one would purchase shoes deemed undervalued by the model and resell those shoes, interpretability is not necessary as much as model performance. Therefore, a Random Forest Model (or a boosted model) would be preferred over the regression models despite their simplicity and interpretability.\nRandom Forest Model Residuals & Feature Importances\n\n\n'], 'url_profile': 'https://github.com/neonbelly', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['Logistic-Regression-Model\nThe logistic regression model implemented in Python\n'], 'url_profile': 'https://github.com/AryanP281', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/emreozb', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'Düsseldorf, Germany', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Abalone-Dataset\nClassification and Regression on Abalone Dataset\nThere are separate two classification and regression in .py format also along with the single jupyter notebook (includes both classification and regression)\n'], 'url_profile': 'https://github.com/prudhvinathreddymalla', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['Linear_Regression\nLinear regression in Keras with Tensorflow backend\n'], 'url_profile': 'https://github.com/annielkn', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Wine-Quality-Project\npredicting wine quality using regression model\n'], 'url_profile': 'https://github.com/rishabh-goel001', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Course Assignment\nMultiple linear regression model in R\n'], 'url_profile': 'https://github.com/snehab15', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'R', 'Updated Feb 25, 2020', 'R', 'Updated Jul 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '936 contributions\n        in the last year', 'description': ['model_flask_lr_post\nFlask Logistic Regression Model API with Postman\n'], 'url_profile': 'https://github.com/yuchild', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/georgrapt', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sriramaraju423', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chandan54', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['advanced-house-price-prediction\nKaggle Competition -House Prices: Advanced Regression Techniques\n'], 'url_profile': 'https://github.com/Septimmius', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '1,061 contributions\n        in the last year', 'description': ['Advance_Housing_Prediction\nAdvance Logistic Regression and XGBoost Parameter tuning\n'], 'url_profile': 'https://github.com/memetics19', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hitarthshah72', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/eamonn2014', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maheshboopathy', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Alexcei', 'info_list': ['Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 29, 2020', 'R', 'Updated Mar 4, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Alexcei', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chandan54', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': [""Multiple_Regression\nMultiple Regression examples of Machine Leaning\nIntroduction\nIn this article,I’m going to walk you through how to perform a multiple linear regression in python using the scikit-learn module.The scikit-learn module in python is basically used to perform machine learning tasks.It contains a number of machine learning models and algorithms that are very good and helpful to use.Among these models are the regression models that are used to perform both simple and multiple linear regression.\nBut one may ask what is machine learning and why do we need it? Machine learning is about extracting knowledge from data.Another way to view machine learning is training computers to learn from data using mathematical models. It is a research field at the intersection of statistics, artificial intelligence, and computer science and is also known as predictive analytics or statistical learning.Machine Learning can be classified into three main parts namely: Supervised Learning,Unsupervised Learning, and Reinforcement Learning.The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, which is known as supervised learning.Supervised learning comprises of regression and classification so it is clearly seen that regression analysis falls into supervised machine learning.\nLinear regression analysis,also known as linear modelling entails fitting a straight line,a plane or polynomial to a data.Like most of the machine learning algorithms,the goal of linear regression regression is to predict an output variable using other variables.Linear regression expresses the output variable or dependent variable as a function or a linear combination of the independent variables or the predictor variables.Linear regression is a widely used technique to model and understand real world phenomena.It is easy to use and understand intuitively.In simple linear regression,the model is just a straight line and for multiple regression,the model could either be a polynomial or a plane.But for this particular article,our main area of discussion is multiple linear regression.\nThe data we will be using for our regression analysis comprises of some technical specs of some cars.The data set is downloaded from UCI Machine Learning Repository.The UCI Machine Learning Repository is a machine learning repository that contains free data you can use for your machine learning projects.You can also go to my github repository for the data files and the python code written in a jupyter notebook. In this project,we try to predict the miles per gallon of a car using the multiple regression.Miles per gallon (mpg) of a car measures how far a car could go given one gallon of fuel.In every part of the world where the use of cars is very common,consumers sometimes consider the efficiency and fuel economy of the car they want to purchase before purchasing it.Everyone wants to buy a car that can travel far and consume less fuel.In this setting,let’s assume that we work for a firm that deals in cars and we have been given the task as data scientists to analyze this data and produce a good model that can predict/estimate the miles per gallon of a car with minimum error given other features of the car.\nTo begin with,we import the necessary modules we will be needing and they’re as follows.\nData preprocessing\nIn this part,all what we will do is to read in the data files,merge them and perform some data cleaning.Data cleansing or cleaning is a very important thing in the field of data analysis and machine learning.It requires detecting and correcting or removing inaccurate records from the data set.It improves your data quality and in doing so,increases overall productivity.When you clean your data,all outdated or incorrect information is gone leaving you with the highest quality information.\nNow that we have our full data set,let’s describe what our columns or features mean to make it easier to understand them and know how useful they are in our analysis.\nmpg — Mileage/Miles Per Gallon\ncylinders — the power unit of the car where gasoline is turned into power\ndisplacement — engine displacement of the car\nhorsepower — rate of the engine performance\nweight — the weight of a car\nacceleration — the acceleration of a car\nmodel — model of the car\norigin — the origin of the car\ncar — the name of the car\nNow let’s print out the info of the data set.\ncars.info()  # print the info of the data\nwe can see that the horsepower column is an object datatype and we will try to see what the odd value is in the horsepower column.We can go about this by getting the unique values in the horsepower column using pandas.unique() function and search for the odd value in the column.\nprint the unique values in the horsepower column\ncars.horsepower.unique()\nWe can see that the odd value is ‘?’ representing null so we now change it to NaN value and fill the spot with the mean horsepower.\nWe will drop the car column from the data set since we won’t be needing it.We also go ahead to check for duplicates and missing values in other columns.If there are any duplicated records,we will drop them and fill in the missing data with the mean value in case there is any.\nwon't be needing the car column so we drop it\ncars = cars.drop('car',axis=1)\ncheck for duplicates and null values\nprint('sum of duplicated values\n{}\\n'.format(cars.duplicated().sum()))\nprint('sum of null values: {}'.format(cars.isnull().sum()))\nWe can see from the output above that there are neither duplicated records nor missing data in our data set.Now we can say that our data is clean and ready to fit a model on,but we will first have to explore the data to find hidden patterns that will be of great help to our analysis.\nExploring the data\nAt this point,we are going to do a short and simple exploratory analysis on our data set to discover the relationships between variables,the distributions of the various features, and display some summary statistics of the data set.\nlet's print the summary statistics of the data\ndisplay(cars.describe())\nlet's visualize the distribution of the features of the cars\ncars.hist(figsize=(12,8),bins=20)\nplt.show()\nWhat we can conclude from the histograms above is :\nThe acceleration of the cars in the data is normally distributed and the most of the cars have an acceleration of 15 meters per second squared.\nHalf of the total number of cars (51.3%) in the data have 4 cylinders.\nOur output/dependent variable (mpg) is slightly skewed to the right.\nWe can also see that our variables are not on the same scale.\nLet’s visualize the relationships between the Mileage Per Galon(mpg) of a car and the other features.\nplt.figure(figsize=(10,6))sns.heatmap(cars.corr(),cmap=plt.cm.Reds,annot=True)\nplt.title('Heatmap displaying the relationship between\\nthe features of the data',\nfontsize=13)\nplt.show()\nLooking at the above correlation heatmap,we can conclude that;\nWe can see that there is a relationship between the mpg variable and the other variables and this satisfies the first assumption of Linear regression.\nThere is a strong negative correlation between the displacement,horsepower,weight,and cylinders.This implies that,as any one of those variables increases,the mpg decreases.\nThe displacement,horsepower,weight,and cylinders have a strong positive correlations between themselves and this violates the non-multi collinearity assumption of Linear regression.Multi-collinearity hinders the performance and accuracy of our regression model.To avoid this, we have to get rid of some of these variables by doing feature selection.\nThe other variables.ie.acceleration,model and origin are not highly correlated with each other. We can also check for multi-collinearity using the variance inflation factor.A variable/feature affected by multi-collinearity will have a value greater than 5 when we print out the series from the variance inflation factor\nAnother way of checking the multi-collinearity is by using the variance inflation factor.If a variable has a variance inflation factor greater than 5,then it is associated with multi-collinearity.We will use the variance_inflation_factor() of statsmodels to perform this task and the code is as shown below.\nX1 = sm.tools.add_constant(cars)\ncalculate the VIF and make the results a series.\nseries1 = pd.Series([variance_inflation_factor(X1.values,i) for i in range(X1.shape[1])],index=X1.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(series1))\nWe can see that there is a problem of multi-collinearity in our data since some of the variables have a variance inflation factor greater than 5.And we can also see clearly that the displacement,horsepower,weight,and cylinders have a strong positive correlations between themselves and they are the cause of the multi-collinearity as shown in the correlation heatmap above.To avoid this, we take out those features from our data and compute the variance inflation factors of the remaining variables and check if multi-collinearity still exists.\nLet's drop the columns that highly correlate with each other\nnewcars = cars.drop(['cylinders','displacement','weight'],axis=1)\nLet's do the variance inflation factor method again after doing a feature selection\n#to see if there's still multi-collinearity.\nX2 = sm.tools.add_constant(newcars)\nseries2 = pd.Series([variance_inflation_factor(X2.values,i) for i in range(X2.shape[1])],index=X2.columns)\nprint('Series after feature selection: \\n\\n{}'.format(series2))\nGreat, we have gotten rid of the multi-collinearity as the remaining variables have a variance inflation factor less than 5.Now we have gotten enough information from our data and it’s time to fit train and fit a model on it and start making some predictions.\nTraining the regression model\nThis is the part where we start training the regression models we imported earlier.Here, we do not train only one model.But we train as many models as we can as model accuracy is what were after.We want to end up with the model that predicts well and gives minimum error.We will split our dataset into two parts.ie. training data and testing data using the train_test_split() function of sklearn.model_selection.Since the variables are not of the same scale,we will scale them using the preprocessing.scale() function from sklearn.Scaling the variables is only necessary for the linear,ridge and lasso regression models as these models penalize coefficients.After scaling the feature or predictor variables,we will therefore go ahead to fit our LinearRegression model on the data and assess the model to see how accurate it is.\nWe can see from the above output that the LinearRegression model fits on the training data 75.5% and 72.7% on the test set.With this model,we do not have a problem of over-fitting or under-fitting but the accuracy of the model isn’t satisfactory so we go ahead and fit a Ridge model on the data to see if we can increase the accuracy and minimize the mean squared error.\nLooks like the Ridge model is no different from the LinearRegression model we first fit.Let’s try to tune the hyper parameters to see if we can make a significant change in the accuracy and minimize the MSE.In doing this,we will perform a grid search cross validation to search for the best parameters using the GridSearchCV() function from sklearn.model_selection\nNothing much from Ridge regression,we move on to fitting a Lasso regression model and straight away perform a grid search for the best parameters.\nI guess it’s clear to us now that the LinearRegression,Ridge, and Lasso are giving us a non-satisfactory model accuracy and a mean squared error so we move on to the ensemble methods for our regression.The most common ensemble methods we will use are the DecisionTree,RandomForest and GradientBoosting.Instead of first fitting the models with single parameters and scoring them,we will straight away begin with the grid search for the best parameters and score the models.Let’s begin with the DecisionTreeRegressor and tune its parameters.\nFrom the output above,we can clearly see that the DecisionTreeRegressor has an accuracy of 79% and a mean squared error of 11.4 which is better than the LinearRegression,Ridge and Lasso models.But it looks like the DecisionTreeRegressor is slightly over-fitting as its prediction accuracy on the training data is 86.6% and on the test data is 79%. Let’s consider the RandomForestRegressor model to see if we can still get a higher accuracy,minimized error,and a generalized model.\nThe RandomForestRegressor is doing great with reducing the mean squared error but also over-fitting the data as its prediction accuracy on the training data is 94% and on the test data is 80.5%. Let’s consider the GradientBoostingRegressor model to see if we can still get a higher accuracy,minimized error,and a generalized model.\nLooks like this model is not too over-fitted and it has low mean squared error which when taken the square root of gives 2.98.This tells us that the average distance from the actual values and predicted values is 2.98 which is better.We will now try to make predictions and see how well our model predicts.We will visualize the actual mpg values recorded and the mpg values predicted by our model to see how close our predictions are to the actual values.\nWe can see from the above scatter plot that our model made a good predictions as the values of the actual mpg and the predicted mpg are very close to each other.We can say that we have succeeded in training a model that predicts the Mileage Per Gallon (mpg) of a car given the acceleration,model,origin and the horsepower of a car.\nEven though we could have continued to train other models like the Adaboost and XGboost which could have given a better accuracy and minimized error as compared to our final GradientBoosting model but I choose to end here since this whole article was just to show you how to train a multiple regression model to make estimations/predictions.I hope to go further next time and thank you for taking your precious time to read this article.The whole project files and code can be accessed at my github account.Thanks again and see you next time.\n""], 'url_profile': 'https://github.com/franciscosalido', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['Simple Linear Regression 2 - Estimating Yacht Hydrodynamics\nHaving briefly gone through the mathematical theory in part 1, let\'s demonstrate how simple linear regression can be used in practice.\nPrevious part: https://github.com/tommyzakhoo/simple-regression1\n\nData Description\nThe dataset that we will be using comes from http://archive.ics.uci.edu/ml/datasets/yacht+hydrodynamics. And is contributed by Dr Roberto Lopez, Ship Hydromechanics Laboratory, Maritime and Transport Technology Department, Technical University of Delft.\nDescription of the variables in the dataset taken from the link above:\n\n Longitudinal position of the center of buoyancy, adimensional. \n Prismatic coefficient, adimensional. \n Length-displacement ratio, adimensional. \n Beam-draught ratio, adimensional. \n Length-beam ratio, adimensional. \n Froude number, adimensional. \n Residuary resistance per unit weight of displacement, adimensional. \n\nHere, we are trying to predict Y = Residuary resistance per unit weight of displacement using the rest of the variables.\n\nCSV Conversion\nThe data comes in a text file ""yacht.txt, which is space separated in a inconsistent way, and is also missing headers for each column. Let\'s add a row of headers, clean up the excess whitespace, and turn it into csv format.\nf = open(\'yacht.txt\',\'r\') # open file\nx = f.readlines() # \nf.close()\n\ng = open(\'yacht.csv\',\'w\')\ng.write(\'buoyancy,prismatic,length-displace,beam-draught,length-beam,froude,resist\\n\') # write header row\n\nfor index in range(0,len(x)):\n  y = x[index].strip() # remove leading/trailing whitespace\n  y = y.replace(\'  \',\' \'); # some rows had double spacing\n  y = y.replace(\' \',\',\'); # replace single spaces with comma\n  g.write(y + \'\\n\') # added a line break after each row\n\ng.close()\n\nPreliminary Exploration\nLet\'s start by looking at the scatter plots of Y with the rest of the variables.\nimport pandas as pd # data structures module\nimport matplotlib.pyplot as plt # for visualization\n\ndata = pd.read_csv(\'yacht.csv\')\n\nfor i in range(0,6):\n    plt.subplot(2,3,i+1)\n    plt.scatter(data.iloc[:,i], data.iloc[:,6])\n\nplt.show()\n\n\n\nWhile some of these variables look like good candidates for splitting the data set into clusters, only the bottom right plot shows some potential for a simple linear regression model. The relationship shown in the scatterplot is not linear, but we can apply a logarithmic transform to Y variable to get a more linear looking scatterplot.\n\n\n\nOriginally, our dependent variable was Y = Residuary resistance per unit weight of displacement. Here, it has become Z = log(Y+1), where log is the natural logarithm. We have added 1 because some values of Y were close to zero, which behaves badly under logarithmic transformation. Our independent variable here is X = the Froude number.\nFitting A Simple Linear Regression Model\nLike we mention in part 1, we want to fit this linear model to our data.\n\ny^i = a^ + b^ xi\n\nLike we mentioned in part 1 of this article, he a^ and b^ can be found by minimizing the sum of squares. In sklearn, this solution is computed using Singular Value Decomposition (SVD). See the documentation for more details.\nimport pandas as pd # data structures module\nimport numpy as np # n-dimensional array module\nfrom sklearn.linear_model import LinearRegression as lr\n\ndata = pd.read_csv(\'yacht.csv\')\n\nx = data.iloc[:,5]\nx = x.reshape(-1, 1)\n\ny = np.log(data.iloc[:,6]+1)\ny = y.reshape(-1, 1)\n\nreg = lr()\nreg.fit(x, y)\nWe can plot the fitted line over a scatter plot to visualize the fit.\n\n\n\nOur simple regression line does looks like a ""good"" estimate at first glance. However, there are mathematical tools we can use to quantify this.\nFitted Coefficients\nLet us take a look at the fitted coefficients.\n>>> reg.coef_[0][0]\n11.957987175940932\n\n>>> reg.intercept_[0]\n-1.7672239620561152\n\n\nWhat this means is that for our line model y^i = a^ + b^ xi, the coefficients a^ = -1.7672 and b^ = 11.9579, to four decimal places.\nbut since resistance cannot go below 0, the interc\nCoefficient Of Determination\nAs mentioned in part 1 of this article, the coefficient of determination R2 can be interpreted as ""how much of the variation in Y is explained by our model"". For simple linear regression, R2 is actually equal to the square of the sample correlation coefficient.\n>>> reg.score(x,y)\n0.9635494120195388\n\nResidual Analysis\nWe can take a look at the residual for clues on how good our model is.\ne = y - y_predicted\nt = np.arange(0,len(e))\n\nplt.scatter(t,e) # scatter plot\nplt.hist(e) # histogram\n\n\n\n\n\nConclusions & Physics\nIt turns out that the Froud number is an important Physics concept for determining resistance. This highlights a particular situation in which linear regression excels: estimating physical laws of nature.\nMore Real World Application\nPlease proceed to part 3 of this article for another application of simple linear regression to data: https://github.com/tommyzakhoo/simple_regression3\n'], 'url_profile': 'https://github.com/tommyzakhoo', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['boston-housing\nLinear Regression on Boston Housing dataset\n'], 'url_profile': 'https://github.com/s-mushnoori', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Training for Linear regression (using Python)\nMarch 2020\n\nContains notebooks to explain:\n\nthe basics of pandas\nintroduction to linear regression\n\n\nGoal is to find which advertising variables are important to predict and improve sales.\n\nThe notebooks are made to work with:\nGoogle Colaboratory\n\n\nInspired by and making use of:\n\nhttp://faculty.marshall.usc.edu/gareth-james/ISL/\nhttps://github.com/justmarkham/DAT4/blob/master/notebooks/08_linear_regression.ipynb\n\n'], 'url_profile': 'https://github.com/SandervandenOord', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Kuala Lumpur , Malaysia', 'stats_list': [], 'contributions': '1,347 contributions\n        in the last year', 'description': ['Linear Regression\nLines to fit data .\nLine is a rough approximation\nbut it allows us the ability to explain and predict variables that have a linear relationship with each other.\n\nor\n\n\nm : The slope is a measure of how steep the line is, while the intercept is a measure of where the line hits the y-axis.\nb : The intercept is a measure of where the line hits the y-axis.\nThe goal is to get the “best” m and b for our data.\n\nLoss\nWhen assign a slope and intercept to fit a set of points, we have to define what the best fit is.\nFor each data point, we calculate loss, a number that measures how bad the model’s (in this case, the line’s) prediction was.\nloss as the squared distance from the point to the line. We do the squared distance (instead of just the distance) so that points above and below the line both contribute to total loss in the same way\n\nsquared_distanceA = 9\nsquared_distanceB = 1\ntotal_squared_distance = 10\nTotal loss is 10 , if found a line that had less loss than 10, that line would be a better model\n\n$$x_{1,2} = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2b}.$$\n\n\nRough Code\nScikit Learn\n'], 'url_profile': 'https://github.com/dnlsyfq', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AryanP281', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/medha-sagar', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Insurance_charges_prediction\npredicts the insurance charges ,linear regression used\n'], 'url_profile': 'https://github.com/ShizaAbid', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 28, 2020', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MohYou007', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Indore', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/artist1327', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Big_Mart_Sales\nSales Prediction using Linear Regression\n'], 'url_profile': 'https://github.com/DSamson24', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Housing-Prices\nRegression Analysis on Housing Prices Data\n'], 'url_profile': 'https://github.com/Joshuayeo95', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Manaus, Brazil', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['SimpleLinearRegression\nModel exemple for simple linear regression\n'], 'url_profile': 'https://github.com/VictorCLucena', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kavyan1392', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/necakovk', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Takoradi-Ghana', 'stats_list': [], 'contributions': '870 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hagios2', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nmishra1708', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Logistic-Regression\nBreast Cancer prediction using Logistic Regression\n'], 'url_profile': 'https://github.com/madhumitha4396', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'Mumbai', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhiisinghh', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Turkey', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['FLR\nFirth Logistic Regression Algorithm for rare events\n'], 'url_profile': 'https://github.com/ezgicn', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deepak2409', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Arunachal Pradesh ,India', 'stats_list': [], 'contributions': '647 contributions\n        in the last year', 'description': ['Machine-Learning\n\nIt includes my work on Machine learning during Coursera Assignment .\nPLEASE DO NOT COPY . USE IT FOR REFERENCE .\nHere are some screenshot from the work.\n\n  \n\n\n'], 'url_profile': 'https://github.com/suubh', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Earth', 'stats_list': [], 'contributions': '456 contributions\n        in the last year', 'description': ['Regression trees project\nLearning how regression trees work and implementing a tool that, given a dataset, builds a regression tree.\nDescription\nThis projects aims to provide a tool that allows the generation of regression trees.\nTool\nThis java-based tool consists of a UI where one can generate a regression tree based on training data specified from a .csv file.\nOptionally, test data can be loaded either by specifying a test file whose data format matches the format of the training data, or by selecting the option “create the tests from the training values” in which case you a % of the training data will be used as evidence.\nIf you have specified test data, once the tree is generated, each of the instances will be tested giving as a final result the average error of predictions made.\nNote that, for both files, it must be specified if it has headers (in which case the first row of the .csv will be interpreted as the header) and which is the separator used in the file (comma, semicolon, tabulation).\nAdditionally, you have the option to enable the option to interpret the nominal attributes as ordinals. If you do so, the conversion of categorical attributes applies.\nFinally, we must establish which are the cut-off limits for both the minimum number of test instances and the minimum percentage of the initial standard deviation.\n'], 'url_profile': 'https://github.com/fedealconada', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '391 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tomasokal', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Akashpawashe', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['LinearRegression_byHand\nSimple implementation of a Linear Regression by hand.\n'], 'url_profile': 'https://github.com/VascoMonteiroNeto', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Bordeaux France ', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mawusicharles', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Xinqi Zhang\nUSF email address: xzhang198@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Jan 8, 2021', 'Python', 'Updated Apr 3, 2020', '2', 'MATLAB', 'Updated Jul 21, 2020', 'Java', 'Updated Feb 29, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Stata', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}"
"{'location': 'Raipur, India', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['Rossmann Store Sales Problem From Kaggle\nKeras Regression example using Rossmann Store Sales from Kaggle.\nProblem Statement - Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\n'], 'url_profile': 'https://github.com/mayank11196', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  YaleiShi\nUSF email address:  yshi52@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/digvijaysonawane', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Sope Ogundipe\nUSF email address:  mogundipe@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Ziling Wang\nUSF email address:  zwang155@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Radha Prashanthi Aakula Chinna\nUSF email address: raakulachinna@usfca.dons.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'Chennai,India', 'stats_list': [], 'contributions': '3,571 contributions\n        in the last year', 'description': ['PyPassCheck\nPasswords are a vital component of system security. Though there are many alternatives to passwords for access control, password is the more compellingly authenticating the identity in many applications. They provide a simple, direct means of protecting a system and they represent the identity of an individual for a system. The big vulnerability of passwords lies in their nature. Users are consistently told that a strong password is essential these days to protect private data as there are so many ways for an unauthorized person with little technical knowledge or skill to learn the passwords of legitimate users. Thus it is important for organizations to recognize the vulnerabilities to which passwords are subjected, and develop strong policies governing the creation and use of passwords to ensure that those vulnerabilities are not exploited\n'], 'url_profile': 'https://github.com/smaranjitghose', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['CKD Patient Prediction\nLogistic regression model to classify high risk CKD patients.\n'], 'url_profile': 'https://github.com/saniya-k', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '213 contributions\n        in the last year', 'description': ['Regression Testing\nSimple app demonstrating basic automated regression testing ideas.\n'], 'url_profile': 'https://github.com/CajucomW', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}","{'location': 'kolkata', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""In this kernal a nice feature engineering where (length , breadth and Height ) has been converted into one feature(volume) which has improved the accuracy of the model by atleast 10%.\nLearning:- IF possible try to merge your features. Lower the number of varibable/features involved better will be the accuracy.\nOnce feature Engineering is Done I have used various models LR,GBR,DTR,SVR,RFR,Stacking to predict the outcome and SVR gave a better result of 98%.\n#DESCRIPTION\nDiamond-Price-Estimate\nThis classic dataset contains the prices and other attributes of almost 54,000 diamonds\nContext\nThis classic dataset contains the prices and other attributes of almost 54,000 diamonds. It's a great dataset for beginners learning to work with data analysis and visualization.\nContent\nprice price in US dollars ($326--$18,823)\ncarat weight of the diamond (0.2--5.01)\ncut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\ncolor diamond colour, from J (worst) to D (best)\nclarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\nx length in mm (0--10.74)\ny width in mm (0--58.9)\nz depth in mm (0--31.8)\ndepth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\ntable width of top of diamond relative to widest point (43--95)\n""], 'url_profile': 'https://github.com/samiran31', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020', 'R', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020']}"
"{'location': 'Karachi,Pakistan', 'stats_list': [], 'contributions': '344 contributions\n        in the last year', 'description': ['stock_market-prediction\nusing linear regression model stock market prices can be predicted\n'], 'url_profile': 'https://github.com/MuntahaShams', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Sukirti Dash\nUSF email address:  sdash@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GitNickProgramming', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['USA-Housing-Data-Analysis\nThis is an analysis on housing data using linear regression\n'], 'url_profile': 'https://github.com/Raksh7777', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:\nUSF email address:\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Air-pollution\nAir pollution prediction using linear regression from  algorithm scratch\n'], 'url_profile': 'https://github.com/disha2000', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Sydney, Australia', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NotArpit', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Boulder, Colorado', 'stats_list': [], 'contributions': '241 contributions\n        in the last year', 'description': ['Logistic-regression-digits-classification\nLogistic regression Machine learning algorithm using digits classifier\nTo build a logistic regression model that implements stochastic gradient ascent and apply it to the task of determining whether a number is 8 or 9\nNormalize your data.\nTransform your outputs into a set of binary features via one-hot encoding.\nWrite get_optimal_parameters - train all ten models at once.\nCalculate the accuracy of your model on Train and Test data.\nGenerate a confusion matrix on test and train Data.\nPredict the labels of the first ten datapoints from your test set alongside the images of those same datapoints.\n'], 'url_profile': 'https://github.com/PreethiVijai', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '371 contributions\n        in the last year', 'description': ['Kaggle Competition Notebook\n\nA notebook for Kaggle Competition of House Prices:Advanced Regression Techniques\n\n\n\nTeam Name DaVinciCode:\n\nTeam Mates\n* Nitika Kamboj\n\n* Samar Srivastava\n\n\n\n'], 'url_profile': 'https://github.com/samacker77', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Aishwarya Parthasarathy\nUSF email address:  aparthasarathy2@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['1', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Insurance\nTo predict the charges using Linear regression and SGDRegressor\n'], 'url_profile': 'https://github.com/rajearumugam', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'Romania', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['ML-Matlab-Linear-Regression\nMatlab implementation of single variable linear regression to predict the Systolic Blood Pressure (SBP). There are two input variables to chose from in the ""age_weight_SBP.txt"" file, age and weight\n'], 'url_profile': 'https://github.com/ionut-banu', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/s-mushnoori', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:\nUSF email address:\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['Logistic-Regression-and-Churn-Prediction\nlogistic regression to predict customer churn using the REASON Method\n'], 'url_profile': 'https://github.com/glockkm', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexskim0912', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['linearRegression\nGraphical implementation of linear regression with gradient descent algorithm\n'], 'url_profile': 'https://github.com/mikhailbudko', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['decision_tree_model_for_regression_on_varoius_datasets\nDecision tree algorithm for regression on various datasets\n'], 'url_profile': 'https://github.com/hoshangk', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'Yangon, Myanmar', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Logistic-Regression-on-Titanic-Dataset(Preprocessing and Training)\nAnalysis Titanic dataset and train with Logistic Regression \nDataset Link --> https://www.kaggle.com/c/titanic/data\n'], 'url_profile': 'https://github.com/MinThuraZaw', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}","{'location': 'Lahore', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['LogisticRegression\na vectorized binary logistic regression implementation in python.\nThe following functions are supported:\n\n\nfit(self, train_X, train_Y, learningRate=0.01, numOfIterations=2000, validation_X=None, validation_Y=None): fit function is passed as parameters training dataset (train_X), training dataset labels (train_Y), learningRate, numOfIterations, validation dataset and validation dataset labels. This funtion then learns weights.\n\n\npredict(self, test_X): predict function is passed as parameter the test set (test_X). It then predicts the labels of each item in the test set and returns the labels in an array.\n\n\nsigmoid(self, Z): sigmoid function (activation function) is used by above two functions.\n\n\nNote:\n-> The input shape for training set, validation set, and test set must be (m, nx) where m is the number of items in the set and nx is the number of features.\n-> The shape of array containing labels for training set, test set and validation set must be (m, 1) where m is the number of items.\n-> The model has been trained and tested in main.py on a dataset containg cat images (dataset has been taken from coursera deep learning course assignment). The model gives 68% test accuracy.\n'], 'url_profile': 'https://github.com/BigWheel92', 'info_list': ['HTML', 'Updated Feb 28, 2020', 'MATLAB', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Siyu Zhao\nUSF email address:  szhao33@usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['MRP-with-pymc3\nThe code belongs to Austin Rochford. Uploaded here for demonstration purposes\n'], 'url_profile': 'https://github.com/tugberkcapraz', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Pengfei Song\nUSF email address:  psong4@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'United Kingdom', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hayfiz', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'San Diego', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mtaufek', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Rubin Johny\nUSF email address: rjohny@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'Columbia, MO', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['DRPnet - Automated Particle Picking In Cryo-Electron Micrographs Using Deep Regression\nThis repository is an implementation in Matlab for the paper DRPnet - Automated Particle Picking In Cryo-Electron Micrographs Using Deep Regression.\nPrerequisites\nThe code was tested successfully on the system of:\nMatlab 2018b (/w Image Processing Toolbox, Computer Vision Toolbox, and Deep Learning Toolbox)\nCUDA 9.2\ncuDNN 7.4\nHow to pick particles\nSetting up the program parameters:\nThe program parameters are stored in file ""Input/file_name.txt"". To pick particles from a dataset of images, we need to set these parameters to appropriate values. There are seven groups of parameters we can change:\n% ------------ input/output paths ---------------------------------------------------------  \ninpath = \'path_to_your_images\'; % folder contains .mrc files  \noutpath = \'output\'; % folder stores coordinate files (.star files) of picked particles  \ngroundruth_path = \'\'; % folder stores ground truth coordinates files  \n\n% ------------- train/test data -----------------------------------------------------------  \nstart_train_detect = 2;  % index of the first micrograph to train detection network  \nnum_train_detect = 2;     % number of images to train detection network  \nstart_test_detect = 2;   % index of the first micrograph to test detection network (and train/test classification network)  \nnum_train_classify = 2;   % number of images to train classification network  \n\n% ------------- input image type: negative stain versus cryo-em  \nis_negative_stain_data = 1;\n\n% ------------ prticle diameter in pixels -------------------------------------------------  \nbox_size = 50;\n\n% ------------ peak detection parameters, used after DRPnet -------------------------------  \nsigma_detect = 3;  \nthreshold = 8;  \nk_level = 3;  \n\n%------------- classification network (CNN-2) parameters ----------------------------------  \nretrain = 0; \nnum_train_mics = 2;  \nnum_epochs = 5;  \nclass_cutoff = 0.7;  \n\n%------------- optional: filter/don\'t filter particles based on their intensity -----------  \ncustomize_pct_training = 0;  \npct_mean_training = 1;  \npct_std_training = 50;  \n\ncustomize_pct_predict = 1;  \npct_mean_predict = 1;  \npct_std_predict = 50; \nfilter_particle = 1;  \n-------------------------------------------------------------------------------------------\n\nRun picking program\nmatlab -r -nosplash -nodesktop DRPnet(\'path/file_name\')\n\nThe coordinate files of pick particles will be generated in ""output"" folder\nHow to retrain classification network (CNN-2)\nUsers can set the retrain paramter to 1 if they need to train the classification network (CNN-2) to adapt better with a specific type of particles.\n%------------- classification network (CNN-2) parameters ----------------------------------\nretrain = 1; \n\nThen run the program\nmatlab -r -nosplash -nodesktop DRPnet(\'path/file_name\')\n\nor open the file ""Test_Detection_Classification_DRPnet.m"" in Matlab to run in interactive mode.\nAt first this program detects postive samples and negative samples, and trains the CNN-2 network. After finishing, the program restarts its iteration to detect particles, and will use the recenlty trained classification network to classify true/false instances of particles, and save them in the ""output"" folder.\nHow to retrain detection network (CNN-1)\nThe pre-trained Fully Convolutional Regression Network (FCRN) works well with multiple types of particles based on blob detection method. Users can also train FCRN network by their own using the files provided in ""train_detection"" folder.\nFirst, users can prepare the training samples by running\nmatlab GetTrainingDetectionSamples_bin.m\n\nThen users begin to train FCRN network (CNN-1):\nmatlab Train_Detection_Network.m\n\n'], 'url_profile': 'https://github.com/emcoregit', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'Warsaw, Poland', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['HR Analytics case study\nDuża firma o nazwie XYZ zatrudnia w danym momencie około 4000 pracowników. Jednak każdego roku około 15% pracowników odchodzi z firmy. Zarząd uważa, że odejście pracowników na własną rękę lub z powodu ich zwolnienia jest niekorzystne dla firmy z następujących powodów:\nProjekty byłych pracowników ulegają opóźnieniu, co utrudnia dotrzymanie terminów i powoduje utratę reputacji wśród konsumentów i partnerów\nDział HR musi być większy w celu rekrutacji nowych talentów\nNajczęściej nowi pracownicy muszą być przeszkoleni do pracy i / lub mieć czas na aklimatyzację w firmie\nCelem przedstawionej pracy jest zaproponowanie modelu przewidującego prawdopodobieństwo odejścia pracownika z pracy. Dane użyte do zadania znajdują się na stronie: https://www.kaggle.com/vjchoudhary7/hr-analytics-case-study#general_data.csv\nModele użyte w projekcie:\n\nregresja logistyczna\nlas losowy\n\nWnioski.  Podsumowanie - porównanie modeli\nRozpatrując confusion_matrix dla zadanego problemu badawczego oraz porównując wyniki dla różnych progów odcięcia ważne jest poleprzenie klasyfikacji TN (Osób, które odeszły z pracy i zostały poprawnie sklasyfikowane) oraz minimalizacja klasyfikacji FP (Osoby które odeszły z pracy, sklasyfikowane jako osoby nadal pracujące).\nW modelu regresji logistycznej wartości dla progu odcięcia Youdena w porównaniu z progiem odcięcia 0,5 zmieniła się następująco:\nTN wzrosło z 10 do 130 (prawdziwie negatywnych klasyfikacji)\nFP zmniejszyło się z 222 do 102 (fałszywie pozytywnych klasyfikacji)\ntzn. dla progu odcięcia optymalnego wartości te znacznie się poprawiły. Zmniejszenie progu odcięcia z 0,5 na próg optymalny Youdena powoduje znaczne zwiększenie sensitivity (czułości modelu), a co za tym idzie zmniejszenie specificity. Sensitivity wzrosło z 0,04 do 0,56.\nFPR = 1- specifity w związku z czym zwiększenie wartości TPR (True Positive Rate) jest związane również ze zwiększeniem wartości FPR (False Positive Rate).\nW modelu lasu losowego wartości dla progu odcięcia Youdena w porównaniu z progiem odcięcia 0,5 zmieniła się następująco:\nTN wzrosło z 188 do 199 (prawdziwie negatywnych klasyfikacji)\nFP zmniejszyło się z 39 do 238 (fałszywie pozytywnych klasyfikacji)\nW modelu lasu losowego wyniki również uległy poprawie jednak zmiana nie jest tak duża, wynika to z tego że sam model przy progu odcięcia 0,5 ma już bardzo wysoką skuteczność i zmiana progu nieznacznie wpływa na jego polepszenie. Sensitivity oraz Specificity praktycznie się nie zmieniło.\nW obu przypadkach próg odcięcia Youdena polepszył wyniki modelu, jednak w przypadku regresji logistycznej różnica jest znacznie większa. W przypadku modelu lasu losowego wyniki klasyfikacji przy obu progach są zbliżone. W obu przypadkach optymalny próg odcięcia jest<0.5.\nEfektywniejszym modelem dla analizowanego zagadnienia w przypadku obu progów odcięcia będzie więc model lasu lasowego. Jego jakość predykcji jest wysoka.\n'], 'url_profile': 'https://github.com/martawa10', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '375 contributions\n        in the last year', 'description': ['Sentiment Analysis of Tweets\nTABLE OF CONTENTS\n\nObjective\nTechnologies\nData\nMODEL\nImplementation\nResults\n\nOBJECTIVE\n\nBuild logistic regression package from scrath and build the model for classification.\nPerform binary classification of tweets based on sentiments using Logistic regression.\n\nTECHNOLOGIES\n\nPython Packages (scikit-learn, Pandas, Numpy, NLTK)\n\nDATA\nTwitter Amazon Reviews Dataset ( 100k Tweets-->90k Train, 10k Test ). It available here, train and test\nMODEL\n\nTF-IDF Vectorization\nLogistic Regression\n\nCode file\nIMPLEMENTATION\n\nData Preprocessing\nTF-IDF vectorization\nLogistic Regression\nK-fold CV\nMetrics- accuracy, precision, recall\n\n'], 'url_profile': 'https://github.com/skotak2', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kouakiradou', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'MATLAB', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Sep 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2021', 'MATLAB', 'Updated Feb 25, 2020']}"
"{'location': 'Mumbai', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['random_forest_regression_model_examples\nRegression using Random Forest on various Data Sets\n'], 'url_profile': 'https://github.com/hoshangk', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Anurag Jha\nUSF email address:  ajha6@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression\nName: Edmund Wong\nUSF email address: ewwong2@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Student-Performance-Analysis\nAn academic project in Statistic using linear and multiple regression\nCollaborated with three groupmates: Shihao(Owen) Tong, Siqi(Nicole) Wang and Ahnaf Tazwar Ayub.\nAnalyzed a dataset about student achievement in secondary education of two schools, which includes stu-dent grades and possible factors in student performance like demographic, social and school-related fea-tures, via linear and multiple regression. The main purpose is to explore the significant factors affecting student performance.\nLink to the dataset and dataset description: https://archive.ics.uci.edu/ml/datasets/Student+Performance#\n'], 'url_profile': 'https://github.com/riiichard', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Interest-Rate-Prediction-Using-SLR\nTo predict the interest rate using Simple Linear Regression\n'], 'url_profile': 'https://github.com/Hemachandan', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Oslo', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['AdvancedRegressionModels\nAssignment and notes for STATS762 Advanved Regression Models @ Auckland Uni\n'], 'url_profile': 'https://github.com/feiyangtang97', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MiguelRodriguezSayeg', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Nishant Mehta\nUSF email address: nmehta6@usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Manali M Patil\nUSF email address:  mmpatil@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Sami Ullah\nUSF email address:  sullah2@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}"
"{'location': 'Maryland', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shruthinair27', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Houston, TX', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['Homework 2: Text Classification with Naive Bayes and Logistic Regression\nDescription\nThis homework will expose you to scikit-learn: a Python API that is used for common NLP\nand Machine Learning tasks. Specifically, you will learn how to use scikit-learn to carry out\nfeature engineering and supervised learning for sentiment classification of movie reviews.\nDownload and unzip the training and test corpora available on the class webpage.\nDatasets are simple plaintext files grouped into two folders: pos and neg. All files in\nthe pos folder have a positive sentiment associated with them; and all files in the neg\nfolder have a negative sentiment associated with them.\n\nUse the CountVectorizer and TfidfVectorizer classes provided by scikit-learn to obtain\nbag-of-words and tf-idf representations of the raw text respectively.\nWith the feature representation as input; train the Naive Bayes and Logistic Regression\nclassifier(s) to carry out text classification.\nTest the performance of your classifier(s) on the test set by reporting accuracy, precision, recall and F-score values for the test set.\nAdditionally, carry out these experiments:\nObserve the effect of using bag-of-words and tf-idf representations on the model’s\nperformance.\nLook into how stop words can be removed. Observe the effect of removing stop words\non model performance.\nObserve the effect of L1 and L2 regularization v/s no regularization with Logistic\nRegression on model performance.\n\nInstructions\n\n\nUnzip the aclImdb_v1.tar.gz file.\n\n\nInstall dependencies:\nLinux or macOS\npip3 install -r requirements.txt\nWindows\npip install -r requirements.txt\n\n\nTo run, type in the command line interpreter:\nLinux or macOS\npython3 hw2.py <path-to-train-set> <path-to-test-set> <representation> <classifier> <stop-words> <regularization>\nWindows\npython hw2.py <path-to-train-set> <path-to-test-set> <representation> <classifier> <stop-words> <regularization>\nValid arguments:\n\nrepresentation ∈ {bow, tfidf}\nclassifier ∈ {nbayes, regression}\nstop-words ∈ {0, 1}\nregularization ∈ {no, l1, l2}\n\n\n\nNOTE: Python version >=3.6.1 is recommended.\n'], 'url_profile': 'https://github.com/nich227', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Karachi,Pakistan', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Stock_market_prediction\nlinear regression model is used to predict stock market prices\n'], 'url_profile': 'https://github.com/ShahzaibAliQaimkhani', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Kunal Sonar\nUSF email address: ksonar@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Kochi', 'stats_list': [], 'contributions': '181 contributions\n        in the last year', 'description': ['GOALKEEPER OVR & POT PREDICTION\n1. This is a simple python application using tkinter gui that predicts Overall(OVR) and Potential of a goalkeeper as such in EA Sports FIFA game.\n2. The program is implemented using Multivariate Linear Regression ML Algorithm.\n3. Data set is of EA Sports FIFA20 game.\n'], 'url_profile': 'https://github.com/pranavvp10', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RCPrushikesh', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Classification of CIFAR-10 dataset\nThe following is a code from a previous assignment for classification of the CIFAR-10 dataset through logistic regression\nResults\nConfusion Matrix:\n\nVisualization of Weight Matrices:\n\n'], 'url_profile': 'https://github.com/guevarrm', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Machine_Learning\nhomework of machine learning: supervised classification, polynomial regression, theoretical measures\n'], 'url_profile': 'https://github.com/Zengxiang-Zhao', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Chennai, India', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['Mini_Projects on Regression - Ganesh Ram Gururajan\nThese are my few mini projects on regression problems\n'], 'url_profile': 'https://github.com/ganeshramg', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}","{'location': 'Mumbai, Maharashtra, India', 'stats_list': [], 'contributions': '227 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nerds-coding', 'info_list': ['Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Python', 'Updated Jul 6, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kouakiradou', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RCPrushikesh', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Classification of CIFAR-10 dataset\nThe following is a code from a previous assignment for classification of the CIFAR-10 dataset through logistic regression\nResults\nConfusion Matrix:\n\nVisualization of Weight Matrices:\n\n'], 'url_profile': 'https://github.com/guevarrm', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Machine_Learning\nhomework of machine learning: supervised classification, polynomial regression, theoretical measures\n'], 'url_profile': 'https://github.com/Zengxiang-Zhao', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'Chennai, India', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['Mini_Projects on Regression - Ganesh Ram Gururajan\nThese are my few mini projects on regression problems\n'], 'url_profile': 'https://github.com/ganeshramg', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'Mumbai, Maharashtra, India', 'stats_list': [], 'contributions': '227 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nerds-coding', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'Anchorage, Alaska', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['Hillari Denny\nCSCE A415 - Machine Learning\nAssignment 2 - Regression and Bayes Net\nImplementing a Regression Model\nThis repo is only for part 2 of Machine Leaning Assignment 2. Bayes Net will not be covered here.\nPart 2 implements a regressor using Scikit learn to determine which variables in the data set affect life expectancy.\n3/10/2020 - Model now implements Linear Regression in Scikit-Learn. Missing values are item nonresponse and\nhave been handled with SimpleImputer. Currently, the only values that have been dropped are missing observations\nin ""Life expectancy"".\n'], 'url_profile': 'https://github.com/hillari', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Yanan Liu\nUSF email address:  yliu295@usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName: Vikas Ramaneti\nUSF email address: vramaneti@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Yuxin Huang\nUSF email address:  yhuang146@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['MATLAB', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['readme.md\nName:  Ni Luo\nUSF email address:  nluo@dons.usfca.edu\n'], 'url_profile': 'https://github.com/USF-CS-663-Spring-2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Akashpawashe', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Linear_Regression\nLinear regression is used to model and predict continuous outcomes with normal random errors. There are nearly an infinite number of different types of regression models and each regression model is typically defined by the distribution of the prediction errors (called ""residuals"") of the type of data. Logistic regression is used to model binary outcomes whereas Poisson regression is used to predict counts. In this exercise, we\'ll see some examples of linear regression as well as Train-test splits.\n'], 'url_profile': 'https://github.com/paroguha', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['logistic-regression\n'], 'url_profile': 'https://github.com/kshubhamshekhar', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Skopje, Macedonia', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Regression-models\nApplication of supervised Regression models\nExploring relationship between model complexities and generalization performance, by adjusting key parameters of the models.\nApplication content:\n\ninserting random data\nvisualization of the data\npolynomial LinearRegression models on training data for function degrees 1,3,6,9\nvisualization of the predicted models\nregression scores for the predicted models\ngraphical presentation of the regression scores\nregression scores for a non-regularized LinearRegression model and a lasso regression model\n\n'], 'url_profile': 'https://github.com/Paskalovski', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Seoul', 'stats_list': [], 'contributions': '832 contributions\n        in the last year', 'description': ['2019-2\nApplied Linear Statistical Models HW\ncode : R markdown\n수식 및 풀이 : pdf\n'], 'url_profile': 'https://github.com/kkminyoung', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '162 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abdulazeez001', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Raraphael', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Monacz', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bharathibharu98', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Python', 'Updated May 6, 2020', '1', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}"
"{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '444 contributions\n        in the last year', 'description': ['Applying-Linear-Regression-on-a-Ecommerce-website-customer-dataset\n'], 'url_profile': 'https://github.com/apoorvdwi', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'Bengalore', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/422Sagar', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shashikant18596', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'Kharagpur, India', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['Implementation of ""A Convex Framework for Fair Regression""\nSummary\nA rich family of fairness metrics for regression models that take the form of a fairness regularizer is applied to the standard loss functions for linear and logistic regression. The family of fairness metrics covers the spectrum from group fairness to individual fairness along with intermediate fairness notion. By varying the weight on the fairness regularizer, the efficient frontier of the accuracy-fairness tradeoff is obtained and the severity of this trade-off is computed via a numerical quantity called Price of Fairness (PoF).\nRequirements\npython_version = ""3.6"" \n[packages] \nnumpy==1.18.2 \npandas==1.0.3 \ncvxpy==1.0.31 \nsklearn==0.22.2.post1 \nmatplotlib==3.2.1 \nxlrd==1.2.0 \nResults\nPaper(Left) vs Our implementation(Right)\n \n \n \n \nRemaining\n\nThe Law School dataset that we managed to have access to, is a much concise version of what the authors used. Therefore, the result we obtained for this concise version of dataset is different from the author\'s and hence, isn\'t shown in the above results.\nBecause of the unavailability of the Sentencing dataset, experimentation with it couldn\'t be performed.\n\nNOTE\n\nThe paper doesn\'t use all the cross-pairs, but rather, random sampling is done for choosing the cross-pairs. In our experiments, we found that some datasets are quite sensitive to which random pairs are chosen and hence the slight difference in the paper\'s and our results.\nExperimenting with various values of lambdas to get smoother curves couldn\'t be performed because for the large datasets, the time to run the experiments on our local machines was quite large (~7-8 hours with 7 cores).\nFor Communities and Crime dataset, the paper says that two groups are formed based on the percentage of Black people, White people, Indians, Asians and Hispanics in a community. However, per capita incomes for these groups are considered for forming groups.\n\nTeam Members\n\nSharik A (19CS60D04)\nManish Chandra (19CS60A01)\nAnju Punuru (19CS60R07)\nKunal Devanand Zodape (19CS60R13)\nAnirban Saha (19CS60R50)\nHasmita Kurre (19CS60R67)\n\nProject Setup\n\nClone the repo\nInstall pipenv\n\npip install pipenv\n\n\ncd to the project directory\nCreate the virtual environment\n\npipenv install --skip-lock\n\n\nActivate the virtual environment\n\npipenv shell\n\nPreprocessing\npython3 -m src.preprocess_compas\n\nReplace preprocess_compas with preprocess_adult, preprocess_default or preprocess_community for \'Adult\', \'Default\' and \'Communities and Crime\' datasets respectively.\nGenerating Accuracy vs Fairness\npython3 -m src.frontier --dataset=compas --proc=<number of cores to use>\n\nReplace compas with adult, lawschool, default or community for \'Adult\', \'Default\' and \'Communities and Crime\' datasets respectively.\nGenerating Price of Fairness Bar Graph\npython3 -m src.pof --dataset=compas --proc=<number of cores to use>\n\nReplace compas with adult, lawschool, default or community for \'Adult\', \'Default\' and \'Communities and Crime\' datasets respectively.\nThe final plots will be saved inside output/\n'], 'url_profile': 'https://github.com/ManishChandra12', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ahmetemintek', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'Pittsburgh, PA, USA', 'stats_list': [], 'contributions': '183 contributions\n        in the last year', 'description': ['UncertaintyRegression\nThe code for ""Simple and Effective Approaches for Uncertainty Prediction in Facial Action Unit Intensity Regression"" at FG 2020\nAll building blocks are part of this repository:\n\nDWAR\nGP-VFE\nMLP ensemble\nLoss Attenuation\nMLP Model for dropout, U-MLP, and the Multi-Task MLP\n\nThe specific functions to handle the datasets, define and train the models, and evaluate uncertainty metrics are added soon.\n'], 'url_profile': 'https://github.com/twoertwein', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'Algiers, Algeria.', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['GradientDescent_linearReg\n'], 'url_profile': 'https://github.com/hanibounoua', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '162 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abdulazeez001', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['linear_regression\n'], 'url_profile': 'https://github.com/sgvvannabe', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/13KumariJyoti', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 13, 2020', 'Python', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'BSD-3-Clause license', 'Updated Sep 26, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Feb 26, 2020']}"
"{'location': 'Mumbai', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['LogisticRegression\nAN Hacker Earth solution for the Great Indian Scientists competition. Though it is simple solution ;P\n'], 'url_profile': 'https://github.com/RajezMariner', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'Portland, Or', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['regression_tools\nPersonal Set of Regression Tools with a focus on quickly trainable and explainable results.\n'], 'url_profile': 'https://github.com/chrisgian', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'New Brunswick, New Jersey', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mik0why', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dsudit05', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-regression\n'], 'url_profile': 'https://github.com/spoorthy24', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'Nalgonda, Telangana', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['regression-model-\n'], 'url_profile': 'https://github.com/Anji99-gif', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression\nA small Linear Regression model on Advertising.csv dataset\n'], 'url_profile': 'https://github.com/bbasiit', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['CFB Win Regression Model\nRepo Guide\n\nACC: All ACC data used in this project in excel format\nB1G10: All B1G 10 data used in this project in excel format\nBIG12: All BIG 12 data used in this project in excel format\nCFB: Saved RDS files and images used throughout project, as well as app.R file\nPAC12: All PAC 12 data used in this project in excel format\nSEC: All SEC data used in this project in excel format\n.gitignore\nREADME.md\nfinal_project.Rmd: R Markdown version of project where data wrangling was done\n\n'], 'url_profile': 'https://github.com/ddiakite1', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Siriratkant', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leonardbinet-overleaf', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Feb 29, 2020', 'Updated Feb 27, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'HTML', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'TeX', 'Updated Feb 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vedhapuneeth', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Multiple-Regression\n'], 'url_profile': 'https://github.com/Elllifa', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deepesh21', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'Mississauaga, Ontario', 'stats_list': [], 'contributions': '1,100 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mihirKachroo', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'Baltimore ', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Baltimore Salary -\nEmployers often make mistake of overpaying or underpaying their employees. This project uses Baltimore City employee salary data to check if the Baltimore government is making that mistake or not. The regression model checks if there is any correlation between the difference of Annual and Gross salaries from last year with the present year.\nThe correlation between the two can help us predict amount of overpayment or underpayment in next year.\n\n\nClustering\nI have clustered different departments based on 1) Overpay, 2) Medium, and 3) Underpay to make the data clear.\n\nAnalysis\nThe R-Squared value for the fitted model is 0.7 which makes it a good model. It also means that 70% of the data can be explained through this model. The standard error is of 9,687, which means that the prediction error could range from +/- $9,687. In simple words, next year (2020) Baltimore government can either over-pay or under-pay their employees given the range of $9687 v/s what they paid this year.\nSteps\n\nRetrieved data of Baltimore Salary from Baltimore City Employee Salaries for year 2019,2018, and 2017\nUsed VLOOKUP function to extract all the data sets into a single Excel sheet\nCleaned the data to remove all the NAs using filters\nTook the difference between Annual and Gross salaries for every year\nUsing Data Analysis, fitted a Linear Regression Model and analysed statistical factors like R Square value, F significance, Standard error, and P value.\nFor clustering I calculated the Mean, Variance, and the Z value.\nMinimum distance was calculated using MIN() function\nI used Excel Solver to come up with optimal minimum value for the distance\n\nNote\n\nClustering gives different values for every iteration\nData sets of newly joined employees (2019) was removed as previous years entries were blank\n\nSource\nThe data was retrieved on 26th February 2020 from https://data.baltimorecity.gov/City-Government/Baltimore-City-Employee-Salaries-FY2019/6xv6-e66h\n'], 'url_profile': 'https://github.com/yash0602', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': [""LOGISTIC REGRESSION\nThe Logistic model is used to model the probability of a certain class or event\n\nI have built a simple Binary Logistic Regression model for Predicting Diabetes.\nThis model predicts if a patient will have or won't have Diabetes in the future using medical History.\nLogistic regression is a classification algorithm which transforms its output using the logistic sigmoid function\n""], 'url_profile': 'https://github.com/iamsrilakshmi', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'Hephzibah, GA', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['games-regression\nUnit 2 Build Week project\n'], 'url_profile': 'https://github.com/ericaburdett', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'Mérida, Yucatán, México', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cruzdany', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sparsh2823', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['LinearRegression\nÜretim Planlama ve Kontrol dersi kapsamındaki projede forecasting için yazılmıştır.\n-Konya 2020\n'], 'url_profile': 'https://github.com/arasbuyukaras', 'info_list': ['Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'MIT license', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'C#', 'Updated Mar 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leonardbinet-overleaf', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,204 contributions\n        in the last year', 'description': ['logistic_regression\n'], 'url_profile': 'https://github.com/bsaund', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Milan', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['BayesianRegression\nBayesian inference and online supervised machine learning\n'], 'url_profile': 'https://github.com/ricnov', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Auckland, New Zealand', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Logistic_regression\nThis is a shiny app that displays a 3d plot of binary outcome data, with continuous independent variables of exposures.\nVarious types of logistic regression model planes may be shown. A corresponding regression nomogram is also drawn.\nThe data comes from a case-control study of cot death carried out in the 1980s in New Zealand.\nThe shiny app is available here: https://sithor.shinyapps.io/Logistic_regression/\n'], 'url_profile': 'https://github.com/sithor', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '237 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akash-alt', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['logistic_regression\n'], 'url_profile': 'https://github.com/kansusha', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2,461 contributions\n        in the last year', 'description': ['auto-regression\n'], 'url_profile': 'https://github.com/happilyeverafter95', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Majixs', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Regression-analysis\nupvote\n'], 'url_profile': 'https://github.com/pandu-cmd', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Peru', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': [""Linear-Regression\nIn this repository I'm going to upload different ways to make a lineal regression with different libraries in python.\n""], 'url_profile': 'https://github.com/GianCarloTG', 'info_list': ['TeX', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 28, 2020', 'Updated Mar 1, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}"
"{'location': 'Russia, Moscow', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Weight Agnostic Neural Networks with Structural Metrics modification\nActive prototype and computational experiment already available in Colab. \nAll experiments mentioned in paper are located in this Notebook.\nWANNs core modification with NEAT-algo is available here (TODO).\nPresentation of our research can be seen here.\nCopyright D. Skachkov, M. Sodikov, V. Strijov, R. Neychev | MIPT, 2020.\n'], 'url_profile': 'https://github.com/MakhmoodSodikov', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Movie-Regression\n'], 'url_profile': 'https://github.com/fearghusson', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mayanksinha1999', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sreeragkiyyath', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deepesh21', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Logistic_Regression\nWe turn our attention to classification. Classification tries to predict, which of a small set of classes, an observation belongs to. Mathematically, the aim is to find  yy , a label based on knowing a feature vector  xx . For instance, consider predicting gender from seeing a person\'s face, something we do fairly well as humans. To have a machine do this well, we would typically feed the machine a bunch of images of people which have been labelled ""male"" or ""female"" (the training set), and have it learn the gender of the person in the image from the labels and the features used to determine gender. Then, given a new photo, the trained algorithm returns us the gender of the person in the photo.\nThere are different ways of making classifications. One idea is shown schematically in the image below, where we find a line that divides ""things"" of two different types in a 2-dimensional feature space. The classification show in the figure below is an example of a maximum-margin classifier where construct a decision boundary that is far as possible away from both classes of points. The fact that a line can be drawn to separate the two classes makes the problem linearly separable. Support Vector Machines (SVM) are an example of a maximum-margin classifier.\n'], 'url_profile': 'https://github.com/paroguha', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'Daejeon, Republic of Korea', 'stats_list': [], 'contributions': '649 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/seanie12', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Linear-regression\nMany college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously.\nIn the article “Beauty in the Classroom: Professors’ Pulchritude and Putative Pedagogical Productivity” (PDF), authors Daniel Hamermesh and Amy M. Parker suggest\n(based on a data set of teaching evaluation scores collected at UT Austin) that student evaluation scores can partially be predicted by features unrelated to\nteaching, such as the physical attractiveness of the instructor.\nIn this notebook, we will use this data to try and predict the average instructor rating with a multiple linear regression.\nData:\nThe data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students looked at a photograph of each professor in the sample, and rated the professors’ physical appearance. More specifically:\nEach of the professors’ pictures was rated by each of six undergraduate students: Three women and three men, with one of each gender being a lower-division, two upper-division students (to accord with the distribution of classes across the two levels). The raters were told to use a 10 (highest) to 1 rating scale, to concentrate on the physiognomy of the professor in the picture, to make their ratings independent of age, and to keep 5 in mind as an average.\nWe are using a slightly modified version of the original data set from the published paper. The dataset was released along with the textbook “Data Analysis Using Regression and Multilevel/Hierarchical Models” (Gelman and Hill, 2007).)\n'], 'url_profile': 'https://github.com/Harsha-reddy98', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieljohnson18', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Linear-Regression\nHouse price prediction with deep learning network.\nDependencies\nnumpy,matplotlib.pyplot\n'], 'url_profile': 'https://github.com/migo-robben', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 4, 2020', '1', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2021', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Jun 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Predictive_Models for\ndelivery time \n\tmodel type:  predictive/regression type\n\tlanguage:  Python, R; \n\tmethodologies:  OLS, random forest \n\tfeatures:    continuous and categorical predictors\n\n\nlate delivery \n\tmodel type:  classifier (binary)\n\tlanguage:    Python\n\tmethodologies:   logistic regression, random forest, Gaussian naive Bayesian\n\tfeatures:     continuous and categorical predictors.\n\n\t\tattempt w/ undersampling to boost signal-noise ratio\n\n'], 'url_profile': 'https://github.com/ashley-wenger', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': [""Logistic-regression-on-donorschoose\nDonorsChoose\nDonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right\nnow, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the\nDonorsChoose.org website.\nNext year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they\nneed to solve:\nHow to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and\nas efficiently as possible\nHow to increase the consistency of project vetting across different volunteers to improve the experience for teachers\nHow to focus volunteer time on the applications that need the most assistance\nThe goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be\napproved, using the text of project descriptions as well as additional metadata about the project, teacher, and school.\nDonorsChoose.org can then use this information to identify projects most likely to need further review before approval.\nAuthor:Siril Sam\n""], 'url_profile': 'https://github.com/SIRILSAM77', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'サイタマ', 'stats_list': [], 'contributions': '3,312 contributions\n        in the last year', 'description': ['linear-regression-wars\nゲーム☆（＾～＾）\n'], 'url_profile': 'https://github.com/muzudho', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['graduate_admission_linear_regression\nThe is the repository of my Linear Regression model on Graduate Admission 2 dataset from Kaggle\nThe dataset and the tasks are defined here: https://www.kaggle.com/mohansacharya/graduate-admissions\n'], 'url_profile': 'https://github.com/VikrantChauhan001', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'Turkey', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['1D-LinearRegression\n1D-LinearRegression in Python\n'], 'url_profile': 'https://github.com/hasanozdem1r', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['LinearRegression_AutoCompany\nProblem Statement:\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThe company wants to know:\n\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\n\nBusiness Goal:\nWe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/PremalMatalia', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'Kansas City', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': [""Part I - WeatherPy\nIn this example, you'll be creating a Python script to visualize the weather of 500+ cities across the world of varying distance from the equator. To accomplish this, you'll be utilizing a simple Python library, the OpenWeatherMap API, and a little common sense to create a representative model of weather across world cities.\nYour first objective is to build a series of scatter plots to showcase the following relationships:\n\nTemperature (F) vs. Latitude\nHumidity (%) vs. Latitude\nCloudiness (%) vs. Latitude\nWind Speed (mph) vs. Latitude\n\nAfter each plot add a sentence or too explaining what the code is and analyzing.\nYour next objective is to run linear regression on each relationship, only this time separating them into Northern Hemisphere (greater than or equal to 0 degrees latitude) and Southern Hemisphere (less than 0 degrees latitude):\n\nNorthern Hemisphere - Temperature (F) vs. Latitude\nSouthern Hemisphere - Temperature (F) vs. Latitude\nNorthern Hemisphere - Humidity (%) vs. Latitude\nSouthern Hemisphere - Humidity (%) vs. Latitude\nNorthern Hemisphere - Cloudiness (%) vs. Latitude\nSouthern Hemisphere - Cloudiness (%) vs. Latitude\nNorthern Hemisphere - Wind Speed (mph) vs. Latitude\nSouthern Hemisphere - Wind Speed (mph) vs. Latitude\n\n""], 'url_profile': 'https://github.com/brettrchadwick', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rupikasree', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'mumbai', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/devraj-patil', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/19PA1AO410', 'info_list': ['Python', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Rust', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ganeshbhrathwaj', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['linear-regression-visualiser\nYou can run this by going to the suitable directory and running python3 app.py\n'], 'url_profile': 'https://github.com/Nush082700', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['BinaryClassifier-LogisticRegression-\nThis is jupyter notebook implementing Logistic Regression based Binary Classifier.It was originally made to classify between images.\nThis model uses a very famous mathematical tool known as Gradient Descent. All the code is written without use of any high level libraries(except for numpy) for the in-depth understanding of the concepts.\n'], 'url_profile': 'https://github.com/Shubhiey', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hughiemak', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mani-vegupatti', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hedgehog612', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['multiple-linear-regression\nThis repository deals with integration of machine learning with flask.\nThe process starts with gathering the data till make machine learning model but here I had saved the model using pickle file and then build a webapp in flask.\nInstallations\nALl the necessary libraries for this model is in the requirement.txt folder. From there you can install all the libraries by using pip install.\n'], 'url_profile': 'https://github.com/Abhishek1236', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""House-Price-Regression\nFinal project developed on Udacity's course AI and Machine Learning Fundamentals. Main Goal was to predict the house prices based on it's features.\n""], 'url_profile': 'https://github.com/luckhassel', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '215 contributions\n        in the last year', 'description': ['Regression_Project\nCreate a linear model regression to predict base salary for NYC employees\nIntroduction We got our dataset from NYC Open Data (https://data.cityofnewyork.us/City-Government/Payroll/4zc2-cuvr). Our data set contained 3,333,080 observations for the fiscal years of 2015-2019. We wanted to learn what factors contributed the most towards an employees base salary.\nData Cleaning\nFor our data we dropped columns that were irrelevant to our predictor such as Last Name, First Name, Mid Init, and Payroll Number. Some columns were variable overlap so we dropped Regular Gross Paid and Total Other Pay. Lastly we dropped column with high variability such as Title Description. We created new variables by turning Agency Start Date to years worked. From there we dropped everyone that was not a salary based, working in the 4 main boroughs (not Staten Island because it was not in the dataset) and not on leave.\nEDA\n$10,000 increase in average city employee salary over the past 5 years.\nAverage pay by borough worked Bronx $64,444 Brooklyn $65,521 Manhattan $72,903 Queens $68,192\n\n20 years is where the average pay usually levels off and that is because a lot of employees pension kicks in at that point and people retire.\n\nThe highest paying agency is the Office of Collective Bargaining\n\nFeature Engineering & Selection\nScaled continuous variables by normalizing. Looked for linear relationship between normalized variables and base salary and realized that if base salary is logged then they will be more linear. Next we split our data into test, train, split and used them to create and test our models. We created Ridge, Lasso and Linear Regression models. After comparing the residual sum mean squared errors we noticed that the Ridge model performed sightly better than Linear. Finally we did a K fold test to determine the best alpha.\nModel\nUsed an alpha of .01 that had a slightly lower residual sum mean squared error. Our model predicted the test data within .634 standard deviations.\n\nTake away\nTo make the maximum amount of money working for the city working for around 20 years in Manhattan for the Office of Collective Bargaining\n'], 'url_profile': 'https://github.com/marmistead91', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}","{'location': ""Cincinnati Children's Hospital"", 'stats_list': [], 'contributions': '572 contributions\n        in the last year', 'description': [""dual_regression_cifti\nPerforms FSL's dual_regression for CIFTI files in addition to permutation based analyses (via FSL's PALM).\nThis script requires that FSL, connectome workbench (wb_command), FSL's PALM, and GNU parallel to be installed and added to the system path for this script to work correctly.\nNote:\n\nIntended to run on LSF platforms (i.e. jobs for PALM are submitted via bsub to run in parallel).\n\n\n  Usage: dual_regression_cifti.sh -i <image> -o <out_dir> -f <list.txt> -tsL <left_surf_template> -tsR <right_surf_template>\n\nPerforms FSL's dual regression for CIFTI files in addition to permutation based analyses.\n\nRequired arguements:\n\n-i, -ics, --ica-maps          Input CIFTI dscalar IC maps\n-f, -files, --file-list       Text file list of CIFTI dtseries subject files\n-o, -out, --out-dir           Output directory\n-tsL, --template-surf-L       Template midthickness (left) surface\n-tsR, --template-surf-R       Template midthickness (right) surface\n\nOptional arguements:\n\n-j, -jobs, --jobs             Number of jobs that can be run in parallel.\n                              The default varies per system and is defined as \n                              N-1 the maximum number of cores available. [default: 11]\n-sL, --surf-list-L            Text file list of subject left midthickness surface files\n-sR, --surf-list-R            Text file list of subject right midthickness surface files\n--atlas-dir                   Surface template atlas directory. If specified, then the \n                              '--template-surf-L' and '--template-surf-R' options do not\n                              need to be specified. Atlas directory name and layout are expected\n                              to be similar to that of the HCP S1200 fs_LR atlas(es).\n--no-stats-cleanup            No clean-up of PALM's IC map sub-directory will be done\n--convert-all                 Converts all output files from (fake) NIFTI-1 images to CIFTI (recommended)\n\nDual Regression specific arguements:\n\n-des, --des-norm              Whether to variance-normalise the timecourses used as the stage-2 regressors (recommended)\n-d, -design, --design         Design matrix for final cross-subject modelling with PALM\n-c, -contrast, --t-contrast   Design t-contrast for final cross-subject modelling with PALM\n-f, --f-contrast              Design F-contrast for final cross-subject modelling with PALM\n-n, -nperm, --permutations    Number of permutations for PALM. If not set or set to 0, then\n                              permutations are performed exhaustively.\n--thr                         Perform thresholded dual regression to obtain unbiased timeseries for connectomics \n                              analyses (e.g., with FSLnets)\n\nPALM specific arguements:\n\n--fdr                         Produce FDR-adjusted p-values.\n--f-only                      Run only the F-contrasts, not the t-contrasts.\n--save1-p                     Save (1-p) instead of the actual p-values (mutually exclusive with '--log-p').\n--log-p                       Save the output p-values as -log(p) (mutually exclusive with '--save1-p', recommended).\n--two-tail                    Run two-tailed tests for all the t-contrasts instead of one-tailed.\n--demean                      Mean center the data, as well as all columns of the design matrix. If the design has \n                              an intercept, the intercept is removed.\n--sig                         Significance threshold for statistical thresholding [default: 0.05]\n--method                      Method used for determining corrected cifti-stat threshold. \n                              Valid options include: Bonferroni ('bonf') and Šidák ('sid') [default: bonf]\n\nLSF specific arguements:\n\n--mem, --memory               The amount of memory to be used when submitting jobs for PALM (in MB) [default: 5000]\n--wall                        The amount of wall-time to be allocated to each job for PALM (in hours) [default: 100]\n-q, --queue                   LSF queue name to submit jobs to (, look up queue names with the command 'bqueues') [default: normal]\n\n----------------------------------------\n\n-h,-help,--help     Prints usage and exits.\n\nNOTE:\n- Requires FSL v5.0.11+\n- Requires Connectome Workbench v1.3.2+\n- Requires FSL's PALM version alpha115+\n- Requires GNU parallel to be installed and\n  added to system path\n- Default LSF arguements are unlikely to result \n  in all PALM jobs running to completion.\n\n----------------------------------------\n\nAdebayo B. Braimah - 2020 02 18 17:31:22\n\ndual_regression_cifti.sh v0.0.1\n\n----------------------------------------\n\n  Usage: dual_regression_cifti.sh -i <image> -o <out_dir> -f <list.txt> -tsL <left_surf_template> -tsR <right_surf_template>\n\n""], 'url_profile': 'https://github.com/AdebayoBraimah', 'info_list': ['Python', 'Updated Feb 28, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Shell', 'Updated Sep 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lakshmichaitanyach', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""PA_MD_regression\nSome clarification for things you'll see in the .rmd if you're not familiar with R.\nThe .rmd file with the code in it is an Rmarkdown file created using Rstudio.\nThe data used to generate the code was from SLF PA & MD operations.\nAny code that looks like the following produces output when using Rmarkdown:\n```{r,...}\nprint(example %>% filter(example_variable <= 32 & example_variable >=5)\n\n```\n\nIn the .rmd file you'll see a lot of the following:\n```{r, echo=FALSE}\n\n\n```\n\nThis means that I did not want the code to actually show up in the final product.\nMany arguments can be used inside of the {r, ...} chunk for multiple different reasons, like handling errors and so on.\nIf there is a # inside of:\n```{r, ...}\n \n# some text\n\nggplot(example_ds, aes(x, y, fill) +\ngeom_point() + ...\n\n```\n\nit's a comment.\nIf there's a # outside of the r code chunk it means the following string is a header.\nOne # is the largest header, and the more you use the smaller it gets. This is how you can organize which headers become clickable\ntabs in the ui for the final output.\nIf there's an asterisk (*) wrapping the text then that means some sort of effect is being added to the text itself.\n\nOne * for italics.\nTwo ** for bold.\nThree *** for italics and bold.\n\nMaking an Rmarkdown file uses a lot of the same rules as making a README.md does here on github, so if you're familiar with\nthat then the .rmd should be more recognizable.\n""], 'url_profile': 'https://github.com/bronsoncj', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Case Study\nScenario:\nYou are an analyst team working for a housing construction firm.\nYou want to build a predictive model to predict sale price for houses built within ten years.\nYour firm is interested in both:\n\nWhat type of houses with what type of features sell for more than others and\nWhere they should build housing, ie, lot/geographic features and location\n\nHousing features example questions:\n\nDo big houses sell for more? How much is 100sqft worth?\nDo houses with porches sell for more? How much is a porch worth?\nHow much does having a pool cost?\nDoes the type of roof on a house impact house cost?\nDoes the ratio of first floor to lot size matter?\nIs there such a thing as too many bathrooms or bedrooms? Does it relate to the square feet of the house?\nHow much does it cost to have a basement? What is 10sqft in a basement worth? Should we finish the basements before selling them?\n\nLocation features example questions:\n\nAre there ""hot"" neighborhoods, neighborhoods where houses sell for more than other neighborhoods?\nDoes the zoning of the lot impact cost?\nHow much does living with an alley behind your host cost?\nShould we build on level ground or are houses built on a gradient sell for more?\n\nTask:\n\nAs a team build one model that includes both housing and location features.\nPresent recommendations for both housing features and location recommendations.\n\nStretch question (if there is time):\n\nAssess, using the last 10 years, how well your model fits the rest of the dataset.  Train your model on the last ten years, and test using the previous years.  What is the RMSE when you predict on the entire dataset vs your test dataset?\n\nSprint 1: Data Cleaning 120 min\n\nDecide how you will subset the data\nDo some EDA to look for correlations between target and independent variables\nIdentify variables you wish to encode\nDecide how you want to treat missing values (drop rows? not include column? replace with zero? replace with mean?)\n\nFeel free to ""split"" the data cleaning work as long as at the end of this sprint you have one Jupyter notebook that can be re-run to clean the whole dataset from start to finish.\nThe final objective of this section is to have a dataset ready for train-test-split.\nBREAK 15 min\nSprint 2: Regression 60 min\n\nIteratively build a model (start with a few variables then add more)\nWhen adding new variables, see if that affects the other coefficients, such as flipping their value or changing their p-values\nDrop variables where the coefficient does not make sense or the p-value is not high enough\n\nThe final objective of this section is to have a model that be interpreted into business decisions\nBREAK 15 min\nSprint 3: Prepare for Recommendations 30 min\n\nMake no more than 5 slides\neach slide should be:\nintro, dataset specs, main guiding questions\nrecommendations for housing type\nrecommendations for location\none slide on what further data you wish you had, what you would look into if you had more time\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['liear-_regression_profit\n'], 'url_profile': 'https://github.com/18amit', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alinakhaliq', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/radhikarangu123', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BhushanDA', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': [""Nutrition-Regression_Tasks\nI have used the nutirion dataset and applied various machine learning algorithms on it.\nTarget variable is Calories\nStep1: Done preprocessing of data i.e imputing NA's , changing categorical values into one hot vectors and removing unwated row/columns\nUsed standard Scaler to scale the dataset\nMethods Used:\n\n\nLinear Regression\n\n\nIntroduced Ridge and Lasso with different values of alpha\n\n\nPolynomialFeatures\n\n\n4.Decision Tree Reression Model\n""], 'url_profile': 'https://github.com/newbieeashish', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'Christchurch', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Regression Analysis\nRegression analysis for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG). They are particularly interested in the following two questions:\n\nIs an automatic or manual transmission better for MPG\nQuantify the MPG difference between automatic and manual transmissions\n\nlink to the full report\n'], 'url_profile': 'https://github.com/myfriendtae', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}","{'location': 'PARIS', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YacineAbd', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Updated Mar 1, 2020', 'R', 'Updated Mar 3, 2020', 'R', 'Updated Jun 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MIT license', 'Updated Nov 29, 2020', 'R', 'Updated Feb 27, 2020']}"
"{'location': 'Singapore', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Agenda: Analysing Customer Behavior on Apps Vs Website \nType: Regression \nAlgorithm: Linear Regression Algorithm \n'], 'url_profile': 'https://github.com/padalianirali', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': [""Logistic-Regression-Model\nIn this project we will be working with a fake advertising data set, indicating whether or not a particular internet user clicked on\nan Advertisement. We will try to create a model that will predict whether or not they will click on an ad based off the features of\nthat user.\nThis data set contains the following features:\n'Daily Time Spent on Site': consumer time on site in minutes\n'Age': cutomer age in years\n'Area Income': Avg. Income of geographical area of consumer\n'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n'Ad Topic Line': Headline of the advertisement\n'City': City of consumer\n'Male': Whether or not consumer was male\n'Country': Country of consumer\n'Timestamp': Time at which consumer clicked on Ad or closed window\n'Clicked on Ad': 0 or 1 indicated clicking on Ad\n""], 'url_profile': 'https://github.com/Hellblazer99', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Guruji752', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'ShangHai', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zpjshiwo77', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['LinearLogisticRegression\nLinear Regression and Logistic Regression\n'], 'url_profile': 'https://github.com/PratheekRebello', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Sklearn_LinearRegression\nAnalysis of New Coronavirus Epidemic Situation Based on Sklearn Linear Regression Model\n'], 'url_profile': 'https://github.com/1013764208', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'College Park, Maryland', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Kaggle House Regression Challenge\nThis is the ""Pandas Express"" submission for the Kaggle House Prices: Advanced Regression Techniques Challenge as part of the BMGT438A data science class. Note that the code in this repository relies on data provided by Kaggle which has been removed from the repository\'s history. Please visit Kaggle to see this dataset.\nFinal Video Presentation\nFinal presentation including analysis can be viewed here:\n\nFinal Poster\n\nMethodology\n\nClean up the data by turning all of the categorical columns (e.g Neighborhood) into a format an ML model can read using pd.get_dummies()\nCreate an initial OLS (Ordinary Least Squares) model to see its r^2 value and see whether there are any other problems with the data\nMake use of SKlearn\'s automated feature selection package by using RFECV (Recursive Feature Selection with Cross Validation) to recursively determine the number of features to use in the final model as well as what those features are\nExplore SKlearn\'s Univariate Automated Feature Selection to see if it performs better than RFECV\nBuild the final model and analyze the residuals to look for outliers and see if there are any patterns in the model\'s inaccuracies\nRun the final model on the test dataset to predict prices needed for the final Kaggle submission\n\n'], 'url_profile': 'https://github.com/gilaniasher', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['adult-income-logisticRegression\nPrediction income with logistic regression on R\n'], 'url_profile': 'https://github.com/Msolhan', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['simple-Linear-Regression\nSimple Linear implementation with python\n'], 'url_profile': 'https://github.com/bhavanasikakolli', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['This repository contains the code for basic Regression model in Machine Learning in the file named regression.py and\nthe dataset used for the model is given in student-mat.csv file.\n'], 'url_profile': 'https://github.com/Sumukhesh', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression-Project-\n'], 'url_profile': 'https://github.com/Emin123456123', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""multiLinear-Regression\ndata file is fake.\nGD means 'gradient descent' & NE means 'normal equation'\nThey are two ways to do with multiLinear regression.\n""], 'url_profile': 'https://github.com/ZhangPHs', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vamsiteja142', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Bend, OR', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Machine_Learning_Linear_Regression\nThis code uses machine learning to run linear regressions on datasets. The code is mostly optimized to work with any inputed ""training"" and ""testing"" datasets. Right now, the code is set up for training and testing datasets pertaining to a bunch of independent variables that are likely to influence housing prices in Boston. The goal is to be able to predict housing prices given a set of inputs (independent variables).\n'], 'url_profile': 'https://github.com/maxwellmckee', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-1\nHi. Here, I used simple linear regression to create a model which would predict the startup cost of a pizza franchise for a given value of annual franchise fee.\n'], 'url_profile': 'https://github.com/DebojyotiRoy15', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['Linear-Regression-Model\nSolve a feature value prediction problem using Linear Regression Model in python\n'], 'url_profile': 'https://github.com/Hellblazer99', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Johannesburg', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Nested Sampling for MLP search\nCan Bayesian evidence be used for machine learning model hyper-parameter optimization?\nIn this short project we search for the optimal number of hidden units in a multi-layer perceptron (MLP) using the\nNested Sampling approach of Skilling. We document the results for the following datasets:\n0) Regression problems - Boston house price dataset \n1) Classification problems - Iris dataset, Taiwan credit default dataset \n\nNested sampling\nNested sampling is an approach for calculating bayesian evidences while simultaneously computing the posterior probability distribution of the parameters. The basics steps of this algorithm are:\n0) Draw N live points from the prior distribution, and calculate their likelihoods\n1) Delete the lowest likelihood live point, and replace it with a new point drawn from the prior, but with higher likelihood\n2) Repeat 1) until some stopping criteria is met. \n\nPrior\nWe assumed a standard normal disrtibution as the prior distribution over the parameters/weights (including the biases) for both problem types (regression and classification). This assumption can be relaxed.\nLikelihood\nWe set out the log likelihood functions as follows:\na) Regression problems - the log likelihood was set to equal the negative of the mean squared error (MSE). This has the effect of us implicitly assuming that the output variable has a normal distribution.\nb) Classification problems - the log likelihood was set to equal the negative of the cross entropy loss. This has the effect of us implicitly assuming that the output variable has a multinomial distribution\n\nNeural network considerations\nWe assumed a tanh activation for the hidden layer for both types of problems. Classification problems had the softmax (or sigmoid) activation at the output layer. No activation function was applied at the output layer for the regression problems.\nResults\nThere is a high correlation between the evidence of the model (calculated using the training dataset) and the models performance on a test dataset. More computational resources required for larger datasets.\n'], 'url_profile': 'https://github.com/WilsonMongwe', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'ShangHai', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zpjshiwo77', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['LinearLogisticRegression\nLinear Regression and Logistic Regression\n'], 'url_profile': 'https://github.com/PratheekRebello', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Sklearn_LinearRegression\nAnalysis of New Coronavirus Epidemic Situation Based on Sklearn Linear Regression Model\n'], 'url_profile': 'https://github.com/1013764208', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'MATLAB', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 26, 2020', 'HTML', 'MIT license', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 5, 2020']}"
"{'location': 'Johannesburg', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Nested Sampling for MLP search\nCan Bayesian evidence be used for machine learning model hyper-parameter optimization?\nIn this short project we search for the optimal number of hidden units in a multi-layer perceptron (MLP) using the\nNested Sampling approach of Skilling. We document the results for the following datasets:\n0) Regression problems - Boston house price dataset \n1) Classification problems - Iris dataset, Taiwan credit default dataset \n\nNested sampling\nNested sampling is an approach for calculating bayesian evidences while simultaneously computing the posterior probability distribution of the parameters. The basics steps of this algorithm are:\n0) Draw N live points from the prior distribution, and calculate their likelihoods\n1) Delete the lowest likelihood live point, and replace it with a new point drawn from the prior, but with higher likelihood\n2) Repeat 1) until some stopping criteria is met. \n\nPrior\nWe assumed a standard normal disrtibution as the prior distribution over the parameters/weights (including the biases) for both problem types (regression and classification). This assumption can be relaxed.\nLikelihood\nWe set out the log likelihood functions as follows:\na) Regression problems - the log likelihood was set to equal the negative of the mean squared error (MSE). This has the effect of us implicitly assuming that the output variable has a normal distribution.\nb) Classification problems - the log likelihood was set to equal the negative of the cross entropy loss. This has the effect of us implicitly assuming that the output variable has a multinomial distribution\n\nNeural network considerations\nWe assumed a tanh activation for the hidden layer for both types of problems. Classification problems had the softmax (or sigmoid) activation at the output layer. No activation function was applied at the output layer for the regression problems.\nResults\nThere is a high correlation between the evidence of the model (calculated using the training dataset) and the models performance on a test dataset. More computational resources required for larger datasets.\n'], 'url_profile': 'https://github.com/WilsonMongwe', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'Houston Texas', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Logistic Regression in Matlab\nProject 4 Machine learning\nBinomial logistic regression with multi-dimensional input\nSubmitted by Siddhartha Gupta, Sittal Aryal and Trey Smith\nAbstract:\nIn this report we provide a summary of our implementation of binomial logistic regression with multi-dimensional inputs using MATLAB R2017b. We will first discuss the critical aspects of our implementation including our choice of data set and implementation strategy. This is followed by the evaluation of our implementation using synthetic data and then for the case of real world data. Finally, we provide a general summary of our results and discuss the training and testing times.\nIntroduction:\nBinomial logistic regression can be used as a binomial classifier to discriminate between data. It works by fitting a sigmoidal curve to the input data which can be multi-dimensional. The standard logistic curve is expressed as:\nf(x)=1/(1+e^(-x) )\nThe output in the case of logistic regression is always between (0,1) because of the monotonic and asymptotic nature of the sigmoidal curve. For a given target vector t, and feature vector x logistic functions can give the probability P(t/x) . Thus, logistic regression can be used to model the probability of the target being 0 or 1 given experimental data. This makes it useful for classification purposes.\nThe generalized linear logistic function for our case can be written as:\nP(Y|X)=1/(1+e^(-w^T x) )\nHere, w indicates the weight vectors and x indicates the input data or the feature vector. Y is the target variable whose probability needs to be estimated given X.\nImplementation of logistic regression model:\nThere are various approaches by which a logistic regression model’s parameter can be estimated given a training set of input and output data such as Newton’s method, gradient descent, stochastic gradient descent, minibatch etc. In this study we have chosen the gradient descent and the stochastic gradient descent method to perform logistic regression. Details of these algorithms can be found in the literature. Gradient descent and stochastic descent were chosen for their simplicity of implementation and for comparing two different approaches. Other approaches such as gradient descent with regularization require extra hyperparameters which may need to be optimized and as a result we decided not to go for them.\nWe first apply both the gradient descent and the stochastic descent algorithm to synthetic data and compare the results. Finally, we apply the above two algorithms to real data with a feature vector length of 5 (multi-dimensional). We validate our results using hold-out method in the case of real data and show the time taken by various algorithms.\nAdditional, we also applied fminunc which is a custom MATLAB function to benchmark our implementation of the algorithms for the case of real-data. We now begin by discussing the case of synthetic data.\nHardware and software:\nFor the purpose of this exercise we used MATLAB R2017B on an Alienware laptop with Windows 10 and 32 GB RAM and core i7 processor with 4GB graphic card. This enabled us to easily run iterations of the order of ~ 3*107 in small amounts of time (few minutes).\nPart A. Validation of logistic regression using synthetic data:\nThe synthetic data was generated using multivariate normal random numbers with means of [0 0] and [4 4] and standard deviation of [1 0.75]. The data obtained is shown in figure 1 a and clearly indicates overlap between the two otherwise distinct regions.\nBoth Stochastic descent implementation and Gradient descent implementation were tested on the synthetic data. In this preliminary case, we have calculated accuracy based on prior knowledge of the data. Therefore, we have defined accuracy for this preliminary case as:\nFigure 1 Synthetic data with 10,000 points showing the two classes with overlap.\n\u2003\nTable 1: A study of performance of stochastic gradient descent vs. gradient descent on synthetic data\nAlgorithm\tNumber of Iterations\tAccuracy\tStep Size\tTime\nStochastic Gradient Descent\t300000\t69.29\t3*(10^-5)\n38.82\nGradient Descent\t300000\t93.76\t3*(10^-5)\n42.33\nStochastic Gradient Descent\t3000000\t49.38\t3*(10^-5)\n423.5363\nGradient Descent\t3000000\t97.62\t3*(10^-5)\n458.5091\nFigure 2 (a) Stochastic decent (left) and gradient descent right after 300000 iterations showing match (red) and mismatch (black) (b) The same classification with 3000000 iterations showing improvement in gradient descent\nThe choice of step size and number of iterations were made after various trials and keeping in mind the time-complexity of the problem and the computational feasibility.\nPart B. Performance of logistic regression on real data:\nTo carry out logistic regression on real data, we obtained data about blood transfusion service center such that the total number of instances is 748 and the number of attributes was 5. The data was obtained from:\nhttps://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center\nA snapshot of the data columns with their headers is shown below.\nTo classify this data using our model we decided to use 3107 iterations each with a step size of 310-5. The step size and the number of iterations were chosen heuristically based on our experience with the synthetic data. However, it does not reflect a choice which was made through a parametric study and thus there is a lot of scope for improvement.\nWe applied the gradient descent method in order to find the regression parameters. We also applied the fminunc function of MATLAB in order to independently check our implementation for this case.\nWe used all 4 attributes of the donor as our input parameters and the target variable as the probability of blood donation (0 or 1). In order to validate the model obtained after logistic regression we decided to use the hold-out validation.  Since our data set is relatively small ( ~ 748 points) and easy to split, hence hold-out is a fast and convenient choice for us.\nValidation using hold-out method: We implemented hold-out method by splitting the data into 80% as training set and 20% as test set in an arbitrary manner. The training set was used to learn the model paramaeters and then the model was applied to the test case. The accuracy for both the training and test cases were calculated separately as the number of mismatches as a percentage of the data set.\nThe validation using the test and training set was done for results obtained using the MATLAB built in function as well as for Gradient descent.\nThe accuracy for the test case using fminunc or gradient descent was found to be ~ 91%, however the accuracy for the training case was lower at 75% .  This accuracy is merely a measure of the mismatch or the accuracy of correctly classifying an observation. Lastly, while we agree that k-fold validation would be a more rigorous method of validation, we adopted holdout method for its simplicity. In order to check if the hold-out method was consistent we carried out the 80-20 split multiple times in a random way and measured the accuracy of applying logistic regression. In general, since we were able to acquire 80 – 90% accuracy, we believe that hold-out method is sufficient to assess the performance of our implementation.\nCross entropy loss function: We also implemented cross-entropy loss function calculations to quantify the loss estimate. Our results showed gradient descent performing better than stochastic descent in terms of number of iterations. It may be possible that computational overhead might change the balance of which algorithm is overall more efficient.\nPart C: Training and testing times\nFor the case of real data, the training and testing times are a sensitive function of convergence / maximum number of iterations, as well as computational algorithm implemented. In our simplistic implementation of the gradient decent we observed time requirements of ~ 90 seconds for 3*107 iterations in the training step. For the test case, the time required to calculate the test error was negligible < 0.001 s. Hence, the training step is the most time consuming by at least 3 orders of magnitude in our case.\nSimilarly, when we used the MATLAB function finance in place of our own implementation, the time required for the training step was ~ 0.36 s and the time required for test case was < 0.001 s. These results reflect the robustness of optimized algorithms in dealing with machine learning problems where a speedup of almost 30 times is achieved by using custom functions.\n\u2003\nSummary:\nIn our study we looked at developing binomial logistic regression models using synthetic as well as real data. We also tried multiple algorithms and highlighted their advantages and disadvantages. We used hold-out validation in case of our real data as it affords simplicity and ease of use. In the case of learning algorithm, we implemented both the stochastic gradient and the gradient descent method for the synthetic data. However, since stochastic descent is sensitive to the choice of number of iterations and step size, we decided to go with gradient descent for the case of real data. We observe that as we increased number of iterations, our accuracy increased. However, we do realize that a lot of optimization needs to be done in our case to arrive at an efficient choice of step size and number of iterations. In sum, our regression models can achieve ~ 75% – 97 % accuracy for the choice of parameters.\nReferences:\nWikipedia contributors. ""Machine learning."" Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 1 Apr. 2018. Web. 3 Apr. 2018.\n“Deep Learning.” Coursera, Deeplearning.ai, www.coursera.org/specializations/deep-learning?\nBishop, Christopher M., and Tom M. Mitchell. ""Pattern Recognition and Machine Learning."" (2014).\n'], 'url_profile': 'https://github.com/BitsAndAtoms', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alisherbaygarin', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'Nairobi,Westlands', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rigelabs', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['SGD-for-linear-regression\nTo implement stochastic gradient descent to optimize a linear regression algorithm on Boston House Prices dataset which is already exists in sklearn as a sklearn.linear_model.SGDRegressor.here,SGD algorithm is defined manually and then comapring the both results.Linear regression is technique to predict on real values.\nstochastic gradient descent technique , evaluates and updates the coefficients every iteration to minimize the error of a model on training data.\nObjective:\nTo Implement stochastic gradient descent on Bostan House Prices dataset for linear Regression\nImplement SGD and deploy on Bostan House Prices dataset.\nComapare the Results with sklearn.linear_model.SGDRegressor\n'], 'url_profile': 'https://github.com/cpriyankab', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahuls0599', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '577 contributions\n        in the last year', 'description': [""Gradient Descent for Univariate Linear Regression\nProgram in Java to demonstrate univariate linear regression with gradient descent.\nUsage\nOn Linux or MacOS:\n$ chmod +x gradient-descent.sh\n$ ./gradient-descent.sh\nOn Windows:\nC:\\DIRECTORY> gradient-descent.bat\nThese will automatically compile and run the program.\nData\nData is stored as .csv files within the /data folder.\nThe program works on numeric data, thus the data should be formatted as two-column numeric data delimited with commas.\nYou can specify the your own file by changing line 3 of GradientDescent.java:\n    static final String DATA_FILE = /* your_file.csv */;\nExample data included:\n\nMacdonellDF.csv\n\nMacdonell's Data on Height and Finger Length of Criminals, used by Gosset (1908)\nFormat: height, finger_length\n\n\n\nPlotting Functions\nOn line 58 of the GradientDescent class, there is a HypothesisFunction functional interface that is used\nin plotting the graph for said function. For example, if you want to plot the function:\nh(x) = x + 2\n\nyou'd change that line to be:\nHypothesisFunction h_x = (x) -> x + 2;\nThis is called a lambda expression, and it's useful because it resembles how you'd write\nthe function mathematically. Lambda expressions are of the form:\n(parameter1, parameter2,...parameterN) -> { function body }\n\nThink of it as just a more compact way of writing methods.\nIf you want to add external variables in the lambda expression you have to\ncreate a temporary variable that is final because of how it is implemented\nin Java. So if you have variables w1 and w2 and want to express the function\nh(x) = (w1 * x) + w0, you'd have to create temporary variables to store the current value. That is:\ndouble w1 = 0;\ndouble w2 = 0;\n////////////////////\n// some code here //\n////////////////////\nfinal double w_1 = w1;\nfinal double w_0 = w0;\nHypothesisFunction h_x = (x) -> (w1 * x) + w0;\nAs of right now, the HypothesisFunction interface can only handle one parameter x.\nIf you want to pass in more parameters you'd have to go to HypothesisFunction.java\nand change the method signature.\nFor example, you want to write the function (x1, x2) -> x1 + x2.\nIn HypothesisFunction.java, change the evaluate method to:\ndouble evaluate(double x1, double x2);\nAs it is, the program is only capable of plotting graphs for univariate regression because it only plots\nin two dimensions.\n""], 'url_profile': 'https://github.com/Nirusu', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hughiemak', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['Salary-Prediction-Polinomial-Regression\n\nWe have a dataset which have a salary of according to different position in a company\nSo if a new person came to join then his sallery can be predicted from the given machine\nGUI USED\n\nSPYDER\nDataset Name\n\nPosition_Salaries_New.csv\nPackages Used\n\n\nPandas\nNumpy\nMatplotlib.pyplot\n\nEXPECTED OUTPUT\n\nFor Linear Regression  -->  306082.81493002\nFor Polynomial Regression  --> 153002.77334568\n'], 'url_profile': 'https://github.com/ks1912', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sscarfone-minerva', 'info_list': ['Python', 'Updated Apr 26, 2020', 'MATLAB', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 27, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['LinearRegressionProject\nMachine learning project using both python and R\n'], 'url_profile': 'https://github.com/ADosier', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'National Institute of Statistics and Applied Economics, Rabat', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['ML-Linear-Regression\n'], 'url_profile': 'https://github.com/Okimdone', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ram71', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sergiu154', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Regression_Analysis_R\nAnalysis of different algorithms (KNN and Linear Regression) in R\n'], 'url_profile': 'https://github.com/simonZhou86', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '577 contributions\n        in the last year', 'description': [""Gradient Descent for Univariate Linear Regression\nProgram in Java to demonstrate univariate linear regression with gradient descent.\nUsage\nOn Linux or MacOS:\n$ chmod +x gradient-descent.sh\n$ ./gradient-descent.sh\nOn Windows:\nC:\\DIRECTORY> gradient-descent.bat\nThese will automatically compile and run the program.\nData\nData is stored as .csv files within the /data folder.\nThe program works on numeric data, thus the data should be formatted as two-column numeric data delimited with commas.\nYou can specify the your own file by changing line 3 of GradientDescent.java:\n    static final String DATA_FILE = /* your_file.csv */;\nExample data included:\n\nMacdonellDF.csv\n\nMacdonell's Data on Height and Finger Length of Criminals, used by Gosset (1908)\nFormat: height, finger_length\n\n\n\nPlotting Functions\nOn line 58 of the GradientDescent class, there is a HypothesisFunction functional interface that is used\nin plotting the graph for said function. For example, if you want to plot the function:\nh(x) = x + 2\n\nyou'd change that line to be:\nHypothesisFunction h_x = (x) -> x + 2;\nThis is called a lambda expression, and it's useful because it resembles how you'd write\nthe function mathematically. Lambda expressions are of the form:\n(parameter1, parameter2,...parameterN) -> { function body }\n\nThink of it as just a more compact way of writing methods.\nIf you want to add external variables in the lambda expression you have to\ncreate a temporary variable that is final because of how it is implemented\nin Java. So if you have variables w1 and w2 and want to express the function\nh(x) = (w1 * x) + w0, you'd have to create temporary variables to store the current value. That is:\ndouble w1 = 0;\ndouble w2 = 0;\n////////////////////\n// some code here //\n////////////////////\nfinal double w_1 = w1;\nfinal double w_0 = w0;\nHypothesisFunction h_x = (x) -> (w1 * x) + w0;\nAs of right now, the HypothesisFunction interface can only handle one parameter x.\nIf you want to pass in more parameters you'd have to go to HypothesisFunction.java\nand change the method signature.\nFor example, you want to write the function (x1, x2) -> x1 + x2.\nIn HypothesisFunction.java, change the evaluate method to:\ndouble evaluate(double x1, double x2);\nAs it is, the program is only capable of plotting graphs for univariate regression because it only plots\nin two dimensions.\n""], 'url_profile': 'https://github.com/Nirusu', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/18amit', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ram71', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['概要\n作業の工数見積もりを行うAI（ディープラーニング）プログラムです。一般的な回帰問題にも応用可能です。\nOptunaによるハイパーパラメータ最適化機能を実装しています。\n使用方法・解説\nこちらのブログ記事をご確認ください。\n・使用方法： https://nine-num-98.blogspot.com/2020/03/ai-hyper-opt-01.html\n・解説： https://nine-num-98.blogspot.com/2020/03/ai-hyper-opt-02.html\n参考\n・Optuna： https://tech.preferred.jp/ja/blog/optuna-release/\n・Optuna実装前のバージョン： https://github.com/kotetsu99/deep_regression\n・logging.conf内容の参考サイト： https://www.sejuku.net/blog/23149\n'], 'url_profile': 'https://github.com/kotetsu99', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/WIRASTA', 'info_list': ['Python', 'Updated Mar 4, 2020', 'Updated Feb 27, 2020', 'R', 'Updated Feb 26, 2020', 'R', 'Updated Feb 28, 2020', 'Updated Feb 26, 2020', 'Java', 'Updated Feb 26, 2020', 'Updated Feb 29, 2020', 'R', 'Updated Feb 26, 2020', 'Python', 'Updated Mar 13, 2020', 'HTML', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChaitraUH', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': [""\nft_linear_regression \n\nDescription\nCe projet sera votre première étape dans l'IA et l'apprentissage automatique. Vous allez commencer par un algorithme d'apprentissage machine simple et basique. Vous devrez créer un programme qui prédit le prix d'une voiture en utilisant un train de fonctions linéaires avec un algorithme de descente de gradient.\n\n""], 'url_profile': 'https://github.com/Drakauf', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'New Delhi, India ', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Polynomial-Regression-from-scratch\nImplemented linear and polynomial regression from scratch and tested it on Boston-Housing Dataset. Used Normal equation method.\n'], 'url_profile': 'https://github.com/nishant3101', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['s Readme file\n'], 'url_profile': 'https://github.com/amitsinghkec2', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'mumbai', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/devraj-patil', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Patrickkp1', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vamsiteja142', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Fish-Weight-Prediction-and-Species-Classification\nI utilized a linear regression model to make predictions on the weight of fishes. To interpret my model, I looked at the coefficient results to determine the relationship of weight and the other independent features. In addition, I also utilized a logistic regression model to classify the species of fish based on other independent features. To interpret my model, I calculated the odds ratio to determine, for example, how great the odds of a fish being a Pike is from an increase of the independent features.\n'], 'url_profile': 'https://github.com/KevinQMBui', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Qingdao, China', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['Hands-On-Machine-Learning\n机器学习实战：\n一、用贝叶斯决策的方法推测12篇存在争议的《联邦党人文集》的作者\n二、分别用KNN、逻辑回归、SVM、K-Means方法对二维高斯分布的两类点（500个）进行分类并绘制分类边界线\n三、分别用潜在因子算法（矩阵分解）、临近相似度算法，实现协同滤波，根据一个涉及影评者及其几部影片评分情况的字典，对用户进行电影推荐\n'], 'url_profile': 'https://github.com/RainFZY', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Karachi', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Regression-and-Time-Series-Analysis\nIn this you will find projects relating to Regression and Time Series\n'], 'url_profile': 'https://github.com/Hassan8725', 'info_list': ['MATLAB', 'Updated Mar 1, 2020', 'Python', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Feb 27, 2020', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gklabs', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'Bellevue, WA', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/whitecw', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['wdio-fefanf-html-visreg-reporter\nA reporter for webdriver.io which generates a HTML report for visual regression tests using wdio-image-comparison-service in the wdio-fefanf-boilerplate\nThis project is a fork of @rpii/wdio-html-reporter, which was itself a fork of a reporter I had previously created wdio-html-format-reporter\nInstallation\nThe easiest way is to keep the wdio-fefanf-html-visreg-reporter as a devDependency in your package.json:\n{\n  ""devDependencies"": {\n    ""wdio-fefanf-html-visreg-reporter"": ""~0.0.2""\n  }\n}\nOr, you can simply do it with:\nnpm install wdio-fefanf-html-visreg-reporter --save-dev\n\nAutomatic PR with new baseline images\nBy selecting the images you want to become the new basline, and clicking ""Update Selected Images"", a Pull-Request will automatically be created in github on the repo you specify. You will need a github token.\nTo create a token, you need a Github account, then go here and follow these instructions. Store the token in a safe place to re-use it.\nConfiguration\nThe following code shows the default wdio test runner configuration. Just add an VisRegHtmlReporter object as another reporter to the reporters array.  Syntax shown requires babel:\n// wdio.conf.js\nimport { VisRegReportAggregator, VisRegHtmlReporter} from \'wdio-fefanf-html-visreg-reporter\' ;\nmodule.exports = {\n\n  \n  reporters: [\'spec\',\n        [VisRegHtmlReporter, {\n            debug: true,\n            outputDir: \'./reports/visreg-reports/\',\n            filename: \'report.html\',\n            reportTitle: \'Visual Regression Test Report Title\',\n            // to override the git repo & branch\n            gitRepo: \'<owner>/<repo>\', \n            // to show the report in a browser when done\n            showInBrowser: true,\n\n            // to use the template override option, can point to your own file in the test project:\n            // templateFilename: path.resolve(__dirname, \'../src/wdio-fefanf-html-visreg-reporter-alt-template.hbs\'),\n            \n            // to add custom template functions for your custom template:\n            // templateFuncs: {\n            //     addOne: (v) => {\n            //         return v+1;\n            //     },\n            // },\n\n            //to initialize the logger\n            LOG: log4j.getLogger(""default"")\n        }\n        ]\n    ]\n    \n \n};\nConfiguration Options:\nTo generate a master report for all suites\nwebdriver.io will call the reporter for each test suite.  It does not aggregate the reports.  To do this, add the following event handlers to your wdio.config.js\n    onPrepare: function (config, capabilities) {\n\n        let reportAggregator = new VisRegReportAggregator({\n            outputDir: \'./reports/visreg-reports/\',\n            filename: \'master-report.html\',\n            reportTitle: \'Visual Regression Master Report\',\n            \n            // to use the template override option, can point to your own file in the test project:\n            // templateFilename: path.resolve(__dirname, \'../src/wdio-fefanf-html-visreg-reporter-alt-template.hbs\')\n        });\n        reportAggregator.clean() ;\n\n        global.reportAggregator = reportAggregator;\n    },\n    \n    onComplete: function(exitCode, config, capabilities, results) {\n        (async () => {\n            await global.reportAggregator.createReport( {\n                config: config,\n                capabilities: capabilities,\n                results : results\n            });\n        })();\n    },\n    \nTo use a logger for debugging\nA new feature for developers is to add a log4js logger to see detailed debug output.  See the test/reporter.spec.js for configuration options\nTo use a custom handlebars template for reports\nUncomment the templateFilename above, and in the VisRegReportAggregator.  You must provide an absolute path to the file you can modify the alt-template above if you wish\nThe template must support all the constructs in the default template.  YOu may add more or just change the formatting and css.\nTo trigger visual regression tests\nYou just need to use wdio-fefanf-boilerplate and use the @visreg tag for specific Scenarios\n  @visreg\n  Scenario: Go to the Home page\n    Given I go to the Home page\n    Then I should now be on the Home page\n    When I click on signInLink\n    Then I should now be on the Signin page\n    ...\nSample Output:\n\n'], 'url_profile': 'https://github.com/aruiz-caritsqa', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Some experiments on data analysis programs, mostly for NBA or PSE\n'], 'url_profile': 'https://github.com/danielpaulotipan', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Titanic-Project\nMy first project in Data Science using Logistic Regression and Decision Tree.\n'], 'url_profile': 'https://github.com/IsaelOliveira', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'Tianjin', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AISKYEYE-TJU', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'Chennai,india', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/venkatchadalavada', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Salary-Prediction\nA project that intends to predict the salary of a person using Linear Regression\n'], 'url_profile': 'https://github.com/cristinamaria01', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'Arizona', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Pesquisa_CreditRiskMethods_MECAI\nResearch completed about different methods of credit-risk scoring - Logistic Regression, Decision Trees, and Random Forest.\n'], 'url_profile': 'https://github.com/ConnorSterrett', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Walmart-Trip-Type-Classification\nClass project: logistic regression, random forest, gradient boosting (data from Kaggle)\n'], 'url_profile': 'https://github.com/YameiW', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'HTML', 'Updated Mar 31, 2020', '1', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Updated Feb 29, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Mar 1, 2020']}"
"{'location': 'Tianjin', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AISKYEYE-TJU', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/enakshidas', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Chennai,india', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/venkatchadalavada', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Classification\nLogistic Regression, Random Forest Trees, SVM , Multi-layer perceptron, Mini-batch Stochastic Gradient descent\n'], 'url_profile': 'https://github.com/lakshmichaitanyach', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Cambridge, England', 'stats_list': [], 'contributions': '194 contributions\n        in the last year', 'description': ['Coursework NLP: Predicting Machine Translation Quality\nAuthor: Maleakhi Wijaya & Faidon Mitzalis & Harry Coppock\nThe aim of the coursework is to develop regression models to predict the quality of machine translated sentence from English to Chinese. The project explore 3 main embedding techniques: Word2Vec, GloVe, and BERT. In addition, we also explore various regressor models: Feed forward Neural Network, LSTM, Bi-LSTM, Random Forest, and SVR.\nThe repository contains the following files:\n\nbaseline_regressors.ipynb: contains various statistical and neural regressor models and baseline word embeddings techniques using Word2Vec and GloVe with some modification.\nbert_regressors.ipynb: contains state of the art neural regressor model and BERT word embedding technique.\n\n'], 'url_profile': 'https://github.com/maleakhiw', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Forest_Fires_Regression\nregression modelling of forest fires in Portugal to predict fire damage\nContributors\nPhilip Sohn\nSource\nData from UCI machine learning repository: http://archive.ics.uci.edu/ml/datasets/Forest+Fires\n'], 'url_profile': 'https://github.com/psohn', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Potsdam', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': [' amifast: simple powerful benchmarking with Python\n\nContents: Installation and usage |\nContributing | Change Log |\nAuthors\n\nInstallation and Usage\nInstallation\nUsage\n\nExamples\nExamples can be found in the Examples directory.\nCommand line options\nWill be added at a later date.\nLicense\nMIT\nContributing\nMore details can be found in CONTRIBUTING\nChange Log\nNothing has been recorded here so far...\nAuthors\nDeveloped by Julian Niedermeier\n'], 'url_profile': 'https://github.com/sleighsoft', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Chennai,india', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': [""Ames Housing Data and Kaggle Challenge\nIn this project I examined a comprehensive housing dataset from the city of Ames in Iowa, USA (source). Homeowners looking to increase the value of their homes often spend too much on remodeling and don't get the return on investment when selling the house. On the other hand, people looking to buy houses want to get the best house possible given a budget. The goal of this project is to address some of these concerns, specifically:\n\nwhat features add the most value to a home, and which hurt home values the most?\ngiven a set of features, what is the expected sale price of a house?\ngiven a budget, what kind of house would one be able to afford?\n\nTo answer these questions, I fitted different linear regression models to the housing data in order to determine the features that are most influential on house price, and those that have the least effect. The performance of the models will be evaluated using the R2 metric, which is a measure of how much the model is able to explain the variance in the dataset.\nDue to the scale of this project, it is split into three Jupyter notebooks: EDA and data cleaning, preprocessing and feature engineering, and model tuning and insights (along with an appendix showing some additional model results where I attempted automated feature selection with the featuretools library).\nSummary of Findings & Recommendations\n\nAn elastic net regression model had the best predictive performance on housing sale price in Ames USA, and outperformed the other linear models tested. The model revealed that square feet area, condition, age, and the location of the house are the most important determinant factors of how much a house sells for. With an R2 of 0.903, it can be used to predict house prices in Ames with relatively high accuracy based on characteristics of the house, of which the following were found to most greatly affect price:\n\nFor house buyers with specific budgets, this model would allow one to figure out what features they would be able to afford given an amount of money. For those looking to invest, houses in the Northridge Heights, Stone Brook, and Northridge neighbourhoods all had higher prices compared to other neighbourhoods. Conversely, people looking to sell their house would be able to use the model to get an estimate of how much they would be able to sell their house for. If one is looking to sell, they should do it sooner rather than later, as the age of the house is one of the biggest contributing factor to the decrease in value. Having a garage in bad condition also negatively affect value, as does having a second floor to the house. And unsurprisingly, if the house is severely damaged, its value drops. As such, those looking to increase the value of their home could consider increasing the square footage of their house, renovating the kitchen and garage, and adding a fireplace.\nThe model may however have limited applicabilities, as it was developed using data on houses sold between 2006 - 2010 in Ames, USA. This dataset is limited in scope both in terms of the time frame captured, as well as location. The small time frame of four years is not enough to capture any annual changes in sale price that could arise as a result of external factors, such as the current economy. Housing prices at present (in 2019) may also have changed as a result of inflation. The model is also specific to houses in Ames and may not be as accurate when applied to data from another city or country, where house prices may be affected by different things than for example, garage quality.\nIn reality, house price may be difficult to predict as it is also affected by buyers' psychology, the economic climate, and other factors not included in the present dataset. There will never be a perfect model; the aim of this model is therefore not to give a perfect prediction, but act as a guideline to inform decisions.\nData Dictionary\n\n   SalePrice - the property's sale price in dollars. This is the target variable.\n   MSSubClass: The building class\n       20 1-STORY 1946 & NEWER ALL STYLES\n       30 1-STORY 1945 & OLDER\n       40 1-STORY W/FINISHED ATTIC ALL AGES\n       45 1-1/2 STORY - UNFINISHED ALL AGES\n       50 1-1/2 STORY FINISHED ALL AGES\n       60 2-STORY 1946 & NEWER\n       70 2-STORY 1945 & OLDER\n       75 2-1/2 STORY ALL AGES\n       80 SPLIT OR MULTI-LEVEL\n       85 SPLIT FOYER\n       90 DUPLEX - ALL STYLES AND AGES\n       120 1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150 1-1/2 STORY PUD - ALL AGES\n       160 2-STORY PUD - 1946 & NEWER\n       180 PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n       190 2 FAMILY CONVERSION - ALL STYLES AND AGES\n   MSZoning: Identifies the general zoning classification of the sale.\n       A Agriculture\n       C Commercial\n       FV Floating Village Residential\n       I Industrial\n       RH Residential High Density\n       RL Residential Low Density\n       RP Residential Low Density Park\n       RM Residential Medium Density\n   LotFrontage: Linear feet of street connected to property\n   LotArea: Lot size in square feet\n   Street: Type of road access to property\n       Grvl Gravel\n       Pave Paved\n   Alley: Type of alley access to property\n       Grvl Gravel\n       Pave Paved\n       NA No alley access\n   LotShape: General shape of property\n       Reg Regular\n       IR1 Slightly irregular\n       IR2 Moderately Irregular\n       IR3 Irregular\n   LandContour: Flatness of the property\n       Lvl Near Flat/Level\n       Bnk Banked - Quick and significant rise from street grade to building\n       HLS Hillside - Significant slope from side to side\n       Low Depression\n   Utilities: Type of utilities available\n       AllPub All public Utilities (E,G,W,& S)\n       NoSewr Electricity, Gas, and Water (Septic Tank)\n       NoSeWa Electricity and Gas Only\n       ELO Electricity only\n   LotConfig: Lot configuration\n       Inside Inside lot\n       Corner Corner lot\n       CulDSac Cul-de-sac\n       FR2 Frontage on 2 sides of property\n       FR3 Frontage on 3 sides of property\n   LandSlope: Slope of property\n       Gtl Gentle slope\n       Mod Moderate Slope\n       Sev Severe Slope\n   Neighborhood: Physical locations within Ames city limits\n       Blmngtn Bloomington Heights\n       Blueste Bluestem\n       BrDale Briardale\n       BrkSide Brookside\n       ClearCr Clear Creek\n       CollgCr College Creek\n       Crawfor Crawford\n       Edwards Edwards\n       Gilbert Gilbert\n       GrnHill Green Hill\n       IDOTRR Iowa DOT and Rail Road\n       MeadowV Meadow Village\n       Mitchel Mitchell\n       Names North Ames\n       NoRidge Northridge\n       NPkVill Northpark Villa\n       NridgHt Northridge Heights\n       NWAmes Northwest Ames\n       OldTown Old Town\n       SWISU South & West of Iowa State University\n       Sawyer Sawyer\n       SawyerW Sawyer West\n       Somerst Somerset\n       StoneBr Stone Brook\n       Timber Timberland\n       Veenker Veenker\n   Condition1: Proximity to main road or railroad\n       Artery Adjacent to arterial street\n       Feedr Adjacent to feeder street\n       Norm Normal\n       RRNn Within 200' of North-South Railroad\n       RRAn Adjacent to North-South Railroad\n       PosN Near positive off-site feature--park, greenbelt, etc.\n       PosA Adjacent to postive off-site feature\n       RRNe Within 200' of East-West Railroad\n       RRAe Adjacent to East-West Railroad\n   Condition2: Proximity to main road or railroad (if a second is present)\n       Artery Adjacent to arterial street\n       Feedr Adjacent to feeder street\n       Norm Normal\n       RRNn Within 200' of North-South Railroad\n       RRAn Adjacent to North-South Railroad\n       PosN Near positive off-site feature--park, greenbelt, etc.\n       PosA Adjacent to postive off-site feature\n       RRNe Within 200' of East-West Railroad\n       RRAe Adjacent to East-West Railroad\n   BldgType: Type of dwelling\n       1Fam Single-family Detached\n       2FmCon Two-family Conversion; originally built as one-family dwelling\n       Duplx Duplex\n       TwnhsE Townhouse End Unit\n       TwnhsI Townhouse Inside Unit\n   HouseStyle: Style of dwelling\n       1Story One story\n       1.5Fin One and one-half story: 2nd level finished\n       1.5Unf One and one-half story: 2nd level unfinished\n       2Story Two story\n       2.5Fin Two and one-half story: 2nd level finished\n       2.5Unf Two and one-half story: 2nd level unfinished\n       SFoyer Split Foyer\n       SLvl Split Level\n   OverallQual: Overall material and finish quality\n       10 Very Excellent\n       9 Excellent\n       8 Very Good\n       7 Good\n       6 Above Average\n       5 Average\n       4 Below Average\n       3 Fair\n       2 Poor\n       1 Very Poor\n   OverallCond: Overall condition rating\n       10 Very Excellent\n       9 Excellent\n       8 Very Good\n       7 Good\n       6 Above Average\n       5 Average\n       4 Below Average\n       3 Fair\n       2 Poor\n       1 Very Poor\n   YearBuilt: Original construction date\n   YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)\n   RoofStyle: Type of roof\n       Flat Flat\n       Gable Gable\n       Gambrel Gabrel (Barn)\n       Hip Hip\n       Mansard Mansard\n       Shed Shed\n   RoofMatl: Roof material\n       ClyTile Clay or Tile\n       CompShg Standard (Composite) Shingle\n       Membran Membrane\n       Metal Metal\n       Roll Roll\n       Tar&Grv Gravel & Tar\n       WdShake Wood Shakes\n       WdShngl Wood Shingles\n   Exterior1st: Exterior covering on house\n       AsbShng Asbestos Shingles\n       AsphShn Asphalt Shingles\n       BrkComm Brick Common\n       BrkFace Brick Face\n       CBlock Cinder Block\n       CemntBd Cement Board\n       HdBoard Hard Board\n       ImStucc Imitation Stucco\n       MetalSd Metal Siding\n       Other Other\n       Plywood Plywood\n       PreCast PreCast\n       Stone Stone\n       Stucco Stucco\n       VinylSd Vinyl Siding\n       Wd Sdng Wood Siding\n       WdShing Wood Shingles\n   Exterior2nd: Exterior covering on house (if more than one material)\n       AsbShng Asbestos Shingles\n       AsphShn Asphalt Shingles\n       BrkComm Brick Common\n       BrkFace Brick Face\n       CBlock Cinder Block\n       CemntBd Cement Board\n       HdBoard Hard Board\n       ImStucc Imitation Stucco\n       MetalSd Metal Siding\n       Other Other\n       Plywood Plywood\n       PreCast PreCast\n       Stone Stone\n       Stucco Stucco\n       VinylSd Vinyl Siding\n       Wd Sdng Wood Siding\n       WdShing Wood Shingles\n   MasVnrType: Masonry veneer type\n       BrkCmn Brick Common\n       BrkFace Brick Face\n       CBlock Cinder Block\n       None None\n       Stone Stone\n   MasVnrArea: Masonry veneer area in square feet\n   ExterQual: Exterior material quality\n       Ex Excellent\n       Gd Good\n       TA Average/Typical\n       Fa Fair\n       Po Poor\n   ExterCond: Present condition of the material on the exterior\n       Ex Excellent\n       Gd Good\n       TA Average/Typical\n       Fa Fair\n       Po Poor\n   Foundation: Type of foundation\n       BrkTil Brick & Tile\n       CBlock Cinder Block\n       PConc Poured Contrete\n       Slab Slab\n       Stone Stone\n       Wood Wood\n   BsmtQual: Height of the basement\n       Ex Excellent (100+ inches)\n       Gd Good (90-99 inches)\n       TA Typical (80-89 inches)\n       Fa Fair (70-79 inches)\n       Po Poor (<70 inches)\n       NA No Basement\n   BsmtCond: General condition of the basement\n       Ex Excellent\n       Gd Good\n       TA Typical - slight dampness allowed\n       Fa Fair - dampness or some cracking or settling\n       Po Poor - Severe cracking, settling, or wetness\n       NA No Basement\n   BsmtExposure: Walkout or garden level basement walls\n       Gd Good Exposure\n       Av Average Exposure (split levels or foyers typically score average or above)\n       Mn Mimimum Exposure\n       No No Exposure\n       NA No Basement\n   BsmtFinType1: Quality of basement finished area\n       GLQ Good Living Quarters\n       ALQ Average Living Quarters\n       BLQ Below Average Living Quarters\n       Rec Average Rec Room\n       LwQ Low Quality\n       Unf Unfinshed\n       NA No Basement\n   BsmtFinSF1: Type 1 finished square feet\n   BsmtFinType2: Quality of second finished area (if present)\n       GLQ Good Living Quarters\n       ALQ Average Living Quarters\n       BLQ Below Average Living Quarters\n       Rec Average Rec Room\n       LwQ Low Quality\n       Unf Unfinshed\n       NA No Basement\n   BsmtFinSF2: Type 2 finished square feet\n   BsmtUnfSF: Unfinished square feet of basement area\n   TotalBsmtSF: Total square feet of basement area\n   Heating: Type of heating\n       Floor Floor Furnace\n       GasA Gas forced warm air furnace\n       GasW Gas hot water or steam heat\n       Grav Gravity furnace\n       OthW Hot water or steam heat other than gas\n       Wall Wall furnace\n   HeatingQC: Heating quality and condition\n       Ex Excellent\n       Gd Good\n       TA Average/Typical\n       Fa Fair\n       Po Poor\n   CentralAir: Central air conditioning\n       N No\n       Y Yes\n   Electrical: Electrical system\n       SBrkr Standard Circuit Breakers & Romex\n       FuseA Fuse Box over 60 AMP and all Romex wiring (Average)\n       FuseF 60 AMP Fuse Box and mostly Romex wiring (Fair)\n       FuseP 60 AMP Fuse Box and mostly knob & tube wiring (poor)\n       Mix Mixed\n   1stFlrSF: First Floor square feet\n   2ndFlrSF: Second floor square feet\n   LowQualFinSF: Low quality finished square feet (all floors)\n   GrLivArea: Above grade (ground) living area square feet\n   BsmtFullBath: Basement full bathrooms\n   BsmtHalfBath: Basement half bathrooms\n   FullBath: Full bathrooms above grade\n   HalfBath: Half baths above grade\n   Bedroom: Number of bedrooms above basement level\n   Kitchen: Number of kitchens\n   KitchenQual: Kitchen quality\n       Ex Excellent\n       Gd Good\n       TA Typical/Average\n       Fa Fair\n       Po Poor\n   TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n   Functional: Home functionality rating\n       Typ Typical Functionality\n       Min1 Minor Deductions 1\n       Min2 Minor Deductions 2\n       Mod Moderate Deductions\n       Maj1 Major Deductions 1\n       Maj2 Major Deductions 2\n       Sev Severely Damaged\n       Sal Salvage only\n   Fireplaces: Number of fireplaces\n   FireplaceQu: Fireplace quality\n       Ex Excellent - Exceptional Masonry Fireplace\n       Gd Good - Masonry Fireplace in main level\n       TA Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n       Fa Fair - Prefabricated Fireplace in basement\n       Po Poor - Ben Franklin Stove\n       NA No Fireplace\n   GarageType: Garage location\n       2Types More than one type of garage\n       Attchd Attached to home\n       Basment Basement Garage\n       BuiltIn Built-In (Garage part of house - typically has room above garage)\n       CarPort Car Port\n       Detchd Detached from home\n       NA No Garage\n   GarageYrBlt: Year garage was built\n   GarageFinish: Interior finish of the garage\n       Fin Finished\n       RFn Rough Finished\n       Unf Unfinished\n       NA No Garage\n   GarageCars: Size of garage in car capacity\n   GarageArea: Size of garage in square feet\n   GarageQual: Garage quality\n       Ex Excellent\n       Gd Good\n       TA Typical/Average\n       Fa Fair\n       Po Poor\n       NA No Garage\n   GarageCond: Garage condition\n       Ex Excellent\n       Gd Good\n       TA Typical/Average\n       Fa Fair\n       Po Poor\n       NA No Garage\n   PavedDrive: Paved driveway\n       Y Paved\n       P Partial Pavement\n       N Dirt/Gravel\n   WoodDeckSF: Wood deck area in square feet\n   OpenPorchSF: Open porch area in square feet\n   EnclosedPorch: Enclosed porch area in square feet\n   3SsnPorch: Three season porch area in square feet\n   ScreenPorch: Screen porch area in square feet\n   PoolArea: Pool area in square feet\n   PoolQC: Pool quality\n       Ex Excellent\n       Gd Good\n       TA Average/Typical\n       Fa Fair\n       NA No Pool\n   Fence: Fence quality\n       GdPrv Good Privacy\n       MnPrv Minimum Privacy\n       GdWo Good Wood\n       MnWw Minimum Wood/Wire\n       NA No Fence\n   MiscFeature: Miscellaneous feature not covered in other categories\n       Elev Elevator\n       Gar2 2nd Garage (if not described in garage section)\n       Othr Other\n       Shed Shed (over 100 SF)\n       TenC Tennis Court\n       NA None\n   MiscVal: $Value of miscellaneous feature\n   MoSold: Month Sold\n   YrSold: Year Sold\n   SaleType: Type of sale\n       WD Warranty Deed - Conventional\n       CWD Warranty Deed - Cash\n       VWD Warranty Deed - VA Loan\n       New Home just constructed and sold\n       COD Court Officer Deed/Estate\n       Con Contract 15% Down payment regular terms\n       ConLw Contract Low Down payment and low interest\n       ConLI Contract Low Interest\n       ConLD Contract Low Down\n       Oth Other\n\n""], 'url_profile': 'https://github.com/venkatchadalavada', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['COMP551-A1 (Applied Machine Learning)\nLogistic regression and linear discriminant analysis applied to the red wine and breast cancer datasets\nAuthors: Daniel Borisov, Jonas Lehnert, and Jeffrey Hyacinthe\n'], 'url_profile': 'https://github.com/DanielBorisov', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['time_series_hw\n'], 'url_profile': 'https://github.com/jaimealv', 'info_list': ['Updated Feb 29, 2020', 'Stata', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'MIT license', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': ['QEA_Project_1\nSmart Airbnb pricing through ratings\n'], 'url_profile': 'https://github.com/jackiezeng01', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'Roorkee', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shikhar2707', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'CHENNAI', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NLP-disaster-twitts\nReal or Fake\ncount vectorizer and TFIDF\nlogistic regression\nMultinomial naive bayes\n'], 'url_profile': 'https://github.com/DINESHMURALI', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Learning-to-Rank\nImplement query-document relevance calculation using Linear Regression for Microsoft LETOR dataset.\n'], 'url_profile': 'https://github.com/pp7788', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Keyril', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Personal-Income-Case-Study\nClassifying Personal Income Case Study using Logistic Regression and KNN- K Nearest Neighbors classifier\n'], 'url_profile': 'https://github.com/rshankarsharma9', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['GDP_life_expectancy_linear_knn_models\nWhat is the purpose of this project? What was my goal?\n\n\nCompare linear regression, and k nearest neighbor algorithms.\n\n\nLoad, and prepare data using pandas.\n\n\nWanted to learn if there is a strong correlation between money and life expectancy.\n\n\nData used\n\n2017-country-life-expectancy.csv\ngdp-per-capita.csv\nBoth datasets are from 2017, and were pulled from data.worldbank.org.\nDataset of size > 200\n\nModel, and Preparation of Data, Plotting data\n\nPandas was used to load the csv files, and also merge them based off Country name.\nNumpy was used to extract the information, and set X, and y values.\nScikit-learn has both linear regression, and knn models built in.I originally created a linear model based off GDP (X),\nand life expectancy (y) and then used both built in regressors to create predictions.\nIn order to choose the ""k"" parameter for knn regressor, I found all countries within a specific range based off GDP of the country I wanted to predict. (Example below)\nAfter both models were set, and predictions were made. I plotted the data with matplotlib\n\nResults, and Tests\n\n\nThere were couple of cases where high GDP resulted in lower life expectancy compared to other countries in the same cohort. There were also countries who had higher life expectancy than expected (based off similar GDP\'s).\n\n\nOverall there is a strong correlation with a countries wealth, and average life expectancy. For the most part of the graph, as the GDP increased the life expectancy increased.\n\n\nFor my test I used Andora\nandora_gdp = 39134.39337\nknn_range = (37500, 40000) <-- I set the range based off Andora\'s 2017 GDP\nlinear_regression_gdp_to_life(andora_gdp, knn_range)\nThe linear regression model outputed near 81 years, while the knn model outputted near 78 years.\n\n\nWhat I learned?\n\nMore data more precision\nHow important it is to extract unnecessary data, or outliers such as null values.\nI only played around with knn, and linear regression but there are more effective models such as logistic regression.\n\nWhat I would like to add in the future?\n\nLogistic Regression\nAdd ""year"" as a feature and see how that plays a factor in the prediciton.\n\n'], 'url_profile': 'https://github.com/sammyjse', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['margZIfit\nFit marginal ZI regression models for clustered data with excess zeros\n'], 'url_profile': 'https://github.com/tbenesi', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/emreozb', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '145 contributions\n        in the last year', 'description': ['ML-Emails\nrepository for nlp and regression on our end of year emails\n'], 'url_profile': 'https://github.com/davearch', 'info_list': ['Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 28, 2020']}"
"{'location': 'London', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['Bachelor Project\nImplementation of SVM for regression in python using libraries as sklearn, scipy, pandas...\nWhat I Learned\n\nTheoretical SVM bases\nHow to read, process, and preprocess data files as CSV.\nDetect, and remove outliers.\nFilter and smooth data with Savitzky-Golay filter.\nVisualize data\nWork with models, train and test them.\nUse different metrics to compare the models (MSE, RMSE, MAE, R2...)\nOptimize hyperparameters with some libraries as Optunity, GridSearchCV, ...\nCreate ensemble from different models with optimized hyperparameters\n\n'], 'url_profile': 'https://github.com/nowrie141', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/x34903', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '477 contributions\n        in the last year', 'description': ['ML-python-sklearn-lrc-cpu-training\nQuickstart project for executing a classifier on the Wine dataset using the SciKit-Learn framework on a CPU.\nThis Quickstart trains the model and persists as in ONNX format. The service runtime will then serve the model on localhost where the user can then send GET requests to perform inference.\nIn accordance with MLOps principles, running requirements.txt then python app.py will train a model and, if threshold metrics are passed, will convert the model to .onnx format, saving it as .model.onnx.\nAdditionally, metrics will be saved to a .metrics/ folder.\nUpon successful training, a Pull Request will automatically be made on the corresponding service project with the model and metrics folder being copied across.\nJenkins X requires the metrics and model to be saved in this format and the defined locations in order to promote the model to the service stage.\n'], 'url_profile': 'https://github.com/jamesthatcher', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/satvik1998', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ZYFFF166', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'Bellevue, WA', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/whitecw', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'Pune, Maharashtra, India', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Board_Game_Review_Prediction\nBoard Game review Prediction using Linear regression and Random Forest models\n'], 'url_profile': 'https://github.com/AnuragGorkar', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'Exeter, Devon, United Kingdom', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['SeniorProjectRVA\nMy Senior Project for BYU-Idaho Data Science Degree - Multiple Linear Regression to Optimize RVA\n'], 'url_profile': 'https://github.com/HanselPalencia', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'Kuala Lumpur', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['machine-learning\nMachine learning algorithms processing with titanic dataset from Kaggle. Logistic Regression,SVM,RF,MLP,Boosting\nAuthor\nAjithlal K\n'], 'url_profile': 'https://github.com/import-ajith', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}","{'location': 'Wichita, KS', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Excel-Linest-Forecast-function\nIf you use linest for linear regression, this function will save time creating the model forecast\nDescription:\nThis Excel Visual Basic function is designed for use with regressions using the LINEST function.  The function allows the user to select\nranges for the regression constants and the model variables (features).\nUsage:\nThe formula is used ""=LmFcast(cell range for constants, cell range for variables)"".  In a cell enter ""=LmFcast("" then Control A ... this pops up a dialog box to help you select the variables and constants correctly.  Make sure to fix the constant cell range using ""$"".  The attached worksheet contains the code and sample to demonstrate usage and a math check.\nFunction:\nThe function code use the first element of the constants as the intercept then adds each pair (in opposite directions) until the remainder of the constants and variables are multiplied and added.\nFunction LmFcast(rConstants As Range, rVariables As Range)\n  Dim ArrV() As Variant: ArrV = Application.WorksheetFunction.Transpose(rVariables)\n  Dim ArrC() As Variant: ArrC = Application.WorksheetFunction.Transpose(rConstants)\n  Dim C As Long\n  Dim V As Long: V = 1\n  LmFcast = ArrC(UBound(ArrC, 1), 1)\n  For C = UBound(ArrC, 1) - 1 To 1 Step -1  \'First array dimension is rows.\n      LmFcast = LmFcast + ArrC(C, 1) * ArrV(V, 1)\n      V = V + 1\n  Next C\nEnd Function\n\n'], 'url_profile': 'https://github.com/koenig741', 'info_list': ['Python', 'Updated May 12, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Apache-2.0 license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Apr 29, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Updated Jul 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': [""Insurance Prediction\nBasic java program to predict insurance price based on few a parameters.\nDescription\nThis project serves as an introduction to\nWeka's machine learning tools.\nIt accepts a csv file containing data as input, generates a linear\nregression model, asks a user a handful of questions, and then outputs\nthe predicted insurance price.\nInputs\n\nAge\nSex\nBMI\nNumber of children\nSmoker(T/F)\nLocation\n\nCourse\nThis project was created for Artificial Intelligence at\nWestmont College\nin the Spring of 2020.\n""], 'url_profile': 'https://github.com/dpoleselli', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['This project was made in collaboration with @lianamehrabyan\nHouse Prices Prediction\nOverview\nGiven a dataset from the Kaggle competition ”House Prices: Advanced Regression Techniques” the goal of this project is to apply different algorithms and optimization techniques to predict House Prices based on the attributes.\n'], 'url_profile': 'https://github.com/ElsaScola', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nkumtakar', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Cincinnati', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Multiple linear regression on the Boston Housing dataset\nCode in this repository is used for evaluating different regression techniques on the Boston Housing dataset\n'], 'url_profile': 'https://github.com/sidharthg', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['LLRE\nR code for A Lasso-Logistic Regression Ensemble(LLRE) Algorithm in\n""Large Unbalanced Credit Scoring Using Lasso-logistic Regression Ensemble""\nPlease cite this paper if your think this code in useful.\nWang, Hong, Qingsong Xu, and Lifeng Zhou. ""Large unbalanced credit scoring using lasso-logistic regression ensemble."" PloS one 10, no. 2 (2015).\n'], 'url_profile': 'https://github.com/whcsu', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Predicting Home Prices in Ames, Iowa\nsubmitted: January 17th, 2020\nExecutive Summary\nZillow is an online housing price estimation and reporting platform used across America. They frequently provide ""Estimated home prices"" to houses for sale, and not for sale, to help users follow housing market rates. Naturally, more accurate estimates of home values are central to increasing user confidence in Zillow\'s product.\nThis report seeks to develop and refine a home sale price prediction tool that can help Zillow improve their current predicition tools. A Kaggle competition dataset containing various features from homes sold in Ames, Iowa was used to create four different prediction models. Linear Regression was the only machine learning method allowed for this competition and the OLS, ridge, and lasso methods were the only methods used. Root mean squared error in sale price prediction was used to score test predictions.\nReport Structure:\n\nData cleaning were done in the data_cleaning.ipynb files\nModules folder contains the function used to run and compare key metrics for various models\nSubmissions folder contains all predictions submitted to kaggle\nRemaining submission.ipynb files contain individual model iterations\n\n\nPrimary findings\nThe data consists of 2,051 homes in the training set and 878 in the test set along with 80 unique features recorded for each home. The average sale price was $181,500 and minimum and maximum prices of $12,800 and $612,000 respectively. The two columns with the most null values were Lot Frontage and Garage Year Built, and the nulls were replaced with their average values. Moreover, 21 home traits with ordinal rankings were mapped to numeric values while 34 categorical home traits were converted to dummy variables.\nThe modeling and evaluation workflow was carried out by iteratively selecting features, applying one of the three regression models allowed, and either adding or removing features depending on whether key metrics indicated high bias or high variance.\nThe first model iteration used OLS regression and only the top 13 variables most highly correlated with sale price to make predictions. This model had high bias and scored an rmse of $36,968 on the kaggle test. This rmse was used as a baseline for comparison going forward.\nThen, in order to get a sense of the upper limit of overfitting the model, the second iteration used an OLS regression on all 216 home features. The kaggle score improved to an rmse of $29,788 and was extremely overfit. Next, the third iteration attempted to minimize this variance by applying a ridge regularized regression to the same 216 variables. This model ended up doing worse than the second with a kaggle score rmse of $36,750.\nFinally, the fourth iteration applied the same Ridge model to a select group of 120 polynomial features. This produced the best kaggle score of $27,579.\n\nConclusions and Next Steps\nThis report set out to predict home prices from a very detailed and complete home feature dataset. Despite there being 80 features, a reasonable best rmse score of $27,579 was achieved by using the polynomial features that were derived from just 12 out of the 80 features. The Ames data offers more data than is necessary to predict sale price. The 12 features used in the fourth iteration consisted of key traits like overall quality, total square footage and year built. The simplicity of this model will likely translate to other towns and housing data sets. In an effort to improve the Zillow Offers program, more research could be done on identifying the bare minimum traits of a home that a home owner could plug into their home in order to get a quick price estimate and draw them in for further evaluation. Other next steps for improving the model would be to iteratively add more relevant categorical features such as neighborhood and house style in order to improve model accuracy.\n'], 'url_profile': 'https://github.com/robertcdavison', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['IMDB dataset and the sentiment classification task\nThe large movie review dataset contains a collection of 50,000 reviews from IMDB. The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\nThe kaggle dataset link is here - https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\nThe sentiment classification task consists of predicting the polarity (positive or negative) of a given text.\nOur next model is a version of logistic regression with Naive Bayes features.\nFor every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment.\nThe approach is insired by the paper Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning.\nThe model created here achieves an accuracy over 92% compared to the 91.22% achieved in the paper.\n'], 'url_profile': 'https://github.com/VishakBharadwaj94', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '477 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jamesthatcher', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Hayward, CA', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnishRaviraj', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'israel', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ranbm', 'info_list': ['Java', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 25, 2020', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'London - UK', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Requirements\nRun the following command to install the Python 3 libraries needed to run the project:\npip install -r requirements.txt\nAlso, run the following commands so the Spacy Vocabulary\nwith the GloVe and Bert Embedding can be loaded:\npython -m spacy download en_core_web_md\npython -m spacy link en_core_web_md en300\npython -m spacy download de_core_news_md\npython -m spacy link de_core_news_md de300\npython -m spacy download de_trf_bertbasecased_lg\npython -m spacy download en_trf_xlnetbasecased_lg\nLastly, be sure to have an en-de/ directory in the same level as the src/ directory,\nwith the training set, validation set and test set.\nExecuting the files\nThe files can be excuted by running, for instance, the following:\npython BertMLP.py\nMake sure to be inside the src/ directory when executing the previous command.\nImplementations\nIndicesRepresentation.py\nThis files implements the indices representations of each sentence.\nPre-processing parameters can be changed easily in the function call tokenize_corpuses.\nIt allows us to try different pre-processing techniques without having to write new code.\nBagOfWords.py\nThis file implements the bag of words idea, the input given to the model is a vector of the size of the vocabulary build\nfrom each training files concatenated.\nMultiple techniques to build the vocabulary can be used with this class. This\nis also the case for training. Here is a list of the possible options that it provides:\n\n\nVocabulary construction possibilities (parameter n_gram):\n\nunigram: The vocabulary built takes only each word into account\nbigram: The vocabulary built takes into account each words and bi-gram present in the training set.\ntrigram: Takes into account, uni-gram, bi-gram and tri-gram when building the vocabulary.\n\n\n\nTraining methods (Depending on the methods called on the instance)\n\nSVR\nMulti Layer Perceptron\n\n\n\nExemples are provided in BagOfWords.py at the end of the files in comments.\nGloveCNN.py\nIn this file, we try to do regression using embedding on each word and different channels for each language.\nTherefore, we use CNN with input of size: (batch_size, nb_channels, max_length, embedding_size)\n\nbatch_size: Size of each batch\nnb_channels: Number of languages (in our case 2) * number of embedding representation\n(For example if we use GloVe and FastText the number of channels will be 4)\nmax_length: The length of the longest sentence of both corpus\nembedding_size_: The size of the embedding representation (300 for Glove)\n\nGloveMLP.py\nIn this file, we try to do regression using embeddings averaged for each word in a sentence.\nThe vectors of each sentence are concatenated into one vector.\nTherefore, we use MLP with input of size: (batch_size, 2*embedding_size)\n\nbatch_size: Size of each batch\nembedding_size_: The size of the embedding representation (300 for Glove)\n\nGloveArc_I.py\nIn this file, we try to do regression using embedding on each word for each language. However the two\nlanguages will be indepedent inputs to the network.\nTherefore, we use an 1d-CNN with input of size: (batch_size, 2*max_length, embedding_size)\n\nbatch_size: Size of each batch\nmax_length: The length of the longest sentence of both corpus. The input here is 2*max_length\nsince the matrices of the two sentences are getting stacked on this dimension. However\nthey are decoupled before forwarding and each matrix is forwared into a differen 1-dimensional\nCNN.\nembedding_size_: The size of the embedding representation (300 for Glove)\n\nBertMLP.py\nIn this file, we try to do regression using embeddings averaged for each word in a sentence.\nThe vectors of each sentence are concatenated into one vector.\nTherefore, we use MLP with input of size: (batch_size, 2*embedding_size)\n\nbatch_size: Size of each batch\nembedding_size_: The size of the embedding representation (768 for Bert)\n\n'], 'url_profile': 'https://github.com/PierreElm', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/Intelligent-Systems-Phystech', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Odrinary-Least-Squares---Multi-Linear-Regression-ML-Implementatio\nMy implementation of the ordinary least squares multi linear regression algorithm. Written in python.\n'], 'url_profile': 'https://github.com/johngriffin21', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '477 contributions\n        in the last year', 'description': ['ML-python-sklearn-lrc-cpu-service\nQuickstart project for executing a wine classifier using the SciKit-Learn framework on a CPU.\nRunning pip install requirements.txt and then python app.py will start the app on localhost where the user can send GET requests to perform inference.\neg .../wine/{index} where index is in the range [0:178]\n'], 'url_profile': 'https://github.com/jamesthatcher', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': [""Linear Regression - Boston House Pricing\nUsing SciKit-Learn and Python to create a linear regression model for Boston House Pricing.\nBoston house prices dataset\nData Set Characteristics:\n:Number of Instances: 506 \n\n:Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n:Attribute Information (in order):\n    - CRIM     per capita crime rate by town\n    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n    - INDUS    proportion of non-retail business acres per town\n    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n    - NOX      nitric oxides concentration (parts per 10 million)\n    - RM       average number of rooms per dwelling\n    - AGE      proportion of owner-occupied units built prior to 1940\n    - DIS      weighted distances to five Boston employment centres\n    - RAD      index of accessibility to radial highways\n    - TAX      full-value property-tax rate per $10,000\n    - PTRATIO  pupil-teacher ratio by town\n    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n    - LSTAT    % lower status of the population\n    - MEDV     Median value of owner-occupied homes in $1000's\n\n:Missing Attribute Values: None\n\n:Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.\n.. topic:: References\n\nBelsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\nQuinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n\n""], 'url_profile': 'https://github.com/emreozb', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""Data-Science-Research\nData cleaning and Explorotary data analysis followed by predicitons using regression\n\nPlease downlaod and use the '.ipynb' file, alll the investigations are easy to follow with top bottom aproach\nyou can downlaod the data from this link: http://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring\n""], 'url_profile': 'https://github.com/HovHak', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'New York, USA', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': [""R-Regression-Discontinuity-2019\nCausal Analysis in Regression Discontinuity Design (R, 2019)\nThis R file contains a replication project of the following paper:\nSanz, C. (n.d.). Direct democracy and government size: Evidence from Spain. Political Science Research and Methods, 1-16. doi:10.1017/psrm.2018.65\nThe dataset can be found here:\nhttps://www.cambridge.org/core/journals/political-science-research-and-methods/article/direct-democracy-and-government-size-evidence-from-spain/E3D106AD332D40789DECC98090EFBA90#fndtn-supplementary-materials\nAbstract:\nSanz (2018) studies the effect of direct democracy on economic policy, namely: revenues, expenditures and deficits. In this study, he used a fixed-effect regression discontinuity design (RDD), controlling for municipality and time fixed effects. According to his models, direct democracy leads to a smaller government, reduces public spending by ~8%, reduces revenues by ~6% but had no statistically significant effect on budget deficits.\nIn this report, I aim to replicate the results as laid out in the paper and aim to further the discussion of the results in the paper with modifications to the model. In this paper, Chapter 3 replicates Table 1 and Figure 2 in Sanz's paper. Chapters 4 to 6 aim to replicate the results for Table 2 and Figure 3. In Chapter 7, I investigate the covariate balance within the optimal bandwidths and compare hypothetical results should conditioning on these covariates be necessary. Thereafter, in Chapter 8, I estimate the LATE of revenues and expenditures without logarithms and found no statistically significant effect of Direct Democracy. Last, in Chapter 9, I used the World Bank's GDP deflators for Spain and estimated the sensitivity of LATE to higher and lower GDP deflators.\n""], 'url_profile': 'https://github.com/danielboeyks', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['\n\n\ntitle\nauthor\ndate\noutput\n\n\n\n\nModeling and Regression Analysis\nAnisha Shrestha\nDecember 06\n\n\n\nhtml_document\n\n\n\n\n\n\n\nkeep_md\n\n\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\n\nModeling and Regression Analysis\nUsed Grand Valley State University’s Allendale student on campus datasets to understanding pattern and exploratory visualizations for predictor variable, usages of indicator variables for categorical values, implementing the best subsets approach to model selection based on the different criteria (adjusted R square, significance gain, p-value) using R.\nLoading all the necessary libraries. options(warn = -1) will not show warning in the output. SuppressMessages function will suppress the messages for loading libraries.\noptions(warn = -1)\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(broom))\nsuppressMessages(library(modelr))\nsuppressMessages(library(rje))\nsuppressMessages(library(ggfortify)) #helps to show all the residual vs fitted plot, normality plot\nLoading the datasets of allendale student\nallendale_students <- read_csv(""data/allendale-students.csv"")\n## Parsed with column specification:\n## cols(\n##   distance = col_double(),\n##   scholarship = col_double(),\n##   parents = col_double(),\n##   car = col_double(),\n##   housing = col_character(),\n##   major = col_character(),\n##   debt = col_double()\n## )\n\nUsing glimpse function to inspect most of the column name in the datasets.\nglimpse(allendale_students)\n## Observations: 200\n## Variables: 7\n## $ distance    <dbl> 40, 30, 130, 120, 30, 0, 30, 50, 10, 10, 40, 140, 40…\n## $ scholarship <dbl> 1532, 7479, 2664, 1998, 1462, 3053, 1301, 1948, 2295…\n## $ parents     <dbl> 0.440, 0.265, 0.115, 0.325, 0.105, 0.335, 0.375, 0.1…\n## $ car         <dbl> 6, 7, 3, 9, 10, 9, 5, 6, 8, 3, 8, 8, 10, 8, 6, 4, 7,…\n## $ housing     <chr> ""off campus"", ""on campus"", ""on campus"", ""on campus"",…\n## $ major       <chr> ""STEM"", ""STEM"", ""business"", ""business"", ""other"", ""ST…\n## $ debt        <dbl> 26389, 21268, 32312, 28539, 34867, 18193, 29990, 343…\n\nExploratory Analysis:\nDebt is our response variable. The predictor (or independent) variable for our linear regression with response variable in the datasets is plotted using geom_smooth. This helps to visualize the pattern of the datasets.\n\nScatter plot like Debt and Distance, Parents and Debt have shightly different range. However, there are no outrageous variations in the dataset which makes it difficult to understand their pattern. So I opt for not using any kind of transformation. Although, we chould use log get the similar range for x and y axis. In this datasets, the pattern is very clear to understand.\n\nggplot(allendale_students, aes(distance, debt)) +\n  geom_point() + geom_smooth()+ ggtitle(""Fig 1: Debt and Distance"")+ xlab(""Distance"") + ylab(""Debt"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n\nAs the distance increases the debt also increases. This shows a positive linear regression line.\n\nggplot(allendale_students, aes(scholarship, debt)) +\n  geom_point() + geom_smooth()+ ggtitle("" Fig 2: Debt and scholarship"")+ xlab(""scholarship"") + ylab(""Debt"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n\nThe graph above is Debt and Scholarship. This is obivious that, if a student has a scholarhsips, the debt will going down which is shown in the graph. When the scholarship amount increases from 1000 to 2000 the point shows lower in debt. But there is one value on the right most end, which might be an outlier as this shows if we have scholarship above 2000, there is stil people who is going to have some debt remaining which could be as there are other determining factors.\n\nggplot(allendale_students, aes(parents, debt)) +\n  geom_point() + geom_smooth()+ ggtitle(""Fig 3: Debt and parents"")+ xlab(""parents"") + ylab(""Debt"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n\nThe graph above is Parents and Debt. Here, there is much variation in the data points. The graph says, if parents is paying the half cost, then the debt is goining to decrease.\n\nggplot(allendale_students, aes(car, debt)) +\n  geom_point() + geom_smooth()+ ggtitle(""Fig 4: Debt and car"")+ xlab(""Car"") + ylab(""Debt"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n\nThe above graphs shows the relationship between Debt and Car\'s age. The line here is in horizontal axis, meaning there is not much difference and the number of year increases the debt is still similar.\n\nggplot(allendale_students, aes(housing, debt)) +\n  geom_point() + geom_smooth()+ ggtitle("" Fig 5: Debt and Housing"")+ xlab(""Housing"") + ylab(""Debt"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n\nThe above graphs shows Debt and Housing relationship. Because the housing is categorical variables here, we can see two different line instead of scattered plots in the graphs. In the off campus and on campus, the people who live on and off campus have similar debt values. We noticed on campus people have more debt to pay with minimum around 1000 in the start.\n\nggplot(allendale_students, aes(major, debt)) +\n  geom_point() + geom_smooth()+ ggtitle("" Fig 6: Debt and major"")+ xlab(""Major"") + ylab(""Debt"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n\nThe above graphs shows Debt and Major relationship. Here, Major is categorical variables, thus this shows three lines. In the graph, the major under STEM, has higher debt ratio compare to rest two and the lowest is major other.\n\nCreating an Indicator variable for categorical values\n\n\nUsing dummy variable as indicating the categorical values for housing and major. I have used 1 for values occured in the variable 0 for other.  A dummy column is one which has a value of one when a categorical event occurs and a zero when it doesn’t occur.\n\n\nFor housing (off campus and on campus) 1 as the dummy variable where present and 0 for on campus and vice versa. Similar is the case for major variables as 1 if there is Steam present in a values for major column. I also separated it into different columns indicating there presence in each column by 1. Lastly, I choose only the necessary columns as we will be playing with these variables in the analysis from here with separate name as allen_data.\n\n\nallendale_students %>%\n  mutate(off_campus= allendale_students$housing<- ifelse(allendale_students$housing ==""off campus"", 1, 0)) %>%\n  mutate(on_campus= allendale_students$housing<- ifelse(allendale_students$housing ==""on campus"", 1, 0)) %>%\nmutate(major_stem= allendale_students$major<- ifelse(allendale_students$major ==""STEM"", 1, 0)) %>%\nmutate(major_business= allendale_students$major<- ifelse(allendale_students$major ==""business"", 1, 0))%>%\n  mutate(major_other= allendale_students$major<- ifelse(allendale_students$major ==""other"", 1, 0)) -> allendale_students\nhead(allendale_students)\n## # A tibble: 6 x 12\n##   distance scholarship parents   car housing major  debt off_campus\n##      <dbl>       <dbl>   <dbl> <dbl> <chr>   <chr> <dbl>      <dbl>\n## 1       40        1532   0.44      6 off ca… STEM  26389          1\n## 2       30        7479   0.265     7 on cam… STEM  21268          0\n## 3      130        2664   0.115     3 on cam… busi… 32312          0\n## 4      120        1998   0.325     9 on cam… busi… 28539          0\n## 5       30        1462   0.105    10 off ca… other 34867          1\n## 6        0        3053   0.335     9 off ca… STEM  18193          1\n## # … with 4 more variables: on_campus <dbl>, major_stem <dbl>,\n## #   major_business <dbl>, major_other <dbl>\n\nkeeps <- c(""distance"", ""scholarship"", ""parents"", ""car"", ""major_stem"", ""major_business"",""major_other"", ""on_campus"",""off_campus"", ""debt"")\nallen_data<-allendale_students[keeps]\ndplyr::tbl_df(allen_data)\n## # A tibble: 200 x 10\n##    distance scholarship parents   car major_stem major_business major_other\n##       <dbl>       <dbl>   <dbl> <dbl>      <dbl>          <dbl>       <dbl>\n##  1       40        1532   0.44      6          1              0           0\n##  2       30        7479   0.265     7          1              0           0\n##  3      130        2664   0.115     3          0              1           0\n##  4      120        1998   0.325     9          0              1           0\n##  5       30        1462   0.105    10          0              0           1\n##  6        0        3053   0.335     9          1              0           0\n##  7       30        1301   0.375     5          0              1           0\n##  8       50        1948   0.185     6          0              1           0\n##  9       10        2295   0.225     8          1              0           0\n## 10       10        4653   0.185     3          1              0           0\n## # … with 190 more rows, and 3 more variables: on_campus <dbl>,\n## #   off_campus <dbl>, debt <dbl>\n\nPowerSet\nUsing Powersets, which will give all possible submodels for all 9 variables. I dont want response variable for creating a formula of submodel so I minus debt and selected all the column names of the allen_data. These gives me all the subsets for our predictor variables. The total number was 512 number of rows and I deleted the character whose value is 0. This means there is no any predictor variables in the analysis making it total to 511 number of rows.\nallen_data %>% \n  select(-debt) %>% \n  names %>% \n  powerSet()->predictors\n\npredictors[lapply(predictors,length)>0]->predictors\nhead(predictors)\n## [[1]]\n## [1] ""distance""\n## \n## [[2]]\n## [1] ""scholarship""\n## \n## [[3]]\n## [1] ""distance""    ""scholarship""\n## \n## [[4]]\n## [1] ""parents""\n## \n## [[5]]\n## [1] ""distance"" ""parents"" \n## \n## [[6]]\n## [1] ""scholarship"" ""parents""\n\nWriting a Function\n\n\nOptions(scipen=4) function as global function will convert the values in the dataframe into scientific notation after 4 decimal point. The first steps is to create the formula from the string.\n\n\nThe argument here is x, and predictors which is all the submodels from the above functions. I collapse them using + and stored it into pdat. I created a formula where I used glance function to get all the values in a table. Glance function will give all the intercept values and tidy will give the residual values in the model.\n\n\nThus, I used spread to get all the intercept values for the explanatory variables. For the tidy function I wanted r square value, adjusted r square, dataframe, formula, so I combined these column using cbind.\n\n\nIn the function, debt being the response variable and used the formula by pasting debt~ for all the subsets. I used lapply and  rbind_list function to combine all the separate dataframe into one. The new dataframe we have shows all the subsets values along with the formula used, response, intercept, slope for all the variables along with adjusted R square and residual values. The total number of rows is equivalent to 2^n-1 (511) in this case.\n\n\noptions(scipen=4)\n\nlapply(predictors, function(x)\n  {\n  paste(x, collapse = \'+\')-> pdat\n  lm(as.formula(paste(\'debt ~\',pdat)), data=allen_data) %>%\n  glance() %>% \n  mutate(""response_variable"" = \'debt\', formula= (paste(\'debt~\', pdat)))-> model_glance\n  paste(x, collapse = \'+\')-> pdat\n  lm(as.formula(paste(\'debt ~\',pdat)), data=allen_data) %>% tidy() %>% spread(term, estimate) %>%\n  mutate(""response_variable"" = \'debt\', formula= (paste(\'debt~\', pdat))) %>% cbind(model_glance[,c(""r.squared"",""adj.r.squared"",""df"",""formula"")])\n}) %>% rbind_list()  %>% arrange(formula) -> model_intercept\n  coalesce_by_column <- function(model_intercept) {\n  return(coalesce(model_intercept[1], model_intercept[2]))}\n  model_intercept %>%\n  group_by(formula) %>%\n  summarise_all(coalesce_by_column)%>%\n  tibble::rowid_to_column( ""Model Number"")-> final_model\nhead(final_model)\n## # A tibble: 6 x 19\n##   `Model Number` formula std.error statistic p.value `(Intercept)` distance\n##            <int> <chr>       <dbl>     <dbl>   <dbl>         <dbl>    <dbl>\n## 1              1 debt~ …      298.   -0.0384   0.969        29554.       NA\n## 2              2 debt~ …      299.   -0.233    0.816           NA        NA\n## 3              3 debt~ …      300.   -0.223    0.824           NA        NA\n## 4              4 debt~ …      299.   -0.211    0.833           NA        NA\n## 5              5 debt~ …      299.   -0.211    0.833           NA        NA\n## 6              6 debt~ …      299.   -0.211    0.833           NA        NA\n## # … with 12 more variables: response_variable <chr>, r.squared <dbl>,\n## #   adj.r.squared <dbl>, df <int>, scholarship <dbl>, parents <dbl>,\n## #   car <dbl>, major_stem <dbl>, major_business <dbl>, major_other <dbl>,\n## #   on_campus <dbl>, off_campus <dbl>\n\nChoosing the best model\n\n\nIn order to check the model, I used significant gain in the model meaning I compare change in the r square value as delta and got the change into separated column called Diff. I sorted based on highest R square value and among them, there were several factors that was considered like nuber of data frame, P values, adjusted R square value and R Square values itself. Under those, the combination of those made up the most significant adjusted R square value, i.e 0.7427 for Model Number 225 with 3 explanatory variables.\n\n\nDelta means the change or bump after adding each to existing values. There is less overlapping meaning less colinearity. Thus, It is very clear that this is our best model.\n\n\nThus, the best model with the highest Adjusted R square values is 225, there are 3 predictor variables. The Adjusted R square value is : 0.7427 and R square value is 0.7466.\n\n\nThe model equation is debt ~ distance -scholarship + parents.\n\n\nsuppressMessages(library(data.table))\nDT <- data.table(final_model)\nDT[ ,list(`Model Number`,df, p.value, r.squared, adj.r.squared, Diff=diff(r.squared))  ] %>%\n  arrange(desc(Diff)) -> model_check\nhead(model_check)\n##   Model Number df      p.value   r.squared adj.r.squared      Diff\n## 1          383  3 1.517970e-01 0.149888137  0.1412575597 0.2635139\n## 2          160  4 2.511871e-14 0.368382552  0.3587149383 0.2465671\n## 3           32  3 9.827729e-01 0.009773765 -0.0002792932 0.2383260\n## 4          447  3 9.636718e-25 0.421025911  0.4151480019 0.1453590\n## 5          319  2 2.080280e-91 0.009771416  0.0047702610 0.1311836\n## 6          224  4 2.450381e-30 0.615989338  0.6101116237 0.1306189\n\n final_model %>%\n    arrange(desc(adj.r.squared)) %>%\n    slice(49)-> bestmodel\nbestmodel\n## # A tibble: 1 x 19\n##   `Model Number` formula std.error statistic  p.value `(Intercept)`\n##            <int> <chr>       <dbl>     <dbl>    <dbl>         <dbl>\n## 1            225 debt~ …    0.0901     -17.1 8.04e-41            NA\n## # … with 13 more variables: distance <dbl>, response_variable <chr>,\n## #   r.squared <dbl>, adj.r.squared <dbl>, df <int>, scholarship <dbl>,\n## #   parents <dbl>, car <dbl>, major_stem <dbl>, major_business <dbl>,\n## #   major_other <dbl>, on_campus <dbl>, off_campus <dbl>\n\nOnly getting the formula for the best model.\nas.vector(bestmodel$formula)\n## [1] ""debt~ distance+scholarship+parents""\n\nA graph with the r-square ($R^2$) value on the vertical axis vs. the number of explanatory variables in the model on the horizontal axis. Here, in total, there are 9 explanatory variables used in the model.\nggplot(final_model, aes(x= df, y= r.squared, fill= r.squared, label=final_model$`Model Number`))+ geom_line( alpha= 0.1, color=""red"")+ geom_point() + geom_text(aes(label=ifelse(final_model$`Model Number` == ""225"", as.character(final_model$`Model Number`),\'\')),hjust=-0.1,vjust=-0.8)+theme_minimal()+ ggtitle(""R Square plot : Model and Explanatory Variables"")\n\nResidual Plot For the best model.\nResiduals are essentially the difference between the actual observed response values (debt to 3 predictor variables in our case) and the response values that the model predicted.\nfit <- lm(debt ~ distance+scholarship+parents, data = allen_data) \nggplot(fit, aes(x = .fitted, y = .resid)) + geom_point()+geom_smooth()+ggtitle(""Residuals Vs Fitted Plot"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\nFrom the above residual plot, for the best model, all the points falls around the horizontal axis straight towards x axis. We can see one outlier point here, points far left top rest everything line up on the horizontal line. It shows homoscedasticity pattern (homogenity of variances) where residual and fitted for this model are uncorelated, but closer look shows slightly heteroscedasticity as variance are more towards lower section of the horizontal line.\nInteraction Term\nIf debt is out response variables, and we compare their relation and interation term then,\nFocusing on the interation term, between scholarship, housing and parents whereas there are no any interaction between (scholarship and parents) and (off_campus and parents). The slope of the interaction for scholarship:off_Campus is 0.513 which is positive and p value = 0.049, which is slightly less than 0.05 . In order to explore more, lets visualize them,\nallen_data$on_campus<- as.factor(allen_data$on_campus)\nallen_data$off_campus<- as.factor(allen_data$off_campus)\n\nmodel3 <- lm(debt~(scholarship + on_campus + off_campus+ parents)^2, data = allen_data)\ntidy(model3)\n## # A tibble: 7 x 5\n##   term                     estimate std.error statistic  p.value\n##   <chr>                       <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)             41926.     1588.       26.4   5.66e-66\n## 2 scholarship                -1.72      0.305    -5.64  6.07e- 8\n## 3 on_campus1               4047.     2035.        1.99  4.82e- 2\n## 4 parents                -27580.     4915.       -5.61  6.91e- 8\n## 5 scholarship:on_campus1     -0.513     0.259    -1.98  4.93e- 2\n## 6 scholarship:parents         0.964     0.864     1.12  2.66e- 1\n## 7 on_campus1:parents      -2138.     5850.       -0.365 7.15e- 1\n\nglance(model3)\n## # A tibble: 1 x 11\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <int>  <dbl> <dbl> <dbl>\n## 1     0.589         0.576 5522.      46.1 8.68e-35     7 -2004. 4023. 4049.\n## # … with 2 more variables: deviance <dbl>, df.residual <int>\n\naugment(model3)\n## # A tibble: 200 x 12\n##     debt scholarship on_campus off_campus parents .fitted .se.fit  .resid\n##    <dbl>       <dbl> <fct>     <fct>        <dbl>   <dbl>   <dbl>   <dbl>\n##  1 26389        1532 0         1            0.44   27809.    920.  -1420.\n##  2 21268        7479 1         0            0.265  23325.    976.  -2057.\n##  3 32312        2664 1         0            0.115  36909.   1000.  -4597.\n##  4 28539        1998 1         0            0.325  32484.    768.  -3945.\n##  5 34867        1462 0         1            0.105  36666.    939.  -1799.\n##  6 18193        3053 0         1            0.335  28428.    576. -10235.\n##  7 29990        1301 0         1            0.375  29819.    761.    171.\n##  8 34333        1948 0         1            0.185  33825.    676.    508.\n##  9 27717        2295 1         0            0.225  34665.    723.  -6948.\n## 10 21398        4653 0         1            0.185  29660.    604.  -8262.\n## # … with 190 more rows, and 4 more variables: .hat <dbl>, .sigma <dbl>,\n## #   .cooksd <dbl>, .std.resid <dbl>\n\n\n\nVisualizing all two interaction term below.\n\n\nThe first two graph shows the interaction between housing and scholarship. There is no any different either for on campus or off campus housing. The first graph shows the interaction between scholarship and housing students.\n\n\nThe curve geom_smooth produces is indeed an estimate of the conditional mean function, i.e. it\'s an estimate of the mean scholarships conditional on the factor on campus and off campus.(it\'s a particular kind of estimator called LOESS). The number we calculate, in contrast, is an estimate for the unconditional mean, i.e. the mean over all the data. The lines shows there is an interaction term so does the summary table from the linear model.\n\n\nThe graphs below shows housing and scholarship have interaction term as the line intersects each other. The significance is also shown by the P value, if its higher or lower to 0.5.\n\n\nFor second graph for parents and Scholarship, Its hard to actually find out any pattern because both of them are continous variable. Thus, when I create then into factor variables, we can see some interaction between scholarship and parents in the summary table and in the graph as well.\n\n\nggplot(data=model3, aes(x=scholarship,y=debt, color=on_campus)) + geom_point()+ geom_smooth()+labs(x = ""scholarships"",  y = ""debt"")+ggtitle(""Interaction Plot for Scholarship and On campus"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\nggplot(data=model3, aes(x=scholarship,y=debt, color=off_campus)) + geom_point()+ geom_smooth()+labs(x = ""scholarships"",  y = ""debt"")+ggtitle(""Interaction Plot for Scholarship and Off Campus"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n#For parents and scholarship\nggplot(data=model3, aes(x=scholarship,y=debt, color=parents)) + geom_point()+ geom_smooth()+labs(x = ""scholarships"",  y = ""debt"")+ggtitle(""Interaction Plot for Scholarship and Parents"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n\n\nThe interaction for parents was not clear as it was continous variables, Thus, I choose to make it into factor and divide the interval into 3 formats. From negative value to 0.2 as one, 0.2 to 0.4 as 2 and 0.4 to above infinity as 3. These values were also named as lower than 0.2, 0.2 to 0.4 and higher than 0.4 respectivily.\n\n\nWe run the summary model for this, We can see that, scholarship and parents group higher than 0.4 has interaction term. When We plot this below, We can see there is an interaction term between 0.2 to 0.4 and lower than 0.2. But this varies depending upon the interval we assign for the parents while making them as a factor variables. I am also using geom_smooth inorder to fit the mean value in the linear regression.\n\n\nallen_data %>%\nmutate(p_group=cut(parents, breaks=c(-Inf, 0.3, 0.4, Inf), labels=c(""lower than 0.2"",""0.2 to 0.4"",""higher than 0.4"")))-> new_data\n  model4 <- lm(debt~(scholarship + on_campus + off_campus+ p_group)^2, data = new_data)\ntidy(model4)\n## # A tibble: 10 x 5\n##    term                               estimate std.error statistic  p.value\n##    <chr>                                 <dbl>     <dbl>     <dbl>    <dbl>\n##  1 (Intercept)                         3.81e+4  1076.       35.4   1.15e-85\n##  2 scholarship                        -1.81e+0     0.226    -8.00  1.22e-13\n##  3 on_campus1                          2.15e+3  1519.        1.41  1.59e- 1\n##  4 p_group0.2 to 0.4                  -6.90e+3  1798.       -3.84  1.68e- 4\n##  5 p_grouphigher than 0.4             -6.97e+3  1830.       -3.81  1.88e- 4\n##  6 scholarship:on_campus1             -1.40e-1     0.274    -0.510 6.11e- 1\n##  7 scholarship:p_group0.2 to 0.4       7.98e-1     0.288     2.77  6.10e- 3\n##  8 scholarship:p_grouphigher than 0…  -6.38e-2     0.341    -0.187 8.52e- 1\n##  9 on_campus1:p_group0.2 to 0.4        8.22e+2  1998.        0.411 6.81e- 1\n## 10 on_campus1:p_grouphigher than 0.4  -6.21e+2  2198.       -0.282 7.78e- 1\n\nglance(model4)\n## # A tibble: 1 x 11\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <int>  <dbl> <dbl> <dbl>\n## 1     0.569         0.548 5703.      27.8 2.26e-30    10 -2008. 4039. 4075.\n## # … with 2 more variables: deviance <dbl>, df.residual <int>\n\naugment(model4)\n## # A tibble: 200 x 12\n##     debt scholarship on_campus off_campus p_group .fitted .se.fit  .resid\n##    <dbl>       <dbl> <fct>     <fct>      <fct>     <dbl>   <dbl>   <dbl>\n##  1 26389        1532 0         1          higher…  28302.   1319. -1913. \n##  2 21268        7479 1         0          lower …  25706.   1173. -4438. \n##  3 32312        2664 1         0          lower …  35097.    913. -2785. \n##  4 28539        1998 1         0          0.2 to…  31912.   1271. -3373. \n##  5 34867        1462 0         1          lower …  35496.    850.  -629. \n##  6 18193        3053 0         1          0.2 to…  28152.   1180. -9959. \n##  7 29990        1301 0         1          0.2 to…  29926.   1334.    64.4\n##  8 34333        1948 0         1          lower …  34616.    791.  -283. \n##  9 27717        2295 1         0          lower …  35816.    941. -8099. \n## 10 21398        4653 0         1          lower …  29718.    729. -8320. \n## # … with 190 more rows, and 4 more variables: .hat <dbl>, .sigma <dbl>,\n## #   .cooksd <dbl>, .std.resid <dbl>\n\nggplot(data=model4, aes(x=scholarship,y=debt, color=p_group)) + geom_point()+ geom_smooth()+labs(x = ""scholarships"",  y = ""debt"")+ ggtitle(""Interaction term between Scholarship and Parents"")\n## `geom_smooth()` using method = \'loess\' and formula \'y ~ x\'\n\n\n'], 'url_profile': 'https://github.com/annie5696', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '122 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dushyant18033', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kishoremanamala', 'info_list': ['Mathematica', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'Python', 'Updated Feb 25, 2020', 'Apache-2.0 license', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'HTML', 'Updated Mar 3, 2020', 'HTML', 'MIT license', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""House-Prices-Advanced-Regression-Techniques\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.  With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, you need to predict the final price of each home.\n""], 'url_profile': 'https://github.com/sudhanshusaurav', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ram71', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['House-Prices: Advanced-Regression-Techniques (Kaggle Competition Challenge)\n\n\nProblem Statement: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\n\n\nGrab Data set: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n\n\nMy solution file is sample_submission.csv\n\n\nI am using XGBoost algorithm to cultivate my accuracy.\n\n\nMy rank is 2387/4649 in just two entry and continuesly trying to go up on leaderboard with using different different methods.\n\n\n'], 'url_profile': 'https://github.com/pnidhi26', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/djdsdjds', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '396 contributions\n        in the last year', 'description': ['Tic Tac Toe RegressionBot\nA simple Tic Tac Toe Bot that learns the best moves with multiple linear regression.\nStart with python main.py\nFor more information please visit:\nMedium Story\n'], 'url_profile': 'https://github.com/Skyy93', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'sangrur,punjab', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/munnupsk', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PriyabrataThatoi', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'Netherlands', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pranavsdev', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'Espoo, Finland', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Logistic-Regression-for-Titanic-Dataset\nUsing Logistic Regression classifier to classify titanic dataset. Predicting the survival rates of the titanic based on the passenger features.\n'], 'url_profile': 'https://github.com/mesushan', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""Advertising Project using Logistic Regression, Decision Tree Classifier and Random Forest Classifier\nIntroduction\nThe goal of this project is to create a model to predict whether or not a particular internet user will click on an Advertisement on a company’s website. I have worked with three Machine learning algorithms Logistic Regression, decision Tree classifier and Random Forest classifier.\nDatasets:\nIn this project, I have used some fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement.\nThis data set contains the following features:\n\n'Daily Time Spent on Site': consumer time on site in minutes\n'Age': cutomer age in years\n'Area Income': Avg. Income of geographical area of consumer\n'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n'Ad Topic Line': Headline of the advertisement\n'City': City of consumer\n'Male': Whether or not consumer was male\n'Country': Country of consumer\n'Timestamp': Time at which consumer clicked on Ad or closed window\n'Clicked on Ad': 0 or 1 indicated clicking on Ad\n\nMachine Learning Algorithm Process\nThe machine learning algorithm process highlights the steps taken to get the models up and running from start to end. The machine learning algorithm process includes:\n\nLoad data into Jupyter notebook and perform exploratory analysis.\nUsed seaborn to explore the data\nSplit the data into input and output columns.\nSince the data was already in standard form I didn’t do any preprocessing\nPass the data into grid search Logistic Regression, Decision Tree classifier and RandomForest classifier\nEvaluate the performance of the models using confusion matrix and classification report.\n\nResults\nThe results of the 3 machine learning models evaluated are as follows:\n\nLogistic regression with Precision, Recall, F1 score as 91%, did not performed well as compared to other models.\nDecision tree with Precision, Recall, F1 score as 94%\nRandom forest with Precision, Recall, F1 score as 95%\n\nConclusion\nRandom forest classifier performed best with Precision, Recall, F1 score as 95%.It accurately predicted whether a particular internet user will click on an Advertisement on a company website or not. Random forest classifier is a combination of decision trees and is protected from the problem of overfitting as it is an ensemble method.\nThis project was very interesting and I learnt a lot from it\nPrecision – What percent of your predictions were correct?\nRecall – What percent of the positive cases did you catch?\nF1 score – What percent of positive predictions were correct?\n""], 'url_profile': 'https://github.com/shrutibasarkar', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 1, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Pandian-1998', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sham10', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Het369', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'kolkata', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""-House-Prices-Advanced-Regression-Techniques\nGiven a set of parameters of a House, we need to predict the sales price. Feature enigneering has been done and a number of ordinal feature has been converted into numerical value(in ordinal order). The categorical features which were nominal in nature converted to number by one hot encoding.\nThen the features were trained using GBR,DTR,SVR,RFR giving a good result.\nCompetition Description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nPractice Skills\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting\nFile descriptions\ntrain.csv - the training set\ntest.csv - the test set\ndata_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\nsample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\nData fields\nHere's a brief version of what you'll find in the data description file.\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale\n""], 'url_profile': 'https://github.com/samiran31', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Logistic-Regression-for-sentiment-classification\nBuilding a logistic regression model to perform binary sentiment classification by using TF-IDF as the feature.\nDataset\nThis dataset contains 100k tweets with their associated binary sentiment polarity labels. The dataset is split into 90k train/test and 10k prediction sets.\nCreated a logistic regression model to assess precision and recall without using inbuilt libraries.\n'], 'url_profile': 'https://github.com/ShambhaviChati', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChaitraUH', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['EDA-and-Feature-engineering-for-Regression\ndetailed explanation of EDA, Stats and Feature engineering\n'], 'url_profile': 'https://github.com/shaurya2611', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Russia, Orel, Moscow', 'stats_list': [], 'contributions': '2,002 contributions\n        in the last year', 'description': ['advanced_statistics_methods-linear_regression\nMachine Learning && Deep Learning\n'], 'url_profile': 'https://github.com/PavelPavells', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'san francisco', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['ML_Apprentice_Project\nProject accomplished for the first machine learning assignment.\nIn This project I have build some regression model using Scikit-learn python library.\nThe file .ipynb contains the analysis of the case and the models built.\nThe file .py contains the most relevant model for our analysis.\n'], 'url_profile': 'https://github.com/skanska1', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Basic-Neural-Network-linear-regression-\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 25, 2020', 'MATLAB', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""House_Prices-Advanced_Regression_Techniques\nI have used Python 3 & Jupyter notebooks to work through the solution of the House Prices: Advanced Regression Techniques competition of Kaggle.\nThis repository contains the solution of the House Prices: Advanced Regression Techniques competition of Kaggle.\nTo know more about the competition please follow the link: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n-- Installations required: I am using Python 3.6.0 for this project with Jupyter notebook as Integrated Development Environment(IDE). You need to install the following libraries.\nNumPy (for documentation:http://www.numpy.org/)\nPandas (for documentation:http://pandas.pydata.org/)\nScikit-Learn (for documentation:http://scikit-learn.org/stable/)\nMatplotlib.pyplot (for documentation:https://matplotlib.org/api/pyplot_api.html)\n-- Code of the project: The code contains in the 'House Prices - Advanced Regression Techniques.ipynb' file.\n-- Data for the problem: The training data is contained in 'train.csv' file and the testing data is contained in the 'test.csv' file. 'Data Description.txt' is the file which contains the explanations of the fields available in the other data files.\n-- To download the data please follow the links:\ntrain.csv (https://www.kaggle.com/c/house-prices-advanced-regression-techniques/download/train.csv)\ntest.csv (https://www.kaggle.com/c/house-prices-advanced-regression-techniques/download/test.csv)\n-- Aim of the project: The goal for the competition is to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\n""], 'url_profile': 'https://github.com/muditgmac', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nakul1010', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Titanic-Data-EDA-Logistic-Regression\n'], 'url_profile': 'https://github.com/kaprideepak', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'Helsinki', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['econometrix\nI store my code of tests, regressions and time-series analyses with R here.\n'], 'url_profile': 'https://github.com/tsunamisabella', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Selmate-a-customizable-test-automation-tool\nSelmate is developed as an extensible and customizable tool to automate regression testing for Web applications. It provides its own scripting language based on spreadsheet/xml and relevant API in Java for external communication. Selenium WebDriver is used internally by Selmate for all browser communication. This is an endeavor to minimize the programming effort required for test automation using Selenium.\n'], 'url_profile': 'https://github.com/IBM', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Stock-Predictor-For-Google\nThis  program can be used to predict the Google stock price for a specific day and displays result in the  form of graph. It makes use of Machine Learning Algorithm of Linear Regression and Support Vector Regression (SVR)\n'], 'url_profile': 'https://github.com/deeppatne', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '321 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Odellc', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/antoniaavadanei', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '742 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kritikseth', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['An Analysis of Engineering Salary Correlation to Skill Level and Tenure Length\nThe purpose of this analysis was to examine if tenure length or skill level affected salary for engineers, and if so, which had a stronger effect on salary. The general findings of this multiple linear regression analysis were that position/title related to skill level were more likely to have an impact on salary for engineers. This could have implications for engnineers working for the city who are looking to earn higher salaries, as well as for the City Department of Human Resources if they are looking to attract the best talent.\nHow Do Position and Tenure Length Affect Salary for Engineers in Baltimore?\nFor any organization looking to hire the most qualified people for a job and have the best outputs, it is important to ensure the best talent isn\'t overlooked or driven away by not being properly compensated.\nThe strategy of rewarding more qualified workers with higher salaries is also a more effective and efficient approach of using company funds instead of rewarding people who have worked in the organization for longer. This method rewards quality of work and acts as investment in the advancement of workers who show potential as opposed to rewarding workers who simply have been there longer and may be satisifed in their position and have no motivation to progress and increase their value to the company. A related study conducted in the UK explored the realities and implications of increasing wages depending on seniority versus experience level. These results may also be pertinent to engineers such as myself searching for jobs, as they would prefer to work somewhere where the value they generate for the company is compensated over their length of employment by the organization.\nUsing Job Title, Hire Date, and Salary to Assess Correlation\nRaw data   was downloaded from the Baltimore City open salary data website for Fiscal Year 2019 (FY2019). The data categories specifically used for analysis were job title, to reflect skill level, hire date, to reflect length of tenure, and gross and annual salary. Data was additionally filtered to only inlude numbers for people with ""engineering"" in the job title to allow for more specific results.\n""Job Levels"" were artificilly determined by ordering all the different engineering job titles by their assumed importance and the assumed progression of positions in an organization, since it was not specified which positions were actually higher than others. Data analysis would have been more significant had progression throughout the entire pomotional hierarchy been available. Once ordering of the relative order of positions (entry, mid, and high) was determined, these positions translated into analyzable data by assigning numbers to every title based on order in the list. Department ID was used in the analysis to see if any relationship existed, but no logical relationship was found as the numbers for each department do not provide any actual numerical information.\nHire date was translated into tenure length by calculating the number of days between the end of FY2019 and the hire date, and therefore provide the amount of time worked by that person in the organization.\nGross and annual salary were not modified in any way, but analysis was conducted with both values to see if there were any meaningful differences.\nResults Show that Job Level and Salary Have a Strong Correlation\nMultiple linear regression was performed on the raw data from Open Baltimore comparing salary, job level, and time working for Baltimore City. The steps to perform this analysis are listed below:\n\nDownload Baltimore Open City Salary Data for FY 2019\nCreate a new document for analysis and tranfer the data into it so as to not affect the raw data file\nFilter out the data for positions that are not for engineers\n\n\nCreated a column to the right of the data titled “Eng_Job_Title” and entered and equation, for example: =IF(ISNUMBER(SEARCH(""engineer"",B2)),B2,0) that would enter the job title if it had ""engineering"" somewhere in the JOBTITLE column and will return a value of 0 if the job title is not related to engineering\nUsed a filter on the new column Eng_Job_Title to hide all the columns that had ""0"" entered, as those were the non-engineering jobs\n\n\nJob titles were then assigned their corresponding ""Job Levels"" using the following system\n\n\n\n\nEngineering Draft Tech\n\n\n\n\nEngineering Associate I\n\n\n\n\nEngineering Associate II\n\n\n\n\nEngineering Associate III\n\n\n\n\nEngineer I\n\n\n\n\nNetwork Engineer\n\n\n\n\nEngineer II\n\n\n\n\nOperations Engineer\n\n\n\n\nMarine Engineer\n\n\n\n\nSupervising Enigneer\n\n\n\n\nProject Engineer\n\n\n\n\nChief of Engineering\n\n\n\n\nIn a new column titled ""Eng_Job_Level"" each job title was assigned its numeric job level using the following code:\n\n=IF(H404=""Civil Engineering Draft Tech"",1,IF(H404=""Engineering Associate I"",2, IF(H404=""Engineering Associate II"",3,IF(H404=""Engineering Associate III"",4, IF(H404=""Engineer I"",5,IF(ISNUMBER(SEARCH(""Network"",H404)),6,IF(H404=""Engineer II"",7,IF(ISNUMBER(SEARCH(""Operations"",H404)),8,IF(ISNUMBER(SEARCH(""Marine"",H404)),9,IF(ISNUMBER(SEARCH(""Supervi"",H404)),10,IF(ISNUMBER(SEARCH(""Project"",H404)),11,IF(ISNUMBER(SEARCH(""Chief"",H404)),12,0))))))))))))\n\n\nThe table was sorted by acesnding job level\nThe length of tenure for each person at their job from their hire date to the end of FY2019 (September 30, 2019) was caluclated in a new column labeled Tenure_Length\n\n\nA column was created with 09/30/2019 entered for each row to indicate the end of FY2019. The difference in time between HIRE_DT and FY2019_End was then calulcated using a simple subtraction equation between the two columns: =F404-E404\n\n\nMultiple regression was then performed to see which variables are strong predictors of annual and gross salary\n\n\nAll variables that were to be analyzed were moved next to the salary column, and all values were ensured to be in number format to allow for analysis\nThe regression tool in the data analysis toolpak was used to perform the regression\nAnnual RT was selected as the input Y range (dependent variable)\nAll the data under DEPTID, Eng_Job_Level and Tenure_Length were selected as the input X range (independent variables)\nThe regression provided the following table of variables and coefficients as part of the output:\n\n\n\n\n\nCoefficients\nStandard Error\nt Stat\nP-value\n\n\n\n\nIntercept\n37240.17581\n2154.893708\n17.28167644\n2.80555E-39\n\n\nEng_Job_Level\n5388.331928\n207.7909961\n25.93149862\n5.64435E-61\n\n\nTenure_Length\n0.136162083\n0.101159015\n1.346020245\n0.180088199\n\n\nDEPTID\n0.076274556\n0.035530871\n2.146712241\n0.033233813\n\n\n\n\nMultiple regression was repeated for Gross Salary to analyze any differences, and the regression provided the following table of variables and coefficients as part of the output:\n\n\n\n\n\nCoefficients\nStandard Error\nt Stat\nP-value\n\n\n\n\nIntercept\n27102.27767\n4839.874351\n5.59978952\n8.47226E-08\n\n\nEng_Job_Level\n6067.256762\n466.6969461\n13.00042096\n2.84914E-27\n\n\nTenure_Length\n1.032291983\n0.227202355\n4.543491571\n1.04557E-05\n\n\nDEPTID\n0.059149079\n0.079802057\n0.741197425\n0.459596375\n\n\n\n\nFor each independent variable (Job level, tenure, deptID) two scatter plots were made with the independent variable as the x-axis and the y-axis set to either Annual RT or Gross income, to see if there were any differences\n\n\nTrendlines, equations, and r^2 values were added to the graphs\nChart title and axis titles were added\n\nThe results in the tables show how closely each variable (Job Level, Tenure Length, and Department ID) correlate to each salary type (Annual and Gross Salary). The Regression Coefficient is an estimate of the unknown population parameters and describes the relationship between a predictor variable and the response. The Standard Error of Regression value represents the average distance that the observed values fall from the regression line as one indication of how correct the regression model is. The T-Statistic represents a measure of the precision with which the regression coefficient is measured. The P-Value determines how likley it is that these results are due to chance and variablity in data, and if the value is below 0.05 the data is considered statistically significant.\nAs can be seen from the first table representing correlation to annual salary, only the Engineering Job Level has a P-value of less than 0.05, indicating the regression equation is a good predictor of annual salary when using job level. The data, regression equation, and R^2 value are shown below.\n\nThe R^2 value of 0.7712 indicates that 77.12% of the data can be predicted by the regression equation of y = 5523.7x + 42298.\nAs can be seen from the second table representing correlation to gross salary, there was still a correlation, if weaker, between salary and job level, but there also appeared to be a significant relationship between length of tenure and salary. The data, regression equation, and R^2 value for both of these variables are shown below.\n\nThe R^2 value of 0.5914 indicates that now only 59.14% of the gross salary data can be predicted by job level with the regression equation of y = 6124.8x + 38203.\n\nWhile there is a significant P-value for the relationship between gross salary and tenure length, R^2 value is only 0.0752, indicating only 7.52% of the gross salary data can be predicted by tenure length with the regression equation y = 0.9801x + 69639.\nJob Level Creates the Primary Impact on Annual Salary, but Tenure Length Can Have Secondary Effects on Gross Salary\nThe conclusion from the data analysis is that job level has the strongest impact on annual salary, which is a good indication for the organization as well as for employees. Employees who are doing harder, more value-producing jobs are being fairly compensated, and the organizations funds are goig towards supporting higher level employees. However, the gross salary for employees is less strongly correlated to job level, and develops a relationship with tenure length. This suggests that while most employees with the same job title have the same base salary, those who have worked there longer are possibly receiving higher bonuses, resulting in higher overall salary. This could be due to the higher experience and value level of those longer-term employees.\nThese results indicate that the methods of Baltimore City in attracting and retaining good engineering talent are promising and attractive to current employees as well as new applicants. Additionally, they are devoting their funds reasonably to people in positions who produce more value for them. Overall, Baltimore City should continue with their practice of rewarding people in higher positions, and further rewarding people who perform particulalry well in those positions.\n'], 'url_profile': 'https://github.com/karinafrank', 'info_list': ['Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Mar 10, 2020', '2', 'Java', 'Apache-2.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 26, 2020', 'R', 'Updated Feb 27, 2020', '1', 'HTML', 'LGPL-3.0 license', 'Updated Feb 28, 2020', 'MIT license', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Car-Sales-prediction-using-linear-regression-\nPrediction of car sales volume.\nThis is a cars dataset having 156 observation and 16 different variables.\nWe have to create a linear regression model using the provided predictor variables for the prediction of sales value of cars.\nJupyter Notebook\n'], 'url_profile': 'https://github.com/sudhirtakke', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'PARIS', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YacineAbd', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Linear-and-Logistic-Regression-from-Scratch\nBuilding Linear and Logistic Regression from Scratch as part of my machine Learning Assignment. While Python’s scikit-learn library provides the easy-to-use and efficient LogisticRegression class, the objective is to create an own implementation using NumPy. Implementing basic models is a great idea to improve your comprehension about how they work.\n'], 'url_profile': 'https://github.com/mxk180040', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Regressions-and-Correlations-with-Baltimore-Incarceration-Rates\nData Sources: Opportunity Atlas Data for Baltimore City based on four indicators: incarceration rates, teen birth rates, Employment rate and household income\nBusiness Question: I am a data analyst for mayor of the city of Baltimore, Bernard Young, and the city council is about to begin voting on the budget. The council’s main goal is to reduce incarceration rates, but the budget is limited and can only allocate funds to one initiative. My job as a data analyst for the city is to answer the following business question: Which factors between household income, employment rates and teenage birth rates and have the highest impact on incarceration rates? Using this information, we can write up a convincing proposal to the city council.\nBased on the business related question the following data related questions are pertinent:\n\nWhat is the relationship between incarceration rates and household income, teenage birth rates and employment rates? What is the curve of this relationship?\nWhich x values are most significant amongst the three for determining incarceration rates?\nIs there a strong correlation between these values? Is it positive or negative?\nWhat are the chances that these independent variables are at the cause of the dependent variable? What do our p values and f significance tell us about this?\n\nBased on the the answer to the data related questions, the city council will decide between:\n\nRaising minimum wage to raise household income\nFunding a comprehensive job creation program to increase employment rates\nFunding a planned  parenthood initiative to tackle teenage birth rates\n\nData Related Findings:\n\nBased on the first regression, 72.5% of variation in incarceration can be explained by household income, employment rates and teen birthrates.\nMoreover, the low significance F signifies that at least one of these values matter to determining incarceration rates\nThat being stated, when looking at the significance of each variable in determining incarceration rates, household income and employment rates remain relatively insignificant. The only significant variable is teenage birth rates.\nBased on this fact a second regression in which the only independent variable was teen birth rates, showed that 72% incarceration rates could be explained by teen birthrates and the p value and f significance showed significance in determining incarceration rates.\nFrom there, data on correlations showed negative correlation and very slight negative correlation between household income and employment rates, respectively, and incarceration rates.\nTeenage birthrates showed a positive correlation meaning that as teenage birth rates rose, so did incarceration rates.\nAn interesting relationship, aside from incarceration rates, is between teenage birth rates and household income, which are very negatively correlated indicating an important relationship.\n\nConclusions:\nBased on our data, it is clear that in evaluating sociological factors, establishing clear correlation and relationships is difficult. It is usually not a single isolated variable, that will have a direct effect on another social variable. Still, the strongest relationship to incarceration, based on our data, seems to be between teenage birth rates and incarceration rates. Thus my suggestion to the city council is the following:\nChoose to spend the allocated funds towards a widespread planned parenthood program. This based on our data, should decrease teen birth rates, and decrease incarceration rates in the long run.\nExcel Step by Step Walkthrough\n\nI used V-Lookup to match the tract numbers of different location data sets, to the tract numbers of location data on inCarceration rates\nI used the Analysis Tool-Pak add-in and under the “Data” tab, selected data analysis. I selected incarceration rates as my dependent variable, and the other three variables as my independent variables, and ran the regression.\nI used similar steps for the second regression, but with teen births as my only independent variable\nOn tab “Correlation, ” I ran the correlation function between my variables. I did this also between teen birth rates and incarceration rates, and made a graph.\nI finally made a line graph to show the different peaks based on each region. I used a secondary axis to adequately map household income.\n\nExcel Files\n\nhttps://github.com/mdia4/Regressions-and-Correlations-with-Baltimore-Incarceration-Rates/blob/master/Mini%20Project%202.xlsx\nhttps://github.com/mdia4/Regressions-and-Correlations-with-Baltimore-Incarceration-Rates/blob/master/Baltimore%20Employment%20rates.xls\nhttps://github.com/mdia4/Regressions-and-Correlations-with-Baltimore-Incarceration-Rates/blob/master/Household%20income%20baltimore.xls\nhttps://github.com/mdia4/Regressions-and-Correlations-with-Baltimore-Incarceration-Rates/blob/master/Baltimore%20Incarceration%20project%202.xls\nhttps://github.com/mdia4/Regressions-and-Correlations-with-Baltimore-Incarceration-Rates/blob/master/Teengage%20Birthrates%20Baltimore.xls\n\n'], 'url_profile': 'https://github.com/mdia4', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Liv0112', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Feb 28, 2020', 'Updated Feb 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'Erlangen', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['reamme\n'], 'url_profile': 'https://github.com/murari-goswami', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'Yokohama, Japan', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Machine_Learning_Methods_For_Regression_Matlab\nDescription\nData has 6 features & 1 output. the target for this is regression by at least 3 machine learning methods. And at least 1 NN method.\n1. 代码文件夹及其代码文件的介绍\n\n\n代码文件夹内容介绍：\n代码文件夹中共有1个文件夹，分别是data。\ndata文件夹下存放的是训练集数据data.xlsx和测试集数据100_test.xlsx（是最新的测试集数据）。\n其中data.xlsx的sheet名称为train_400，即400个训练数据，100_test.xlsx的sheet名称为valid_data，即100个测试数据。\n\n\n代码介绍：\n由于模型训练较快，因此将模型的训练和测试统一写成了run_model.m脚本文件。其中第一种模型对应的函数脚本文件为svr_train.m, svr_predict.m, gaussKernel.m；第二种模型对应的函数脚本文件为fmincg.m, linearRegCostFunction.m, trainLinearReg.m。\n由于模型训练较快，因此将带ACO的ML模型的训练和测试统一写成了ACO_model.m脚本文件。其中模型对应的函数脚本文件为trainAnts.m。\n\n\n2. 运行代码方法\n\n\n直接打开matlab软件运行run_model.m脚本文件。\n\n\n运行脚本文件后，命令行终端输出选择模型，共有三个模型提供选择，分别对应数字1、2、3，比如要选择第一个模型，则在终端中输入1。\n\n\n程序就会根据输入的模型进行训练，最后输出验证集的预测准确率。\n\n\n若使用ACO模型，则可以直接打开matlab软件运行ACO_model.m脚本文件。\n\n\n3. 验证集准确率介绍\n\n\n将验证集准确率定义为模型输出和验证集输出值偏差在5%范围内的数据占比。\n\n\n模型输出和验证集输出值偏差定义为二者的绝对值之差与标准输出的占比。\n\n\n比如模型输出为90，标准输出为96，则偏差为(96-90)/96 = 0.0625,即6.25%。\n\n\n而验证集中数据有30个满足以上偏差范围，则准确率为30/40=75%。\n\n\n4. 模型介绍\n\n\n模型1： 模型使用SVR方法(支持向量回归机)，是支持向量机的回归形式，使用带有松弛变量的最小化数据点距平面的距离和。使用梯度下降求解平面法向量，得到分界平面，根据不同的核函数可以求解不同分界曲面。\n\n\n模型2： 使用线性映射（非线性化）的线性回归模型，线性映射是将5个特征投影到高维的空间进行非线性化，然后再高维空间中使用梯度下降法进行线性回归拟合，其中添加正则化项用参数λ进行控制，参数λ的大小意味着模型对于正则化项的重视程度。该正则化项防止过拟合，而使模型不具有很好的泛化性。\n\n\n模型2使用的线性映射有变化，映射方式为特征2、4、6分别平方，其他特征不变。\n\n\n正则化项系数改变为0.0002。\n\n\n\n\n模型3： 模型的结构为3层BP网络结构。其中输入层有6个结点，隐藏层分别有200个结点，最后输出层有1个输出结点。隐藏层使用对数S型激活函数，输出层使用双曲正切激活函数。\n\n\n带ACO算法的线性回归模型： 模型使用线性映射（非线性化）的最小二乘法的回归模型，同时训练中使用蚁群智能算法。首先将数据按维度进行线性映射，使其适应非线性模型，最后使用蚁群智能算法和梯度下降法进行拟合。蚁群智能算法使用在最优化问题中，因此只适用于训练中，在最小化损失函数(最小二乘法是最小化均方误差)条件下，使用该算法寻找全局最优解，即蚁群智能算法寻求最能拟合训练数据的模型，其中添加正则化项用参数λ进行控制和探索，参数λ的大小意味着模型对于正则化项的重视程度。该正则化项防止过拟合，而使模型不具有很好的泛化性。\n\n\n5. 模型结果分析\n\n\n对于ACO模型：\n\n\n经过多次训练后固定参数权重得出，ACO模型的训练集的准确率为80%，测试集的准确率为87%。\n\n\n蚁群智能算法旨在寻求最能拟合训练数据的模型，但容易过拟合，泛化性能不是很好。\n\n\n\n\n其他模型：\n\n\n经过多次训练后固定参数权重得出，模型1的训练集和测试集准确率都为61.0%；模型2的训练集准确率为85.0%，测试集准确率为87%；模型3的训练集的准确率为98.0%，测试集准确率为87%。\n\n\n因此，可以得出模型2、3的预测准确率最高，模型性能最好。\n\n\nBp算法性能能提升的原因：\n\n\n网络层宽度就是结点数能减少过拟合的可能。\n\n\n\n\n\n\n6. 模型训练参数\n\n\nACO模型：\n\n\n信息遗留因子α：0.5\n\n\n蚁群蚂蚁个数m：50\n\n\n信息素增强系数Q：1500.0\n\n\n信息素挥发系数ρ：0.7\n\n\n训练最大回合数：50\n\n\n\n\n模型1训练参数：\n\n\n正规化因子C：40，又称惩罚系数\n\n\n松弛变量：0.001\n\n\nγ核函数系数：0.150\n\n\n核函数：gaussian Kernel\n\n\n\n\n模型2训练参数：\n\n\n正规化因子λ：0.0002\n\n\n\n\n模型3训练参数：\n\n\n学习率：0.01\n\n\n训练最大回合数：1000\n\n\n动态因子：0.9\n\n\n最小平方误差目标：10^-7\n\n\n梯度下降法：trainlm\n\n\n\n\n'], 'url_profile': 'https://github.com/CraKane', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}","{'location': 'Washington DC', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['Airbnb Price Prediction using Lasso-Linear-Regression\n'], 'url_profile': 'https://github.com/thindwan', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'MATLAB', 'GPL-3.0 license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '103 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/IJzerenSteen', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': [""Multinomial-Regression-using-Hyperparameter-Optimization-with-SVM\nGauging how Support Vector Machine Algorithm behaves with Hyperparameter Tuning\nData Description\nThe “juice.csv” data contains purchase information for Citrus Hill or Minute Maid orange juice.  A description of the variables follows.\n\nPurchase: A factor with levels CH and MM indicating whether the customer purchased Citrus Hill or Minute Maid Orange Juice\nWeekofPurchase: Week of purchase\nStoreID: Store ID\nPriceCH: Price charged for CH\nPriceMM: Price charged for MM\nDiscCH: Discount offered for CH\nDiscMM: Discount offered for MM\nSpecialCH: Indicator of special on CH\nSpecialMM: Indicator of special on MM\nLoyalCH: Customer brand loyalty for CH\nSalePriceMM: Sale price for MM\nSalePriceCH: Sale price for CH\nPriceDiff: Sale price of MM less sale price of CH\nStore7: A factor with levels No and Yes indicating whether the sale is at Store 7\nPctDiscMM: Percentage discount for MM\nPctDiscCH: Percentage discount for CH\nListPriceDiff: List price of MM less list price of CH\nSTORE: Which of 5 possible stores the sale occured at\n\nMore crucial attribute than the prices of both these brands is the price difference between both these brands.\nWe already have the Store ID in the dataset so STORE and Store7 need not be included in the dataset.\nList Price difference is a redundant attribute as we already have the Sales Price Difference in the data.\nConclusion\nAfter comparing the Train and Test Scores for all the models:\nBasic Models\nSVM with Linear Kernel is the best in case of Basic Models with the least Error scores for both Train and Test datasets.\nTuned Models\nFor both RBF and Linear Kernels, the cost parameter for the best model is 0.31 and the scores are almost equal.\nFor Polynomial, the cost parameter is 9.61 but the model isn't as good.\nTaking the Accuracy rate and Error Rate into Consideration, both tuned models - SVM with Kernel RBF and Kernel Linear are good but RBF is slighlty better as there are lower chances of Overfitting.\nRemoving such redundant variables in the dataset and keeping more relevant attributes will help us avoid the overfitting of the model.\n""], 'url_profile': 'https://github.com/niadel91', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Saksham19', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ychen39', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'University of Warwick', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kbnyakundi', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Jersey City, NJ', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/siddheshmanjrekar', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'R', 'Updated Jan 29, 2021', 'R', 'Updated Feb 26, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Spotify’s 2019 Worldwide Daily Song Rankings \nThe 200 daily most streamed songs in 10 countries\n\nAcknowledgment: This task is inspired by a project done by http://eduar.do shared on Kaggle with the context of exploring how artists and song’s popularity varies overtime here. This work is an extension of the same by analyzing daily ranking of the 200 most listened songs by Spotify users in 10 countries that contribute most to Spotify’s bottom line here.\nIntroduction\n\nSpotify transformed music listening forever when it launched in Sweden in 2008 and became one of the most popular global audio streaming subscription service with 271m users, including 124m subscribers, across 79 markets. All throughout the last year, users have streamed countless hours of their favorite songs, artists, playlists, and podcasts. In this analysis, using Spotify’s chart of 2019 I will highlight the common trends and use regression models to forecast the number of tracks streamed and their associated popularity.\nData\nDataset contains the daily ranking of the 200 most listened songs in USA, UK, Mexico, Germany, Brazil, Canada, Australia, Netherlands, France and Sweden. It contains 730,000 rows, which comprise 2574 artists, 8825 songs for a total count of eighty-eight billion stream counts.\nThe data spans from 1st January 2019 to 31st December 2019 and includes 7 columns.\nIt has been collected from Spotify\'s regional chart data.\nObjective and MVP\n\nUsing the information given in the database I predicted the rank position that a song will have in the future and expanded the analysis to learn factors such as:\n\nDuration songs “resist” on the top 3, 5, 10, 20 ranking.\nWhat are the signs of a song that gets into the top rank to stay\nDo countries share same top-ranking artists or songs\nAre people listening to the very same top-ranking songs in countries far away from each other?\n\nData Scrapping, Cleaning & Feature Engineering\n\nScrapping: Data is scrapped by downloading the CSV file using Spotify top 200 daily API and converted to a Pandas data frame.  Cleaning:  Missing data has been removed from the table as it was insignificant and region codes are all replaced with the country’s name. Miscategorized datatypes are also converted to the right format, for example the ‘date’ column has been converted from an object to datetime and set as index, and rank position changed from continuous to a discrete. Also, categorical variables are encoded as part of the data processing.\nFeature Engineering : To run the models faster I created separate data frame for each country and used USA table as a pilot and then tested the model for each country individually. Since the rank position for an artist could vary depending on song’s popularity, number of streams and the region, I have created new variables for average rank, top rank, low rank based on average, maximum and minimum streams. The new variables drastically improved the model’s R2 score.\nData Analysis\n\nFor easier analysis and collaboration, I have built a dashboard to visualize and track Spotify music trends. For a more in-depth insights, download and interact with this [sample dashboard] on Tableau Public. \n\nModeling\n\nRank Position\nInitially, I have computed Linear Regression and used Multi-output regression to predict multiple output/target variables. I took artist and streams to forecast average/max/min rank variables simultaneously. As tuning /fitting process, after standardizing(scaling) data I used PCA to find linear combinations of current predictor variables and created new ""principal components"". This process helped to reduce dimensionality and understand the most important ""directions"" in the data.  However, after plotting the Linear Regression of actual vs prediction residual I realized that the outcome did not change in proportion to a change in any of the inputs, in other words, the plot indicated nonlinearity:\n\n\nAnd therefore, I computed the Random Forest Regressor that uses  averaging of classifying decision trees on sub-samples of the dataset to improve the predictive accuracy and control over-fitting. The result looked promising:\nSummary of Regression Results:\nThe train R2 Score on the Rank Position data is:  98.6% \nThe train RMSE on the Rank Position data is:  6\n\nThe test R2 Score on the Rank Position data is: 91.9% \nThe test RMSE on the Rank Position data is:  15\n\n\nPredicting Rank Resistance\nResults indicated that choosing only one attribute (# of Streams ) is not suffcient to predict the duration of a song remaining on its current rank, however EDA can still be used for some directional insights:\n\n\nConclusion and Next Steps\n\nIn general, the popularity rating is based on total number of streams compared to other tracks as well as how recent those streams are, however that alone does not factor in determining the songs success. As shown in the data, countries share the same top-ranking songs, and that shows how factors like social trends, location, artists and genre are significant to track’s popularity. Nonetheless, exploring songs features could provide further insights for artists and music agents who are looking for Spotify users view and their perceived values.\nNext steps identified to expand on this project are as follows:\n\nReal Time KPI Dashboard:  connecting to Spotify data and get the latest data from Spotify’s charts and bring it into Tableau for analysis.\nExtend the data set to include all countries where Spotify is available and add podcast as a separate category.\nFurther analysis to learn engagement by streaming device e.g. phone, computer or tablet.\nExplore impact of attributes such as song genre, keys, energy, danceability, instrumentals, etc. on its popularity and create a regression model to predict song’s popularity based on these features.\n\nSources & Requirements\n\n• Spotify | Charts\n• Scikitlearn==0.21\n• Spotipy== fycharts.SpotifyCharts Tweet\n• Pandas==1.0.\n'], 'url_profile': 'https://github.com/moriesam', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Data Mining Algorithm Implementation\nPractice the implementation of main data mining methods with Python. From csv handling to result explanation.\nLanguage: Python\nTopics:\n\nregression (linear/logistic)\nDecision Tree\nSVM\nClusters: K-means, GMM, DBSCAN\nApriori\nnaive bayes\npLSA\n\nEach directory contains detailed usage of its Python code.\nSitu Ma\n'], 'url_profile': 'https://github.com/Situ-Ma', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '230 contributions\n        in the last year', 'description': [""Working with the UCI Bike Sharing Dataset\nThe purpose of this repository is myself practice of neural networks. I am a student that is learning, let me know if you find any errors, the original code is from examples and exercises found in books, tutorials, lectures and other sources all mentioned in this file. I am just practicing what I have learned, the proper authors and creators of the algorithms/code are the ones mentioned in the file.\nI will be using Streamlit to develop an App where I will be sharing what I've learned. To run the app, first clone the repository, install the project dependencies, then run the app with streamlit.\nBefore installing the requirements, it's recommended to create a virtual environment.\npip install -r requirements.txt\nstreamlit run net.py\n\n\n\nResources:\n\nLectures: Building a regression model with numpy, Alura: https://www.alura.com.br/curso-online-rede-neural-numpy\nML glossary: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n3blue1brown: https://www.youtube.com/watch?v=aircAruvnKk&t=551s\nILOC tutorial: https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n\n""], 'url_profile': 'https://github.com/parismollo', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/andreitivga', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Kaggle-Competition-Supervised-Learning-LR-estate-price-prediction\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shivamduseja', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['linear_regression Model\nIll advice you gather some knowledge on Statistics and linewar algebra(Not much though)\nThis is the code for the ""How to Do Linear Regression the Right Way""\nOverview of the code and everything\nI\'m using a small dataset of student test scores and the amount of hours they studied.\nIntuitively, there must be a relationship right(The more the test score and study time, the higher the chances of them passing\nThe more you study, the better your test scores should be.\nWe\'re going to use  to prove this relationship.\nImportant Links\n#(1) formular link = ""https://medium.com/meta-design-ideas/linear-regression-by-using-gradient-descent-algorithm-your-first-step-towards-machine-learning-a9b9c0ec41b1""\n#(2) [linear regression] https://onlinecourses.science.psu.edu/stat501/node/250\n#(3) Use pip to install any dependencies.\nHere are some helpful links:\nSum of squared distances formula (to calculate our error)\nhttps://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png\nPartial derivative with respect to b and m (to perform gradient descent)\nSome calculus\nhttps://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png\nDependencies\n\nnumpy for numerical compuation esp in the partial derivatives\n\nPython 3 is a preferable one for this\nUsage\nJust run python3 my_demo.py to see the results in your terminal and command line(Linux/Mac) and cmd for windows:\n\nStarting gradient descent at b = 0, m = 0, error = 5565.107834483211\nRunning...\nAfter 1000 iterations b = 0.08893651993741346, m = 1.4777440851894448, error = 112.61481011613473\n\n\nCredits\nSiraj, Vallery Atieno and Setdex -Teachers\nArnold Suolo -Internet provider\n'], 'url_profile': 'https://github.com/OkomoJacob', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Houston, TX', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['concrete\nUse Multiple Linear Regression to model the relationship of multiple exploratory variables on a single explanatory variable.\n'], 'url_profile': 'https://github.com/4klud', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '749 contributions\n        in the last year', 'description': ['Neural_Network_school_project\n\nSolving regression problems using Neural Network.\nCreating and training feedforward Neural Network that could classify not linearly separable classes.\nSearching for optimal hyperparameters using cross-validation method.\n\n'], 'url_profile': 'https://github.com/markovicanja', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prateekpr', 'info_list': ['Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Mar 31, 2020', 'C++', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 25, 2020', 'MATLAB', 'MIT license', 'Updated Mar 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}"
"{'location': 'Chennai, India', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': [""regime-detection\nDataset:\nBig Macroeconomic dataset from FRED St. Louis desiged by McCracken and Ng (2015).\nInvolves 129 macroeconomic monthly time series data from 1959 to 2018.\n8 Categories: Output and income, labor maket, housing, consumption, orders and inventories, money and credit, interest and exchange rate, prices in the stock market.\nNBER Recession Dates:\n• Labelling based on NBER dataset\n• 8 recession periods during the time period in consideration\n• 628 'normal' periods and '93' recession periods\nData Cleaning:\n• Removal of variables with missing observation/ imputation of some sort\n• Add lags of all variables as additional features\n• Test stationarity of the time series\n• Standardize the dataset\nAdd lags of the variables as additional features:\n• Add 3, 6, 9, 12, 18 month lags for each variable\n• Shift labels for 1 month ahead prediction\n• 699 observation points and 710 features\nStationarity:\n• Augmented Dickey Fuller Test. Null hypothesis of ADFis that the time series is non stationary with the alternative that it is stationary\n• If p value > significance level, we cannot reject null hypothesis. Then take first order difference\n• adfuller function from statsmodels is used\nStandardization:\n• Standardization of feature vectors by removing mean and scaling to unit variance\n• StandardScaler from scikit-learn is used\nMethodology:\n• Perform feature selection to get the most important variables for the forecasts\n• Separate dataset into training and validation datasets. 1960 - 1996: Training, 1996 - 2018: Validation\n• Evaluate the performance of ML Algos on training set with Cross Validation\n• Select the best performing models based on average accuracy and std dev of the CV results. Logistic Regression chosen as benchmark\n• Make predictions on the validation dataset with selection models. Use GridSearchCV to find the best combination of hyperparameters. Evaluate the validation modela nd report accuracy metrics.\nCross Validation:\n• K Fold CV used:\n○ Train the model on (k-1) folds of the training data\n○ The resulting model is validated on the remaining part of the data\n• 'TimeSeriesSplit' is CV technique for time series data. Use first k sets as training, (k+1) as test set\nEvaluation Metric:\n\nROC AUC Score\n\n""], 'url_profile': 'https://github.com/atheesh1998', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '586 contributions\n        in the last year', 'description': ['Vandermonde\nWhat\'s a Vandermonde matrix?\nA Vandermonde matrix is a square matrix that satisfies\na_ij = x_i ^ (j − 1)\na_ij = x_j ^ i\na_ij = x_i ^ (n − j)\n\nUsually what you want is the first form, i.e.\n[1 x x^2 x^3 ... x ^ (n - 1)]\n[1 y y^2 y^3 ... y ^ (n - 1)]\n...\n[1 s s^2 s^3 ... s ^ (n - 1)]\n\nImporting\nimport ""github.com/AnthonyHewins/vandermonde"" \n\nUsage\nGenerally, what you want is this:\nStandard Vandermonde:\nmy_data := []float64{1,2,3}\nmatrix, err := vandermonde.Vandermonde(my_data, 0, 0)\n\n// matrix = [1 1 1]\n//          [1 2 4]\n//          [1 3 9]\nTransposed version:\nmy_data := []float64{1,2,3}\nmatrix, err := vandermonde.Vandermonde(my_data, 0, 1) // axis=1, similar to pandas and other data science langs\n\n// matrix = [1 1 1]\n//          [1 2 3]\n//          [1 4 9]\n\n// Note: the returned data is a gonum *mat.Dense, so you can also\n// do matrix.T() if you need both the matrix and its transpose.\nStarting with a different exponent:\nmy_data := []float64{1,2,3}\nmatrix, err := vandermonde.Vandermonde(my_data, 2, 0)\n// matrix = [1  1  1]\n//          [4  8 16]\n//          [9 27 81]\n\nmy_data := []float64{1,2,3}\nmatrix, err := vandermonde.Vandermonde(my_data, -1, 0)\n// matrix = [1   1 1]\n//          [1/2 1 2]\n//          [1/3 1 3]\nFor more advanced usage when you only a slice of the Vandermonde matrix, which\ncan be larger or smaller:\nmy_data := []float64{1,2,3}\n\n// start with exponent 2, generate 4 terms (instead of 3), default axis\nmatrix, err := vandermonde.VandermondeWindow(my_data, 2, 4, 0)\n// matrix = [1  1  1   1]\n//          [4  8 16  32]\n//          [9 27 81 243]\n\n\n// start with exponent -1, generate only 1 row, transposed axis\nmatrix, err := vandermonde.VandermondeWindow(my_data, -1, 1, 0)\n// matrix = [1 1/2 1/3]\n'], 'url_profile': 'https://github.com/AnthonyHewins', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Medellín, Antioquia, Colombia', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['Technologies:\nPython, Jupyter Notebook, Machine Learning, EDA\nPredicting House Prices in Colombia\nDeveloped and using more than one million data given by property portal a model to predict house prices in Colombia\nMetrics\n\nAchieved 0.2 (RMSLE) on principal cities (twenty).\n\n'], 'url_profile': 'https://github.com/MiguelMque', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['ZomatoRatingPrediction\nPredicting Rating for restaurant.\n'], 'url_profile': 'https://github.com/Muktan', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Espoo, Finland', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['ANN-for-car-purchasing-amount\nUsing Artificial Neural Networks(Regression Task) to predict how much a customer is willing to pay for a car using the different features of Customer.\n'], 'url_profile': 'https://github.com/mesushan', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Jakarta Utara', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/diushalimi', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Stockholm', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['CSV-ML\nRunning Machine Learning Linear Regressions on any structured CSV dataset\nThis is a python program that uses Tensorflow to read data from a CSV file, and run a Deep Neural Network with either Ftrl or Adam optimizers for linear regression.\nCreate a tfconfig.py Python configuration file (as seen in the samples directory) to specify the configurations of your model. The example below is for the prediction of housing prices in Stockholm:\nclass TfConfig:\n    OUTDIR = \'./model_trained\'\n    ## Location of training CSV file\n    TRAINING_DATA_FILE = ""samples/housing/stockholm-housingprices.csv""\n    PREDICT_INPUT_FILE = ""samples/housing/predict_input.json""\n    LABEL_NAME = ""final_price""\n    NUMERICAL_FEATURE_COLUMNS = [""num_of_rooms"",""size"",""initial_price""]\n    ## The categorical features as an array of tuples containing feature name and hash bucket size\n    ## the size of the hash bucket should be at least equal to the number of expected categories\n    CATEGORICAL_FEATURE_COLUMNS = [(""street_name"",1600),(""location"",160),(""sold_month"",16)]\n    ## A divider to define dimension of the categorical features in relation to bucket size. \n    ## This allows for dimensionality reduction of the categories. A value of \'n\' means a dimension\n    ## equals the bucket size divided by \'n\'\n    CATEOGRICAL_FEATURE_DIMENSION_DIVIDER = 2\n    ## Categorical features that have bucketized columns. Defined as an array of tuples in the format:\n    ## (feature_name, min_value, max_value, step). For example, (""my_feature"", 0, 11, 4) will define \n    ## the category buckets [0-3], [4-7], [8-11] - so all values falling in the same bucket are treated\n    ## as the same category.\n    CATEGORICAL_FEATURE_BUCKETIZED_COLUMNS = [(""year_built"",1700, 2020, 5),(""sold_year"",2010, 2020, 1),(""floor_num"",0, 11, 3)]\n    ## A filter to apply on the source data. Rows not matching the filter will not be included in the \n    ## training dataset. Must be defined as dictionary where the keys are column names, and values their\n    ## desired values (more than one value will be considered with an \'OR\' operator, i.e. either one of\n    ## the values are accepted)\n    FEATURE_COLUMN_FILTER = {\n        ""type"": [""Lägenhet""]\n    }\n    ## A dictionary of multipliers for numerical features \n    ## i.e. values for each feature column (defined as dict\n    ## keys) will be multiplied by the specified value. Use\n    ## fractions to perform a division.\n    TRANSFORM_DATA = {\n        ""size"": 1/10,\n        ""initial_price"": 1/1000000,\n        ""final_price"": 1/1000000\n    }\n    BATCH_SIZE = 64\n    DNN_REGRESSOR_NUM_OF_STEPS = 5000\n    DNN_CONFIG = {\n        ""optimizer"": ""Ftrl"",\n        ""hidden_units"": [32,64,32],\n        ""Ftrl"" : {\n            ""optimizer_learning_rate"": 0.1,\n            ""optimizer_learning_rate_power"": -0.5,\n            ""optimizer_initial_accumulator_value"": 0.1,\n            ""optimizer_l1_regularization_strength"": 0.1,\n            ""optimizer_l2_regularization_strength"": 0.2,\n        },\n        ""Adam"" : {\n            ""optimizer_learning_rate"": 0.2\n        }\n    }\n\n\nMake sure TRAINING_DATA_FILE points to your CSV file, and PREDICT_INPUT_FILE points to a JSON file containing the input non-labeled values you want to use for prediction. Below is an example for a single input for this problem:\n{\n    ""size"": [50],\n    ""num_of_rooms"": [2],\n    ""year_built"": [2002],\n    ""floor_num"": [4],\n    ""street_name"": [""Kungsgatan""],\n    ""sold_year"": [2020],\n    ""sold_month"": [""jan""],\n    ""location"": [""Stockholm""],\n    ""type"": [""Lägenhet""],\n    ""initial_price"": [2795000]\n}\n\nUsage\nTo run the application, set up a virtual environment with Python 3 and install all the requirements.\npip install -r requirements.txt\nBefore running the app, edit the following line in main.py:\nfrom samples.wine.tfconfig import TfConfig as cfg\nwith the relative path to import your tfconfig.py file.\nThen run:\npython main.py\nThe RMSE and predicted value(s) will be shown in standard output.\n'], 'url_profile': 'https://github.com/vmehmeri', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Go', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '1', 'Python', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abcgz133', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Paris Nanterre - La Défense', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AurDataAnalyst', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Breast-cancer-diagnosis-using-Logistic-regression-using-L2-Regularization\nA Logistic regression model is built using SGD algorithm for weight and bias update and the final model is compared with Inbuilt function\nData Set Information:\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nAttribute Information:\n\nID number\nDiagnosis (M = malignant, B = benign)\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\na) radius (mean of distances from center to points on the perimeter)\nb) texture (standard deviation of gray-scale values)\nc) perimeter\nd) area\ne) smoothness (local variation in radius lengths)\nf) compactness (perimeter^2 / area - 1.0)\ng) concavity (severity of concave portions of the contour)\nh) concave points (number of concave portions of the contour)\ni) symmetry\nj) fractal dimension (""coastline approximation"" - 1)\nModelling:\nImplemented logistic regression with SGD and L2 regularization from scratch and compare with inbuilt function\nDataset Source:\nhttps://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\nAuthor:\nSiril Sam\n'], 'url_profile': 'https://github.com/SIRILSAM77', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Pima-Indian-Diabetes-Dataset-Prediction\nThis notebook show to you Data Visualisation and various Machine Learning Classification algorithms on a dataset.\nI use three different classification algorithms: (K Neighbors Classifier, SVM Classifier and Logistic Regression Classifier)\n'], 'url_profile': 'https://github.com/MalekDeminion', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'Arlington, Texas, USA', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/darshan-lal', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lazylooserr', 'info_list': ['Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Updated Feb 28, 2020']}"
"{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '444 contributions\n        in the last year', 'description': ['Using-Logistics-Regression-to-classify-if-a-person-will-click-on-the-Advertisement-or-not\n'], 'url_profile': 'https://github.com/apoorvdwi', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Marketing-Mix-And-Panel-Data-Analysis\nPerformed data mining to find the effect of detailing on physicians script behavior. Performed regression and fixed effects to eliminate bias.\n\nClick here for the data files\nClick here to check out the business questions answered\n\n'], 'url_profile': 'https://github.com/bharatimalik', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['ImageClassification2016\nThis repository consists of several scripts written in python and IDL for implementing a softmax regression classifier on high resolution imagery for supervised classification\n'], 'url_profile': 'https://github.com/Skunkler', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Building Artificial Neural Networks in Keras\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'Copenhagen ', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['PySpark-and-predicting-taxi-demand-spike\nImplementation of ML classifiers such as logistic regression, decision tree, random forest and gradient boosted tree with cross validation and parameter sweep\n'], 'url_profile': 'https://github.com/Meghdad-DTU', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['xrnet: R Package for Hierarchical Regularized Regression to Incorporate External Data \n\n\n\n\n\nIntroduction\nThe xrnet R package is an extension of regularized regression\n(i.e.\xa0ridge regression) that enables the incorporation of external data\nthat may be informative for the effects of predictors on an outcome of\ninterest. Let (y) be an n-dimensional observed outcome vector, (X)\nbe a set of p potential predictors observed on the n observations,\nand (Z) be a set of q external features available for the p\npredictors. Our model builds off the standard two-level hierarchical\nregression model,\n\n\nbut allows regularization of both the predictors and the external\nfeatures, where beta is the vector of coefficients describing the\nassociation of each predictor with the outcome and alpha is the vector\nof coefficients describing the association of each external feature with\nthe predictor coefficients, beta. As an example, assume that the outcome\nis continuous and that we want to apply a ridge penalty to the\npredictors and lasso penalty to the external features. We minimize the\nfollowing objective function (ignoring intercept terms):\n\nNote that our model allows for the predictor coefficients, beta, to\nshrink towards potentially informative values based on the matrix (Z).\nIn the event the external data is not informative, we can shrink alpha\ntowards zero, returning back to a standard regularized regression. To\nefficiently fit the model, we rewrite this convex optimization with the\nvariable substitution (gamma = beta - Z * alpha). The problem is then\nsolved as a standard regularized regression in which we allow the\npenalty value and type (ridge / lasso) to be variable-specific:\n\nThis package extends the coordinate descent algorithm of Friedman et\nal.\xa02010 (used in the R package glmnet) to allow for this\nvariable-specific generalization and to fit the model described above.\nCurrently, we allow for continuous and binary outcomes, but plan to\nextend to other outcomes (i.e.\xa0survival) in the next release.\nInstallation\nFrom CRAN\ninstall.packages(""xrnet"")\nFrom Github (most up-to-date)\n\nOS-specific prerequisites\n\nWindows: Install\nRTools (not an\nR package)\nMac: If using R version >= 3.6.0, verify your GNU Fortran\nversion is >= 6.1. If you have an older version, go\nhere to install\nthe required version\n\n\nInstall the R package devtools\nInstall the xrnet package with the install_github() function\n(optionally install potentially unstable development branch)\n\n# Master branch\ndevtools::install_github(""USCbiostats/xrnet"")\n\n# Development branch\ndevtools::install_github(""USCbiostats/xrnet"", ref = ""development"")\nA First Example\nAs an example of how you might use xrnet, we have provided a small set\nof simulated external data variables (ext), predictors (x), and a\ncontinuous outcome variable (y). First, load the package and the example\ndata:\nlibrary(xrnet)\ndata(GaussianExample)\nFitting a Model\nTo fit a linear hierarchical regularized regression model, use the main\nxrnet function. At a minimum, you should specify the predictor matrix\nx, outcome variable y, and family (outcome distribution). The\nexternal option allows you to incorporate external data in the\nregularized regression model. If you do not include external data, a\nstandard regularized regression model will be fit. By default, a lasso\npenalty is applied to both the predictors and the external data.\nxrnet_model <- xrnet(\n  x = x_linear, \n  y = y_linear, \n  external = ext_linear, \n  family = ""gaussian""\n)\nModifying Regularization Terms\nTo modify the regularization terms and penalty path associated with the\npredictors or external data, you can use the define_penalty function.\nThis function allows you to configure the following regularization\nattributes:\n\nRegularization type\n\nRidge = 0\nElastic Net = (0, 1)\nLasso / Quantile = 1 (additional parameter quantile used to\nspecify quantile, not currently implemented)\n\n\nPenalty path\n\nNumber of penalty values in the full penalty path (default = 20)\nRatio of min(penalty) / max(penalty)\n\n\nUser-defined set of penalties\n\nAs an example, we may want to apply a ridge penalty to the x variables\nand a lasso penalty to the external data variables. In addition, we may\nwant to have 30 penalty values computed for the regularization path\nassociated with both x and external. We modify our model call to xrnet\nfollows.\n\npenalty_main is used to specify the regularization for the x\nvariables\npenalty_external is used to specify the regularization for the\nexternal variables\n\nxrnet_model <- xrnet(\n  x = x_linear, \n  y = y_linear, \n  external = ext_linear, \n  family = ""gaussian"", \n  penalty_main = define_penalty(0, num_penalty = 30),\n  penalty_external = define_penalty(1, num_penalty = 30)\n)\nHelper functions are also available to define the available penalty\ntypes (define_lasso, define_ridge, and define_enet). The example\nbelow exemplifies fitting a standard ridge regression model with 100\npenalty values by using the define_ridge helper function. As mentioned\npreviously, a standard regularized regression is fit if no external data\nis provided.\nxrnet_model <- xrnet(\n  x = x_linear, \n  y = y_linear, \n  family = ""gaussian"", \n  penalty_main = define_ridge(100)\n)\nTuning Penalty Parameters by Cross-Validation\nIn general, we need a method to determine the penalty values that\nproduce the optimal out-of-sample prediction. We provide a simple\ntwo-dimensional grid search that uses k-fold cross-validation to\ndetermine the optimal values for the penalties. The cross-validation\nfunction tune_xrnet is used as follows.\ncv_xrnet <- tune_xrnet(\n  x = x_linear, \n  y = y_linear, \n  external = ext_linear, \n  family = ""gaussian"",\n  penalty_main = define_ridge(),\n  penalty_external = define_lasso()\n)\nTo visualize the results of the cross-validation we provide a contour\nplot of the mean cross-validation error across the grid of penalties\nwith the plot function.\nplot(cv_xrnet)\n\nCross-validation error curves can also be generated with plot by\nfixing the value of either the penalty on x or the external penalty on\nexternal. By default, either penalty defaults the optimal penalty on\nx or external.\nplot(cv_xrnet, p = ""opt"")\n\nThe predict function can be used to predict responses and to obtain\nthe coefficient estimates at the optimal penalty combination (the\ndefault) or any other penalty combination that is within the penalty\npath(s). coef is a another help function that can be used to return\nthe coefficients for a combination of penalty values as well.\npredy <- predict(cv_xrnet, newdata = x_linear)\nestimates <- coef(cv_xrnet)\nUsing the bigmemory R package with xrnet\nAs an example of using bigmemory with xrnet, we have a provided a\nASCII file, x_linear.txt, that contains the data for x. The\nbigmemory function read.big.matrix() can be used to create a\nbig.matrix version of this file. The ASCII file is located under\ninst/extdata in this repository and is also included when you install\nthe R package. To access the file in the R package, use\nsystem.file(""extdata"", ""x_linear.txt"", package = ""xrnet"") as shown in\nthe example below.\nx_big <- bigmemory::read.big.matrix(system.file(""extdata"", ""x_linear.txt"", package = ""xrnet""), type = ""double"")\nWe can now fit a ridge regression model with the big.matrix version of\nthe data and verify that we get the same estimates:\nxrnet_model_big <- xrnet(\n  x = x_big, \n  y = y_linear, \n  family = ""gaussian"", \n  penalty_main = define_ridge(100)\n)\n\nall.equal(xrnet_model$beta0, xrnet_model_big$beta0)\n#> [1] TRUE\nall.equal(xrnet_model$betas, xrnet_model_big$betas)\n#> [1] TRUE\nall.equal(xrnet_model$alphas, xrnet_model_big$alphas)\n#> [1] TRUE\nContributing\nTo report a bug, ask a question, or propose a feature, create a new\nissue here. This project\nis released with the following Contributor Code of\nConduct.\nIf you would like to contribute, please abide by its terms.\nFunding\nSupported by National Cancer Institute Grant #1P01CA196596.\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': [""Point-Spread-NBA\nhttps://medium.com/the-sports-scientist/the-million-dollar-model-7c0b11208bf7\nWant to win millions of dollars betting on sports? So did I, which is why I decided to try to build a predictive model that would estimate the point spread in regular season NBA games. If you don’t know what point-spread is, that is okay, if you don’t have a clue about basketball that is also okay too. It's simple, it is a Linear Regression model predicting the point spread and winner of any NBA regular season games and is based on in-game statistics form 2018-2019 season and 2019-2020 (up until February 1st). Here is the model:\nPoint spread = 0 + 1.454(Difference-Field Goal %) + 0.275(Difference-Three Point %) + 0.200(Difference-Free Throw %) + 0.286(Difference-Rebounds) + 0.478(Difference-Offensive Rebounds) + 0.068(Difference-Assists) + (-0.999)( Difference-Total Turnovers) + 0.265(Difference-FT attempts) + 0.352(Difference-3P attempts)\n""], 'url_profile': 'https://github.com/markus-proesch', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MarioVicuna', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kishoremanamala', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}","{'location': 'Minneapolis, Minnesota', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Breast-Cancer-Classification-Algorithm\nLeverage advanced predictive learning techniques like logistic regression, k nearest neighbours, decision trees, random boost, gradient boost and neural networks to predict whether an individual has breast cancer\n'], 'url_profile': 'https://github.com/PriyankaRamadas', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated Mar 2, 2020', '1', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020']}"
"{'location': 'Mexico', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Bayesian-Inference-Class\nThis repo cointains a brief introduction to bayesian statistics, incluiding:\n\nparameter estimation\nhypothesis testing\nlinear regression\n\nand a bried example of bayesian neural networks, unfortunaly it is only in Spanish.\n'], 'url_profile': 'https://github.com/NunezKant', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marce-edem', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Stock-Predictor-for-Facebook\nA Python program which makes use of Machine Learning Algorithm named Support Vector Regression (SVR) Model to predict stock price of FB for a specific day\n'], 'url_profile': 'https://github.com/deeppatne', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '328 contributions\n        in the last year', 'description': [""\nAnalysis and Prediction of Successful Kickstarter Projects\nTeam\n\nSamuel Guo\nJoanna Kim\nSpencer Zou\nBrandon Lee\n\nAbstract\nThis project is aimed at the following questions:\n\nWhat are the overarching similarities and key features of successful Kickstarter projects?\nHow accurately can one predict project success or failure, given the characteristics of a new project?\n\nAs follows, the purpose of this project is to gain insight on what truly makes up a successul Kickstarter campaign, and whether or not that success can be replicated. The importance of having the regression analysis in addition to the machine learning model is to highlight the reasons and concepts behind an accurate prediction. Just as essential as the ability to have a ML program predict project success is the understanding of the underlying forces and factors that guide the prediction, and as such we have made it a priority of this project to draw real, conversable conclusions for data scientists and casual readers alike.\nOn the technical side, the project is done in R and Python, as is standard. Notable R packages include dplyr, ggplot, and Shiny, while the bulk of the Python work is done with the assistance of the essential machine learning library sklearn. An honorable mention to Microsoft Excel for many of the graphs and visuals.\nPremise and Motivation\nKickstarter has been one of the premiere crowdfunding platforms since its launch in 2009. It is now home to over 445,000 successfully backed projects. While crowdfunding remains a great resource for any aspiring product designers and entrepreneurs, taking care of a campaign still requires a decently significant amount of time and money, with no guarantee or indicator of success. This project attempts to provide potential campaign managers with information and insight that can be used to maximize the probability of success for a certain project, as well as provide areas of interest that can later be researched further by the project manager.\nTable of Contents\n\nPrepping Data\nExploratory Insight\nStatistical Models and Analysis\nMachine Learning\nRelevant Conclusions and Applications\nNext Steps\n\nPrepping Data\nThe raw data for this project comes from Kaggle: https://www.kaggle.com/yashkantharia/kickstarter-campaigns/data. It's a 32 MB dataset with 170731 unique projects, along with the following variables:\n\nID\nName\nCurrency\nMain Category\nSubcategory\nLaunch Time\nDeadline\nDuration\nGoal in USD\nCity\nState\nCountry\nBlurb Length\nName Length\nStart Month\nEnd Month\nStart Quarter\nEnd Quarter\nStatus (Successful/Failed)\nAmount Pledged in USD\n\nWhich looks like this pixelated mess in the R Data Viewer:\n\nDuplicate rows were removed and time-and-date formats for Launch Time and Deadline were converted to simple numbers in Excel. Somehow, there were no missing values for any of the 170,731 projects. With the dataset cleaned, we then moved on to narrowing down the variables.\nMany of the variables were easy to identify as being correlated with other variables. For example, Launch Time had a direct relationship with Deadline through the Duration variable (and the same for Start Month and End Month, which also happened to be closely related to Start Quarter and End Quarter). Similarly, City, State, Currency, and Country also shared a relationship, and Currency was an obvious choice for removal. City and State were also ultimately removed due to the lack of data within most of the levels. More on that in the next section.\nTo confirm our suspicions, we simply had to check the correlation between the variables in question. Using the previous example, because we believed that Deadline and Launch Time were not both necessary to the project because they were closely linked, we ran the following:\n> cor(data$launched_at, data$deadline)\n[1] 0.9998613\nA correlation of almost 1 meant that these variables were far too correlated to be both included in the model, so we decided to drop Deadline from the starting model. But just because Launch Time and Deadline were correlated through Duration did not mean that Duration was necessarily a bad variable. So we tested the correlation between Launch Time and Deadline:\n> cor(data$launched_at, data$duration)\n[1] -0.05183889\nWe were correct. Deadline was (mostly) independent of Launch Time, so it was allowed to stay in the model. So on and so forth until we were sure that all of our variables were independent and useful on their own, each bringing something different to the party.\nSome variables were simply not helpful for the project, such as ID and Name. Other variables were counterproductive, such as Amount Pledged, which defeated the purpose of trying to predict success before the project finished its campaign. Amount Pledged was ultimately replaced with another variable: Percentage Funded. More on that in the next sections as well.\nAfter removing multicollinearity and irrelevant variables, we were left with the following model to start with:\n\nMain Category\nLaunch Time\nDuration\nGoal in USD\nCountry\nBlurb Length\nName Length\nStart Month\nStatus (Successful/Failed)\n\nOne final issue that would come up eventually was the discrepancy in success to failure ratio between our sample data and the population, which would lead to some trouble down the road in the Machine Learning section.\n\nBut leaving that foreshadowing for now, we pressed on into looking for initial trends in Exploratory Insight.\nExploratory Insight\nUsing Excel, we created visuals that would help us to gauge which variables would be the most helpful or interesting in the scope of this project.\n\nThe top five countries that had the most successes were either countries from Asia or Europe. This finding could indicate that citizens of each country may be more inclined to support kickstarters, European or Asian- based kickstarters could be more popular with the public, kickstarter-funding methods could be more popular in Asia or Europe, or there could be no correlation.\n\nThe categories with the highest amount of successes are (1) music, (2) film & video, and (3) publishing, all categories that pertain more to artistic ventures. The categories with the highest amount of failures are (1) technology, (2) food, and (3) film & video. Technology and food are distinct in terms of categorization. Based on this data, it is possible that artistic kickstarters will be more likely to be successful and kickstarters that pertain to technology or food may be more likely to fail. However, because film & video is also the top third failure, this may show that the factor in high successes or failures may depend on the number of the kickstarters in each category, which means a higher success or failure count. This muddles the clarity of what category may lead to a more succesful kickstarter.\n\nThe percentage of success and failure will be more indicative of what leads to a successful kickstarter. Although music, film & video, and publishing had the highest success count, (1) comics, (2) dance, (3) publishing have the highest percentage of success. The kickstarters with the highest percentage of failure are (1) food, (2) journalism, (3) technology. There are some similarities with the percentage of success and failure versus the literal count of success and failure. Comics have the highest percentage of success while food has the highest percentage of failure.\n\nThe measures of central tendency for the kickstarter goals may indicate that a higher goal leads to a decreased likelihood of success, based on the median goal, mean goal, and trimmed mean goal of failed kickstarters being significantly higher than that of successful kickstarters. Interestingly, however, the mode goal of successful and failed kickstarters were both the same at $5000.\n\nBased on the means of name length and blurb length, there seems to be little to no correlation between name length and success or blurb length and success, as the means of name length and blurb length for successes and the means of name length and blurb length for failures are quite similar.\n\nThis graph displaying the relationship between starting month and success/failure seems to indicate that the starting month of a kickstarter project may be influential in determining its success, as the the proportions of success for each month varies considerably. The starting months that seem to have the lowest proportions of success are (1) December, (2) July, and (3) August. It is also worth noting that in general less kickstarters seem to be started during the period of December to February.\n...and that's about as pretty as the graphs get. In the next section we dive deep into R, with a lot more statistics and less aesthetically pleasing visuals.\nExtra Notes: Why did we remove Subcategory, City, and State as variables? Well, we can't even show the graphs here because they're too cluttered, but there were over 100 levels for each of those variables. Essentially, there were some subcategories with a good amount of training data, but the vast majority were random subcategories and cities that only belonged to one or two projects, which would lead to overfitting later down the line. And we were comfortable removing them because we had better versions of them in the model, namely the Main Category and Country variables, where each level had a suitable amount of examples to work with.\nStatistical Models and Analysis\nRunning multilinear regression on a binary response variable usually leads to muddled and not-too-impressive results, so the next question for us was: how can you quantify success or failure?\nTo answer that question, we replaced the Status variable with Percentage Funded, which was just Amount Pledged divided by Goal in USD. Having a numerical response variable would give us more insight into how the variables actually influenced the progress of each project rather than just a 1 or 0.\nWe began by generating scatter matrixes and heatmaps to give us an idea of which variables were most likely to prove useful in the model. And because we were careless, we realized that we forgot to remove Launch Time because it was heavily intertwined with Start Month, as shown below in the scatter matrix.\n\nThis graph may look pretty daunting but what we're really after is just any obvious patterns on the plots. Patterns between the x variables indicate a multicollinearity issue, while patterns between an x and the y variable indicate a possible trend to look into. And it looked like there weren't really any of those, so we checked the correlation matrix of the numerical variables:\n> cor(numerics)\n                launched_at      duration      goal_usd  start_month blurb_length   name_length percent_funded\nlaunched_at     1.000000000 -0.0518388879  0.0031828420  0.057145032 -0.102879638  0.0230973728   0.0032271150\nduration       -0.051838888  1.0000000000  0.0259268388  0.009222666  0.026420766 -0.0176895277   0.0001854683\ngoal_usd        0.003182842  0.0259268388  1.0000000000  0.002731183 -0.003440994 -0.0057589789  -0.0006447432\nstart_month     0.057145032  0.0092226664  0.0027311827  1.000000000 -0.032152307  0.0107324617   0.0033825942\nblurb_length   -0.102879638  0.0264207656 -0.0034409942 -0.032152307  1.000000000  0.1371042990  -0.0031606274\nname_length     0.023097373 -0.0176895277 -0.0057589789  0.010732462  0.137104299  1.0000000000   0.0007393314\npercent_funded  0.003227115  0.0001854683 -0.0006447432  0.003382594 -0.003160627  0.0007393314   1.0000000000\nThis graph shows the correlation between all variables on a scale from 0 to 1. You can see that every variable has a perfect correlation with itself. At this point we got a little worried, because the numbers we were concerned about (the bottom row) were very, very small. It was clear that Percentage Funded was far too nuanced of a variable to be influenced by our chosen predictors, so as final tests, we ran a basic stepwise regression to see which model it would pick:\n> mod0 = lm(y~1) # The most simple model: y versus 1\n> mod.upper = lm(y~x1+x2+x3+x4+x5+x6+x7+x8) # The most complex model: y versus all\n> step(mod0, scope = list (lower = mod0, upper = mod.upper)) # The step function will find the best middle ground\nStart:  AIC=3461112\ny ~ 1\n\n       Df  Sum of Sq        RSS     AIC\n<none>               1.0877e+14 3461112\n+ x6    1 1.2445e+09 1.0877e+14 3461112\n+ x1    1 1.1328e+09 1.0877e+14 3461112\n+ x7    1 1.0866e+09 1.0877e+14 3461113\n+ x8    1 5.9455e+07 1.0877e+14 3461114\n+ x5    1 4.5215e+07 1.0877e+14 3461114\n+ x3    1 3.7415e+06 1.0877e+14 3461114\n+ x2   14 1.5628e+10 1.0875e+14 3461116\n+ x4   21 3.5854e+09 1.0877e+14 3461149\n\nCall:\nlm(formula = y ~ 1)\n\nCoefficients:\n(Intercept)  \n      467.8 \nAnd not too surprisingly, it chose the model with none of our variables in it. Which means that our entire model is unsuitable for any sort of regression with Percentage Funded as the response variable.\nAs a final nail in the coffin, these are the stats for the full model:\nResidual standard error: 25240 on 170688 degrees of freedom\nMultiple R-squared:  0.0002117,\tAdjusted R-squared:  -2.845e-05 \nF-statistic: 0.8815 on 41 and 170688 DF,  p-value: 0.686\nMeaning that our model explains 0.02117% of the variance in Percentage Funded. So we were forced to conclude that Percentage Funded was a big mistake. While discouraging, that's okay. It was a good try and it made sense at the time, and now we came out with valuable insight: this data is entirely insufficient at predicting a wide-range continuous number like Percentage Funded.\n\nMachine Learning\nRelevant Conclusions and Applications\nNext Steps\nIn conclusion, our regression models and random forest machine learning do a mediocre job at predicting. There are no strong correlations that point to any solid conclusions and the machine learning predictions have a couple of false postives and false negatives. In the future, we will learn how to use different models to derive more evidence towards concrete results. To make the significance of our project outcome more convincing, we are planning to learn how to analyze our statistical models in greater depth, allowing us to better communicate any conclusions we are able to deduce.\n""], 'url_profile': 'https://github.com/TokyoExpress', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/golfalot', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'Nashville, TN', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['DataMining-DS740-FinalProject\nThis project uses machine learning (linear and penalized regression) to see which attributes may best predict the total volume and average price of avocados.\n'], 'url_profile': 'https://github.com/dbuckin', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Piyush1535', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': [""ad_click_prediction\nPredicting from a user's data if he/she will click on the ad. I have used logistic regression model for prediction. The model gives accuracy of 91%\n""], 'url_profile': 'https://github.com/Vrushil', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/plaskas', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['strm\nstrm\nstrm is an R package that fits spatio-temporal regression model based on Chi & Zhu Spatial Regression Models for the Social Sciences (2019). The approach here fits a simultaneous spatial error model (SAR) while incorporating a temporally lagged response variable and temporally lagged explanatory variables.\nThis package builds on the errorsarlm() function from the spatialreg package.\nThis package is still under development. Please report bugs or constructive tips to issues here.\nInstallation\nstrm was built on R version 4.0.2 (""Taking Off Again"").\nPackage dependencies include:\n- R (>= 3.6),\n- spatialreg (>= 1.1-5),\n\nPackage imports include:\n- dplyr (>= 1.0.0),\n- rlang (>= 0.4.6),\n- tidyr (>= 1.0.0),\n- purrr (>= 0.3.4),\n- magrittr (>= 1.5),\n- rgdal (>= 1.5.10),\n- spdep (>= 1.1.3),\n- lazyeval,\n- stats,\n- grDevices,\n- methods,\n- graphics,\n- utils,\n- knitr,\n- testthat (>= 2.3.2),\n- rmarkdown (>= 2.3)\n\nPackage suggests include:\n- splm (>= 1.4.11),\n- spdep (>= 1.1-3),\n- sf (>= 0.9-4),\n- Ecdat (>= 0.3-7),\n- tidycensus (>= 0.9.9),\n- ggplot2 (3.3.2),\n- patchwork (>= 1.0.1),\n- broom (>= 0.7.0)\n\nTo download the latest version of strm:\nlibrary(""devtools"")\ndevtools::install_github(""mkamenet3/strm"")\n\n\n'], 'url_profile': 'https://github.com/mkamenet3', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'C#', 'Updated Feb 24, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'R', 'Updated Sep 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '606 contributions\n        in the last year', 'description': ['Introduction\nThis repo stores my work for an intro level course to data science. It includes data cleaning and clearing, plotting, principal component analysis, multi-dimmensional-scaling, linear regression, bi-variate and multi-variate logistic regression, various clustering techniques including:Agglomerative methods, K-Means, Spectral. Classification via the Receiver operating characteristic (ROC) curve and Maximum Likelihood Estimation.\nExamples\nFigure 1.1: Classical Multi-Dimensional Scaling, showing grouping based on Pulsar status.\n\nFigure 1.2: Ward clustering, similar grouping compared to MDS of pulsar data.\n\nFigure 2.1: Ad-Hoc testing using Mult-Comp after ANOVA testing.\n\nFigure 3.1: Bi-Plot showing  results of principal component analysis, shows varibles most responsible for variance, signified by magnitude of line.\n\nFigure 4.1: Receiving Operating Characteristic Curve, showing FPR TPR rates for various threshholds chosen.\n\nFigure 6.1: Linear Regression.\n\n'], 'url_profile': 'https://github.com/dg022', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': 'Stillwater, Oklahoma', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['Churn-Analysis-using-Logistic-Regression, Decision Trees and Random Forest\nCustomer churn, also known as customer attrition, is the loss of clients or customers. Churn is an important business metric for subscription-based services such as telecommunications companies.\nDataset        : https://www.kaggle.com/blastchar/telco-customer-churn\nTools used     : Python\nLibraries used : pandas, numpy, matplotlib, sklearn, seaborn,\nConcepts       : Imputation, SMOTE, chisquare test of independence, over-sampling, data partitioning\nAlgorithms     : Logistic Regression, Decision Trees, Bagging, Random Forest, Gradient Boosting\n'], 'url_profile': 'https://github.com/HarishPatlolla', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': 'Rochester, NY', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Risk Prediction for Home Equity Line of Credit(HELOC)\n\nProject Outline\nWe develop a predictive model and a decision support system(DSS) that evaluates the risk of Home Equity Line of Credit (HELOC) applications with Streamlit.\nData\nThe dataset and additional information can be found on here. This also excel sheets\nalong with its decision rules\nDetails\nUsing the dataset, we developed a predictive model to assess credit risk. We compared various models and different algorithms and shortlisted the most appropriate algorithm for prediction depending upon the accuracy. We have also created a prototype of an interactive interface that sales representatives in a bank/credit card company can use to decide on accepting or rejecting applications.\nApproach\nData cleaning\nFirstly, we see the percentage of pf -7, -8, -9 of the total rows. We drop the rows containing special values such as -9. As we have a significant number of values of -7 & -8, dropping those values would result in less number of datapoints which would result in in accurate evaluation of our models. Besides, if we take average of these special values, it would result in complete change of original meaning of these values. Hence, we could not move forward with same. Thus, post initial research we can say that the special values -7, -8 does have certain relevance in order to arrive at our results. Further, we separated Independent (X) variables from Independent Variables (Y). We considered all the variables except RiskPerformance as an\nindependent variables and Risk Performance was the predicted variable.\nPost separation, we had reconstructed the variables Max Delqz Public’/ ‘Max Delq Ever’ using get_dummies and we were able to recreate Independent Variables. We do so because the category represents delinquency status and not all values are numeric. They are majorly categorical variables.\nModeling and evaluation\nWe used various models to create prediction and followed by certain parameters such RSME and accuracy to evaluate these models. Some of the models included SVM, Logistic Regression KNN, Gaussian, Decision Tree classifier, Random Forest, Ada Boost classifier. We had evaluated these models on the lines of methods we learned during our classes. We had splitted\nthe data set into training and testing data on a loose ratio of 75:25 split.\nMoving forward, we used Hyper parameter tuning using GridSearch CV in order to find the best model among all. We created data frame which containing each of the model. Post this, we calculated the values of Root Mean Square error, accuracy, recall, precision and CV score to choose the best model having best accuracy and least error.\nLastly, we summarized the whole models and checked the value of best model\nAs we see from below, the accuracy of Ada Boost is highest which is 0.7287 and that the RMSE is the the lowest which is 0.520855 among other models. We also took into consideration Precision and Recall for arriving at better results.\nModel Comparison: \nInterface\nAn interface was developed using in order to represent respective models. We did data cleaning using pipeline and stream lit package for the interface.\nWe can input the row of information in the dataset using the side bars or the user can select the one of the rows in the data frames to make the prediction.\n\n\n'], 'url_profile': 'https://github.com/tongxinguo', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Материалы курса ""Интеллектуальный анализ данных"", НТУ ""Днепровская политехника"", 2020 г.\n\nЗнакомство с анализом данных и R\nРазведочный анализ данных\nКластерный анализ. Иерархическая кластеризация\nКластерный анализ. Метод k-средних\nПроверка статистических гипотез\nКорреляция и регрессия\nМногомерная линейная регрессия\nПрогнозирование временных рядов\nКлассификация. Метод k ближайших соседей. Кроссвалидация\nКлассификация. Деревья классификации и регрессии (CART)\nАнсамбли моделей\nПодготовка данных\n\n'], 'url_profile': 'https://github.com/dkhramov', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': 'College Station, TX', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['\n\n\noutput\n\n\n\n\ngithub_document\n\n\n\nsgslasso\nThis package implements the sparse group-subgroup lasso which  identifies important regressor groups, subgroups and individual variables.\nResults from the sparse group subgroup lasso (SGSL) are compared against combinations of group lasso and lasso. The corresponding references are:\nGarcia, T.P., MÃ¼ller, S., Carroll, R.J., and Walzem, R.L. (2013).\nIdentification of important regressor groups, subgroups, and individuals via regularization methods: application to gut\nmicrobiome data. Bioinformatics, DOI: 10.1093/bioinformatics/btt608.\nInstallation\nYou can install the development version from GitHub with:\n# install.packages(""devtools"")\ndevtools::install_github(""tpgarcia/sgslasso"", force=TRUE)\nExample\nThis is a simple example of using SGSL:\nlibrary(sgslasso)\n\n## Generate the data\nset.seed(1)\nN=30;\nL=10;\np.in.group =8;\np=L * p.in.group;\nsigma <- sqrt(1);\nbeta.coef <- matrix(0,nrow=2*L,ncol=(p/L)/2)\nbeta.coef[1,] <- c(6,6.4,6.6,8)/2\nbeta.coef[2,] <- c(6,6.4,6.6,8)/2\nbeta.coef[3,] <- c(6,6.6,6.6,8)/2\nbeta.coef[5,] <- c(12.5,12.5,0,0)/2\nbeta.coef <- beta.coef *2\np.group <- rep(p/L,L)\nindex.subgroup <- matrix(NA,nrow=L,ncol=p)\ntmp <- 0\nfor(k in 1:L){\n  if(k==1){\n    index.subgroup[k,1:p.group[k]] <- c(rep(1,(p/L)/2),rep(2,(p/L)/2))\n  } else {\n    ind <- 1:p.group[k] + sum(p.group[(k-1):1])\n    index.subgroup[k,ind] <- c(rep(k+tmp,(p/L)/2),rep(k+tmp+1,(p/L)/2))\n  }\n  tmp <- tmp + 1\n}\nout <- data.group(N,p.group,beta.coef,sigma)\ny <- out$y\nx <- out$X\n\n## Lasso variable selection\nout_lasso <- sgsl(x,y,type=""lasso"",index.subgroup = index.subgroup)\n\n## Group lasso variable selection among the groups only\n##out_group <- sgsl(x,y,type=""group"",index.subgroup = index.subgroup,tau=0.94)\n\n## Group lasso variable selection among the groups and subgroups\n##out_ggroup <- sgsl(x,y,type=""ggroup"",index.subgroup = index.subgroup,tau=0.94)\n\n## Group lasso variable selection among the groups and subgroups, and lasso among variables within each subgroup\n##out_ggroupind <- sgsl(x,y,type=""ggroupind"",index.subgroup = index.subgroup,tau=0.94)\n\n## Sparse group subgroup lasso\nout_sgsl <- sgsl(x,y,type=""sgsl"",index.subgroup=index.subgroup,tau=0.94)\n'], 'url_profile': 'https://github.com/tpgarcia', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}","{'location': '@Beijing, China', 'stats_list': [], 'contributions': '103 contributions\n        in the last year', 'description': ['https://github.com/Linjiayu6/Machine-Learning-Practice/wiki/ML-Notebook\n'], 'url_profile': 'https://github.com/Linjiayu6', 'info_list': ['HTML', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '2', 'HTML', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', '1', 'MATLAB', 'Updated Apr 19, 2020']}"
"{'location': 'Coimbatore', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kavinila-Annadurai', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ines2607', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Boston,MA', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kajasaran', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': [""Loan_Default_Analysis\nAll business including the business of banking requires top line growth in terms of volumes of business to increase the bottom line of profit growth. Loan Default is the possibility of a loss resulting from a borrower's failure to repay a loan or meet contractual obligations. Loan Default also describes the risk that a bond issuer may fail to make payment when requested or that an insurance company will be unable to pay a claim. Thus, Loan default is always the threat to any financial institution and should be predicted in advance based on various features of the applicant.\nThis study aims at applying machine learning models, including Decision tree, Logistic regression, Random forest and Ada Boost classifier to classify applicants with and without loan default from a group of predicting variables, and evaluate their performance, based on the loan issued by XYZ Co-operation through 2007-2015.\n""], 'url_profile': 'https://github.com/ArchanaKPrasad', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '290 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sahilee26', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Tirunelveli, Tamilnadu', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sridhar1997', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kavinila-Annadurai', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '303 contributions\n        in the last year', 'description': ['Image-Compression\n'], 'url_profile': 'https://github.com/alireza-alizadegan', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Nashville', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NLP-LogisticRegression\n'], 'url_profile': 'https://github.com/karunagujar13', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['cryptocurrency_price_analysis-\nIt’s a data analysis project. Getting data from an exchange, plotting graphs of price evolution, comparing several cryptocurrencies, outputting distribution returns. I went further by using clustering methods and linear regression. I mainly used Pandas, matplotlib, numpy, sklearn libraries.\n'], 'url_profile': 'https://github.com/veusti75', 'info_list': ['Jupyter Notebook', 'Updated Feb 28, 2020', 'HTML', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2021', '1', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 28, 2020', 'MATLAB', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Akabhishek067', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HananGit', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Earned-Salaries-Compared-to-their-Tenure-in-Baltimore-City\nUsing open data from Open Baltimore to predict the annual earned salary based on the tenure in Excel\nBackground\nWith the data from Open Baltimore, I tried to predict the annual salary by regressing the annual earned salary over the tenure in Baltimore City.\nOpen Baltimore contains salary data for each officers working in the government. For each individual, there is name, job titile, department, identifier, hired data, annual salary, gross salary.\nApproach\nI calculated the tenure for each individuals based on their employed date. I built regression model for health department and the general services department. The predication for general services department is better than that for health department.\nThe reasons why that happpened is probaly the wide range of job positions in the health department made it difficult to find a good fit.\nConclusion\nThe chart below shows the relationship between the tenure and annual earned salary for health department:\n\nThe chart below shows the relationship between the tenure and annual earned salary for General Services department:\n\n'], 'url_profile': 'https://github.com/lshan6', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'Recife, Brazil', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': [""Future-Sales-Prediction\nO objetivo desta competição disponível no Kaggle é a obtenção de modelos de predição para obter estimativas sobre o volume de vendas futuro de alguns itens. Na abordagem aqui apresentada, optou-se por comparar os resultados entre a aplicação de alguns modelos de machine learning.\n1. Exploração dos dados\nA primeira etapa do pipeline de análise definido foi explorar a base de dados a fim de compreender os registros existentes, verificar a existência de missing values e outras anomalias nos dados. A partir dessa análise, foi possíevl extrair insights como pro exemplo:\n\nRelação entre o preço do item e seu volume de vendas mensais\nQuais produtos são os mais vendidos\nQuais lojas possuem os maiores volumes de vendas\n\n\n2. Preparação dos dados para aplicação do modelo\nApós identificar as particularidades dos dados e identificar potenciais features relevantes para o modelo, a próxima etapa foi converter os dados em formatos adequados para a aplicação do modelo, o que envolve o tratamento de dados anômalos e a conversão de variáveis categóricas.\n\n3. Aplicação do modelo e avaliação dos resultados\nCom o dataframe preparado, é hora de aplicar o modelo e avaliar os resultados:\nRandom Forest\n\nRegressão Linear\n\n4. Melhorando o modelo com feature engineering\nComo os resultados obtidos não foram satisfatórios, buscou-se formas de aumentar a performance do modelo através de um processo mais intenso de feature engineering. Foram criadas 14 novas features a partir das variáveis já existentes, utilizando valores máximos, mínimos, médias e relações entre as variáveis, além de procedimentos de feature scaling. Após essas mudanças, o modelo de regressão linear saiu de um erro quadrático médio (RMSE) de 72 para aproximadamente 13, uma melhora de quase 80% na performance do modelo. Essas modificações estão no aqruivo ipynb 'model_improvement'.\n5. Instalação Apache Spark\nOS & Linux: https://medium.com/luckspark/installing-spark-2-3-0-on-macos-high-sierra-276a127b8b85\nWindows: https://changhsinlee.com/install-pyspark-windows-jupyter/\n6. Competição e dados\nhttps://www.kaggle.com/c/data-science-bowl-2019\n""], 'url_profile': 'https://github.com/luisgustavob78', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'Maasmechelen, Belgium', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VincentBuekers', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': [""Optimal Real Estate Investment in Ames, Iowa\nREADME Contents\n\nIntroduction\nMethodology\nData Dictionary Used\nAnalysis and Findings\n\nComparison of Regressions for Models\nCoeffecients on Individual Features\n\n\nConclusions, Recommendations, & Further Research\nSources\n\nIntroduction and Problem Statement \nReal estate investing is a research and capital intensive endeavor. When considering a property to invest in, one must consider the factors that contribute to property value in that area. For example, square footage in Manhattan makes a much bigger difference than a few hours in upstate New York where land is abundant. For example the table below illustrates the differences in ‘important factors’ between New York State and New York City according to their official government websites.\n\n\n\nNew York State\nNew York City\n\n\n\n\nNeighborhood\nIncome and Expense\n\n\nSchool District\nUtility Company\n\n\nCurb Appeal\nNearby office buildings, factories, stores, hotels\n\n\nAppliance Condition\nProperty Value of Nearby Units Last 3 Years\n\n\n\nAdditionally, firms and individual investors typically take on two different styles of property investing. Either holding for the long term and expecting the price of a property to go up based on the price of land in the area, the value of the neighborhood, the house style, etc. Other types of investors take a more active role and look to make a profit from remodelling or renovating a house, placing more value on things such as the functionality of a house, the appliance quality, whether or not it has pools, fences, etc.\nThis model aims to figure out which investing style is best fitted for Ames, Iowa.\nMethodology \nOur dataset contained many features (listed in the data dictionary) that could be used to predict the price of a house in Ames. Instead of looking at correlations for all the features on sale price and building a model around that, we decided to select our features based on what a firm or individual investor typically looks at.The logic behind this was that these models would better fit the skillset of investors. This means that we still included features that might have low individual correlations because we still deemed them part of a 'type' of investor's 'toolbox'. Investors that are more passive have more knowledge and experience investing in property based on land prices, the region, the building type, features that are ‘static’ and can’t be changed. Firm and individuals that take a more active role in investing and flip houses might not know too much in those areas but are good with upgrading pools, fences, appliances, the basement, functionality of a home, features we call ‘dynamic’ or features that can be changed. Our model aimed to represent the expertise of each type of investor looking at Ames, Iowa. Our results and regressions tell us if that type of investor should look at Ames, Iowa or not.\nBecause of this our feature selection was very discretionary. The static model chose features that we considered static. This included\nYear of House Built\nLot Area\nNeighborhood\nLand Contour\nPavement\nFoundation\nBuilding Type\nZoning\nOur dynamic model contained features that were scores determining the quality and abundance of various features including.\nPool quality\nFences\nFireplaces\nBasement quality\nHouse functionality (heating, electrical, kitchen quality)\nGarage score\nExterior Quality\nWe then ran several regressions on our scaled data to determine which model best predicted the price of a house in Ames. We based on how ‘good’ a model performed based on the bias presented in the training data, and variance after looking at the test data.\nData Dictionary \n\n\n\nDataset\nDescription\nLink\n\n\n\n\nData Dictionary\nAmes Iowa 2010 Real Estate Data Dictionary\nhttp://jse.amstat.org/v19n3/decock/DataDocumentation.txt\n\n\n\nAnalysis and Findings \nComparison of Regressions for Models \n\n\n\nModel 1\nLinear\nLasso\nRidge\n\n\n\n\nTrain\n0.603\n0.559\n0.558\n\n\nTest\n0.646\n0.646\n0.645\n\n\n\n\n\n\nModel 1\nLinear\nLasso\nRidge\n\n\n\n\nTrain\n0.549\n0.543\n0.541\n\n\nTest\n0.602\n0.602\n0.602\n\n\n\nFor linear regression our models were not scaled however for our scaled models and for that reason is why our Model 1 looked like a clear winner. However with our scaled data, Model 2 performed better having close to the same R-squared as Model with half the variance. Model 1 was significantly over fit in our Lasso and Ridge regressions and Model 2 had significantly less variance than Model 1.\nCoeffecients and R-Squared on Individual Features \nSome interesting observations are that the extra features for the house such as pool, fences, porches, fireplaces, etc don't really contribute to the price of a house however the overall functionality largely determines price. This however could also be correlated with the year a house was built as the quality of appliances is better, this could possible represent a collinearity that was not considered at first as shown by the correlation between Year Built and house price.\n\n\nAdditionally there doesn't seem to be much of a correlation because Lot Area (square footage) and house price. This could possibly be because Iowa is a very rural area where land price is not as expensive therefore does not have a significant effect on house price.\n\nConclusions and Recommendations \nThe recommendation here would be for investors looking in Ames Iowa to focus more on the functionality of a house as shown by our model. Although only a slight difference, the models have shown that the features inside a house, or the functionality of a house are better determinants of house price.\nSources\nNY State Property Value Factors: https://www.tax.ny.gov/pubs_and_bulls/orpts/mv_estimates.htm\nNYC Property Value Factors: https://www1.nyc.gov/site/finance/taxes/property-determining-your-market-value.page\n\n\n""], 'url_profile': 'https://github.com/rnsfrd', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshallbangar', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['python-machine-learning-basic\nIn order to develop all of the exercises of this repo, the instructor follows Python Machine Learning 2nd Edition by Sebastian Raschka, Packt Publishing Ltd. 2017 as a reference book.\nHomework1: Logistic Regression and SVM\nI/ Logistic Regression v.s. SVM with Iris dataset:\n\n\nLoad (all features) and Split the Iris dataset to training and test sets with ratio 80% and 20%, respectively (c.f. block [4] and [5] of the nbviewer of Chapter 3).\n\n\nFit your data using Logistic Regression in sklearns. Then plot the decision regions with C=10,100,1000,5000 (generate one figure for each value of C). The x-axis and y-axis of each figure correspond to feature ""sepal length"" and ""petal length"", correspondingly (c.f. block [16] of the nbviewer of Chapter 3).\n\n\nPlot the accuracy_score (in sklearns) of your predictor with respect to C=10−x,x=−4,−3,...,3,4.\n\n\nRepeat steps 1~3 above, using SVM instead of logistic regression, with kernel ""RBF"", for two cases of  γ=0.1 and 10.\n\n\nCompare two types of predictor (give insight comments)\n\n\nII/ SVM for XOR data:\n\n\nGenerate 1000 random samples of XOR data (c.f. block [23] of nbviewer of Chapter 3). Split 70% and 30% for training and test sets.\n\n\nUse SVM to fit your data with kernel ""RBF"" and γ=0.2. Then plot the decision regions with C=10,100,1000,5000 (generate one figure for each value of C).\n\n\nPlot the accuracy score with respect to γ=0.1,…,100\n\n\nHW2: PCA and LDA\nI/ PCA Iris dataset:\n\n\nLoad (all features, using Pandas), standardize this d-dimension dataset (d is number of features) and Split the Iris dataset to training and test sets with ratio 70% and 30%, respectively.\n\n\nWrite your own function to calculate the covariance matrix. Then compute the eigenvalues and eigenvectors of this matrix.\n\n\nPlot the cumulative variance ratio (c.f. block [11] of the nbviewer of Chapter 5).\n\n\nChoose the k=3 eigenvectors that correspond to the k largest eigenvalues to construct a d×k-dimensional transformation matrix W ; the eigenvectors are the columns of this matrix\n\n\nProject the samples onto the new feature subspace, and plot the projected data using the transformation matrix W (c.f. block [14] of the nbviewer of Chapter 5)\n\n\nII/ LDA for Iris:\nRepeat the steps above of PCA, but using the matrix S−1WSB instead of covariance matrix.\nHW3: Model Evaluation and Hyperparameter Tuning\nIn this HW, we will use wine dataset, which can be loaded as input [4] of this notebook\n\n\nSplit dataset into ratio 7:3 for training and test sets, respectively. Then use pipeline with StandardScaler(), PCA (n=3), and SVM with RBF kernel to fit the training set and predict the test set. Report the accuracy score.\n\n\nUse StratifiedKFold cross-validation to report the accuracy score (mean with std). What are differences between standard k-fold and StratifiedKFold? What are differences between StratifiedKFold and cross_val_score (in [12] and [13] of this notebook)?\n\n\nWhat are the differences between ""learning curve"" and ""validation curve"" tools in sklearns.model_selection? Report the figures of learning curve and validation curve similar to input [15] and [16] of this notebook, respectively. Based on the figure, indicate which is the best value C you should choose.\n\n\nUsing GridSearchCV to find the best hyperparameter (similar to [17] of this notebook). Compare the accuracy score using these GridSearchCV parameters with previous methods.\n\n\nReport the confusion matrix of the above prediction model using GridSearchCV.\n\n\nReport the precision and recall scores as in [29] and the best scores and best parameter of GridSearchCV as in [30] of this notebook.\n\n\nPlotting the ROCs for every pair combination of classes as in [31] of this notebook, or use the 3 dimension ROCs if it is available.\n\n\nHW4: Sentiment Analysis\nFor sentiment analysis, apply similar the technique starting from input [24] of this notebook  with\n\n\nOnly use 10,000 documents for training and test sets.\n\n\nUse the classification SVM.\n\n\nHW5: Clustering\nIn this homework, we use the Iris dataset for comparing different clustering algorithms:\nI) Use K-means with following parameters:\nn_clusters=3,\ninit=\'k-means++\',\nn_init=10,\nmax_iter=300,\ntol=1e-04,\nrandom_state=0,\na) Plot the clusters in 3-D plot with 3 features and 2-D plot with 2 features (similar to input     [6] of this  notebook, you can choose any combination that shows  clear clusters)\nb) Use elbow method to plot and choose the best ""k"", similar to inputs [7] and [8] of this notebook.\nc) Show the silhouette plots of these clusters similar to input  [9] of this  notebook.\nd) Show the silhouette plots of K-means with n_clusters=2, 3, and 4.\nII) Performing the hierarchical clustering on a distance matrix, similar to inputs [14] to [19] of this notebook.\nIII) Performing the DBSCAN and compare it to K-means and agglomerative clustering, similar to inputs [24] to [25] of this notebook.\nHW6 - Feedforward Neural Networks\nI) Theory: Derive the following parts:\n\n\nThe backpropagation algorithm\n\n\nThe derivative of sigmoid function\n\n\nII) Coding: Following the content and codes of this chapter, the dataset is of course MNIST\n\n\nCompare the accuracy results between 20 and 40 hidden neuron network,  using the same 1,000 training images, cross-entropy cost function, learning rate of η=0.5, mini-batch size of 10,  and 300 epochs.\n\n\nWith a 40 hidden neuron network, compare the accuracy results between 1,000 and 5,000 training images, using cross-entropy cost function, learning rate of η=0.5, mini-batch size of 10,  and 300 epochs.\n\n\nWith a 40 hidden neuron network, compare the accuracy results between with and without regularization, using the same 1,000 training images, cross-entropy cost function, learning rate of η=0.5, mini-batch size of 10,  and 300 epochs, λ is chosen in {0.1, 1, 10}\n\n\nUsing heuristic approach, find the best hyper-parameters for learning a  40 hidden neuron network using the same 3,000 training images, cross-entropy cost function, mini-batch size of 10,  and 300 epochs. Show the accuracy of learning epochs with these chosen parameters.\n\n\nHW7 - Deep Neural Network using TensorFlow or PyTorch\nPerform the following tasks using either TensorFlow or PyTorch (Many reports that PyTorch is more intuitive)\n\n\nCompare the accuracy results between 30 and 50 hidden neuron network,  using the same 1,000 training images, cross-entropy cost function, learning rate of η=0.5, mini-batch size of 10,  and 300 epochs.\n\n\nWith a 40 hidden neuron network, compare the accuracy results between 1,000 and 5,000 training images, using cross-entropy cost function, learning rate of η=0.5, mini-batch size of 10,  and 300 epochs.\n\n\nWith a 40 hidden neuron network, compare the accuracy results between L1 and L2 regularization, using the same 1,000 training images, cross-entropy cost function, learning rate of η=0.5, mini-batch size of 10,  and 300 epochs, λ is chosen in {0.1, 1, 10}\n\n\nComparing different activation functions: ReLU, sigmoid, tanh, by using best hyper-parameters for learning a  40 hidden neuron network using the same 3,000 training images, cross-entropy cost function, mini-batch size of 10,  and 300 epochs. Show the accuracy of learning epochs of compared methods.\n\n\n'], 'url_profile': 'https://github.com/munir-bd', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""ai-accelerator-recipe-Churn-Modelling\nThis use case helps to find out customer churn in banking. It uses the 'Churn_Modelling.csv' dataset. It uses standard Python modules. After necessary data cleaning, EDA, feature engineering and data preparation for fitting models, Logistic Regression, SVM and Ensemble models have been used to predict customer churn.\n""], 'url_profile': 'https://github.com/hcl-rbs-ai-accelerator-3', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MarioVicuna', 'info_list': ['Updated Feb 25, 2020', 'R', 'Updated Mar 6, 2020', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 13, 2020', 'R', 'Updated Sep 19, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'MIT license', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020']}"
"{'location': 'New York', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rutvik8795', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sahanashreedhar27', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kavinila-Annadurai', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['Automated_PCA_MDIBL\nThis repository contains a pipeline that identifies unexpected variables in an expression data matrix. It performs normalization on the count matrix, PC Analysis, and regression on the PCs vs. experimental design. Once meaningful principal components are identified, their coordinates are captured into a modified design file, to perform further regression and, in case no correlation is found between PC and design equation variables, to be used for downstream analysis as surrogates of unexpected variable(s).\n\nRequirements\nInput files format\nOperating instructions\nOutputs\nProvided files list\nCopyright and licensing\nContact information\nKnown bugs\nCredits and acknowledgments\n\n1. Requirements\nThe pipeline is written in R scripts called from a bash script.\nR version 3.6.2 (2019-12-12)\nRequired packages\n\nDESeq2\ndplyr\nfactoextra\nforestmangr\ngenefilter\nggplot2\njsonlite\nknitr\nreadr\nrmarkdown\nstringr\n\n2. Input files format\nBelow are the format requirements for the input files.\n\nEstimated counts matrix.\n\nText file, tab (""\\t"") separated.\nGene IDs are in rows, samples are in columns\nThe first column contains the row names (gene_ids)\nThe first row contains the column headers (""gene_id"" and then sample names)\n\n\n\nExample:\n\n\n\ngene_id\nsample1\nsample2\nsample3\nsample4\nsample5\nsample6\nsample7\nsample8\n\n\n\n\nENSMUSG00000000001\n2\n3\n4\n7\n3\n5\n1\n3\n\n\nENSMUSG00000000037\n10190\n4432\n2244\n2797\n2540\n15565\n4369\n12606\n\n\nENSMUSG00000000078\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nENSMUSG00000000085\n44\n8\n64\n59\n18\n32\n37\n7\n\n\n\n\nDesign matrix\n\nText file, tab (""\\t"") separated\nThe first row contains column headers\nThe first column contains the sample names that need to be exactly the same as column names 2:N of the count matrix\nNote: if this is not the case, the program will throw an error and stop.\nThe other columns contain information about each sample\n\n\n\nExample:\n\n\n\nsample\ntreatment\nsite\nsex\n\n\n\n\nsample1\ndrug\nliver\nF\n\n\nsample2\ndrug\nliver\nM\n\n\nsample3\ndrug\nkidney\nF\n\n\nsample4\ndrug\nkidney\nM\n\n\nsample5\ncontrol\nliver\nF\n\n\nsample6\ncontrol\nliver\nM\n\n\nsample7\ncontrol\nkidney\nF\n\n\nsample8\ncontrol\nkidney\nM\n\n\n\n\nJSON file\n\nContains the variables and file paths needed to run the pipeline\nTemplate found in the /data directory of this repository\nIt should be structured as follows:\n\n\n\n\n{\n  ""input_files"": {\n    ""infile1"":""/full/path/2/design_file.txt"",\n    ""infile2"":""/full/path/2/counts_file.txt"",\n    ""experiment_name"": ""experiment_name""\n  },\n  \n  ""folders"":{\n    ""parent_folder"":""/full/path/to/parent_folder""\n  },\n  \n  ""input_variables"":{\n    ""min_gene_tot_raw_count"": 1,\n    ""min_count_mean"":0,\n    ""mean_precentage_threshold"":0.25,\n    ""sd_precentage_threshold"":0.25,\n    ""R_squared_threshold"":0.95,\n    ""max_number_PC_regression"":9\n    \n  },\n    \n    ""design_variables"":{\n            ""design1"":""variable1"",\n            ""design2"":""variable2""\n            \n    },\n    \n     ""design_formula"":{\n            ""design"":""design ~ formula""\n            \n    }\n    \n}\n\nNumeric values displayed above correspond to default values. Follows a description of what each numeric parameter is used for:\n\n""min_gene_tot_raw_count"": 1,\n""min_count_mean"":0,\n""mean_precentage_threshold"":0.25,\n""sd_precentage_threshold"":0.25,\n""R_squared_threshold"":0.95,\n""max_number_PC_regression"":9\n\n3. Operating instructions\nTo run the pipeline, do the following:\n\nCreate a folder on your machine (parent folder), containing the following subfolders:\n\n\nscripts\ndata\n\n\n\nSave your data (estimated count matrices and design files) in the data folder, together with the JSON input file (found in the /data folder of this GitHub repository). Note: there are specific formatting requirements for the design and count matrices files, as specified in the Input files section.\n\n\nSave the scripts from in the scripts folder (scripts are in the /scripts folder of this GitHub repository).\n\n\nOpen your JSON input file (stored in parent_folder/data). Change the following variables to fit your file paths and desired parameters:\n\n\n\n""infile1"": full path to your design file. e.g. ""/home/user/projects/pipeline/data/exp_design.txt""\n""infile2"": full path to your counts file. e.g. ""/home/user/projects/pipeline/data/exp_estcounts.txt""\n""experiment_name"": name of your experiment. This is used to name output files. e.g. ""exp""\n""parent_folder"": full path to your parent folder e.g. ""/home/user/projects/pipeline""\n""design_variables""$""design1"": Column header from the design file e.g. ""site"". It should be identical to the header in the design file (no typos, careful with white spaces). This parameter is used to calculate the correlation between each meaningful PC and the parameter itself. The program calculates linear correlation with no interaction terms. It is also used to generate a correlation plot and to label points in a PC plot.\n""design_variables""$""design2"": Column header from the design file e.g. ""treatment"". It should be identical to the header in the design file (no typos, careful with white spaces). This parameter is used to calculate the correlation between each meaningful PC and the parameter itself. The program calculates linear correlation with no interaction terms. It is also used to generate a correlation plot and to label points in a PC plot. It can be empty.\n""design_formula""$""design"": The design formula used to construct a DESeq2 data set e.g. ""~ group + treatment"". This will be fed as the \'design\' argument in DESeqDataSetFromMatrix(). Refer to the package documentation for more information on the design formula. Note: if there is no design formula, ""~1"" can be used for no design.\n\nThe other variables in the JSON file are numeric parameters that can be optionally changed to fit the analysis. Under the Input files section there is a description of what each numeric parameter is used for.\n\nIn the terminal, cd to the parent_folder/scripts and call the bash script ""bash_automated_pca.sh"" with two arguments:\n\nthe full path to your parent folder\nthe name of the JSON input file (stored in parent_folder/data)\n\n\n\nThe command should look as follow:\nbash bash_automated_pca.sh ~/path/2/your/parent/folder/ name_of_your_json.json\n\nThe pipeline will run and save its outputs in sub-folders in the parent directory. See Outputs for more information.\n\n4. Outputs\nThe outputs of the pipeline can be found in the following subdirectories:\n\n/results\n/figures\n/report\n\nFollows a description of each output file by storing directory. All .txt files are tab-separated.\n\n\n/results:\n\n\nExperimentName_design_meaningful.txt -> a copy of the design matrix with coordinates of the samples on each meaningful PC appended as new columns.\n\n\nExperimentName_design.txt -> A copy of the input design file.\n\n\nExperimentName_genecounts_means.txt ->\n\n\nExperimentName_genecounts_sd.txt ->\n\n\nExperimentName_json_copy.json -> A copy of the input JSON file, with appended file paths to output tables, figures and objects.\n\n\nExperimentName_meaningful_pc_loading_scores.txt -> Loading scores only of the PC that were found to be meaningful through linear regression (see step 6).\n\n\nExperimentName_pca_eigenvalues.txt -> Eigenvalues for all PC, including raw values, explained variance in percent, and cumulative explained variance in percent.\n\n\nExperimentName_pca_loading_scores.txt -> Loading scores of all PCs.\n\n\nExperimentName_pca_object.rds -> PCA R-object (RDS). Output of prcomp().\n\n\nExperimentName_regression_pc_eigen.txt -> Results of the regression of Eigenvalues vs. component number as performed in step 6.\n\n\nExperimentName_rld_normalized.txt or. ExperimentName_vst_normalized.txt -> Respectively, result of the rlog() or vst() normalization performed in step 2. Choice of function depends on matrix size (cut-off: ncol count matrix <= 30 for rlog(), else vst()).\n\n\nExperimentName_xxxxxx_correlation.txt -> Output of the correlation test between each meaningful PC and the variable indicated in JSON -> design_variables -> design1. xxxxxx stands for that variable.\n\n\nExperimentName_yyyyyy_correlation.txt -> Output of the correlation test between each meaningful PC and the variable indicated in JSON -> design_variables -> design2. yyyyyy stands for that variable. Optional, only if yyyyyy exists.\n\n\nExperimentName_Z_mean_stdev.txt -> The counts table, after Z-transformation, with mean and sd after rlog() or vst() normalization (used for the Z-transformation itself).\n\n\nExperimentName_Z_normalized.txt -> The counts table, after Z-transformation (step 3).\n\n\nExperimentName_Z_threshold.txt -> The counts table after Z-transformation, and filtered for sufficient variation and expression level (step 4). Thresholds are indicated in JSON -> input_variables -> sd_precentage_threshold and JSON -> input_variables -> mean_precentage_threshold\n\n\n\n\n/figures:\n\n\nExperimentName_cor_plot_1.png : Correlation plot between design_variables$design1 from the JSON and the coordinate of each sample on each meaningful PC.\n\n\nExperimentName_cor_plot_2.png : Optional plot, only if design_variables$design2 exists. Correlation plot between design_variables$design2 from the JSON and the coordinate of each sample on each meaningful PC.\n\n\nExperimentName_log10scree_plot.png : Scree plot with Eigenvalues converted to log10.\n\n\nExperimentNamemean_histogram.png : Histogram of the raw standard deviations (of each gene across all samples). The dotted line represent filtering threshold as indicated in json$input_variables$mean_precentage_threshold.\n\n\nExperimentNamePC1_PC2.png : PC1 vs. PC2 coordinates with percentage of variance explained. Text is determined by json$design_variables$design1, color by json$design_variables$design2.\n\n\nExperimentNamePC2_PC3.png : PC2 vs. PC3 coordinates with percentage of variance explained. Text is determined by json$design_variables$design1, color by json$design_variables$design2.\n\n\nExperimentNameraw_mean_sd.png : mean vs. sd for each gene across sample of the raw count matrix.\n\n\nExperimentName_regression_plot.png : Linear regression of the log10 Eigenvalues vs. PC number for 1->N, 2->N.... x->N where N is the total number of PCs and x is the max number of regressions as indicated in json$input_variables$max_number_PC_regression.\n\n\nExperimentName_rlog_vsd_mean_sd.png : mean vs. standard deviation for each gene across sample of the standardized count matrix (with DESeq2::rlog or DESeq2::vst).\n\n\nExperimentNamescree_plot.png : Regular scree plot of the experiment.\n\n\nExperimentNamesd_histogram.png : Histogram of the raw standard deviations (of each gene across all samples). The dotted line represent filtering threshold as indicated in json$input_variables$mean_precentage_threshold.\n\n\nExperimentNameZ_mean_sd.png : mean vs. standard deviation of each gene across all samples after Z-transormation (sd should be == 1, mean should be very close to 0).\n\n\n\n\n/report:\n\nExperimentName_results.html : The automated report with a summary of each step, plots, and outputs.\n\n\n\n5. Provided files list\nThe files provided and needed for the correct functioning of the pipeline are the following:\n\n\n/scripts:\n\nautomated_report.R\nbash_automated_pca.sh\nfinal_report.Rmd\nstep_00.R\nstep_01.R\nstep_02.R\nstep_03.R\nstep_04.R\nstep_05.R\nstep_06.R\nstep_07.R\n\n\n\n/data:\n\npipeline_input_file.json\n\n\n\n6. Copyright and licensing\n7. Contact information\nFor any questions please contact Bianca M. Massacci at bianca.massacci@gmail.com\n8. Known bugs\n9. Credits and acknowledgements\nI thank Joel Graber, Ph.D. Senior Staff Scientist and Director of Computational Biology and Bioinformatics Core at the MDI Biological Laboratory, and Daniel M. Gatti, Ph.D. Faculty of Computer Science at College of the Atlantic for helping me in the process of designing and completing this project.\n'], 'url_profile': 'https://github.com/BiancaMass', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['HospitalFormularyPlanning\nUtilized Ordinal Logistic Regression to analyze a ten years historical data with over 100 thousand encounters.\nI also explored a new R package Mlogit to build the model, which followed naturally with a three level response.\nWe finally managed to give four recommendations out of twenty two medications to optimize the cost for the hospital.\n'], 'url_profile': 'https://github.com/xihuixu', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MarioVicuna', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['geo-fencing-and-crop-prediction\nhere you get the code for load google map on your browser and we will also add linear regression algo. for crop prediction.  #which file contain what data The main.py file contain the UI but the actual code of ""Prediction"" button is in test.py so if prediction button not working then run the test.py\n'], 'url_profile': 'https://github.com/manjiri13', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Predict-Taxi-Fares-with-Random-Forests\nAnalyzing the taxi fare in new york. Data which we are using is from a large number of taxi journeys in New York from 2013.  We are using regression trees and random forests to find places where New York taxi drivers earn the most.\n'], 'url_profile': 'https://github.com/JeebanRawat', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['House-Sales-in-King-County-USA-Homes-sold-between-May-2014-and-May-2015-\nThis dataset contains house deal costs for King County, which incorporates Seattle. It incorporates homes sold between May 2014 and May 2015. In this project, I have Preprocessed the dataset(checking for missing values, duplicate values and deleting some attributes which are not useful), Performed some visualizations and Implemented Correlation Analysis to show which factors influencing the price of the house and what would be the important factors we need to consider before buying the house in that Area. Moreover, I also predicted the price of the houses using Random Forest Regression and Ordinary Least Square Regression techniques. Additionally, Conducted Hypothesis test for predicted value to determine the probability that a given condition is true and computing the p-value to know whether the null hypothesis is true or alternative is true.\n'], 'url_profile': 'https://github.com/Prasanna-1998', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['\nMake sure python3 and pip are installed\nCreate a virtual environment with venv: python3 -m venv venv\nPopulate requirements.txt with required python libraries\nActivate the virtual environment: source venv/bin/activate\nUpgrade pip: pip install --upgrade pip\nInstall required python libraries with pip: pip install -r requirements.txt\nCreate .env file to hold environmental variables: touch .env\nPopulate .env file with said variables\n\n'], 'url_profile': 'https://github.com/ianscottknight', 'info_list': ['Jupyter Notebook', 'Updated Feb 27, 2020', 'Tcl', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 28, 2020', '1', 'R', 'Updated May 29, 2020', 'R', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', '1', 'Jupyter Notebook', 'Updated Nov 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Project 2 Code: Modeling Property Sale Prices in Ames, Iowa\nAuthor: Holly Bok\nProblem Statement\nThis project concerns sale prices for properties in Ames, Iowa. The goal of this project is to create a supervised machine learning model that can accurately predict the Sale Price of a property given information about its features. This model will be created using housing data from Ames, Iowa that includes 79 independent features as well as the sale price of the property. A selection of independent features will be used to predict values for the target variable, sale price. 70% of the housing dataset has been put aside as ""train"" data. This data will be used twofold, 1) to identify strong correlations between independent features and sale price, and 2) to fit machine learning models that closely predict sale price. The remaining 30% of the data has been put aside in a different dataset called ""test."" This dataset does not include the sale prices of the properties and will be used to test the strength of the model.\nPredictions will be made using 3 supervised, machine learning models: Linear Regression, LASSO, and RidgeCV Linear Regression models are supervised, machine learning models that create best-fit lines based on existing information (independent variables) with the purpose of making predictions about a target variable. In this project, the target variable is \'SalePrice\', the sale price of the properties. Linear Regression requires manual selection of features through data exploration. The two other models shown here, LASSO and RidgeCV models, input all numeric features and regularize them to create appropriate coefficients for each feature. Features that are good predictors of price are given higher coefficients and features that are not good predictors of price are given coefficients near, or at, 0. LASSO and other regularized models are different from Linear Regression because they include penalties for features that are not as strong predictors of the target variable.\nSuccess for these models will be evaluated using R2 scores*. Higher R2 scores (with a max of 1) indicate more successful models. The R2 score is the performance of the model in comparison to the mean of the target variable. Put another way, the R2 scores are interpreted as the variability in the target feature that is explained by our model in comparison to the average value of that feature. R2 scores closer to 1 indicate higher explanation of variabilty by the model; An R2 score of 1 would mean that 100% of the variability in the response feature is explined by the model.\n\nAlso known as the coefficient of determination.\nEqual to the sum of squared errors divided by the sum of total squares\n\nIn this project I will identify features of the Ames housing dataset that have linear relationships with sale price and fit a Linear Model using these features. I will also fit two regularized models, a LASSO model and a RidgeCV model, using polynomial features to include interaction terms. The models will be assessed for their success and the best model will be chosen and discussed. Furthermore, I will identify the specific features that best predit sale price. Lastly, I will make recommendations for property listing companies and other interested parties as well as discuss issues in the data and make suggestions for further research.\nData Loading and Cleaning\nHousing data has been obtained from https://www.kaggle.com/c/dsi-us-10-project-2-regression-challenge/data. The data dictionary is available for this housing data set therefore it is not outlined in detail here. Exploration and discussion of specific categories and/or aspects of the data will be outlined here for relevant variables.\nTrain and test data has been read in from the datasets folder of this repository. The original train data has been read from filepath \'datasets/train.csv\' and the test data has been read from filepath \'datasets/test.csv\' Both dataframes have been read as pandas dataframes.\nThe training data has 2,050 rows and 81 columns including the dependent variable \'SalePrice\'. The test data has 878 rows and 80 columns. Each row in the data corresponds to a property that has been sold. Of the 81 columns in the training data set, 78 are possible dependent variables (the remaining 3 are SalePrice, ID, and Property ID). All properties were sold in Ames, Iowa between 2006 and 2010. Data has been obtained from the Ames Assessors Office.\nSeveral variables in the raw data set are ranked on non-numerical, but hierarchical, string grading scales. The most common of these grading scales is a quality ranking of Po, Fa, TA, Gd, and Ex for Poor, Fair, Typical / Average, Good, and Excellent. This scale is used for 8 variables (Exterior Quality, Exterior Condition, Heating Quality, Kitchen Quality, Garage Quality, Garage Condition, Pool Quality, and Basement Condition). Each of these variables have been replaced with a 1-5 scale (where 1 is Poor and 5 is Excellent) and converted from objects to integers for use in the modeling process. The assumption made henceforth is that null values represent a lack of the variable they describe (for example, a score of 0 for Garage Quality will represent the lack of a garage). Null values for all of these categories have been replaced with a score of 0. A few other ranking scales exist for non-numerical, heirarchical categories in this data including Paved Drive, Functional, and Garage Finish. The specific conversions for these variables are available as notation in the \'01 Data Clean\' file of this repository. One variable, PID, was read as numeric data and converted to the more appropriate object datatype. 9,822 total null values are present in the raw dataset. All null values are assumed as 0 for all variables.\nThe cleaned dataset has been written to .csv as clean_train. This dataset is used for the modeling process for both the multi linear regression model and the LASSO model. Predictions for the Sale Price of properties in the test dataset have been saved in two separate datsets, one for the linear regression and one for the LASSO model. These files can be found in the datasets folder of this repository as \'clean_train.csv\', \'Holly_Preds_LR\', and \'Holly_Preds_LASSO\'\nModeling\nLinear Regression Model:\nThe majority of EDA is done during the selection process for the variables in the multi linear regression (MLR) model. The tools used for initial feature selection and feature analysis for this model include a seaborn heatmap, histograms of dependent variables, and a seaborn pairplot.\nA seaborn heatmap is a visual created to present the linear correlations between all possible independent variables and the dependent variable \'SalePrice.\' The strongest correlated variables are identified by their correlation scores; The features selected have scores above .40 or below -.40. The threshold of .40/-.40 is chosen because this provides a number of features that will not cause the model to be overfit. Of these variables 10 are selected as the independent features for the model. The process of selecting or eliminating these variables is in the \'02 Linear Regression\' file of this repository, under the ""EDA and Feature Selection"" subheader.\nThe features selected for the model include: Overall Quality, Exterior Quality, Above ground living area, Kitchen Qual, Garage Area, Total basement SF, Year Built, Year Remodeled, Full Bath, and Masonry Veneer Area. A new dataframe, called \'train_1\' is created to include only these features and Sale Price. All MLR modeling is done using this \'train_1\' dataframe rather than the original, raw \'train\' dataframe.\nA logarithmic transformation is done to the two independent variables with the highest correlation scores. These variables are Overall Quality and Exterior Quality. A logarithmic transformation is applied to the target variable as well. The purpose of the logarithmic transformation is to create a more homoskedastic model. In addition, outliers are identified that could affect the normality and homoskedasticity of the data. These outliers are identified and removed for the Linear Regression model as well as each model hereafter.\nA train_test_split function has been applied to our train_1 dataframe to create a training and test subset of train_1. A Linear Regression model (called \'my_model\') is instantiated and fit to the training subset using our X and y variables (where X is all the independent variables included in train_1 and y is the dependent variable). Using this model a set of predictions is made for the train_test_split test subset. A cross_val_score (with 5 folds), training score, and test score are all produced from my_model. A detailed summary of the performance and coefficients for each independent variable can be found in the \'02 Linear Regression\' file of this repository under the ""Model Scoring"" subheader.\nThe Linear Regression model, my_model, preformed as follows:*\ncross_val_train().mean() : 0.8644905636927491\ntrain subset: 0.8705915566195076\ntest subset: 0.8319067601632991\n\nAll scores presented are R2 scores\n\nThe original test data is transformed to match the training data for the MLR in order to make appropriate predictions for the test data using my_model. Kitchen Quality and Exterior Quality are transformed from non-numerical, heirarchical objects to ranked integers in the same way the train data was. Overall Quality and Exterior Quality are transformed through log functions, just as the train data. Predictions are made for the Sale Price of the original test data using my_model.predict(). These predictions are\nconverted back from log functions to return to the original units. A dataframe consisting of property IDs and predicted sale prices is written as a .csv file called \'Holly_Preds_LR\'\nLASSO Model:\nA second, regularized machine learning model is created from the clean_train dataset. This model is created and tested in competition with the MLR model to produce a better fit model with higher cross_val_train, train subset, and test subset R2 scores. Features are not selected using EDA as they are in the MLR model. Instead, all numerical independent variables are input as features into the LASSO regularization model and their coefficients are scaled to fit their correlation to sale price. Additionally, this model is fit with polynomial interaction features. Polynomial interaction features add features for the interaction terms between all independent variables. A new dataframe called \'numeric_train\' is created that includes all numeric variables from clean_train and all new polynomial features. This results in 1,274 independent model variables. A train_test_split is run on the numeric_train dataset, creating a training and testing subset.\nA LASSO model is instantiated (with max iterations set at 1500) and fit. A requirement of the LASSO model is that all variables have been scaled to ensure that all variables are on the same numeric scale. A standard scaler is used to accomplish this on the training and testing subsets. The model is then fit and scored. The coefficients for each variable are shown in a dataframe called \'lasso_coef\' is shown in the \'03 LASSO Model\' file of this repository under the subheader \'Fitting and Scoring the LASSO Model."" More details on the specifics of the LASSO model, including a list of features that were regularized to a coefficient larger or smaller than 0, can be found there.\nThe LASSO model, lcv, preformed as follows:*\ncross_val_train().mean() : 0.916942076487099\ntrain subset: 0.9400702246522403\ntest subset: 0.8930934913207212\n\nAll scores presented are R2 scores\n\nAs with the Linear Regression model, the original test dataset is manipulated to match the dataset numeric_train that the LASSO model is fit to. As this process is outlined above under the Linear Regression Model subheader, it is not detailed here. Specifics can be found in the \'03 LASSO Model\' file of this repository under the \'Generating Predictions for Test Dataset\' subheader. The property IDs and sale price predictions are saved to a dataframe and written as a .csv file named \'Holly_Preds_LASSO.csv\'\nRidgeCV Model\nA RidgeCV model is created and tested in competition with the LASSO model. This model is created in a very similar fashion to the LASSO model, as the RidgeCV and LASSO models are both regularized models. RidgeCV and LASSO models differ in the measures they use to assign penalties to coefficients that are not good predictors of the target variable. The ridgeCV model will not assign a coefficient of 0 to any feature but will instead spread the coefficients out smoothly towards 0.\nThe Ridge CV Model did not score as high as the LASSO model or the Linear Regression model.\nThe RIDGECV model, ridge_cv, preformed as follows:*\ncross_val_train().mean() : 0.6570085100344006\ntrain subset: 0.885667792472507\ntest subset: 0.880501632158954\n\nAll scores presented are R2 scores\n\nPredictions from the RidgeCV model are saved under \'Holly_Preds_Ridge\'\nConclusions\nOverall\nThe goal of this project was to create the best possible supervised, machine learning model to predict the sale price of properties sold in Ames, Iowa. These models were created by manipulating a series of informational variables representing the presence and/or quality of housing features. Model success was evaluated using R2 scores, a measure of how strong a model is at making predictions. Each model was tested and given an R2 score based on its ability to make accurate predictions.\nThe R2 scores for these models suggest that the LASSO model is the best at predicting housing prices, followed by the Linear Regression model and the RidgeCV model. The R2 scores for the LASSO model were highest in all three scoring categories (cross_val_score, test subset, and train subset). This means that the LASSO model is the most successful of our models. The Linear Regression model preformed second best and the RidgeCV model preformed the worst. The highest scores for all 3 models were seen in the training subsets. The highest R2 score received on the test subset is 0.89 using the LASSO model.\nThe LASSO model also balanced bias and variance better than the other two models. This suggests that the best models for predicting the sale price of houses need to be careful to not overfit or underfit. The LASSO model choice is much like the choice of porridge that Goldilocks must make. The LASSO model is the \'just right\' model, while the Linear Regression model has not been engineered enough to more closely examine prices (the \'too cold\' model), and the RidgeCV model has been overengineered to the point that the testing data is unlikely to follow the same pattern (the \'too hot\' model). In selecting the best model (and in turn, the best independent features) we will be able to more accurately predict price.\nPolynomial features preformed higher than the original features in the LASSO model, indicating polynomial features are better predictors of Sale Price. The strongest indicators of Sale Price among all models included Overall Quality, Exterior Quality, Above Ground SF, Total Basement SF, Kitchen Quality, and Masonry Veneer Area. These features are seen independently and in many of the strong interaction features that were created during polynomial transformation.\nLASSO Model\nThe LASSO model preformed the strongest of all 3 models. The model preformed best both overall and in each 3 seperate scoring categories (cross_val_score, train subset, and test subset). The strongest R2 score for the LASSO model was the training subset, with a score of 0.94. The cross_val_score was slightly lower, at 0.92, and the test subset score was the lowest at 0.89. The test subset R2 score is lower than the training and cross_val scores. This suggests that slight overfitting to the training data has resulted in higher variance. However, as all 3 scores are relatively consistent (and all 3 scores are high) we can conclude that the LASSO model is the best at predicting Sale Price as compared to the other models.\nThe LASSO model uses all numerical features as input and assigns coefficients for each features based on how that variable affects the dependent variable. Indepdent features that are determined to have low effect on the target variable are assigned coefficients of 0, meaning they do not have an effect on Sale Price. Of 1,274 features (including polynomial features) that are input into the LASSO model, only 123 features are assigned coefficients. The distribution of these coefficients can be seen in the \'02 Linear Regression\' file of this repository under the ""Fitting and Scoring to Lasso Model"" subheader. The majority of coefficients are small and cluster towards 0 with very few coefficients above the 8,000 mark or below the 2,000 mark. This suggests that the feature with high coefficients are doing the majority of the work in predicting Sale Prices.\nAlmost all strong coefficients in the LASSO model are polynomial features. All of the top 50 positively and bottom 50 negatively most correlated coefficients are polynomial features. The strongest 5 coefficients are as follows:\nMasonry Veneer Area x Pool Quality:          - 20990\nOverall Quality x Above Ground Living Area:    13669\nOverall Quality x Total Basement SF:           11474\nMasonry Veneer Area x Miscelaneous Value:     -10918\nAbove Ground Living Area x Kitchen Quality:   - 8592\nAlthough the large coefficients are all polynomial features, the polynomial features are interaction terms between many of the feature variables identified in the Linear Regression model. This suggests that the feature selection in the Linear Regression model resulted in features that affect Sale Price in all models. For example, Overall Quality, Above Ground Living Area, Total Basement SF, Kitchen Quality, and Masonry Veneer Area are all features that were selected for the Linear Regression models. These features are seen frequently in interaction terms in the LASSO model.\nLinear Regression Model\nThe Linear Regression does not preform as well as the LASSO model but is still a high preforming model. The strongest R2 score for the LASSO model was the training subset, with a score of 0.871. The cross_val_score was slightly lower, at 0.865, and the test subset score was the lowest at 0.83. Much like the LASSO model, this suggests slight variance and overfitting, but the model is preforming well regardless.\nShow some of the coefficients and how some are really not that strong. Of the 10 features selected for the Linear Regression model, only 3 features have coefficienst above .01. In the case of Linear Regression, we can interpret the value of these coefficients as ""a one unit increase of this feature will lead to a .46 unit increase in the target variable."" For example, in this Linear Regression, a 1 unit increase Overall Quality will result in a .46 unit increase in predicted Sale Price.\nThe strongest coefficients of this model were:\nOverall Quality :  .46\nExterior Quality*: .06\nKitchen Quality:   .05\n*The relationship between Exterior Quality and Sale Price has a p value above .05\nAmong the variables with small coefficients we see a higher instance of large scale variables. The variables with large coefficients (Overall Quality, Exterior Quality, and Kitchen Quality) tend to be variables with rank scales (1-10) as opposed to variables with square footage or other, large scales. It is difficult to interpret the importance of the variables on coefficients alone for this reason. This problem is avoided in the LASSO and RidgeCV models as those models are regularized and keep all of the feature variables on the same scale.\nThere are several possible reasons that the Linear Regression is not as strong as the LASSO model. Assumptions of Linear Regression are violated in the dataset. Many of the selected features are not completely linearly related to the target variable, Sale Price. Although we do see somewhat linear coorelations, they are not completely linear and thus violate one of the assumptions of the regression model. Linear relationships can be observed in the \'02 Linear Regression\' file of this repository under the ""EDA and Feature Selection"" subheader. Additionally, our original, raw data is not distributed normally. This results in a non-normal distribution of errors and an increase in heteroskedasticity. While steps are taken to reduce the effects of these problems, we still see our assumptions violated with this data. Lastly, we are violating the independence of variables assumption of LR. Some redundancies are eliminated by dropping features that are redundant, but it is near impossible to avoid non-independence of independent variables in the case of housing data. These assumption violations likely cause the Linear Regression model to underpreform as compared to the LASSO model.\nRidgeCV Model\nThe third model, RidgeCV, preformed the worst of the three models. The highest R2 score for the RidgeCV model was 0.89 for the training subset, followed by 0.88 for the testing subset, and 0.66 for cross_val_score.\nWhile initially this cross_val_score seems incredibly low, this is likely due to the presence of an outlier in the testing data. The cross_val_score is generated by average the R2 scores of a 3-fold train/test split. The average R2 score is presenting as very low compared to the other scores (0.66), but this is because the cross_val_score is an average of a few R2 scores created in different train/test folds. The average cross_val_score for the RidgeCV model is thrown off because of the individual R2 scores: 0.85403624, 0.85500432, and 0.26198497. The average of is brought down by the R2 score for the 3rd fold, 0.27.\nThe preformance of the RidgeCV model is closer to the preformance of the Linear Model while the LASSO model outpreformed both. The RidgeCV model and LASSO model do not preform the same way even though they are both regularized models. The goal of both models is to raise the coefficients of important features and lower the coefficients of less important features. However, while the LASSO model will create coefficients of exactly 0, the RidgeCV model will only bring them very close to 0. Because of this, the RidgeCV model is including many, many more independent features (all 1274) because it is not fully rejecting any one feature. This is likely why we saw better preformance with the LASSO model than the RidgeCV model; the RidgeCV model has so many inputs that it becomes complicated and it is less likely to make accurate predictions.\nIn general, the coefficients in the RidgeCV model showed consistency of predictor features with the other two models. Overall quality and exterior quality made up the majority of the polynomial features that were assigned the largest coefficients. We see negative correlations here with masonry veneer area and pool quality.\nRecommendations\nListing Corporations:\nIn order to best predict property sale prices I recommend the use if the LASSO model. This model is the best choice for a home listing corporation, such as a real estate firm, to accurately predict the sale price of their properties. This is in the interest of such corporations because it balances the expectations of both the home buyers and home owners and will provide the best results for ensuring clients on both sides are satisfied with their listing or buying experience. This will also increase turnover and result in more homes being sold and listed through the company.\nMore generally, when designing supervised machine learning models to predict housing prices it is best to use models that regularize as opposed to models that require manual feature selection. This is the best way to optimize the preformance of the model and minimizes the chance of error due to hidden features of the data. However, when utilizing regularized models care should be taken in which data is regularized. It is important to be discriminatory when selecting features that go into the model or to use a model that will set coefficients of 0 for less important features.\nWhen getting information about properties that will be sold it is important to ask questions first and foremost about the overall quality of the home. The interior and exterior quality / condition of the property overall as well as the quality of specific features are good indicators of sale price. Variables that have information about the size of the property are also good indicators of sale price. Many of the variables that are indicators of price concern quality, size, or interaction terms of one or both. It is important to note that some features, such as masonry veneer area and number of baths, are indirect indicators of size. For example, a property with a large value for masonry veneer square footage is likely also going to be an otherwise very large property. Variables of size and condition are successful in both the Linear Regression and the regularized models.\nWhile it is good to ask about more specific features such as driveway quality or alley access, these do little to predict sale price. These features may attract specific buyers to spend more money, but home buyers are more likey to weigh quality and size when doing the cost / benefit analysis of purchasing a new home.\nIn order to make the best possible predictions for the prices of homes that will be listed, property listing corporations and interests should focus on the overall quality of the property as well as the size.\nHousing Buyers:\nIn order to purchase a home of high quality for the best value it is good practice to look for homes that are within a reasonable size. It can be tempting to want to purchase as large of a house as possible, but these homes are often priced high because of their size. Instead of purchasing the largest home you can find under budget, purchase a slightly smaller home with excellent internal features. Similarly, the home that will be the most valuable for the sale price will need repairs on ""appearance quality"" features. A paint job will likely be cheaper than replacing a bad HVAC system, but a home with fresh paint is going to be valued higher than a home with great HVAC.\nFor Sale by Owner:\nIn order to sell a home at a high price while investing as little as possible into renovations, focus on the overall appearance and quality of the home. It is best to prepare the property for sale starting with the largest and most damaged feature, as repairing these will likely result in a good return in sale price increase. In addition, increasing the square footage of the home, even in non-expensive ways such as setting up a carport or cutting back brush / wilderness, can increase the sale price.\nFurther exploration\nI recommend that this dataset be explored further in several ways. The data dictionary provided has very little information on the meaning of null and 0 values. Several homes have square footage listed as 0. It is impossible to know looking at this data whether this represents a lack of information or if it supposed to represent an empty lot or a home that is still in the process of construction. Additionally, many homes list bedrooms and/or baths above 0 while still listing square footage as 0. This is impossible and points bad data collection. This data needs to be corrected or a new, similar dataset needs to be created through new research. It is also not known for any feature, such as \'Paved Driveway,\' if the presence of a null value means that that feature does not exist at this property or if the reporter does not know if the property has this information or it is not disclosed for another reason.\nI recommend that this general model be broken down into several smaller models. The predicts for sale price will likey be more accurate if there is a separate model for extremely large homes or homes with many great features. Other models could be made to predict the sale price of foreclosures, vacant lots, or homes to and from family members. For further research I would like to construct a similar LASSO model that only includes homes that have no null data in the raw dataset.\nIn addition, the models used could be improved. The linear regression model could be improved by including more interaction or polynomial terms, as well as eliminating more outliers and improving the homoskedasticity and normality of the data. I would like to build another RidgeCV model with a smaller selection of polynomial features. I would also like to identify the data in the RidgeCV dataset that is causing the cross_val_score preform so low in one fold. Identifying and removing this variable could improve the overall preformance of the model. Lastly, I would like to create a model that does not include masonry veneer area. This feature seemed to be somewhat inconsistent between models and often had a negative cooeficient. This is likely because homes with large masonry veneer areas will be very large, but not all very large homes will have masonry veneer area. This heavily violates the independence of variables term and I would like to explore the effects of this further.\nNote: small parts of this code may need to be adapted to fit the requirements of packages or programs in future use. These code warnings appear in red when code is run and do not affect the current preformance of the code.\n'], 'url_profile': 'https://github.com/HollyBok', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['ai-accelerator-recipe-Bank\nThis use case helps to predict if a client subscribed a term deposit or not(yes/no). Dataset used for this case is Bank.csv. After necessary EDA using different plots to have proper understanding of data, classification models were built using Logistic Regression, SVC, KNeighborsClassifiers , RandomForestClassifier , etc. to predict the target variable & then accuracy metrics were taken into consideration to see the model performance.\n'], 'url_profile': 'https://github.com/hcl-rbs-ai-accelerator-3', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'Chicago, US', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anaghadudihalli', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""ai-accelerator-recipe-Bank-loan-status\nThis use case helps in predicting the loan status of a customer. The dataset used in this use case is 'loan_status.csv'. Initially, the data is analysed, EDA is done and then collinear variables are removed after some feature engineering to improve the model's performance. After standardization of data, KNN, Logistic Regression, SVM, Naive Bayes and Random Forest algorithms are used to predict the target variable.\n""], 'url_profile': 'https://github.com/hcl-rbs-ai-accelerator-3', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MarioVicuna', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KristiKo', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'British Columbia, Canada', 'stats_list': [], 'contributions': '157 contributions\n        in the last year', 'description': [""Flight-Data-Analysis-using-R\nThis project aims to build a prediction model for predicting flight delays for the year 2013. The project consists data of domestic flights that departed from the three major New York City airports in 2013. The data was collected over the entire year of 2013 and consists of various factors such as scheduled flight time, scheduled arrival time, weather details, carrier details, etc. This data pertains to three major airports viz, John F. Kennedy International Airport (JFK), Newark Liberty International Airport (EWR) and LaGuardia Airport (LGA).\nIn this project, various statistical learning frameworks for both classification and regression were explored to build efficient prediction models. The models were explored and improved by standard statistical tests and tuning and the results were compared to in turn select the best one for prediction on the held-out test set.\nData\nThe data is taken from the ‘nycflights13’ package in R. The folder 'DATA' consists of a csv file named fltrain.csv which was combined using the following 4 datasets from this package:\n§ Flights: all flights that departed from NYC in 2013 (schedule and logistical modifications)\n§ Weather: hourly meteorological data for each airport\n§ Airports: airport names and locations\n§ Planes: construction information about each plane\nIt contains 43 variables measured on 200,000 flights. The dataset contains information about all flights departed from NYC in 2013, hourly meteorological data for each airport, airport names and locations, construction information about each plane.\nRunning instructions\n\nDownload the datasets and unzip files\nInstall Rstudio and open the .rmd file using the application.\nRun the file.\nThe folder 'PLOTS' has some interesting findings discovered.\n\n""], 'url_profile': 'https://github.com/slavvy-coelho', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Scottl2', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Scottl2', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}","{'location': 'Santa Clara, California', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshtandon23', 'info_list': ['Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020', 'Updated Feb 24, 2020']}"
"{'location': 'Toronto', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['House-Prices\nBuying a house remains one of the biggest decisions that individuals can make in their lifetime. With the data at hand, the goal is to predict the final price of each house in Ames, Iowa, by using every aspect of a residential home. The insights through this model could be used by every person to aid their decision-making process when purchasing a home. The dataset mostly contains categorical variables stores as factors or integers. We will be focusing on advance regression techniques such as LASSO and Ridge to create our model and accurately predict the house prices in Iowa.\n'], 'url_profile': 'https://github.com/kishoreshaurya', 'info_list': ['R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Default-of-credit-card-clients\nThis project presents and discusses data-driven predictive models for predicting the defaulters among the credit card users. Data used include details like limit balance, age, sex, amount of bill statement, repayment status and amount of previous payment. The paper discusses which variables are the strongest predictors of default, and to make predictions on which customers are likely to default. Four machine learning models were trained with repeated cross validation and evaluated in a testing set: (a) logistic regression,(b)KNN, (c) decision tree, (d) Naïve Bayes ,(e)Random Forest and Ensembling methods like Boosting and Bagging. The base model gave 81% R-square value with all the variables without any treatment.\n'], 'url_profile': 'https://github.com/nehakhanzode2508', 'info_list': ['R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Real-Estate-Housing-Price-and-Location-Predictor\nSolved the problem of predicting appropriate locations for the building an on demand house for real estate business, as for a builder to see which is the highest selling house type, which enables the builder to make house based on that, The algorithm used was Gradient Boosting Regressor and used Sea Born for aesthetic Data Visualization based on longitude and latitude, which gives clear picture of the market needs and demand that need to be produce, Achieved 91.4 percent of accuracy\n'], 'url_profile': 'https://github.com/sheikhmujahed97', 'info_list': ['R', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}",,,,,,,
