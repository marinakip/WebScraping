"{'location': 'London', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': [""RobustGP\nProcedures for robust initialisation and optimisation of Variational Sparse Gaussian processes. This code accompanies\nBurt et al (2019, 2020) (see sitation below), and implements the recommendations.\nThe bottom line\nIn Burt et al (2020), we recommend Sparse GP Regression (SGPR) (Titsias, 2009) models to be trained in the following way:\n\nInitialise the inducing inputs using the ConditionalVariance methods.\nAlternately optimise the hyperparameters only and reinitialise the inducing inputs using ConditionalVariance.\nSee FullbatchUciExperiment for an example on how to implement this (training_procedure == 'reinit_Z').\n\nWe find that when using ConditionalVariance we obtain the same performance as gradient-optimised inducing inputs\nwith a slightly larger number of inducing varianbles. The benefit is not having to do gradient-based optimisation, which\nis often more of a pain than it is worth.\nA few anecdotal suggestions for practitioners:\n\nWe suggest using ConditionalVariance even for non-Gaussian likelihoods for initialisation, although you may want\ntest yourself whether to use the periodic reinitialisation method, or gradient-based inducing input optimisation.\nWhen getting Cholesky errors, consider reinitialising the inducing inputs with ConditionalVariance rather than\ne.g. raising jitter. ConditionalVariance will repel the inducing inputs based on any new hyperparameters which\ncaused high correlation between old inducing variables, leading to better conditioning of Kuu.\n\nExample\nM = 1000  # We choose 1000 inducing variables\nk = gpflow.kernels.SquaredExponential()\n# Initialise hyperparameters here\ninit_method = robustgp.ConditionalVariance()\nZ = init_method.compute_initialisation(X, M, k)[0]\nmodel = gpflow.models.SGPR((X_train, Y_train), k, Z)\nfor _ in range(10):\n    # Optimise w.r.t. hyperparmeters here...\n    Z = init_method.compute_initialisation(X, M, k)[0]  # Reinit with the new kernel hyperparameters\n    self.model.inducing_variable.Z = gpflow.Parameter(Z)\nWhat the code provides\nInducing input initialisation\nWe provide various inducing point initialisation methods, together with some tools for robustly optimising GPflow\nmodels. We really only recommend using ConditionalVariance for initialising inducing inputs, with the others being\nincluded for the experiments in the paper.\nAutomatic jitter selection\nIn addition, we provide versions of the GPflow classes SGPR and GPR that have objective functions that are\nrobust to Cholesky/inversion errors. This is implemented by automatic increasing of jitter, as is done in e.g.\nGPy. This process is a bit cumbersome in TensorFlow, and to do it we provide the\nclasses RobustSGPR and RobustGPR, as well as a customised Scipy optimiser RobustScipy. To see how this\nis used, see the class FullbatchUciExperiment in the robustgp_experiments directory.\nExperiments\nAll the experiments from Burt et al (2020) are included  in the robustgp_experiments directory.\nCode guidelines\nFor using the initialisation code:\n\nMake sure that GPflow is installed, followed by running pip setup.py develop.\nTests can be run using pytest -x --cov-report html --cov=robustgp.\n\nFor running the experiments\n\nWe use code from Bayesian benchmarks to handle dataset\nloading. Some assembly needed to get all the datasets.\nSome scripts are paralellised using jug.\n\nMake sure it's installed using pip install jug.\nYou can run all the tasks in a script in parallel by running jug execute jug_script.py multiple times.\nJug communicates over the filesystem, so multiple computers can paralellise the same script if they share a networked filesystem.\nUsually, a separate script takes care of the plotting / processing of the results.\n\n\n\nCitation\nTo cite the recommendations in our paper or this accompanying software, please refer to our JMLR paper.\n@article{burt2020gpviconv,\n  author  = {David R. Burt and Carl Edward Rasmussen and Mark van der Wilk},\n  title   = {Convergence of Sparse Variational Inference in Gaussian Processes Regression},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {131},\n  pages   = {1-63},\n  url     = {http://jmlr.org/papers/v21/19-1015.html}\n}\n\nThis JMLR paper is an extended version of our ICML paper.\n@InProceedings{burt2019gpviconv,\n  title = \t {Rates of Convergence for Sparse Variational {G}aussian Process Regression},\n  author = \t {Burt, David and Rasmussen, Carl Edward and van der Wilk, Mark},\n  booktitle = \t {Proceedings of the 36th International Conference on Machine Learning},\n  pages = \t {862--871},\n  year = \t {2019},\n  editor = \t {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},\n  volume = \t {97},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Long Beach, California, USA},\n  month = \t {09--15 Jun},\n  publisher = \t {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v97/burt19a/burt19a.pdf},\n  url = \t {http://proceedings.mlr.press/v97/burt19a.html},\n}\n\n""], 'url_profile': 'https://github.com/markvdw', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'Kerala', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['Algo-Trading-Math-Models\nMath Techniques viz. ARIMA, Frequency Decomposition, Fourier Filtering, Linear Regression &  Bi-directional LSTMs on Feature Engineered Stock Market Data.\nQuant Trading\nQuant strategies follow a data-driven approach to pick stocks. This approach  which seeks to reduce the role of human bias conceptually fall in between active and passive trading. The stock data is a classic example of "" time series"" where the prices are sampled at regular intervals.\nFourier Filtering\nFourier Filtering helps to de-noise the signal in order to find out the significant curve. This technique can be used before feeding the prediction model or even to smooth the model output.\n\nARIMA Model on TCS Stock Data\naka. Box-Jenkins model, ARIMA was created in 1976. It is conceptually similar to a linear regression model applied on time series data.\xa0\nARIMA has 3 parts:\n- Auto Regression\n- Integration (Differencing)\n- Moving Average\n\n\nLinear Regression\nWe were using only \'p\' previous values and \'q\' errors to predict. But we can use other features such as day of week, time of day, holidays etc. This technique, known as Feature Engineering is more of an art than science.\nHere we take a CSV file containing daily record of the price of the S&P500 Index from 1950 to 2015. Lets try to predict response variable, i.e. closing price, prior to a day. We can use features the features below: \n\nAverage Price of past 365 days.\nRatio of average price for the past 5 days & past 365 days.\nMean and Standard Deviation of previous 365 days.\n\n\nFrequency Decomposition using Power Spectral Density Curve \nAuto correlation is one way to compute periodicity. But a more scientific way to find periodicity is Fourier Transform. This technique can be used to find out the most significant periodic changes in historical data, which gives a dependable hint about future.\n\n\n\n\nLSTM on S&P500 Time Series with Fourier Filering\n\nLong Short-Term Memory networks, can be used to learn from the series of past observations to predict the next value in the sequence.\nA vanilla LSTM model has a single hidden layer of LSTM units, and an output layer used to make prediction. Here we are working with a uni-variate series, so the number of features is one.\nWe apply LSTM on the same S&P 500 data taken for Fourier filtering. First we draw the auto correlation graph to estimate the lag. \n\n\nWe tried to model stock market behaviour using supervised learning approaches viz. Linear Regression or LSTM. But Reinforcement Learning is more robust to account for various environmental factors that affects stock market, as it aims to maximise reward in a given situation.\nFourier analysis works best with waves or wavelets that are regular and predictable, for which stock market is an antithesis. Hence it is beneficial to look into spectral analysis and signal extraction also.\n'], 'url_profile': 'https://github.com/AdroitAnandAI', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['taiko screencheck\n \nInstallation\nnpm install taiko-screencheck\nIntroduction\n\ntaiko-screencheck is a taiko plugin that assists visual regression testing by tracking visual changes to websites and web apps. It introduces a global screencheck method that has the same call signature as the built-in screenshot method, except that it returns the result of comparing a screenshot to a reference screenshot.\nBy default taiko-screencheck will create an automatically numbered directory to store screenshots. It is recommended that test developers use the screenshotSetup method to override this behaviour to give complete control of taiko-screencheck output.\nTaiko Methods\nscreencheck()\nscreencheck(screenshotOptions = {}) => { \n   result:string = ""SAME"" | ""DIFFERENT"" | ""NO_BASE_IMAGE"",\n   data:Buffer, referenceData:Buffer = undefined, pixelCount:number \n}\nThis method takes and saves a screenshot and compares it to the reference screenshot in the detected or configured reference directory (see screencheckSetup#refRunId).\nThe value of the returned result property indicates whether or not the screenshots differ.\nTaiko CLI example:\nawait openBrowser()\nawait goto(""dcdc.io"")\nhomepage = await screencheck()\nassert.equal(homepage.result, ""SAME"")\n\nscreencheckSetup()\nscreencheckSetup(options = { \n   runId:string = <auto>, refRunId:string = <auto>, baseDir:string = <cwd>\n}) => options\nThis method optionally configures screencheck to use custom directories for output and comparison.\nFAQs\n\n\nMy headless screen captures never match my headed screen captures of the same page.\nThis may be happening because you have a high DPI display. Try setting your desktop as a 1:1 pixel ratio to your display device. Alternatively you may want to avoid headed mode for the creation of reference images.\nOn Windows it may help to disable ClearType while running tests in headed modes.\n\n\nHow do I use taiko-screencheck?\nWith an initialised node project, run npm install taiko-screencheck and from thereonin taiko will automatically enable taiko-screencheck.\n\n\nDoes taiko-screencheck work on the taiko CLI and in node?\nYes. You can use the plugin in both node and taiko CLI.\n\n\nWhat happens when I don\'t configure using screencheckSetup?\ntaiko-screencheck will create a directory per run, named 000n.auto where n is computed based on the current contents of the base directory. This is to say, the first time it runs the output will go to $pwd/0001.auto and the next time $pwd/0002.auto and so on.\nYou can use screencheckSetup(options) to change this behaviour.\n\n\nCan I see the current configuration at runtime?\nYes. Call screencheckSetup() without providing options and taiko-screencheck will return the current configuration.\n\n\nWhen I call openBrowser the viewport size is 1440 x 900. Why is this happening?\ntaiko-screencheck overrides the built in openBrowser() command by adding a call to setViewPort() that matches the headless mode viewport. This feature mitigates the need to rewrite test scripts written in headed mode that later run in headless mode. If you don\'t want this behaviour, you can add a second boolean parameter to openBrowser(options, true) to force use of the original openBrowser command which will have a viewport ""appropriate"" for your desktop display.\n\n\nContributing\nClone the git repository and use the _test_harness subdirectory to test the plugin in the taiko environment. The repository includes a Visual Studio Code configuration for debugging in a taiko context.\ngit clone https://github.com/dcdc-io/taiko-screencheck\ncd taiko-screencheck\nnpm install\nnpm run build\nnpm run test\n'], 'url_profile': 'https://github.com/dcdc-io', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['LightAutoML - automatic model creation framework\n\n\n\nLightAutoML project from Sberbank AI Lab AutoML group is the framework for automatic classification and regression model creation.\nCurrent available tasks to solve:\n\nbinary classification\nmulticlass classification\nregression\n\nCurrently we work with datasets, where each row is an object with its specific features and target. Multitable datasets and sequences are now under contruction :)\nNote: for automatic creation of interpretable models we use AutoWoE library made by our group as well.\nAuthors: Ryzhkov Alexander, Vakhrushev Anton, Simakov Dmitry, Bunakov Vasilii, Damdinov Rinchin, Shvets Pavel, Kirilin Alexander\nLightAutoML video guides:\n\nLightAutoML webinar for Sberloga community (Ryzhkov Alexander, Simakov Dmitry)\nLightAutoML framework general overview, benchmarks and advantages for business (Ryzhkov Alexander)\nLightAutoML practical guide - ML pipeline presets overview (Simakov Dmitry)\n\nSee the Documentation of LightAutoML.\n\nInstallation\nInstallation via pip from PyPI\nTo install LightAutoML framework on your machine:\npip install -U lightautoml\nInstallation from sources with virtual environment creation\nIf you want to create a specific virtual environment for LightAutoML, you need to install  python3-venv system package and run the following command, which creates lama_venv virtual env with LightAutoML inside:\nbash build_package.sh\nTo check this variant of installation and run all the demo scripts, use the command below:\nbash test_package.sh\n\nDocs generation\nTo generate documentation for LightAutoML framework, you can use command below (it uses virtual env created on installation step from sources):\nbash build_docs.sh\nBuilded official documentation for LightAutoML is available here.\n\nUsage examples\nTo find out how to work with LightAutoML, we have several tutorials:\n\nTutorial_1. Create your own pipeline.ipynb - shows how to create your own pipeline from specified blocks: pipelines for feature generation and feature selection, ML algorithms, hyperparameter optimization etc.\nTutorial_2. AutoML pipeline preset.ipynb - shows how to use LightAutoML presets (both standalone and time utilized variants) for solving ML tasks on tabular data. Using presets you can solve binary classification, multiclass classification and regression tasks, changing the first argument in Task.\nTutorial_3. Multiclass task.ipynb - shows how to build ML pipeline for multiclass ML task by hand\n\nEach tutorial has the step to enable Profiler and completes with Profiler run, which generates distribution for each function call time and shows it in interactive HTML report: the report show full time of run on its top and interactive tree of calls with percent of total time spent by the specific subtree.\nImportant 1: for production you have no need to use profiler (which increase work time and memory consomption), so please do not turn it on - it is in off state by default\nImportant 2: to take a look at this report after the run, please comment last line of demo with report deletion command.\nFor more examples, in tests folder you can find different scenarios of LightAutoML usage:\n\ndemo0.py - building ML pipeline from blocks and fit + predict the pipeline itself.\ndemo1.py - several ML pipelines creation (using importances based cutoff feature selector) to build 2 level stacking using AutoML class\ndemo2.py - several ML pipelines creation (using iteartive feature selection algorithm) to build 2 level stacking using AutoML class\ndemo3.py - several ML pipelines creation (using combination of cutoff and iterative FS algos) to build 2 level stacking using AutoML class\ndemo4.py - creation of classification and regression tasks for AutoML with loss and evaluation metric setup\ndemo5.py - 2 level stacking using AutoML class with different algos on first level including LGBM, Linear and LinearL1\ndemo6.py - AutoML with nested CV usage\ndemo7.py - AutoML preset usage for tabular datasets (predefined structure of AutoML pipeline and simple interface for users without building from blocks)\ndemo8.py - creation pipelines from blocks to build AutoML, solving multiclass classification task\ndemo9.py - AutoML time utilization preset usage for tabular datasets (predefined structure of AutoML pipeline and simple interface for users without building from blocks)\ndemo10.py - creation pipelines from blocks (including CatBoost) to build AutoML , solving multiclass classification task\ndemo11.py - AutoML NLP preset usage for tabular datasets with text columns\ndemo12.py - AutoML tabular preset usage with custom validation scheme and multiprocessed inference\n\n\nContributing to LightAutoML\nIf you are interested in contributing to LightAutoML, please read the Contributing Guide to get started.\n\nQuestions / Issues / Suggestions\nWrite a message to us:\n\nAlexander Ryzhkov (email: AMRyzhkov@sberbank.ru, telegram: @RyzhkovAlex)\nAnton Vakhrushev (email: AGVakhrushev@sberbank.ru)\n\n'], 'url_profile': 'https://github.com/sberbank-ai-lab', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['Meshed GPs for spatial or spatiotemporal regression\nInstall with devtools::install_github(""mkln/meshgp""). A full documentation for the code is planned.\nRefer to examples in the analysis folder for some simulations and comparisons.\nList of models that can be implemented currently:\n\nmeshgp::meshgp(): spatially-varying coefficients regression (SVC) using the Z argument to store the dynamic inputs\nmeshgp::mvmeshgp(): latent GP regression on multivariate outputs\nmeshgp::meshgp_dev(): latent GP regression on univariate output with gridded reference set != observed locations\n\nNotes: I\'m always updating the source and some older functions/examples may break. Tested on Ubuntu 18.04 (R-4.0.2 w/ Intel MKL 2019.5 or 2020.1) and CentOS 8.2 (R-4.0.2 w/ OpenBLAS 0.3.10). Not tested on macOS or Windows yet. On CentOS, the default OpenBLAS 0.3.3 shipping with R causes segfaults due to possible conflicts with OpenMP. With OpenBLAS 0.3.10 compiled from source (using make NO_AFFINITY=1 USE_LOCKING=1 USE_OPENMP=1) there are no issues.\n\nCitation: M. Peruzzi, S. Banerjee & A. O. Finley (2020). Highly Scalable Bayesian Geostatistical Modeling via Meshed Gaussian Processes on Partitioned Domains. Journal of the American Statistical Association, in press. DOI: 10.1080/01621459.2020.1833889\nLink to arXiv version with supplement\n\nBibTeX:\n@article{doi:10.1080/01621459.2020.1833889,\nauthor = {Michele Peruzzi and Sudipto Banerjee and Andrew O. Finley},\ntitle = {Highly Scalable Bayesian Geostatistical Modeling via Meshed Gaussian Processes on Partitioned Domains},\njournal = {Journal of the American Statistical Association},\nvolume = {0},\nnumber = {0},\npages = {1-31},\nyear  = {2020},\npublisher = {Taylor & Francis},\ndoi = {10.1080/01621459.2020.1833889},\nURL = {https://doi.org/10.1080/01621459.2020.1833889},\neprint = {https://doi.org/10.1080/01621459.2020.1833889}\n}\n\n'], 'url_profile': 'https://github.com/mkln', 'info_list': ['13', 'Python', 'Apache-2.0 license', 'Updated Dec 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TypeScript', 'MIT license', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '184', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'C++', 'Updated Dec 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Deep Mercer Gaussian Process (DMGP) Regression\nWe provide the code used in our paper Faster Gaussian Processes via Deep Embeddings.\nPrerequisites\nTensorFlow version 2.1.0\nTensorFlow Probability version 0.9.0\nGPflow version 2.0.0 or newer\nSource code\nThe following files can be found in the src directory :\n\ndmgp_model.py: implementation of the DMGP model\nhelper.py: various utility gunctions\nhermite_coeff.npy: a numpy array containing the Hermite polynomial coefficients needed for the DMGP model\nmain_realworld.py: code for replicating the results over the real-world datasets\nmain_simulated_data.py: code for replicating the results over the two simulated datasets\n\n'], 'url_profile': 'https://github.com/aresPanos', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['linear-regression-with-multiple-variables\n'], 'url_profile': 'https://github.com/paramveer1999', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Valencia, Spain', 'stats_list': [], 'contributions': '889 contributions\n        in the last year', 'description': [""Input Uncertainty for Gaussian Processes\n\nAuthor: J. Emmanuel Johnson\nEmail: jemanjohnson34@gmail.com\nDocumentation: jejjohnson.github.io/uncertain_gps\nRepo: github.com/jejjohnson/uncertain_gps\n\n\n\n\n\nCaption: A graphical model of a GP algorithm with the addition of uncertainty component for the input.\n\nThis repository is home to my studies on uncertain inputs for Gaussian processes. Gaussian processes are a kernel Bayesian framework that is known to generalize well for small datasets and also offers predictive mean and predictive variance estimates. It is one of the most complete models that model uncertainty.\n\n\nCaption: Demo showing the error bars for a standard GP predictive variance and an augmented predictive variance estimate using Taylor expansions.\n\nIn this repository, I am interested in exploring the capabilities and limits with Gaussian process regression algorithms when handling noisy inputs. Input uncertainty is often not talked about in the machine learning literature, so I will be exploring this in great detail for my thesis.\n\nMethods\nTaylor Approximation\n\n\n\nCaption: The predictive mean and variance equations with the correction term in red to augment the predictive variance to account for uncertain inputs with a known variance.\n\nWe can approximate the predictive mean and variance equations via a Taylor expansion which adds a corrective term w.r.t. the derivative of the function and the known variance. This assumes we know the variance and we don't modify the predictive mean of the learned GP function.\n\nExact Moment Matching\nTODO\n\nVariational Inference\nTODO\n\nMonte Carlo Estimation\nTODO\n\nMy Resources\nLiterature Review\nI have gone through most of the relevant works related to noisy inputs in the context of Gaussian processes. It is very extensive and it also offers some literature that is relevant but may not explicitly mentioned uncertain inputs in the paper.\nDocumentation\nI have some documentation which has all of my personal notes and derivations related to GPs and noisy inputs. Some highlights include the Taylor approximation, moment matching and variational inference.\nGP Model Zoo\nI have documented and try to keep up with some of the latest Gaussian process literature in my repository.\n""], 'url_profile': 'https://github.com/jejjohnson', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Plainsboro, NJ', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Normal-Equation-vs-Sci-kit-Learn-Linear-Regression\n'], 'url_profile': 'https://github.com/azfar154', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Manish442kumar', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnandPrasadDS', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pjozefek', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Regression\nlearning various techniques of linear regression\n'], 'url_profile': 'https://github.com/rishabhm76', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Kuala Lumpur', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Regression\nFit model using regression\n'], 'url_profile': 'https://github.com/zuhailinasir', 'info_list': ['5', 'Python', 'Apache-2.0 license', 'Updated Apr 6, 2020', '4', 'Jupyter Notebook', 'Updated Feb 10, 2020', '6', 'Jupyter Notebook', 'MIT license', 'Updated Nov 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/AI-club-NDSU', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'San Jose', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression\nLinear and polynomial regression\n'], 'url_profile': 'https://github.com/anuragn4', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lokilk', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Bandung, Indonesia', 'stats_list': [], 'contributions': '285 contributions\n        in the last year', 'description': ['support-vector-reggression\nthis repository is created to learn how implement SVM for classification on specific purpose\nRequirements:\npython      = as Programming language\npandas      = as data manipulation\nmatplotlib  = as data visualization\nseaborn     = as data visualization too (Require import matplotlib to use this module)\nnumpy       = for matrix manipulation\nsklearn     = as KNN algorithm framework\njupyterlab  = as IDE\nyou can install all in once by using this step, but you must install python 3.x.x first!:\n\nopen terminal / cmd inside this file directory, then type this following syntax\npip install -r requirements.txt\nthat all, enjoy!\n\n'], 'url_profile': 'https://github.com/cahyoardhi', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/paramveer1999', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Virginia, USA', 'stats_list': [], 'contributions': '777 contributions\n        in the last year', 'description': ['Two baseline classifiers in Natural Language Processing\nLoad TrainSet: 1181/3000 positive/negative samples.\nVocabulary: 6666 items.\nVocabulary (stem): 5106 items.\nLoad TestSet: 1182/3000 positive/negative samples.\nNaive Bayes Classifier\n\n\n\n\nnoStem\nStem\n\n\n\n\nfreq\n89.43%\n90.20%\n\n\nbin\n89.41%\n89.93%\n\n\ntfidf\n88.47%\n87.97%\n\n\n\nLogistic Regression\nparameters: learning rate = 0.10, iternum = 30000\nWithout regularization:\n\n\n\n\nnoStem\nStem\n\n\n\n\nfreq\n90.00%\n89.89%\n\n\nbin\n89.69%\n90.03%\n\n\ntfidf\n90.17%\n90.15%\n\n\n\nWith L2 regularization:\n\n\n\n\nnoStem\nStem\n\n\n\n\nfreq\n90.05%\n90.27%\n\n\nbin\n89.50%\n89.91%\n\n\ntfidf\n90.17%\n89.43%\n\n\n\nThanks\n'], 'url_profile': 'https://github.com/shuwang127', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['logistic\nlogistic regression\n'], 'url_profile': 'https://github.com/vinay8442', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['StoryTeller\nRegression StoryTeller\n'], 'url_profile': 'https://github.com/NageshShantharaju', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Logistic Regression\nLogistic Regression\n'], 'url_profile': 'https://github.com/DanthuluriSatya', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Diwakar1997', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 16, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Feb 19, 2020', '2', 'Jupyter Notebook', 'Updated Jan 3, 2020', '2', 'Python', 'Updated Feb 27, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['Regression-forecasting\n'], 'url_profile': 'https://github.com/Francimaria', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['Truth-or-bluff-detecting-in-salaries\nPolynomial regression ; comparison between polynomial regression and linear regression ; till degree 4\n'], 'url_profile': 'https://github.com/Prerna99-star', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Arkansas', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['#Objective:\nThe repository is a learning exercise to:\n\nApply the fundamental concepts of machine learning from an available dataset\nEvaluate and interpret my results and justify my interpretation based on observed data set\nCreate notebooks that serve as computational records and document my thought process.\nIdentifying the problem and Data Sources\nExploratory Data Analysis\nPre-Processing the Data\nBuild model to predict the the distance travel by auto\n\n'], 'url_profile': 'https://github.com/madandahal', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['This repository contains the final project for my Applied Regression Analysis (MATH 5387) course taken at the University of Colorado Denver campus in Spring 2019. The project analyzes rental prices using the Data to Policy (D2P) housing data. More information about the Data to Policy project can be found here: https://library.auraria.edu/d2pproject.\n'], 'url_profile': 'https://github.com/JessMurphy', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Davanagere ,Karanataka ', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['SIMPLE_LINEAR_REGRESSION\nsimple regression using Python\n'], 'url_profile': 'https://github.com/vinodrsrs', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regularization\nLasso and Ridge regression\n'], 'url_profile': 'https://github.com/Rishisai', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kesskisspass', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Regression-Template\nnon linear regression template\n'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Scilab', 'Updated Sep 14, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020']}"
"{'location': 'Davanagere ,Karanataka ', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['SIMPLE_LINEAR_REGRESSION\nsimple regression using Python\n'], 'url_profile': 'https://github.com/vinodrsrs', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regularization\nLasso and Ridge regression\n'], 'url_profile': 'https://github.com/Rishisai', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AbhavThakur', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'Buffalo, NY', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['LogisticRegression\nClassification using Logistic Regression\nThis project is used to perform classification using Logistic Regression in python. In this project, a logistic regressor is used to classifying using pre-computed data from the images of a fine needle aspirate(FNA) cells of a breast mass into Benign(class 0) and Malignant(class 1) in this two class problem. The project includes a logistic regression algorithm to validate and test the given dataset and compute accuracy, recall and precision using confusion matrix for the same. Various plots explaining the learning process are included in this project.\n'], 'url_profile': 'https://github.com/Danton25', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/satyan-dot', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['RFR\nRandom Forest Regression\n'], 'url_profile': 'https://github.com/dhirajk1000', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '375 contributions\n        in the last year', 'description': [""doc2date: A Study in Document Regression\nDocument classification is a common application of machine learning techniques. Examples include sentiment analysis, the classification of texts into a (typically small) number of moods (such as positive and negative); as well as authorship attribution in stylometry, in which texts are grouped according to their original author,\nUnsupersived learning methods have also been applied to the analysis of documents. For instance, doc2vec is a dimensionality reduction technique that extends [word embeddings] to documents.\nBut what about document regression? In this notebook, we investigate the problem of learning the date of a publication from the text contained therein. Since the target space, a range of years, can be viewed as a continuum, this problem presents a natural test case for applying regression techniques to document analysis.\nRead more about it\nTry it out (Update 2021.02 I've taken doc2date down)\n""], 'url_profile': 'https://github.com/bencwallace', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/premsugan84', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NeldaRaju', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression_Analytics_Frontend\nAnalyses failures of regression\n'], 'url_profile': 'https://github.com/Srilakshmi-J', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Jupyter Notebook', 'Updated Jun 22, 2020', 'Updated Feb 14, 2020', 'Updated Feb 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Linear-Regression\nSports analytics on Baseball (Money Ball Movie)\nOne of the most compelling stories of sports analytics made popular by the 2011 movie Moneyball, the Oakland Athletics team of 2002 created history by winning 20 consecutive games between August 13 and September 2002. Much of the Oakland Athletics (popularly referred to as the A’s) success in that season is attributed to their General Manager, Billy Beane and former Assistant General Manager, Paul DePodesta.\n'], 'url_profile': 'https://github.com/VivekGutti', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'RABAT', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Regression Analysis and Data visualisation\n'], 'url_profile': 'https://github.com/hamzaboufikr', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Logistic-Regression\nWhat is Logistic Regression\n'], 'url_profile': 'https://github.com/sanjeevy94', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['ML HW3\n'], 'url_profile': 'https://github.com/arjun-natarajan99', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Kuala Lumpur', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression-Till-Deployment https://mllearningdeployment.appspot.com/\nCloud Deployment (Google Cloud Platform)\nOnce the training is completed, we need to expose the trained model as an API for the user to consume it. For prediction, the saved model is loaded first and then the predictions are made using it. If the web app works fine, the same app is deployed to the cloud platform. The application flow for cloud deployment looks like:\n\nPre-requisites:\n\nBasic knowledge of flask framework.\nAny Python IDE installed(we are using PyCharm).\nA Google Cloud Platform account.\nBasic understanding of HTML.\n\nFlask App\nAs we’ll expose the created model as a web API to be consumed by the client/client APIs, we’d do it using the flask framework.\nThe flow of our flask app will be:\n\nCreate the project structure, as shown below:\n\nDeployment to G-cloud:\n\nGo to https://cloud.google.com/ and create an account if already haven’t created one. Then go to the console of your account.\nGo to IAM and admin(highlighted) and click manage resources\n\n\n\nClick CREATE PROJECT to create a new project for deployment.\nOnce the project gets created, select App Engine and select Dashboard.\n\n\n\nGo to https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe to download the google cloud SDK to your machine.\nClick Start Tutorial on the screen and select Python app and click start.\n\n\n\nCheck whether the correct project name is displayed and then click next.\n\n\n\nCreate a file ‘app.yaml’ and put  ‘runtime: python37’ in that file.\nCreate a ‘requirements.txt’ file by opening the command prompt/anaconda prompt, navigate to the project folder and enter the command ‘pip freeze > requirements.txt’.\nIt is recommended to use separate environments for different projects.\nYour python application file should be called ‘main.py’. It is a GCP specific requirement.\nOpen command prompt window, navigate to the project folder and enter the command gcloud init to initialise the gcloud context.\nIt asks you to select from the list of available projects.\n\n\n\n\nOnce the project name is selected, enter the command gcloud app deploy app.yaml\n--project \n\n\nAfter executing the above command, GCP will ask you to enter the region for your application. Choose the appropriate one.\n\n\n\n\nGCP will ask for the services to be deployed. Enter ‘y’ to deploy the services.\nAnd then it will give you the link for your app,and the deployed app looks like:\n\n\n\n'], 'url_profile': 'https://github.com/Santy2208', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Proteem', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '280 contributions\n        in the last year', 'description': ['Client code for linear learner regression\n'], 'url_profile': 'https://github.com/vbloise3', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['DSC-Mini-Project-Linear-Regression\nDSC Mini-Project:Linear Regression\n'], 'url_profile': 'https://github.com/elfudge35', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'Taiwan', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['forecast-exchange-rate-of-TWD-USD\nTime series # Regression # OLS\n\n\n\n\n'], 'url_profile': 'https://github.com/An-ICheng', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/tarOzone', 'info_list': ['Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/Miqyas', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '442 contributions\n        in the last year', 'description': ['Running the app\n1). use to cd command to navigate into the RealEstatePriceEstimator-LinearRegression directory\n$ cd RealEstatePriceEstimator-LinearRegression\n2). Run the following list of commands to initialize a virtual Python environment:\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ pip install --upgrade pip\n3). Install all project dependencies by running the following command:\npip install -r requirements.txt\n4). You may now run the application by running the following command:\n $ env FLASK_APP=server.py flask run\nYour output will look as below:\n(venv) [vismarkjuarez@Vismarks-MBP:~/Documents/Github/RealEstatePriceEstimator-LinearRegression (master)\n $ env FLASK_APP=server.py flask run\n * Serving Flask app ""server.py""\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\nRunning the application in Windows...\npython -m venv venv\nvenv\\scripts\\activate.bat\nset FLASK_APP=server.py\nflask run\n'], 'url_profile': 'https://github.com/VismarkJuarez', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Decision Tree Regression in Machine Learning with Python\n'], 'url_profile': 'https://github.com/karakusfurkan', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'Kochi', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['logistic regression\n'], 'url_profile': 'https://github.com/ashwin951', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Linear-Regression\nThis repository explores Univariate and Multivariate Linear Regression, as well as the Normal Equation Method of solving an equation.\nAuthor\nYou can get in touch with me on LinkedIn!\nIf you liked my repository, kindly support my work by giving it a ⭐!\nAbout this Repository\nLinear Regression Cost Function\nThe cost function of Linear regression is given as:\n\n\n\n\n\nParameter Update\nThe parameter(s) theta, are updated with the equation below.\n\nNotice that alpha is the learning rate/step size.\n\n\n\n\n\n\nVectorization\nThe equations can be vectorized and implemented as matrices.\n\n\n\n\nPlotting Cost against Number of Iterations\n\n\nThis is an effective tool for debugging an algorithm.\n\nThe cost MUST go down over the number of increasing iterations.\n\n\n\n\n\nNormal Equation Method\nThis method is an alternative to regression, and solves for the parameters directly.\n\n\n'], 'url_profile': 'https://github.com/ibrahimzafar', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['DSC-Mini-Project-Logistic-Regression\nDSC Mini-Project:Logistic Regression\n'], 'url_profile': 'https://github.com/elfudge35', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '781 contributions\n        in the last year', 'description': ['regression-algorithm-for-estimation\nRegression Algorithm for Estimation (R)\nRequirement\n\nProgramming Language : R\nIDE Tools : R Studio\n\n'], 'url_profile': 'https://github.com/abdul23lm', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['CISC 4631 Data Mining Project\n\nNYC Airbnb Listing Price Regression\nTerm Paper.pdf contains the term paper I wrote for the project\nlistings-full-08.csv and listings-full-09.csv come from the August and September 2019 NYC dataset respecively on http://insideairbnb.com/get-the-data.html\n\n'], 'url_profile': 'https://github.com/mchen118', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['Statistics-Projects\nCreate linear regression(simple&multiple)\n'], 'url_profile': 'https://github.com/bianca150100', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '274 contributions\n        in the last year', 'description': [""Housing Price Regression\nBy: Ibrahim Patel\nExecutive Summary\nI wanted to make a regresion model that can predict the price of a house based on ceratin features. I gathered my Data from a Kaggle dataset, which is free for anyone to access. According to my finding I have found that there are some features that people should take into consideration when purchasing a home.\nContents\n\nIntroduction\n\nProblem Statement\nDataset\n\n\nAnalysis\n\nData Cleaning\nExploratory Analysis\nModeling\n\n\n\nIntroduction \nI am really interested in real estate, so I thought this would be a fun project. I am going to go through what features have an effect on the price of the house. I will be using an OLS model for this project.\nProblem Statement \nI want to make a Linear Regression model that can predict my Target varaible(Price) as accurately as possible. The way this can be figured out is by Feature Selection. After we have our selected features we will run an OLS. These features variables will help me predict my target(Price).\nDataset \nhttps://www.kaggle.com/lodhaad/house-prices\nAnalysis \nAfter cleaning and dropping many features I did a train test split to split my data into training and test categories. Then I ran an OLS and got an R Squared of .44. An R squared of .44 is not bad, however, I would like to make it better. My model is under fit due to the lack of features. However, there is a simple fix to an underfit model. I started again from the begining and dropped less features than before. After adding more feautres to my model I ran an OLS I got an R Squared of .595. My model improved my roughly 15% by adding 4 extra features.\nData Cleaning \nThis dataset was very messy, and very clean at the same time. I had a lot of values that were placed in as 0's instead of NaN. That made dropping things a little more tricky. Some of the features where not normally distibuted so I had to normalize them. I also had to deal with a good amount of outliers. I did not want the outliers to mess up my model, so I Used SciPy Z- scores. I removed anything that had an absolute value Z-score over 2.5. I ended up removing roughly 3000 outliers.\nExploratory Analysis \nI found that the 3 top features in my dataset are view, bathrooms, and sqft_living. These 3 features have the highest Coefficents, and they are all significant. I used 6 Features for my model the first time around which captured 44% of my data. Then I added 5 more features into my model. I ran the model a second time with 11 features and it raised my R-Squared to .595, which is good because this new model captures 59.5% of the data.\nModeling \nI used Lasso Regression first to see which are the most important features. After doing that I used an OLS stats model to run my Regression model. My final model gave me an R-Squared of appx .60. So this shows that my model captures 60% of my data, which is an improvement from my first model. To improve my model I would need to implement some new features so my model can have a better fit and accuracy.\n""], 'url_profile': 'https://github.com/ipatel467', 'info_list': ['Updated Feb 16, 2020', 'HTML', 'MIT license', 'Updated Apr 21, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jun 6, 2020']}"
"{'location': 'Rwanda', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Linear-regression-from-scratch\nLinear regression from scratch practice.\nFind the good parameter theta using Maximum likelihood estimate (MLE), Maximum a posteriori estimation (MAP), Bayesian linear regression.\nCorrection needed :)\n'], 'url_profile': 'https://github.com/AngelHa', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/julian-ramos', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '232 contributions\n        in the last year', 'description': ['RegressionProject\nBusiness Problem\n\nIn market , There are so many used car . Company who sell used car except luxery used car want to know which factures are important on selling strategy . How can they determine price of used car . There are too many features like mileage , year of model , color , power , condition etc .\n\n\nGoal\n\nThis project helps to company to predict price  of usedcar as their features . Data has been taken kaggle compitition. We explored data with visualization . We deleted some outliers and features which are not important. We did lot of data manipulating section like caetgorical features , delete multicollinearities features. Most important features are detected like Year_box , Engine_log .\n\nResult summarty by OLS and qqplot\n\n\nData\n\nData is on excel format like Data.xlsx.\nData has 6019 volumes and 13 features.\n\n'], 'url_profile': 'https://github.com/Mukesh8688', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['This project was bootstrapped with Create React App.\nAvailable Scripts\nIn the project directory, you can run:\nyarn start\nRuns the app in the development mode.\nOpen http://localhost:3000 to view it in the browser.\nThe page will reload if you make edits.\nYou will also see any lint errors in the console.\nyarn test\nLaunches the test runner in the interactive watch mode.\nSee the section about running tests for more information.\nyarn build\nBuilds the app for production to the build folder.\nIt correctly bundles React in production mode and optimizes the build for the best performance.\nThe build is minified and the filenames include the hashes.\nYour app is ready to be deployed!\nSee the section about deployment for more information.\nyarn eject\nNote: this is a one-way operation. Once you eject, you can’t go back!\nIf you aren’t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.\nYou don’t have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.\nLearn More\nYou can learn more in the Create React App documentation.\nTo learn React, check out the React documentation.\n'], 'url_profile': 'https://github.com/rafi9898', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'Noida', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['BoardGameReview\nPredicting Board Game Review using Linear Regression and   Random Forest Regression ,then comparing the results.\nIt invloves\n\ndata cleaning and preproccessing,i.e dropping rows with na values and removing unneccessary features etc.\nBuilding Linear Regression and  Random Forest Regression Models\n\n'], 'url_profile': 'https://github.com/prince721', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/skjain10', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'Plainsboro, NJ', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Logistic Regression made with Scratch vs Sci-Kit Learn\nLesson about Logistic Regression and the logistic function.\nImportant files\nLogistic_Regression_from_Scratch.ipynb\nUsefullness\nLearn sigmoid functions and logistic regression loss. Also, the model made with numpy and its compared to a LogisticRegression model.\n'], 'url_profile': 'https://github.com/azfar154', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Natural-Language-Processing\nGenerate a Conv1d model with nonlinear regression\n'], 'url_profile': 'https://github.com/Hitesh2176', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gangwarv', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Test-Repo\nHeart Diseases prediction using Logistic Regression Algorithm\n'], 'url_profile': 'https://github.com/anku1412', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'TypeScript', 'Updated Mar 1, 2020', 'Python', 'Updated Apr 8, 2020', 'HTML', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020']}"
"{'location': 'Montreal, QC, Canada', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['Logistic Regression\nImplementation of Logistic Regression from scratch.\n'], 'url_profile': 'https://github.com/talha-riaz', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'Atlanta, Georgia', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Sklearn-Linear-Regression-digits-dataset-\nTesting linear regression model on preloaded dataset\n'], 'url_profile': 'https://github.com/Vnixon7', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'Arkansas', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Welcome to my GitHub repository on Using Predictive Analytics model to predict the house price.\n#Objective:\nThe repository is a learning exercise to:\n\nApply the fundamental concepts of machine learning from an available dataset\nEvaluate and interpret my results and justify my interpretation based on observed data set\nCreate notebooks that serve as computational records and document my thought process.\nIdentifying the problem and Data Sources\nExploratory Data Analysis\nPre-Processing the Data\nBuild model to predict the price of the House\n\n'], 'url_profile': 'https://github.com/madandahal', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sebastian118', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'São Paulo - Brazil', 'stats_list': [], 'contributions': '221 contributions\n        in the last year', 'description': ['Kaggle_HousePrices\nKaggle competition - House Prices - Advanced Regression Techniques\n'], 'url_profile': 'https://github.com/rpdieego', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['DecisionTreeClassifier\nA Decision Tree for classification problems\n'], 'url_profile': 'https://github.com/rmahajani31', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'IIT Bombay', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yashkhem1', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/geo-lev', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PoobalanMuthu', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': [""linear_regression\nLinear Regression using gradient descent in Python3\nI used the data from Andrew Ng's 'Machine Learning' coursera class. The data is from exercise1.\nGradient descent is used to update the parameters.\n""], 'url_profile': 'https://github.com/mhiyer', 'info_list': ['Python', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 12, 2020', '1', 'Roff', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Logistic Regression with Python\n'], 'url_profile': 'https://github.com/karakusfurkan', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'Atlanta, Georgia', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Sklearn-Linear-Regression---CPU-Efficency-prediction\nPredicting Computer efficiency using linear regression\n'], 'url_profile': 'https://github.com/Vnixon7', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/babbarutkarsh', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/helamhiri', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dl19947', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Titanic_LogisticRegression\nTitanic Survivors Prediction using Logistic Regression\n'], 'url_profile': 'https://github.com/PythonKatie', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'Agadir Morocco', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AbdoElfathi', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Santhosh5411', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'Egypt', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['SimpleLinearRegression\nSimple Linear Regression Python Raw methods\n'], 'url_profile': 'https://github.com/MGhandour92', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['LBB-Regression-Analysis\nRegression Analysis on Life Expectancy using R\n'], 'url_profile': 'https://github.com/yevonnaelandrew', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 16, 2020']}"
"{'location': 'New Brunswick, NJ', 'stats_list': [], 'contributions': '1,042 contributions\n        in the last year', 'description': ['RegressionFormulae\n\n\n\n\n\nExtended StatsModels.jl\n@formula syntax for\nregression modeling.\nExamples\nusing RegressionFormulae, StatsModels, GLM, DataFrames\n\nSupported syntax\nNesting syntax\na / b expands to a + fulldummy(a) & b.\nRaising terms to a power\nGenerate all main effects and interactions up to the specified order.  For\ninstance, (a+b+c)^2 generates a + b + c + a&b + a&c + b&c, but not a&b&c.\nApproach\nExtended syntax is supported at two levels.  First, RegressionFormulae.jl\ndefines apply_schema methods that capture calls within a @formula to the\nspecial syntax (^, /, etc.).  Second, we define methods for the\ncorresponding functions in Base (Base.:(^), Base.:(/), etc.) for arguments\nthat are <:AbstractTerm which implement the special behavior, returning the\nappropriate terms.  This allows the syntax to be used both within a @formula\nand for constructing terms programmatically at run-time.\n'], 'url_profile': 'https://github.com/kleinschmidt', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Polynomial Regression\nPolynomial Regression model built in TensorFlow.js\nDescription\nThe model trains at runtime according to the data points drawn on canvas, optimizes coefficients (by minimizing the MSE Loss function using Adam optimizer with a Learning Rate of 0.2) to get a best curve fitted line for the data points drawn on canvas.\n'], 'url_profile': 'https://github.com/prince3103', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""A Pokemon classifier using logistic regression.\nIn Jupyter Notebook form so you can play with it easily and test your own models. Other approaches were considered, such as SVM, GloVe (with logistic regression) and Naive Bayes. But logistic regression and tf-idf had the highest F1 score (~75%).\nUses minimal data, which have been adapted from veekun's Pokedex repo, which contains all the data you never knew you wanted about Pokemon.\n""], 'url_profile': 'https://github.com/GiorgioPorgio', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'St Louis', 'stats_list': [], 'contributions': '338 contributions\n        in the last year', 'description': ['Robust-Linear-Regression\nRobust Linear Regression Against Training Data Poisoning\n'], 'url_profile': 'https://github.com/yahoochen97', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NeuralNetwork\nLogistic Regression with a Neural Network mindset\nWe are going to build a logistic regression from a neural network point of view to separate 2 classes from the MNIST dataset.\nThe neural network will have 28x28 = 784 input neurons, no hidden layers and 1 output with a sigmoid as activation function.\nThe output (𝑦) will take values from 0 to 1 and can be thresholded at 0.5 to predict the classes.\n'], 'url_profile': 'https://github.com/carlosmateo10', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'State College, PA', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yhy188', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Syracuse, New York', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shloak26', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': [""UV Resistance predictor\nThis software is used to predict a UV resistance test result of a coating mix. In paritcular, given a dataset of measurements for different glasses in the past, this software can be able to predict whether or not a UV resistance value of a new coating mix will be within the target intervals.\nUsage\nThis software was implemented in Python. To run it, a machine must have installed Python version 3.7 with necessary libraries such as numpy, pandas, matplotlib, seaborn, sklean.\nTo launch the program, using the following command:\npython3 UV_resistance_predictor.py training_data predicting_data\n\nParameters:\n\ntraining_data is a xlsx file that contains coating measurements in the past.\npredicting_data is a csv file that contains a set of coating mix required to be predicted.\n\nInput data\n\ntranining data: The format of training data is the same coating measurements that experts provided.\npredicting data is a csv file. The first line presents a list of column names. Each following line corresponds to a coating mix which is needed to be predicted. *** Note that, the csv file uses Tab to seperate values ***\n\nFor example, predicting data given in triazine-predict-data.csv contains following information:\nEmulsion Thickness\tTriazine % in Paint 1\tTriazine % in Paint 2\tPaint 1 Cohesion\tPaint 2 Cohesion\tPaints 1 & 2 Mix Ratio (%)\tGlass temp. prior coating\tMin Resistance\tMax Resistance\n13\t60\t85\t50.4\t54.0\t45.5\t30\t0.783\t0.919\n14\t60\t85\t57.2\t54.8\t76.0\t31\t0.62\t0.751\n11\t60\t85\t43.2\t53.2\t82.0\t32\t0.637\t0.758\n10\t60\t85\t58.8\t52.0\t58.0\t32\t0.862\t1.002\n19\t60\t85\t58.8\t52.8\t70.0\t32\t0.848\t1.039\n\nOutput\nFor each coating mix, the software produces the following information:\n\nsample : id\ninput values: the input values of coating mix\npredict label: [1] or [0]. [1] means that the final UV resistance satisfies the given intervals. In contrast, 0 means that the final UV resistance doesn't satisfy the given intervals.\nprobability: [x y]. x is a probabily to have prediction label of 0; y is a probability to have prediction label of 1\n\nFor example, belows are results of the software when executing the following command:\npython3 UV_resistance_predictor triazine-coating-measurements.xlsx triazine-predict-data.csv\n==================================\n      PREDICTION RESULTS        \n==================================\nsample        : 1\ninput values  : [[13.    60.    85.    50.4   54.    45.5   30.     0.783  0.919]]\npredict label : [0]\nprobability   : [[0.53454755 0.46545245]]\n--------------------------------\nsample        : 2\ninput values  : [[14.    60.    85.    57.2   54.8   76.    31.     0.62   0.751]]\npredict label : [1]\nprobability   : [[0.04501496 0.95498504]]\n--------------------------------\nsample        : 3\ninput values  : [[11.    60.    85.    43.2   53.2   82.    32.     0.637  0.758]]\npredict label : [1]\nprobability   : [[0.09615758 0.90384242]]\n--------------------------------\nsample        : 4\ninput values  : [[10.    60.    85.    58.8   52.    58.    32.     0.862  1.002]]\npredict label : [0]\nprobability   : [[0.8338067 0.1661933]]\n--------------------------------\nsample        : 5\ninput values  : [[19.    60.    85.    58.8   52.8   70.    32.     0.848  1.039]]\npredict label : [1]\nprobability   : [[0.23789435 0.76210565]]\n--------------------------------\n\nThe best coating mix\n==================================\nsample       : 2\ninput values : [[14.    60.    85.    57.2   54.8   76.    31.     0.62   0.751]]\nprobability  : 0.9549850424353914\n\nBased on the results, we can see that the UV resistance test results of samples 2, 3 and 5 satisfy the intervals with the probabilities of 95%, 90% and 76% respectively. Whereas, the UV resistance test results of samples 1 and 4 are predicted 'failed test'. The best coating mix is the sample 2 because it has the highest probability.\n""], 'url_profile': 'https://github.com/phamsonit', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['House_price_prediction\nHouse price prediction using advanced regression techniques\n'], 'url_profile': 'https://github.com/hgdivya', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Logistic-Regression-on-the-famous-Titanic-Dataset\nData Cleaning, EDA and Logistic Regression on the famous Titanic Dataset\n'], 'url_profile': 'https://github.com/Sorooshir', 'info_list': ['1', 'Julia', 'MIT license', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Apr 22, 2020', 'Python', 'Updated Feb 12, 2020', 'R', 'MIT license', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'San Antonio, Texas', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danbar0', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NNK Graph\nPython source code for the paper: \nGraph Construction from Data using Non Negative Kernel regression (NNK Graphs).\nTo be presented at ICASSP 2020.\nCiting this work\n@article{shekkizhar2020graph,\n    title={Graph Construction from Data by Non-Negative Kernel regression},\n    author={Sarath Shekkizhar and Antonio Ortega},\n    year={2020},\n    booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \n}\n\n@misc{shekkizhar2019graph,\n    title={Graph Construction from Data using Non Negative Kernel regression (NNK Graphs)},\n    author={Sarath Shekkizhar and Antonio Ortega},\n    year={2019},\n    eprint={1910.09383},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n\nKnown Issues\n\nThe graph plot does not show edges.\n\nThis is a shortcoming of the backend being used.\nOnly graphs with number of edges ~10,000 or less shows edges in plot.\nThe visualization is for demo purposes and this issue will not be fixed in the near future.\n\n numpy.linalg.LinAlgError: {m}-th leading minor of the array is not positive definite\n\nOne or more of the data point is too close to each other.\nPossible fixes: Try changing sigma to better distinguish data points or\nincreasing epsilon_high parameter used in non_neg_qpsolver function.\nThe issue can also be fixed by increasing command line parameter thresh.\n'], 'url_profile': 'https://github.com/STAC-USC', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Linear_Regression_2\nIts my second project on Linear Regression\n'], 'url_profile': 'https://github.com/ganguly-cloud', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""PolyRegression\nFinding polynomial regression using simple NN. Following Horner's method to represent apolynomial of degree n as a composition of n linear function as following: p(x) = a_0+a_1 x+a_2 x^2+...+a_n x^n = a_0+x(a_1+x(a_2+x(...(a_n-1+a_n x))). Horner's method is proven to be quite useful for handling and computing polynomial expressions.\nThis simple form provides a unique insight to generate a deep NN with m layers with Linear activation function (ax+b), however, mulliplying each node with the input, x.\n""], 'url_profile': 'https://github.com/fqassemi', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['House-Price-Prediction-Model\nBackground of Problem Statement : The US Census Bureau has published California Census Data which has 10 types of metrics such as the population, median income, median housing price, and so on for each block group in California. The dataset also serves as an input for project scoping and tries to specify the functional and nonfunctional requirements for it.\nProblem Objective : The project aims at building a model of housing prices to predict median house values in California using the provided dataset. This model should learn from the data and be able to predict the median housing price in any district, given all the other metrics. Districts or block groups are the smallest geographical units for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). There are 20,640 districts in the project dataset.\nDomain: Finance and Housing\nLoad Data and Packages\nAnalyzing the Test Variable (Sale Price)\nMultivariable Analysis\nImpute Missing Data and Clean Data\nFeature Transformation/Engineering\nModeling and Predictions\nAnalysis Tasks to be performed:\n1.Build a model of housing prices to predict median house values in California using the provided dataset.\n2.Train the model to learn from the data to predict the median housing price in any district, given all the other metrics.\n3.Predict housing prices based on median_income and plot the regression chart for it.\n'], 'url_profile': 'https://github.com/sheebavar', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Various Regression models implementation from scratch\nImplemented models\nLinear Regression\nRidge Regression\nLasso Regreesion\nLocally Weighted Regression\n'], 'url_profile': 'https://github.com/ram-asari', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Regression-Analysis-on-Red-Wine-Quality-DataSet\nOverview:\nThis is a practice of using regression analysis to predict Red Wine quality.\nInstall:\nThis project requires Python 3.x and the following libraries installed:\n\nNumpy\nPandas\nsklearn\nseaborn\nmath\n\n'], 'url_profile': 'https://github.com/YanZhang0623', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['RegressionModelProject\n'], 'url_profile': 'https://github.com/nima14', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Sep 20, 2020', 'Python', 'MIT license', 'Updated Oct 7, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['SAS-Project\nA statistical project using SAS to estimate and compare multiple linear regression and multiple logistic regression models\nChoose 3 lists of 36 stocks on the TSX from\nhttp://clouddc.chass.utoronto.ca.ezproxy.library.yorku.ca/ds/cfmrc/\nPick 36 stocks based on My name- YA JUN BAI\n• 36 stocks whose symbols begin with Y or A (call this the 1st group from 2013)\n• 36 stocks whose symbols begin with J or U (call this the 2nd group from 2014)\n• 36 stocks whose symbols begin with B or A (call this the 3rd group from 2015)\nand get an end of day price for ALL of them on the same date (April 9).\nWe are using SAS to estimate a multiple regression model, 𝑦 = 𝑓(𝑥1, 𝑥2),\nY= B0+B1X1+B2X2\ny- Stock price of 2015\nx1- Stock price of 2013\nx2- Stock price of 2014\nand justify the scientific significance of the variable(s) and the estimated model with t test, F test,\nR-squared, adjusted R squared. For each test, we identify the null and alternative hypotheses, the\nstatistic value, and the p-value and conclusion.\nWe are using SAS to estimate a multiple regression model, log𝑦 = 𝑓(𝑥1, 𝑥2),\nlogY= B0+B1X1+B2X2\ny- Stock price of 2015\nx1- Stock price of 2013\nx2- Stock price of 2014\nand justify the scientific significance of the variable(s) and the estimated model with t test, F test,\nR-squared, adjusted R squared. For each test, we identify the null and alternative hypotheses, the\nstatistic value, and the p-value and conclusion.\nFinally, make Comparation of two regression model\n'], 'url_profile': 'https://github.com/AUSB', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['masters-machine-learning-1\nCodes encompassing topics such as linear regression, logistic regression, clustering and pca.\n'], 'url_profile': 'https://github.com/anishmahapatra', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': [""ML-Model-for-Price-Prediction\nMACHINE LEARNING ADVENTURE SERIES\nThe model I have tried is **Decision Tree Regressor, Random Tree Regressor and Support Vector Regression (From SVM)\nBuilding a Machine Learning model of housing prices in a state using California Census data.\nObjective: To output a prediction of a district's median housing price. (District comprises 600-3000 people in the area)\nSolution: A supervised learning task, a multiple univariate regression problem using batch learning process as data is old.\nPython Libraries used: Numpy, Pandas, Scikit-Learn\nI will go through various steps required to build a ML model from scratch.\n\nFraming the problem by looking at the big picture\nGet the Data\nVisualise data to gain insights and discover trends and anomalies\nPrepare the data for Machine Learning algorithms\nSelect a model and train it\nFine tune my model\nPresent my solution\nPending-- Launch my model with a visual interface using cloud\n\n""], 'url_profile': 'https://github.com/SharmaSapan', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': [""Board-Game-Rating-Predictor\nThis is a basic regression problem in which I tried to\ntrain Linear Regression Model and Random Forest Regressor\nto predict  ratings of games.\nIt's just a practice project to know the problem better.\n""], 'url_profile': 'https://github.com/Danish64', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['""#Feature-Engineering""\n'], 'url_profile': 'https://github.com/premsugan84', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '202 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deeprajbasu', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3,211 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShaikMohamedIrfan', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Simple_LR\nWeather prediction using Simple Linear Regression with sklearn\n'], 'url_profile': 'https://github.com/Triple-S-India', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Titanic-Prediction-Using-Logistic-Regression\nPredicting the titanic survival using logistic regression algorithm and Flask\nAuthor\nDinisuru Gunaratna\n'], 'url_profile': 'https://github.com/dinisurunisal', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'CSS', 'Updated Feb 13, 2020', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020']}"
"{'location': 'Banglore', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Linear_Regression_3\nMy 3rd project of Linear Regression including future engineering\n'], 'url_profile': 'https://github.com/ganguly-cloud', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Remotely or anywhere you need! ', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/evamintz', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Mumbai, Maharashtra, India', 'stats_list': [], 'contributions': '226 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nerds-coding', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['#python\nq1.py contains lin regression with single variable(dataset: ex1data1)\nq2.py contains lin regression with multiple variable(dataset: ex1data2)\nq3.py contains the converge using normal equations (dataset: ex1data2)\n'], 'url_profile': 'https://github.com/haider-tariq37', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Mumbai, Maharashtra, India', 'stats_list': [], 'contributions': '226 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nerds-coding', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Montreal, QC, Canada', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Reddit Commnets Popularity\nIn this project, we will predict a popularity score for Reddit comments using Linear Regression. The dataset is a collection of\ncomments from r/AskReddit forum, which is a question-answering forum within Reddit. Since these comments are directly downloaded from\nReddit, they may contain inappropriate and foul language.\nWe conduct the Linear Regression using two approaches:\n\nClosed-form solution:\n\n\n\n\nGradient descent: \n\n\n\nFor evaluation, we use Mean Squared Error (MSE) and the time it takes to run each of the above approaches.\nDataset\nThe dataset is a .json file of a NumPy list containing 12000 comments. Each comment is a dictionary with the following keys:\n\ntext: the actual comment\ncontroversiality: This is a metric of how ""controversial"" a comment is. It is a proprietary metric\ncomputed by Reddit and takes on binary values.\nis_root: A binary variable indicating whether this comment is the ""root"" comment of a discussion thread.\nchildren: This counts how many replies this comment recieved.\npopularity_score: The target score which we are trying to predict.\nWe split the dataset into Train/Validation/Test partitions. \nThe dataset file is in the data folder.\n\nPreprocessing\nAs the comments are taken from real discussions, we need to remove irrelevant information (such as numbers, punctuations, etc.)\nfrom each comment. This increases the performance of the regressor.\nGetting Started\nIn order to run the code, you can either use a Python3 kernel in your Jupyter Notebook or any Python IDE.\nPrerequisites\nInstall the following packages:\n\nMatplotlib\njson\nNumPy\n\nExperiment\nDownload the repository in your local computer.\ngit clone https://github.com/PouriaCh/RedditComments.git\n\nIf all of the required packages are installed, you will have the results.\n'], 'url_profile': 'https://github.com/PouriaCh', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""Regression-Technique-and-TensorFlow\nHouse Prices: Advanced Regression Technique and TensorFlow (DeepLearning)\nToday, Data Science is nearly everywhere. Data science in the business domain includes the rate at which the business proliferates, using various models for reason I used linear regression and deep leaning to predict sales prices and practice feature engineering. When you ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad, this dataset proves that much more influences price negotiations than the number of bedrooms or a fence. The goal is to estimate the probability at which someone will quote the price rather than the features of their dream house.\nRegression-Technique\nRidge is linear least squares model with l2 regularization (using squared difference).\nRidgeCV is Ridge regression with built-in cross-validation.\nLasso is Linear Model trained with l1 regularization (using module).\nLassoCV is Lasso linear model with iterative fitting along a regularization path.\nRandom Forest is usually good in cases with many features.\nAnd XGBoost is a library which is very popular lately and usually gives good results.\n""], 'url_profile': 'https://github.com/mercykoko', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Atlanta, Georgia', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Sklearn-Linear-regression-model-Income-\nTesting the Sklearn Linear Regression model on income data set\n'], 'url_profile': 'https://github.com/Vnixon7', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'San Francisco, California', 'stats_list': [], 'contributions': '1,557 contributions\n        in the last year', 'description': ['The Talk\n\nhttps://slides.com/forres/testing-for-lazy-people\n\n'], 'url_profile': 'https://github.com/fforres', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Orange County', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['FruityRegression\nLogistic Regression Model Trained to Identify Fruits on Sample with Acute Variability\nSummary\nThis Logistic Regression Model uses scikit-learn (https://scikit-learn.org/) method to train a model using logistic regression to identify fruits. \nMethods:\n-The data set of images was split utilizing ""train_test_split"" : 75% training and 25% testing.\n-Converted image arrays into 2-d arrays without degrading the numeric values associated with RGB values, ""jpgarray.reshape(-1).""\n-Plotted a confusion matrix. \n-Tested the model on fruit images outside the dataset. \n-Modified the confusion matrix to plot outside images on their predicted and actual values. \nConclusions:\n\n-The model predicted every image in the training set accurately. \n-We received a mean accuracy score of 1.00, a perfect score.v\n-We believe the data source lacks variability, give the model no room for error when test on images not too different from the images \n\n-When utilizing the model to predict the fruit classification of images outside the dataset, the model was found to be highly erroneous. \nData Source:\nImages are of the fruit subjects with a white background with the overall img dimmensions of 100 x 100. \n""Fruits and vegetables were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded.\nA Logitech C920 camera was used for filming the fruits. This is one of the best webcams available.\nBehind the fruits we placed a white sheet of paper as background.""\nHorea Muresan, Mihai Oltean, Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018.\n\n\n\nThree examples above are a snapshot of the images in the dataset. \n'], 'url_profile': 'https://github.com/Aquanewbie', 'info_list': ['Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Feb 22, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'JavaScript', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 24, 2020']}"
"{'location': 'Trichy, India', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jaiganesh-MK', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Linear_Regression\nLinear Regression\n'], 'url_profile': 'https://github.com/ajaymuktha', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['House Prices:Advanced Regression Techniques\nPredicting the house prices using  xgboost regression technique\n'], 'url_profile': 'https://github.com/ashitaag', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['logisticregressiondiabetes\nLogistic Regression and binary classification on Diabetes database kaggle\n'], 'url_profile': 'https://github.com/mar33', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shkev', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Predict-Customer-Churn\nPredict telecommunications customer churn using logistic regression in R (Markdown file) using ~7K observation data set\n'], 'url_profile': 'https://github.com/DataIsEverywhere', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['linear regression with sklearn library\nhere we performed linear regression using python famous sklearn library\n'], 'url_profile': 'https://github.com/rishisankhla', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Syracuse, NY', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': [""ID BLOCK\n\nEric Zair\nGradient Descent (Singular)\n02/03/2020\n\nProject Description\nThe goal of this project was to implement 3 different ways to do linear regression on a database that contains used car information. After collecting all of the data using turicreate, the goal was simple, given only the mileage of the car how can we estimate the expected cost for the car?\nNote this program is built specifically for single feature gradient descent not multiple.\nThe three different methods\n\n\nBuilt in linear regression model in turicreate\n\n\nCalculating the closed for for our gradient equations\n\n\nHand implementing gradient decent\n\n\nUsing the built in linear regression model\nUsing the built in features was rather easy and yields a better answer than my implementation of gradient descent. Granted this hold true because...my implementation of gradient descent will generate nans and loop for quite the long time due to not finding the correct step size. As it turns out if your step size and magnitude used are not sufficient, then your descent gradient never actually converges, which becomes quite the issue.\nClosed form\nClosed form appears to be the way that linear regression should really be done (at least for a small amount of features). The closed form method provides you with a straightforward mathematical way of accomplish your linear regression. Since you do not have to worry about things like step size and all we are really doing are a few simple linear calculations (and later with more features matrix multiplications), we get straight to the point. The speed of doing this is arguably faster than that of gradient descent. We do not loop and loop until we find the estimated or close to perfect solution, but rather we know for a fact that the solution is 100% optimal, since it was mathematically calculated.\nMy gradient descent\nAs it turns out, my implementation of gradient descent is truly flawed. When run, it may appear to be working, but then shortly after you will run into things such as an inf values, which clearly is a problem. I believe that the result of this has to do with having to find the proper step size or magnitude, but I could be wrong as well. Assuming that this implementation...worked properly, we should be receiving a number that is near similar to that of what is produced in the turicreate linear regression model.\nSetup\nIf you want to make sure that you have the proper python packages to run any of the implantation (that are stored in the src/ folder) run the dev_setup.py script.\nRunning each implementation\nEach implementation is stored in it's own .ipynb file. Simply use something like anaconda, conda, jupyter notebook, or google collab in order to run the programs.\nclosed_form.ipynb contains the closed form implementation.\nmy_gradient_implementation.ipynb contains my implementation.\nturicreate_linear_regression_using.ipynb contains the linear regression model implementation.\n""], 'url_profile': 'https://github.com/ezair', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amank56', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Non-Linear-Regression-with-One-Dimension-Convolution-Neural-Network\nBuild a regression-based  One-Dimension Convolution Neural Network\n'], 'url_profile': 'https://github.com/jugalshah291', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 17, 2020', 'C', 'Updated Feb 23, 2020', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ndzamela', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Atlanta, Georgia', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['#Sklearn Linear Regression and KNearestNeighbor Prediction for Final Grades\nPredicting final grade of students using linear regression, and KNearestNeighbor methods\n'], 'url_profile': 'https://github.com/Vnixon7', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amit-peng', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '249 contributions\n        in the last year', 'description': ['california-housing-regression\nregression analysis of california housing dataset using Neural Network\nSeveral iterations and architecturs of a multi-layer perceptron built and compiled with Keras API and tensorflow back-end.\nThis is compared against K-Nearest Neighbors Algorithm.\nThe dataset is normalized for use with KNN and MLP to reduce large variance accross many features.\n'], 'url_profile': 'https://github.com/mmitk', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Berkeley, California', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['binary_classification_R\nTutorial for binary classification (logistic regression, SVMs) in R.\n'], 'url_profile': 'https://github.com/seantrott', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Simple_Linear_Reg_Scratch\nSimple linear regression implementation using Scratch( Without SKLearn)\n'], 'url_profile': 'https://github.com/Triple-S-India', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Planar-Data-Classification\nPlanar Data Classification using Logistic Regression and Neural Network approach\n'], 'url_profile': 'https://github.com/prasannamarudhu', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Jasmine Pixelmatch\n\nHTML5 canvas visual difference checking in the Jasmine test framework with pixelmatch.\nFeatures\n\nSee differences\nBase64 image dump of images in headless environments\n\nInstallation\nnpm install @recreatejs/jasmine-pixelmatch\nExample\nTo obtain a reference image to compare against, just set up your test with no reference image, like so:\ndescribe(""visual canvas test"", function() {\n  it(""looks right"", function(done) {\n    let canvas = drawACanvas(200, 200);\n    let context = canvas.getContext(""2d"");\n    let canvasData = context.getImageData(0, 0, canvas.width, canvas.height);\n    expect(canvasData).toVisuallyEqual(); // no argument\n  });\n});\nThen save the image shown, and make it available to the test runner.\nUpdate your test to load and pass in the image:\ndescribe(""visual canvas test"", function() {\n  it(""looks right"", async function(done) {\n    let canvas = drawACanvas(200, 200);\n    let context = canvas.getContext(""2d"");\n    let canvasData = context.getImageData(0, 0, canvas.width, canvas.height);\n    let referenceData = await fetchImageData(""saved_image.png"");\n    expect(canvasData).toVisuallyEqual(referenceData);\n  });\n});\n'], 'url_profile': 'https://github.com/ReCreateJS', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Bremen, Germany', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': [""Testing random utility discrete choice for housing location. UK census Brent\nThis project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 713639.\n""], 'url_profile': 'https://github.com/RoccoPaolillo', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Bhopal Madhya Pradesh', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hitechgaurav', 'info_list': ['MIT license', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'HTML', 'Updated Mar 8, 2020', 'Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'MIT license', 'Updated Feb 26, 2020', 'TeX', 'MIT license', 'Updated Nov 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""Kaggle Exercise: Regression Model\n1. Introduction\nRegression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables.\nIn this project, I want to practice how to use regression model and find out how to improve the regression model. On the first run, I would like to use all numerical data without linearity assessment. I assume the R-squared and RMSE for this model could be the baseline.\nIn the next move, I would like to implement some assumptions on regression model:\n\nData is free of missing values and outliers\nAll variables are continuous numeric, not categorical\nThere is a linear relationship between predictors and predictant\nAll predictors are independent of each other\nResiduals (aka prediction errors) are normally distributed\nNo heteroscedacity\nAbsence of multicollinearity abd auto-correlation\n\nThe linear regression model can be represented by the equation below.\n\nhθ(x) = θo + θ1x1 + θ2x2 + ... + + θnxn\n\n2. The Data: House Prices\nI got the data from the Kaggle Competition. The goal from this competition is to predict sales price for each house. Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.\nDataset has been splitted into train and test. Not like train-dataset, test-dataset didn't have complete data, it's miss the SalePrice column. To complete test-dataset, we merged it with submission.csv.\n2.1. Data Processing\nThe data downloaded, as mentioned above, was cleaned up and processed before using it for model fitting. There was missing value in the data that we need to handle. The data with more than 90% missing value were utterly removed. For data with less than 50%, the missing value was imputed by their median (numeric) or mode (categoric). There was a particular case for correlation in missing value, which means the data was null because data in another column was null. The data after this initial cleanup is shown in Fig 2.\n\n\nAfter initial cleanup, next step was to check variation in categorical data. I used a simple matrix by compared number of each value and their average. The higher result means that column almost contains one value and not fairly distributed. So it can be completely removed when the result more than 70.\n\n\n2.2. Feature Selection and Engineering\nFirst step of feature engineering would be based on regression's assumptions. I would like to compare each assumption.\n2.2.1. Numeric Values Only\nRegression model assume that all features are numeric, more specific conitnuous numeric. For baseline purpose, all numeric value were selected, without scaling process. All categorical features were removed completely. As we know that not all number is continuous, there are categorical in numeric form, we can handle it later.\n\n['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea','BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'MoSold', 'YrSold']\n\n2.2.2. Standardization with StandardScaler\nThe next step is standardization scaling using standardscaler. It was not only to improve score but also to make easier model interpretation and the coefficients more reliable. Standardization is when a variable is made to follow the standard normal distribution ( mean =0  and standard deviation = 1).\nIn a multivariate analysis when variables have widely different scales, variable(s) with higher range may overshadow the other variables in analysis.\n2.2.3. Outlier Handling\nOutlier is a data point that differs significantly from other observations. Outlier can be expressed as value more than $Q3 + 1.5IQR$ or less than $Q1 - 1.5IQR$, with $IQR = Q3 - Q1$. In houseing price data, we did not remove the observation that have outlier value. We transformed the outlier value into $Q3 + 1.5IQR$ as maximum value and $Q1 - 1.5IQR$ as minimum value.\n2.2.4. One Hot Encoding and Correlation Analysis\nOne hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. Or refer to scikit-learn documentation defines one hot encoding is encode categorical integer features using a one-hot aka one-of-K scheme.\nA correlation coefficient is a numerical measure of some type of correlation, meaning a statistical relationship between two variables. They all assume values in the range from −1 to +1, where +1 indicates the strongest positive correlation, -1 the strongest negative correlation and 0 is the weakest correlation. In this training, we would compare three threshold of coefficinet variable: correlation more than +0.5/-0.5, more than +0.3/-0.3 and more than +0.1/-0.1.\n3. Training of Regression Model\nUsing the dataset with features selection in previous section, various models were trained and compared.  The description and results from these models are shown below.\n3.1. Training Result: Numeric Values Only\nThe regression model performs prediction by combining all numeric value without scaling/standardisation. This model would be a baseline for next feature engineering model.\nIntercept:\n\n411780.8240664884\n\nCoefficient:\n\n[-2.09665894e+02, -7.70098616e+01,  4.34117749e-01,  1.80732818e+04, 5.12955066e+03,  2.43820188e+02,  1.19090279e+02,  3.20337074e+01, 9.90873289e+00,  1.43098504e+00, -8.23605447e-01,  1.05161125e+01, 2.12458809e+01,  2.37681600e+01,  2.68109637e+01,  8.57148052e+03, 1.97965513e+03, -1.55984055e+03, -9.63530530e+03,  4.02416519e+03, 5.63014201e+03,  1.73565734e+02,  1.27677216e+04, -4.81269106e+00, 2.06707347e+01, -1.84572283e+00, -4.71000321e+01, -7.66864445e+02]\n\nTraining Score\n\nR-Squared: 0.812\n\n\nRMSE: 34475.861\n\n3.2. Training Result: Standardization with StandardScaler\nWhen all features in the same scale, it's easier to interpret. From these coefficients we could take short conclusion that OverallQual have the most positive impact when MSSubClass have the most negative impact. They means higher OverallQuall will increase SalePrice, but higher MSSubClass will decrease SalePrice. Perhaps it doesn't make sense for higher class will get cheaper sales, or it should remind me that MSSubClass even in numeric, it is still categorical and should get another handling.\nIntercept:\n\n180921.19589041095\n\nCoefficient:\n\n[-8865.9491972 , -1695.76732218,  4331.56009023, 24986.72482286, 5706.20539294,  7361.55537745,  2457.82509304,  5787.51288932, 4581.05657485,   253.13770241,  -302.52250405,  4551.05588833, 8210.58374253, 10371.92384808, 14083.80977611,  4446.30866026, 1090.24972779,  -784.15232517, -7857.57817491,  6538.61069285, 3628.32009093,  4347.1917959 ,  9538.24180844, -1028.62419943, 2589.95753742,  -122.2483754 ,  -127.29726391, -1018.12007687]\n\nTraining Score\n\nR-Squared: 0.812\n\n\nRMSE: 34475.861\n\n3.3. Training Result: Outlier Handling\nAfter we handle the outliers, higher training score was obtained. It should be no high prediction and high error like two previous models. From these coefficients we got another interpretation that GrLivArea and OverallQuall gave the highest impact to increase SalePrice. But feature 1stFlrSF gave the highest impact to decrease SalePrice. It still didn't make sense, so we had to try another feature engineering and model.\nIntercept:\n\n177331.52636986302\n\nCoefficient:\n\n[-4.07203337e+03,  8.23429604e+02,  5.05064923e+03,  1.90011681e+04, 6.25755958e+03,  1.00238015e+04, 4.18254655e+03,  1.67846929e+03, 2.68923125e+03, -3.60387276e-11, -3.07680570e+03,  1.03189793e+04, -5.11900643e+03, -2.51051326e+03,  2.97461941e+04,  2.08124211e+03, -2.06774032e+02, -1.61478651e+03, -4.78380363e+03, -2.06226297e+02, 3.81012858e+03,  2.61730267e+03,  5.60258311e+03,  1.47873951e+03, 1.95300023e+03,  1.42020535e+03,  7.33315555e+02, -7.30005756e+02]\n\nTraining Score:\n\nR-Squared: 0.880\n\n\nRMSE: 23314.928\n\n4. Results\nObservations and additional results obtained after training the models described in the previous section are shown below. Just did standardization in features could made a little improvement.\n\n\n\n\nModel\nTraining: R^2\nTesting: R^2\nTraining: RMSE\nTesting: RMSE\nKaggle Score\n\n\n\n\nNumeric only\n0.812\n0.845\n34475.861\n31671.964\n0.452\n\n\nStandardScaler\n0.812\n0.846\n34475.861\n31556.481\n0.254\n\n\nOutlier handling\n0.880\n0.840\n23314.928\n32156.249\n0.215\n\n\nOne Hot, Corr > .5\n0.787\n0.828\n36671.335\n33418.579\n-\n\n\nOne Hot, Corr > .3\n0.844\n0.872\n31352.439\n28779.365\n-\n\n\nOne Hot, Corr > .1\n0.853\n0.883\n30412.357\n27522.537\n0.168\n\n\n\n5. Conclusion\nStandardization could make good interpretation and good improvement. For the next submission, we should use all data.\n""], 'url_profile': 'https://github.com/fahimhm', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'Orlando, Florida', 'stats_list': [], 'contributions': '760 contributions\n        in the last year', 'description': ['aggregress\nA simple package for regression analysis with aggregated data\n'], 'url_profile': 'https://github.com/LeeMorinUCF', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'NYC', 'stats_list': [], 'contributions': '924 contributions\n        in the last year', 'description': ['from event When Good Assumptions Go Bad in Linear Regression\n'], 'url_profile': 'https://github.com/seanreed1111', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'Zurich Switzerland', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['BIDS-Project-Prestige-Dataset-in-R\nLinear Regression - Predicting Occupational Prestige for Human Resources & Career Planning\n'], 'url_profile': 'https://github.com/justpatrice', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Setul0712', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'Briones, California', 'stats_list': [], 'contributions': '348 contributions\n        in the last year', 'description': ['Egress - barebones regression testing for Rust\nEgress is a super simple regression testing framework for Rust. It\ndoesn\'t currently support much, but if all you want is to make sure\nsome test outputs don\'t change from run to run, it\'ll do the trick.\nBy default, Egress will make an Egress.toml config file in the same\ndirectory as your Cargo.toml and an egress folder in the same place\nto hold the artifacts it writes to disk.\nExample\nlet mut egress = egress!();\nlet artifact = egress.artifact(""basic_arithmetic"");\n\nlet super_complex_test_output_that_could_change_at_any_time = 1 + 1;\n\n// using `serde::Serialize`:\nartifact.insert_serialize(""1 + 1 (serde)"", &super_complex_test_output_that_could_change_at_any_time);\n\n// or using `fmt::Debug`:\nartifact.insert_debug(""1 + 1 (fmt::Debug)"", &super_complex_test_output_that_could_change_at_any_time);\n\n// or using `fmt::Display`:\nartifact.insert_display(""1 + 1 (fmt::Display)"", &super_complex_test_output_that_could_change_at_any_time);\n\n// More options available; please check the docs.\n\negress.close().unwrap().assert_unregressed();\nTo see the artifacts produced by this example, check egress/artifacts/rust_out/basic_arithmetic.json.\n'], 'url_profile': 'https://github.com/sdleffler', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Multiple linear regression model for the prediction of car prices.\nCar Price Prediction are divided into the following sections:\nData understanding and exploration\nData cleaning\nData preparation\nModel building and evaluation\n'], 'url_profile': 'https://github.com/Ankit5Shri', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nSimple Linear Regression using Linear Model of Least Squares\n'], 'url_profile': 'https://github.com/Aditya1598', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['MATH651-Project-Insurance\nPredicting Medical Expenses with Linear Regression (RF & KMeans)\nCo-work with Tong Li, Yujie Gao and Junhong Liu\nI am mainly responsible for Random Forest and K-Means Clustering (including algorithm applications and plotting with ggplot2)\nAbstract\nFor insurers, it’s important to develop models that accurately forecast medical expenses so that they can make money. Through this project, we aim at pretending to work for insurance companies and build different models to best predict the medical cost of individuals given their basic information. Also, through different methods, the article tries to detect inner relationship among different dependent variables to help better understand the impact of different variables on the medical charges. The final prediction result can be used as a benchmark for the insurance company to establish appropriate insurance claim coverage for their contractors.\nWe first applied linear regression models to find significant variables to help predict the medical charges. By adding interaction variables, changing the continuous variable to be categorical variable, we fit several different linear regression models. We detected the abnormality of residuals. We raised some hypothesis and did several tests to try to explain this issue.\nWe also used random forest method to better forecast the charges of individuals and find the importance of different variables.\nAfter that, we applied K-Means to cluster the individuals and try to find some statistical differences among different groups, further confirming what key variables have a very large impact on charges.\n'], 'url_profile': 'https://github.com/YuxinMa', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['Logistic-Regression\nA basic implementation for training a Logistic Regression model\nThe octave implementation trains a Logistic Regression model for binary classification using the given dataset.\nYou can train a model on your own dataset as well.\n'], 'url_profile': 'https://github.com/anishk74', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'R', 'Updated Jun 25, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Updated Feb 17, 2020', 'Updated Feb 13, 2020', 'Rust', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 13, 2020', 'R', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020']}"
"{'location': 'Paris', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Adaptative Huber Regression\n""Adaptative Huber Regression"" project for the course ""Theoretical Guidelines""\nContext\nImplementation\nWe provide here our implementation of the https://arxiv.org/pdf/1706.06991.pdf.\nWe follow scikit-learn template for regressors.\nResults\nWe try to reproduce the results from the paper with the scripts in the Scripts folder.\n'], 'url_profile': 'https://github.com/fd0r', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['sds291-team73\nTeam project for SDS 291 Multiple Regression group project.\nTeam Members\nEmma Livingston\nClara Rosenberg\nProject Description\nWe are still looking for data :)\n'], 'url_profile': 'https://github.com/emmal73', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'Waterloo', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['qrfit\nR package of Inference for the Conditional Quantile Regression Model\n'], 'url_profile': 'https://github.com/Ironarrow98', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression-for-Loan-Prediction-III\nLogistic Regression for Loan Prediction III\n'], 'url_profile': 'https://github.com/Bo12345692', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'Ahmedabad, India', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['MultipleLinearRegression\nMultiple Linear Regression using gradient descent and MSE cost function.\n'], 'url_profile': 'https://github.com/shashwatrathod', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bedilkarimov', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Rossman-Sales-Prediction\nPredicting sale of Rossman stores using advanced regression algorithms\n'], 'url_profile': 'https://github.com/Navyatha-Kapu', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shadman77', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'Boston,MA USA.', 'stats_list': [], 'contributions': '347 contributions\n        in the last year', 'description': [""Title : Melbourne-Housing-Data-Analysis\nDescription\nThe obective of the project is to analyse the Melbourne's Housing data and build a model to make predict the price of the house.\nThe dataset includes data for the 21 mentioned paramters for the houses in the Melbourne city.\n\nSuburb,Address,Rooms,Price,Method,Type,SellerG,Date,Distance,Regionname,Propertycount,Bedroom2,\nBathroom,Car,Landsize,BuildingArea,YearBuilt,CouncilArea,Lattitude,Longtitude\n\nTechnology Used\n\nPredictive Modeling\nRegression\nDescriptive Statistics\n\nEnvironment\n\nR Studio , R (ggplot2,dplyr,randomForest,corrplot)\n\nAnalysis\n\nData cleaning with variable research to ensure meaningful and analysable data for modelling.\nExploratory Data Analysis to analyse trends in the housing data :\nDashboard \nImplemented Linear regression, Decision tree and Random forest regressor models to predict the target feature price.\nEfficient features selection using stepwise selection and lasso regression. \nRandom Forest regressor was the best fit model with efficient R square score.\n\n""], 'url_profile': 'https://github.com/aashish-bidap', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['CorReg\n \n  \nThe code was originally on an R-forge repository.\nCorReg\'s Concept\nThis package was motivated by correlation issues in real datasets, in particular industrial datasets.\nThe main idea stands in explicit modeling of the correlations between covariates by a structure of sub-regressions (so it can model complex links, not only correlations between two variables), that simply is a system of linear regressions between the covariates. It points out redundant covariates that can be deleted in a pre-selection step to improve matrix conditioning without significant loss of information and with strong explicative potential because this pre-selection is explained by the structure of sub-regressions, itself easy to interpret. An algorithm to find the sub-regressions structure inherent to the dataset is provided, based on a full generative model and using Monte-Carlo Markov Chain (MCMC) method. This pre-treatment does not depend on a response variable and thus can be used in a more general way with any correlated datasets.\nIn a second part, a plug-in estimator is defined to get back the redundant covariates sequentially. Then all the covariates are used but the sequential approach acts as a protection against correlations.\nThis package also contains some functions to make statistics easier.\nInstallation\nlibrary(devtools)\ninstall_github(""CorReg/CorReg"", build_vignettes = TRUE)\nVignette\nOnce the package is installed, a vignette showing an example is available using the R command:\nRShowDoc(""CorReg"", package = ""CorReg"")\nCredits\nCorReg is developed by Clément Théry with contributions from Christophe Biernacki, Gaétan Loridant, Florian Watrin and the A106 team: Quentin Grimonprez, Vincent Kubicki, Samuel Blanck, Jérémie Kellner.\nCopyright ArcelorMittal\nLicence\nCeCILL\nReferences\nModel-based covariable decorrelation in linear regression (CorReg): application to missing data and to steel industry. C Thery - 2015, http://www.theses.fr/2015LIL10060\n'], 'url_profile': 'https://github.com/CorReg-community', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Updated Feb 21, 2020', 'R', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2021', '2', 'R', 'Updated Aug 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Time-Series-Asg3\nVector Auto Regressions (VAR)\n'], 'url_profile': 'https://github.com/swatisharma-github', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'Paris Area', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Tool-wear-classification\nIn this project we are going to work on the classification method of Logistic Regression. We are going to : 1. Describe a set of images by means of a set region descriptors. 2. Using the computed descriptors, we are going to simulate a classification experiment and calculate some performance metrics from the results of the Logistic Regression classifier.\nLanguage used : MATLAB\nThe main file is : main_TP_LogisticReg.m\nThe files used to code the Logistic Regression classifier from scratch (fTrainLogisticReg, funsigmoid, fCalculateCostLogReg, fClassify_LogisticReg.m, fGetShapeFeat.m) were coded by Pr. Víctor González Castro from the Universidad de Leon (Spain), and then completed by me.\nTB3 : Image and Pattern recognition 2019-\nProject (tool wear classification)\nAbdelhakim Benechehab\nDecember 2019\nIn this project we are going to work on the classification method of Logistic\nRegression. We are going to :\n\nDescribe a set of images by means of a set region descriptors.\nUsing the computed descriptors, we are going to simulate a classification\nexperiment and calculate some performance metrics from the results of the\nLogistic Regression classifier.\nIn this experiment, we are going to divide the data set into two disjoint sets :\ntraining and test.\n\n1 PART 1 : Description of the images\nHere is a sample of the images we are willing to process (both unrefined and\nbinary format) :\n\nThe images are binary regions corresponding to cutting edges of insert tools\nused in milling processes, from which we will extract 9 features detailed in [3].\nThe functionfGetShapeFeatcompute the said features for a given region, here\nis the script corresponding to what it does :\n1 stats = regionprops(region,\'Area\',\'Eccentricity\',\'Perimeter\',\n2 \'EquivDiameter\',\'Extent\',\'FilledArea\',\'MajorAxisLength\',\n3 \'MinorAxisLength\',\'Solidity\');\n4\n5 Area = getfield(stats,\'Area\');\n6 Eccentricity = getfield(stats,\'Eccentricity\');\n7 Perimeter = getfield(stats,\'Perimeter\');\n8 EquivDiameter = getfield(stats,\'EquivDiameter\');\n9 Extent = getfield(stats,\'Extent\');\n10 FilledArea = getfield(stats,\'FilledArea\');\n11 MajorAxisLength = getfield(stats,\'MajorAxisLength\');\n12 MinorAxisLength = getfield(stats,\'MinorAxisLength\');\n13 Solidity = getfield(stats,\'Solidity\');\n\n2 Division of the data between Train and Test sets\nAfterwards, the data is randomly split in two disjoint sets : one will be used\nfor training the classifier and the other one will be used for making the test and,\ntherefore, assess how good the classifier is.\n1 % SPLIT DATA INTO TRAINING AND TEST SETS\n2\n3 num_patterns_train = round(p_train*num_patterns);\n4\n5 indx_permutation = randperm(num_patterns);\n6\n7 indxs_train = indx_permutation(1:num_patterns_train);\n8 indxs_test = indx_permutation(num_patterns_train+1:end);\n9\n10 X_train = X(indxs_train, :);\n11 Y_train = Y(indxs_train);\n12\n13 X_test= X(indxs_test, :);\n14 Y_test = Y(indxs_test);\n\n3 PART 2 : Classification\n3.1 Training of the classifier and classification of the test set\n3.1.1 Training\nThe goal of the training of the Logistic Regression classifier is to estimate\nthe parameters  Θ = [θ 0 ,θ 1 ,...,θn]  used to estimate the probability that a certain\npattern belongs to the positive class Xi= [xi 0 ,xi 1 ,...,xin]\nDuring the training phase, the parametersΘwill be modified so that they\nminimise a cost function, which will yield the average error between the outputs\nof the function h and for the training elements and their real classes. In Logistic\nRegression, this cost function is given by the following equation :\nJ(Θ) =−1/m∑[yi.ln(hθ(xi)) + (1−yi).ln(1−hθ(xi))]\nThis optimization is carried out by means of thegradient descentalgo-\nrithm, which is an iterative procedure in which each of the components of the\nvectorΘare updated on each iteration :\nθkj=θkj−^1 −α1/m∑(hθ(xi)−yi).xij\nwhere the element xij is the j-th feature of the feature vector that represents the\ni-th object (i.e. the i-th pattern), and hθ(x) is the hypothesis function, which is\nin this case the sigmoid function :\nhθ(x) =g(θTx) = 1/(1+e−θTx)\nThe implementation of the described classifier is in the files :fTrainLogisticReg, funsigmoid, fCalculateCostLogReg.\n3.1.2 Classification\nOnce the classifier is trained (i.e., the vector Θ for which the cost reaches a\nminimum has been obtained), the classification of elements that have not been\nused in the classification is carried out using Θ.\nThe following file fClassifyLogisticReg returns the probability for each element\non the test set to belong to the positive class.\nIn order to quantify the convergence of our classifier, we are able to track\nthe evolution of the cost function, in the following figures (2), the cost function\nis computed depending on two variables, the learning rateαand the number of\niterations.\nRegarding the learning rate α, we can see that when its value is excessively\nsmall (0.1) the convergence is so slow taking a big number of iterations to achieve\nits stable value. However, if it is so big (6) the algorithm does not converge\nanymore and the gradient does not fall in the bottom of the valley and keep\njumping between its walls which explains the sinusoidal behavior.\nFor the number of iterations, beyond 100 iterations the convergence speed\nis not significant, hence we can stop there for a better CPU running time. For\nalpha = 2, the cost function at 100 iterations is equal to 0.29, and 0.25 at 1000\niterations which are relatively close.\nNow after getting the probability of belonging to the positive class, we assign\nto every observation a class using a decision threshold normally equal to 0.5 :\n\n1 % CLASSIFICATION OF THE TEST SET\n2 Y_test_hat = fClassify_LogisticReg(X_test, theta);\n3\n4 %%\n5 % Assignation of the class\n6 Y_test_asig = Y_test_hat≥0.5;\n\n3.2 Assessment of the performance of the classifier\n3.2.1 Confusion Matrix\nOnce the estimated classes of all the elements of the test set, we can calculate\nperformance metrics, based on the confusion matrix. The most basic one is the\naccuracy, Which is the percentage of the elements that have been well classified.\nOther metrics are also necessary in case of our Data are imbalanced, a detailed\nexplanation of all those metrics is given here [1].\nA key tool to computing all these metrics is the Confusion matrix, it gives\nthe values of all four categories : True positive, False negative, False positive\nand True negative. Here is a code that compute this matrix and its values for\nthe first classifier :\n1 Cfman = zeros(2,2);\n2\n3 for i = 1:\n4 if (Y_test(i) == 1) && (Y_test_asig(i) == 1)\n5 Cfman(1,1) = Cfman(1,1)+1;\n6 elseif (Y_test(i) == 0) && (Y_test_asig(i) == 1)\n7 Cfman(2,1) = Cfman(2,1)+1;\n8 elseif (Y_test(i) == 1) && (Y_test_asig(i) == 0)\n9 Cfman(1,2) = Cfman(1,2)+1;\n10 elseif (Y_test(i) == 0) && (Y_test_asig(i) == 0)\n11 Cfman(2,2) = Cfman(2,2)+1;\n12 end\n13 end\n14\n15 Cf = Cfman;\n16\n17 %For better graphics, the figure is made using the plotconfusion ...\nMatlab function\n18 figure;\n19 plotconfusion(Y_test, Y_test_asig);\n\nThe confusion matrix (figure 3) has strong values in its diagonal hence we\ncan already tell that our classifier is not bad. We can now compute the accuracy\nand the F-score for this classifier :\n1 accuracy = trace(Cf)/sum(sum(Cf));\n2\n3 Precision = Cf(1,1)/(Cf(1,1)+Cf(2,1));\n4\n5 Recall = Cf(1,1)/(Cf(1,1)+Cf(1,2));\n6\n7 FScore = 2*((Precision*Recall)/(Precision+Recall));\n8\n9 fprintf(\'\\n******\\nAccuracy = %1.4f%% (classification)\\n\', ...\naccuracy*100);\n10 fprintf(\'\\n******\\nFScore = %1.4f (classification)\\n\', FScore);\n\n\nAccuracy= 85. 1852\n\nFScore= 0. 6000\n\nLooking at the accuracy value (0.85), We can assume that our classifier is\nvery efficient. But like we saw in the course material the accuracy rate can be a\nvery falsy criterion if our data is strongly asymmetric.\nTo get rid of any doubt we will check the value of the F-Score that takes in\nconsideration the asymmetry of the problem, this indicator (0.6) confirms our\npre-thoughts about our classifier, since the value is relatively high and that our\ndata are quite unbalanced.\n3.2.2 ROC analysis\nThe ROC curve is a curve that shows the evolution of the sensitivity (True\nPositive) to 1-specificity (False positive) depending on the decision threshold t\nthat we have previously chosed as 0.5. A detailed explanation of this performance\nmetric is explained in [2].\nHere is a function that calculates the ROC curve for a given validation set :\n1 function [TPR,FPR] = ROC(Y0,Ytest)\n2 T = 0:0.001:1;\n3 [n, m] = size(T);\n4\n5 TPR = zeros(1,m);\n6 FPR = zeros(1,m);\n7\n8 for j=1:m\n9 Y1 = Ytest≥(T(j));\n10\n11 Cfman = zeros(2,2);\n12\n13 for i=1:\n14 if (Y0(i) == 1) && (Y1(i) == 1)\n15 Cfman(1,1) = Cfman(1,1)+1;\n16 elseif (Y0(i) == 0) && (Y1(i) == 1)\n17 Cfman(2,1) = Cfman(2,1)+1;\n18 elseif (Y0(i) == 1) && (Y1(i) == 0)\n19 Cfman(1,2) = Cfman(1,2)+1;\n20 elseif (Y0(i) == 0) && (Y1(i) == 0)\n21 Cfman(2,2) = Cfman(2,2)+1;\n22 end\n23 end\n24\n25 Cf = Cfman;\n26\n27 tpr = Cf(1,1)/(Cf(1,1)+Cf(1,2));\n28 fpr = Cf(2,1)/(Cf(2,1)+Cf(2,2));\n29\n30 TPR(j) = tpr;\n31 FPR(j) = fpr;\n32 end\n33 end\n\nLet’s try it on our test Data :\n\nThe curve we got is more than sufficient to tell that our classifier is robust,\nit is entirely above the first bisector. To make sure we will calculate the value\nof its integral on the [0,1] domain -the area under the curve-\n1 [n, m] = size(TPR);\n2 q = 0;\n3 %Integral is computed using the rectangle rule\n4 for i=1:(m-1)\n5 q=q+(FPR(i)-FPR(i+1))*TPR(i);\n6 end\n\nq= 0. 8242\n\n4 Support Vector Machines (SVM) Classifier\nAccording to wikipedia an SVM model is a representation of the examples\nas points in space, mapped so that the examples of the separate categories are\ndivided by a clear gap that is as wide as possible. New examples are then mapped\ninto that same space and predicted to belong to a category based on the side of\nthe gap on which they fall.\nDetails about the method are given in [7] chapter ""More About"".\nFor an SVM model, the parameters that make the most difference in the classification performance are the soft margin parameter C (BoxConstraint) of\nthe classifier and the kernel type. The kernel type is either ’linear’ (usual scalar\nproduct), ’Gaussian’ or ’Polynomial’ (see [7] for formulas)\nIn order to optimize these parameters, a built-in function in MATLAB exists\nand would give us the best BoxConstraint and the best kernelScale for a given\nkernel type, here is the implementation of such optimization and its results :\nfigure 5\n1 %HyperParameters optimization and kernelType selection\n2 Modl = fitcsvm(X,Y,\'KernelFunction\',\'polynomial\',\n3 \'OptimizeHyperparameters\',\'auto\',\'HyperparameterOptimizationOptions\',\n4 struct(\'AcquisitionFunctionName\',\'expected-improvement-plus\'));\n5\n6 %tried on three kernels : linear, gaussian and polynomial\n\nThis process was tried on the three kernel types cited above, and the best\nresult (smallest value of the Estimated objective function value which is a mea-\nsure of the difference between prediction and reality using a cross validation\nmethod) is obtained with kernel type polynomial.\nThe hyper-parameters optimization gives the following results as well : figure\n6\n\n\nAt this stage, we have the parameters and the kernel type of our model\nchosed, all we have to do is to train this model using built-in MATLAB functions\nfor SVM binary classification as following :\n1 Modlfinal = fitcsvm(X_train,Y_train,\'KernelFunction\',\'polynomial\',\n2 \'kernelScale\',25.872,\'BoxConstraint\',846.1);\n3\n4 [label,score] = ...\npredict(fitPosterior(compact(Modlfinal),X_train,Y_train),X_test);\n5\n6 Y_test_asig = label\';\n7\n8 table(Y_test\',Y_test_asig\',score(:,2),\'VariableNames\',...\n9 {\'TrueLabel\',\'PredictedLabel\',\'PostProb\'})\n10\n11 figure;\n12 plotconfusion(Y_test, Y_test_asig);\n\nHere is the corresponding Confusion Matrix and ROC curve : figure 7\n\nThe performance metrics give the following as well :\nAccuracy= 88. 8889\n\nFScore= 0. 6897\n\nConclusion : The SVM methods are quite better than the logistic regression\nclassifier for our data, though they require more CPU running time in order to\nchose the best parameters to fit our model.\n5 Neural Networks Classifier\nThe classical shallow neural networks are very strong tools to fit any function\nand might be very robust in binary classification problems, an introduction to\nthe mathematics behind can be found in [4].\nIn MATLAB, a GUI for neural networks exists to facilitate their use, however\nI chose to write its script down directly in command line.Details given in [6] and\n[5].\nThe number of hidden layers and neurons in every layer was decided after\nseveral tries in order to achieve a maximum of accuracy for the validation set,\nthe result is the following script :\n1 inputs = X\';\n2 targets = Y;\n3\n4 % Create a Pattern Recognition Network\n5 hiddenLayerSize = [64 32 16 8]; % 4 hidden layers\n6 net = patternnet(hiddenLayerSize);\n7\n8 % Set up Division of Data for Training, Validation, Testing\n9 net.divideParam.trainRatio = 70/100;\n10 net.divideParam.valRatio = 15/100;\n11 net.divideParam.testRatio = 15/100;\n12\n13 % Train the Network\n14 [net,tr] = train(net,inputs,targets);\n15\n16 % Test the Network\n17 outputs = net(inputs);\n18 errors = gsubtract(targets,outputs);\n19 performance = perform(net,targets,outputs);\n20\n21 % View the Network\n22 view(net)\n\nThe function view(net) gives you a summary of your neural network and its\nperformance using this window : figure 8\n\nWe can see a diagram of our neural network at the top of the window, it\nalso displays the number of iterations needed to train the network (epochs) and\nhow many validation checks our network has passed (6/6 in this case).\nIt offers us the performance metrics we used to compute manually for other\nclassifiers, let’s visualize the confusion matrix and the ROC curve for example :\nfigure 9 and 10\n\nOur network achieved 96.7% of accuracy for the validation set and 93.1%\nglobally, which is very satisfying comparing to the previous classifiers we have\ntried.\nThis is confirmed by the ROC analysis that shows an under-curve surface\nvery close to 1.\nIf trained multiple times, the network might not give the same results, this\nis due to the fact that the data separation is arbitrary and the training pro-\ncess depends strongly on the input data. In some cases the input data are not\nsufficient for a good accuracy, in other cases we may face over-fitting.\n\nAn improvement of this network would be a CNN (convolutional neural\nnetworks) with multiple dropout, Max-Pooling and regularization layers that\nwill avoid the problems mentioned in the previous paragraph.\n6 Conclusion\nDue to the simplicity of the images we are classifying, the three of the classi-\nfiers we tried (Logistic regression, SVM, neural network) worked relatively well\nand gave satisfying results.\nWe applied multiple performance metrics to validate our classifiers and com-\npare them.\nThe best accuracy we could achieve is 93.1% using a neural network with\nfour hidden layers.\nRéférences\n[1] RahulAgarwal.The 5 Classification Evaluation metrics every Data Scien-\ntist must know.url: https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226.\n[2] KaranBhanot.Machine Learning Classifier evaluation using ROC and\nCAP Curves.url: https://towardsdatascience.com/machine-learning-classifier-evaluation-using-roc-and-cap-curves-7db60fe6b716.\n[3] Víctor González-Castro Rocío Alaiz-RodríguezMaría Teresa García-\nOrdásEnrique Alegre-Gutiérrez. “Combining shape and contour features\nto improve tool wear monitoring in milling processes”. In :International\nJournal of Production Research Volume 56 (2017), p. 3901-3913.doi:\nhttps://www.tandfonline.com/doi/full/10.1080/00207543.2018.1435919.\n[4] ChrisNicholson.A Beginner’s Guide to Neural Networks and Deep Lear-\nning.url: https://pathmind.com/wiki/neural-network.\n[5] Unknown.Classify Patterns with a Shallow Neural Network.url:\nhttps://fr.mathworks.com/help/deeplearning/gs/classify-patterns-with-a-neural-network.html.\n[6] Unknown.Fit Data with a Shallow Neural Network.url: https://fr.mathworks.com/help/deeplearning/gs/fit-data-with-a-neural-network.html.\n[7] Unknown.fitcsvm.url: https://fr.mathworks.com/help/stats/fitcsvm.html.\n'], 'url_profile': 'https://github.com/Hakiiiim', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SolomiyaSynytsya', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'Rupnagar,Punjab', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['Regression-Analysis\nThis contains code as well as report for the different variants of linear regression (OLS,RIDGE,LASSO) on the BOSTON HOUSING PRICE DATASET.\nFurther it also includes various inferences corresponding to the comparison done between variants of linear regression.\n'], 'url_profile': 'https://github.com/harshit37', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Regression-Overview\nAn overview on multiple regression models and usage of wrappers to improve re-usability.  Please refer to the pre-processed data auto-mpg-processed as the raw data for the Jupyter notebook.\n'], 'url_profile': 'https://github.com/Maha05', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['ISO Regression Analysis\nBy: Jeff Lindberg\nExecutive Summary\nThe goal of this analysis find significant indicators to describe and predict a player’s power production. Using ISO as my target variable, I ran a multivariable regression model to find the most important features. These features can be used to empower the front office to make informed decisions on evaluating players for arbitration and free agent contracts. Also, to developing players in the organization to maximize value and put the best product on the field. This can even be used in drafting and scouting other leagues where the data is available.\nContents\n\nIntroduction\n\nProblem Statement\nDataset\n\n\nAnalysis\n\nData Cleaning\nExploratory Analysis\nModeling\n\n\n\nIntroduction \nProblem Statement \nAs a front office in an organization, it’s important to find the best players available and be able to make your players better. Can we find significant indicators to describe and predict a player’s power production?\nDataset \nThe analysis is based on a dataset from Statcast (https://baseballsavant.mlb.com/) that includes observations from 2015-2019. The observations include metrics the describe the quality and direction of contact a player makes. The size of my initial dataset was (729,18).\nAnalysis \nData Cleaning \nThe inital data set was very clean but I still had to drop unnecessary columns. I also used a standard scaler and then dropped the few outliers that were in there.\nExploratory Analysis \nI found that all my features were normally distributed and most of them had a linear relationship with my target variable. There was some multicollinearity among the features that needed to be accounted for. Some of the variables did not have a significant relationship with the target amd were not used in the final model.\nModeling \nMy analysis used matplotlib and seaborn to create scatter plots and heatmaps to explore the data. I used statsmodels and sklearn to create my regression model and validate it.\n'], 'url_profile': 'https://github.com/jlindy2', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/clarineanslum', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'Warsaw, Poland', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Liniear Regression (Master Student of Warsaw University of Life Science)\nSimple Linear Regresion for predicting Salary as independent variable and Years of Experience as dependent variable\n'], 'url_profile': 'https://github.com/irawan09', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'Nairobi', 'stats_list': [], 'contributions': '234 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MarkMburu', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}","{'location': 'Vaughan, Ontario', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Lab 4\nLab 4 in the CMTH 642 course at Ryerson University is made to teach aspiring data scientists to learn how to build regression models with the MTCars data set already build into the standard RStudio application.\nRegression Analysis\nRegression is a form of predictive modeling techniques in which it investigates the relationship between a dependent variable and independent variable(s). It is used to forecast, in time series modelling and finding the effect relationship between the variables. The output variable from a regression model is numerical because it is about predicting quantity.\nTo begin I would recommend reading the PDF file that explains each question and why it is done.\nLets get started!!\n'], 'url_profile': 'https://github.com/Chrisboatto', 'info_list': ['Updated Feb 10, 2020', '4', 'MATLAB', 'Updated Apr 4, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistics-Regression\n\nkaggle python, Pandas\n\n'], 'url_profile': 'https://github.com/DS-Jan2020', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'Gothenburg, Sweden', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Linear Regression\nLinear Regression | Example code and own notes while taking the course ""Intro to Machine Learning"" on Udacity.\n\nPerformance metrics (In sklearn)\nR-squared (sum of the errors) = Performance of your regreession: reg.score()\nSlope: reg.coef_\nIntercept: reg.intercept_\nErrors\n\nMinimizing the Sum of Squared Errors (SSE)\nThe best regression is the one that minimizes the sum of squared errors.\n\n\n\nactual: training points\n\n\npredicted: predictions from regression (y = mx + b)\n\n\nThere can be multiple lines that minimizes ∑|error|, but only one line will minimize ∑error²!\n\nSSE is an evaluation metric, however, if you have more data you might get larger SSe, so this does not mean that you have worse fit. right?\nSeveral algorithms\n\nOrdinary last squares (OLS): Used in sklearn linear regression\nGradient descent\n\nr² (""R-squared"") of a regression\nHow much of my change in the output (y) is explained by the change in my input (x)?\n\n\n0.0: Line isn\'t doing a good job of capturing trend data\n1.0: Line does a good job of describing relationship between input(x) and output(y)\n\nR-squared is independent of the number of training points.\nComparing Classification & Regression\n\n\n\nProperty\nSupervised Classification\nRegression\n\n\n\n\nOutput Type\nDiscrete(class labels)\nContinuous (number)\n\n\nWhat are you trying to find?\nDecision boundry\nBest fit line\n\n\nEvaluation\nAccuracy\nSum of squared error -or- r²\n\n\n\n'], 'url_profile': 'https://github.com/gultekingokhan', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic_Regression\nTesting\n'], 'url_profile': 'https://github.com/soham3029', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amank56', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'Whitby, Ontario', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jawadm96', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maheshdhaka147', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '758 contributions\n        in the last year', 'description': ['Linear_Regression\n'], 'url_profile': 'https://github.com/MphoKhotleng', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Logistic_Regression on the uploaded dataset Voice.csv\n'], 'url_profile': 'https://github.com/abinj', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Turjo7', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Salikcr', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020']}"
"{'location': 'Jaguariuna, Brasil', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': [""Regressions_ML\nMy studies of Linear regression, Polynomial Regression, and other regressions...\n\n\n\n\nFor this you will need Python 3 that can be downloaded here\nAnd you will need some libraries, so, open your terminal and install them with this commands:\nhello@world:~$ pip3 install numpy\nhello@world:~$ pip3 install matplotlib\nhello@world:~$ pip3 install -U scikit-learn\nI'm still learning Machine Learning, I'm reading the book Hands-On Machine Learning with Scikit-Learn & TensorFlow to learn it.\nYou can find it's GitHub by clicking here\n""], 'url_profile': 'https://github.com/jonatasfernandespimenta', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': '广州', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Linear-Regression\n基本线性回归模型\n消除多重共线性：岭回归、Lasso\n多项式回归\n'], 'url_profile': 'https://github.com/tuqing3', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Regression_Techniques\nNumerous regression techniques applied to make future predictions based on the existing data.\nRegression_Analysis\nThe regression_analysis.ipynb analyses the dataset by plotting it and making comparisons between curves. The dataset used is a single column containing data of increasing number of people infected by the novel coronavirus-19 on a daily basis both in and out of China.\nLinear_Regression\nThe Linear_Regression.ipynb applies multiple linear regression techniques to examine which would be the best technique that would be applied.\nSupport_Vector_Regression\nThe Support_Vector_Regression.ipynb applies the Support Vector Machine but for regression instead of classification. It deals with different problems like why scaling the data is important.\nNeural_Network_Regression\nThe Neural_Network_Regression.ipynb applies a basic Neural Network for regression to predict future values and also covers the respective problems found.\nStatistical_Models_Regression\nThe Statistical_Models_Regression.ipynb applies many different statistical and forecasting models to clean the data starting from simple average techniques and going up to the ARIMA model.\n'], 'url_profile': 'https://github.com/MatthewSamson', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '326 contributions\n        in the last year', 'description': ['Assignment\nWe will predict employee salaries from different employee characteristics (or features). We are going to use a simple supervised learning technique: linear regression. We want to build a simple model to determine how well Years Worked predicts an employee’s salary. Import the data salary.csv to a Jupyter Notebook. A description of the variables is given in Salary Metadata. You will need the packages matplotlib, pandas and statsmodels.\nSteps and questions\n\n\nSplit your data into a training and test set. Leave the test set for now. Examine the training data for missing and extreme values. Create histograms to show the distribution of the variables and a scatterplot showing the relationship between Years Worked and Salary. Are the data appropriate for linear regression? Is there anything that needs to be transformed or edited first?\n\n\nUsing the statsmodels package and the training data, run a simple linear regression for Salary with one predictor variable: Years Worked.\n\nDoes the model significantly predict the dependent variable? Report the amount   of variance explained (R^2) and significance value (p) to support your answer.\nWhat percentage of the variance in employees’ salaries is accounted for by the   number of years they have worked?\n\n\n\nWhat does the unstandardized coefficient (B or ‘coef’ in statsmodels) tell you   about the relationship between Years Worked and Salary?\n\n\nWhat do the 95% confidence intervals [0.025, 0.975] mean?\n\n\nCalculate the expected salary for someone with 12 years’ work experience.\n\n\nCalculate the expected salary for someone with 80 years’ work experience. Are there any problems with this prediction? If so, what are they?\n\n\nWe have only looked at the number of years an employee has worked. What other employee characteristics might influence their salary?\n\n\nNow fit your model to your test set. DO NOT BUILD A NEW MODEL ON THE TEST SET! Simply use your existing, model, to predict salaries in the test set.\n\nHow does your model compare when running it on the test set - what is the difference in the Root Mean Square Error (RMSE) between the training and test sets? Is there any evidence of overfitting?\n\n'], 'url_profile': 'https://github.com/Shalom91', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sunilkumarsukesan', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This is a linear regression project completed by Allen Mkandla and Tshepo Rampai\n'], 'url_profile': 'https://github.com/tshepojrampai', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AkarshaSravs', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['regression_labs\n'], 'url_profile': 'https://github.com/chega8', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Regression-Regulization\nRegularization is a means to reduce overfitting, reducing the degrees of freedom of the model in order to reduce the variance in the model and thereby increasing its ability to generalize.\nThis model takes a childhood respiration dataset to illustrate various techniques in regression for regularization.\nLasso implements L1 norm regularization. Lasso takes the absolute value of the weights as a penalty for the loss function.\nRidge implements L2 norm regularization Ridge takes a square of the weights as a penalty for the loss function.\nElasticNet implements a mix of Lasso and Ridge\n'], 'url_profile': 'https://github.com/williamwparker', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Ankara, Turkey', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/muhammetbektas', 'info_list': ['Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AkarshaSravs', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['regression_labs\n'], 'url_profile': 'https://github.com/chega8', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Regression-Regulization\nRegularization is a means to reduce overfitting, reducing the degrees of freedom of the model in order to reduce the variance in the model and thereby increasing its ability to generalize.\nThis model takes a childhood respiration dataset to illustrate various techniques in regression for regularization.\nLasso implements L1 norm regularization. Lasso takes the absolute value of the weights as a penalty for the loss function.\nRidge implements L2 norm regularization Ridge takes a square of the weights as a penalty for the loss function.\nElasticNet implements a mix of Lasso and Ridge\n'], 'url_profile': 'https://github.com/williamwparker', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Ankara, Turkey', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/muhammetbektas', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Logistic_Regression on the uploaded dataset Voice.csv\n'], 'url_profile': 'https://github.com/abinj', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Turjo7', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Salikcr', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Çankaya / ANKARA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ferdiakdogan', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['LinearRegression\nML_hw2\n'], 'url_profile': 'https://github.com/WanderDra', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Stock-Regression\nplaceholder\n'], 'url_profile': 'https://github.com/rezan21', 'info_list': ['Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 10, 2020', '1', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020']}"
"{'location': 'Banglore', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Linear_Regression\nIts a Supervised ML Algo.\n'], 'url_profile': 'https://github.com/ganguly-cloud', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Salikcr', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Çankaya / ANKARA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ferdiakdogan', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['LinearRegression\nML_hw2\n'], 'url_profile': 'https://github.com/WanderDra', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Stock-Regression\nplaceholder\n'], 'url_profile': 'https://github.com/rezan21', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear-Regression\n\nLinear Regression\n\n'], 'url_profile': 'https://github.com/DS-Jan2020', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'San Jose, CA', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Regression-Algorithms\nThe code describes a sample data of integers. The data has one input feature and a labelled data.\nSimple Linear regression performed using Linear Regression Algorithms and SVR Algorithm.\nThe performance metrics for both are displayed in the code.\nThe covariance and the covariance coefficient is also calculated between the features and labelled data.\nVisualisation done using scatter plots and bar graphs.\n'], 'url_profile': 'https://github.com/shashankbhagat', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Bhubneswar,Odisha', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LOGISTIC-REGRESSION-\nHere is the code for a logistic regression on a data set with 74.8% accuracy ,if want to increase the accuracy rate then use feature scaling on the data set.\nThe data set contains the different diabetic patient data and the model is used to detect if a person has diabetic or not is a logistic model with 2 binary values 0 and 1.\n0 for the person not having diabetics and 1 for having diabetics.\n'], 'url_profile': 'https://github.com/swapnil0070', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Johannesburg, South Africa', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['Simple Regression Model\nThis\n'], 'url_profile': 'https://github.com/touchafrika', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Logistic-Regression\nThe logistic regression classifier is built to recognize cats. This assignment was built through a Neural Network mindset where logistic regression represents a single neuron.\nRun the python notebook file(.ipynb) to execute the project.\n'], 'url_profile': 'https://github.com/prasannamarudhu', 'info_list': ['Python', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '374 contributions\n        in the last year', 'description': ['LinearRegression\n#Prerequisite\n\npip install sklearn\npip install pandas\npip install numpy\n\n'], 'url_profile': 'https://github.com/animeshsingh04', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Logistic-Regression\nanalyzing car purchases, based on salary and age\nImporting the Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nImporting  the dataset\ndataset = pd.read_csv('Social_Network_Ads.csv')\nX = dataset.iloc[:,[2,3]].values\nY= dataset.iloc[:, 4].values\nEnconding categorical data (When is request)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nX[:, 3] = labelencoder_X.fit_transform(X[:, 3])\nonehotencoder = OneHotEncoder(categorical_features = [3])\nX = onehotencoder.fit_transform(X).toarray()\nAvoiding the Dummy Variabel Trap (When is request)\nX = X[:, 1:]\nSplitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)\nFeature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nFitting Logistc Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train,Y_train)\nPredicting the Test set results\ny_pred = classifier.predict(X_test)\nMaking the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\nVisualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\nnp.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\nalpha = 0.75, cmap = ListedColormap(('red','green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\nplt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1],\nc = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training Set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\nVisualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_test, Y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\nnp.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\nalpha = 0.75, cmap = ListedColormap(('red','green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\nplt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1],\nc = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training Set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n""], 'url_profile': 'https://github.com/IvanPmenta', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Dhaka,Bangladesh', 'stats_list': [], 'contributions': '436 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nilldiggonto', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Linear-regression\n'], 'url_profile': 'https://github.com/RodionSkrupskiy', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': '中国', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['RF-regression\n'], 'url_profile': 'https://github.com/tianzhaotju', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Andi2795', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': [""NYCAirbnbRegression\nProject Overview:\nCost is a big problem in many parts of the world, especially in Bay Area. As someone who first lived in New York city then San Francisco, the two most expensive cities in the U.S., I am always interested in understanding the attributes of living cost. Kaggle provides a great dataset of New York city Airbnb prices that allows me to explore what factors play a role in the cost of Airbnb.\nPart I (New_York_Airbnb.ipynb)\nThis dataset includes customers’ reviews, neighborhoods, boroughs, GPS coordinates, room typse, and other information of each Airbnb unit instance. In this part of the project, I will first explore the following questions based on this Kaggle dataset.\n\nHow does the Airbnb price vary based on its location?\nDo different room types lead to different prices?\nWhat is the overall sentiment of the customers’ reviews?\nCan we predict Airbnb prices with a model?\n\nData source:\nNYC Airbnb Prices (Kaggle):\nhttps://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data\nFuture Studies:\nSo far, from the input features, including neighborhood groups, neighborhoods, availability in a year, last review dates, the best regression model works only moderately, with an R2 value around 0.5.\nOne hypothesis I formed was that there were too many distracting features. I tested this out by only providing the neighborhood information with or without the room type information to the model, as they were shown to have impact on the prcies in the first two sections. However, removing input features decreased the accuracy of the model (data not shown).\nI took a step back and asked myself what I care when I book an Airbnb. Information such as last review date, number of reviews, and room type is important. But for a visitor to New York, what matters most is how close the Airbnb unit is to the big apple's landmark and how safe it is. Next, I will import other databases and determine how these new factors impact the prices.\nPart II (New_York_Airbnb2.ipynb)\nIn this part of the project, I will import two additional datasets of New York subway stations and crime records from New York State webpages and examine the following three questions:\n\nHow does the Airbnb price correlate with the apartment's proximity to subway stations?\nIs the Airbnb price negatively correlated with the crime rate in the neighbourhood?\nIs the model better at predicting Airbnb prices with additional subway and crime information?\n\nData source:\nNYC Transit Subway Entrance And Exit Data (NY State government):\nhttps://data.ny.gov/Transportation/NYC-Transit-Subway-Entrance-And-Exit-Data-API/rwat-jhj8\nNYPD Arrests Data (Government Catalog):\nhttps://catalog.data.gov/dataset?tags=crime&organization=city-of-new-york\nWork in progress. Stay tuned.\n""], 'url_profile': 'https://github.com/LiWangSH', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Linear-Regression\n\nWe will predict employee salaries from different employee characteristics (or features). We are going to use a simple supervised learning technique: linear regression. We want to build a simple model to determine how well Years Worked predicts an employee’s salary\n\n'], 'url_profile': 'https://github.com/Sibusiso1995', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Varanasi', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""Regression_Model\nIn this project, I developed a simple regression model, to predict the column 'Discharge Pressure (psig)' minimizing ‘rmse’ loss function.\n""], 'url_profile': 'https://github.com/ShivanshGupta55', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'New York, New York', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['\n Linear Regression Lab\nThis lab walks you through implementing linear regression on a housing dataset from Sacramento.\n\nMaterials We Provide\n\n\n\nTopic\nDescription\nLink\n\n\n\n\nLab\nLinear Regression Jupyter Notebook\nLink\n\n\nData\nSacramento Real Estate Transactions\nLink\n\n\n\n\nPrerequisites\nBefore this activity, students should already be able to:\n\nPerform simple statistical analysis in Pandas\nExplain simple and multiple linear regression\nDefine bias, variance, and correlation\nDefine and interpret common regression metrics\n\n'], 'url_profile': 'https://github.com/varunganti33', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Machine-learning-projects\nProjects on supervised learning algorithms like Naive Bayes , KNN , Logistic Regression, Linear regression , SVM, Decision Trees, Random Forest and Ensemble models\n'], 'url_profile': 'https://github.com/divya-murali18', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Machine-leaning-project\nA project using Python to implement linear least squares regression to compare Empirical risk minimization (ERM) and Regularized least squares regression (RLM)\nStep 1 - load the data\nThe data is stored in two files, dataset1_inputs.txt and dataset1_outputs.txt\nwhich contain the input values (i.e., values xi) and the target values (i.e., values ti)\nrespectively. These files are simple text files which can be loaded with the load function\nin Matlab/Octave. Plot the outputs as a function of the inputs (ie plot the datapoints,\nnot a curve) and include this plot in your write-up.\nStep 2 - ERM\nFor degrees W = 1, . . . 20, fit a polynomial of degree W to the data using (unregularized) least squares regression. For each learned function, compute the empirical square\nloss on the data and plot it as a function of W. Include this plot in your report. Which\nvalue of W do you think would be suitable?\nStep 3 - RLM\nRepeat the previous step using regularized least squares polynomial regression. Each\ntime train polynomial of degree 20 for regularization parameters λ so that ln(λ) =\n−1, −2, · · · − 20. This time plot (and include) the empirical loss as a function of i.\nCompare and discuss the two curves you get for ERM and RLM.\nStep 4 - cross validation\nImplement 10-fold cross validation for ERM. That is, randomly divide that data into\n10 chunks of equal size. Then train a model on 9 chunks and test on the 10th that\nwas not used for training. For each model you train, average the 10 test scores you\ngot and plot these again as a function of W. Which value of W do you think would be suitable\n'], 'url_profile': 'https://github.com/AUSB', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Sunnyvale, California', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhaskarnn9', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': [""SciKit_Learn_Regression_Optimization\nLearnt about optimisation technique of the error function using two alogorithms:\n1.Gradient Descent(GD)\n\nHuber Regressor/Loss (HL)\n\nSince GD squares the error term while minimising, it also squares the outliers. This disturbs the performance. However a counter\nto this would be using the absolute error as the cost function. This too, proves deleterious as at the origin, when curve passes, slope\ncan't be calculated as a discontinuity has been produced.\n\nHence, as a sort of middle ground to this, Huber Loss function is implemented wherein at the origin and near it's region,\na quadratic function graph is initiated and just starting from it's tips the straight line originates, thereby curbing the issue of\ndiscontinuity.\nAs seen below, the Huber line is a bit laid back, trying to encapsulate mre datapoints, than the MSE.\nIt also solves the issue of outliers.\n\n""], 'url_profile': 'https://github.com/chinmay7g', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'New York, New York', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['lab-regression_and_model_validation\nLab: Linear Regression and Model Validation\n'], 'url_profile': 'https://github.com/varunganti33', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/josetellis', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['Yelp-regression-project\nYelp was founded in 2004 to help people find great local businesses. We have partnered with Yelp Data Science team for the Linear Regression Cumulative Project and you will be working with real data from Yelp to predict the Yelp rating of a restaurant\n'], 'url_profile': 'https://github.com/IuliiaKurchutskaia', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Linear-Regression-Models-using-Keras\nDeep Learning\nModels made for Final Assignment for Coursera IBM AI Specialization.\n'], 'url_profile': 'https://github.com/coutinhocf', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Ecommerce_Linear_regression\nDATA DESCRIPTION:\nThis dataset is having data of customers who buys clothes online. This file has customer email, avg. session time with stylist, Time spent on the app and website, Length of Membership. Our main objective is to predict the Yearly amount spent by the customers.\nATTRIBUTES:\nEmail: Email of the customer\nAddress: Address of the customer\nAvatar: Avatar chosen by the customer\nAvg. Session Length: Average duration of the online session\nTime on App: Time spent on App\nTime on Website: Time spent on website\nLength of Membership: Time period of membership\nYearly Amount Spent: Yearly amount spent by the customer\n'], 'url_profile': 'https://github.com/jasminedas7', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['RealEstate_regression\nContains the Linear Regression of a dataset of a RealEstate , containing the Prices and Size .\nTools used : Matplotlib , Statsmodel, Pandas\n'], 'url_profile': 'https://github.com/schau00', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AkhilRautela', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AllenMkandla', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['linear_regression_project\nMultiple Linear Regression Housing Case Study Problem Statement: Consider a real estate company that has a dataset containing the prices of properties in the Delhi region. It wishes to use the data to optimise the sale prices of the properties based on important factors such as area, bedrooms, parking, etc.  Essentially, the company wants —  To identify the variables affecting house prices, e.g. area, number of rooms, bathrooms, etc.  To create a linear model that quantitatively relates house prices with variables such as number of rooms, area, number of bathrooms, etc.  To know the accuracy of the model, i.e. how well these variables can predict house prices.\nLinear Regression¶\nHousing Case Study\nProblem Statement:\nConsider a real estate company that has a dataset containing the prices of properties in the Delhi region. It wishes to use the data to optimise the sale prices of the properties based on important factors such as area, bedrooms, parking, etc.\nEssentially, the company wants —\nTo identify the variables affecting house prices, e.g. area, number of rooms, bathrooms, etc.\nTo create a linear model that quantitatively relates house prices with variables such as number of rooms, area, number of bathrooms, etc.\nTo know the accuracy of the model, i.e. how well these variables can predict house prices.\n'], 'url_profile': 'https://github.com/debjeetdebnath', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Netherlands', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pranavsdev', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinay8442', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['R-Simple-Linear-Regression\n'], 'url_profile': 'https://github.com/seniorfoua', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Linear-Regression-Assignment\nProblem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/Shriya-Shashidhar', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nikhilbhardwaj44', 'info_list': ['Jupyter Notebook', 'Updated Nov 21, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 16, 2020', 'R', 'Updated Feb 10, 2020', 'Updated Feb 13, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}"
"{'location': 'Pullman', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['Fitting-Regression-models\nR code for implementing Linear,Polynomial,Random Forest,Ridge,Lasso Regression with Cross Validation for large Data Set\nTwo Data set were used for analyzing different fitting models.\nThe R code represents the regression models for boston Housing data and weekly workflow data.\nThis is an exploratory project on curve fitting techniques.\n'], 'url_profile': 'https://github.com/chandan0013', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Stony Brook', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Logistic-Regression-POS-Tagger\nA POS Tagger that learns and develop accuracy through Logistic Regression Classification on the twpos-data-v0.3 dataset of Tweets\nAlso includes functions for Feature Extraction using One-Hot Encoding as well as a Tokenizer using regular expressions from the Python re package.\n'], 'url_profile': 'https://github.com/KhiemPhi', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['multiple-linear-regression-tribble\ncontains all commands\n'], 'url_profile': 'https://github.com/vinay8442', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kegerber', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['📣 Visual Regression Testing Using WebdriverIO 📣\nThis is a webdriverIO project, created to automate visual regression testing.\nBenefits\nCatches visual issues such as spacing, colors, alignment\nCovers a lot with a little effort\n💻 Application URL 💻\nhttps://learn.visualregressiontesting.com/\n\n📃 Clone 📃\n$ git clone https://github.com/tux7P/WebdriverIO-The-Internet.git\n\n⚙️ Install ⚙️\n$ npm install\n\n🏃 Start 🏃\n$ npm test\n\n'], 'url_profile': 'https://github.com/tux7P', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Whitby, Ontario', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jawadm96', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '303 contributions\n        in the last year', 'description': ['House-Pricing-Linear-Regression\nWe have data of sales of house in some area. For each house we have the complete information about its size, price, number of bedrooms, Number of Bathrooms, whether it is made of bricks, Neighbourhood.\nObjective of this case study is to check which factors are effecting house price.\nExpectations\n\nDo an exploratory data analysis to identify the initial patterns in this data i.e. identification of outliers, missing values, univariate, and bivariate patterns.\nCheck multicollinearity among independent variables.\nUse OLS Regression to see price predictors.\nDoes performance check using decile analysis and lift charts.\n\nData Dictionary\nDescription of the variables:\n\nHome - Home number (identity)\nPrice - Retail price of home.\nSqFt - Square feet (area).\nBedrooms - Number of Bedrooms\nBathrooms - Number of Bathrooms\nOffers\nBricks - Made of Bricks\n\nNo - 0\nYes - 1\n\n\nNeighbourhood - Neighbourhood\n\nCredits\nThis project contains data and other related material that is developed by AnalytixLabs.\nI acknowledge and grateful to Professor ChandraMouli Kotta for his continuous support throughout the Certification (Data Science using Python that helped me learn the skills of Linear Regression and develop a Linear Regression model for predicting House Prices.\nDeveloper\nBirender Singh (birender@buffalo.edu)\n\nLicense\nCopyright {2020}\n{Birender Singh birender@buffalo.edu}\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n'], 'url_profile': 'https://github.com/IamBirender', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Taipei, Taiwan', 'stats_list': [], 'contributions': '731 contributions\n        in the last year', 'description': ['segmented-regression-python\nReference\n\nhttps://datascience.stackexchange.com/questions/8457/python-library-for-segmented-regression-a-k-a-piecewise-regression\n\n'], 'url_profile': 'https://github.com/Cuda-Chen', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '162 contributions\n        in the last year', 'description': [""Predicting house prices based on various characteristics.\nGetting started\n\n\nFrom the project root directory, create a virtual environment\n  python -m virtualenv venv\n\n\n\nActivate environment\n  source venv/bin/activate\n\n\n\nInstall packages\n  python -m pip install -r requirements.txt\n\n\n\nFollow the instructions here to setup the kaggle API (needed for downloading data and pulling/pushing kernels)\n\n\nDownload files associated with the competition\n  kaggle competitions download -c [COMPETITION]\n\n\n\n\xa0Kaggle API\n\n\nCreate a kernel\n  kaggle kernels init -p /path/to/kernel\n\n\n\nPush a kernel and run on Kaggle\n  kaggle kernels push -p /path/to/kernel\n\n\n\nPull kernel\n  kaggle kernels pull -p /path/to/kernel\n\n\n\nSubmit to competition (assumes you've already signed up to the competiton and accepted the T&Cs)\n  kaggle competitions submit -c [COMPETITION NAME] -f [FILE PATH]\n\n\n\n""], 'url_profile': 'https://github.com/Ahhj', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Denver, CO', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': [""Ames Housing Regression Model\nAuthored by: Scott Rosengrants\nHome prices predicted using linear regression and the Kaggle Ames Housing dataset\nProblem Statement/ Hypothesis:\nCan a predictive price model be built based on the Ames, Iowa assessor data to predict the value and subsequent sale price of a home. Will the model will be accurate enough to be used in Opendoor’s business model? Will the model will work in new markets?\nThe stakeholders in this analysis are the business stratigists at Opendoor. A recomendation will be made at the end of the analysis stating whether an accurate home price predicting model can be built from the data provided in the Ames, Iowa dataset and whether this model is applicable to other markets Opendoor may be interested in.\nThe data used in this analysis was taken from the Kaggle DSI-US-10 Project 2 Regression Challenge : https://www.kaggle.com/c/dsi-us-10-project-2-regression-challenge/data\nThe data files can be found in this repository under the 'datasets' folder.\nThe folder contains:\n\ntest.csv\ntrain.csv\nclean.csv\n\nData Dictionary:\nThe data dictionary for this data set is quite extensive, for brevity, it has been excluded from this read me file. It can be found in this repository 'ames_data_dictionary.txt' and at http://jse.amstat.org/v19n3/decock/DataDocumentation.txt\nRepo Structure:\nIncluded in this specific repo\n\nJupyter notebooks\n\names_clean.ipynb (data cleaning)\names_eda.ipynb (data analysis and feature engineering)\names_modeling.ipynb (several regression models applied and submission code for Kaggle)\n\n\nimages folder (contains all images used for the presentation\ndatasets\nSubmissions (contains csv submission versions for Kaggle competition)\names_data_dictionary\npresentation slides (pdf format) for Opendoor stakeholders\n\nExecutive Summary:\nFor this analysis the ames housing data set was cleaned, analyzed, underwent feature creation, and was fit to several linear regression models. Each step is summarized below with the Jupyter Notebook it occurs in specified.\names_clean.ipynb\n\nread in data\ncheck data types\ncheck for null values\ncorrect, delete, or impute rows and values based on specific case \nChanges made to:\n\n'Lot Frontage'\n'Alley'\n'Mas Vnr Type'\nBasement columns : 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1', 'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF', 'Bsmt Full Bath', 'Bsmt Half Bath'\n'Fireplaces','Fireplace Qu'\nGareage columns:'Garage Type', 'Garage Yr Blt','Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual',\n'Garage Cond', 'Paved Drive'\n'Pool QC'\n'Fence'\n'Misc Feature'\n'Id'\n'Street'\n-check for outliers\nremove homes greater than 4000 square feet\n\n\n\nData is exported as clean.csv to the datasets folder\nContinue to next notebook, ames_eda\names_eda.ipynb\n\nread in data\nExplore non-numeric features\nDummify:\n\n'MS Zoning' , 'Neighborhood' , 'Condition 1' , 'Condition 2' , 'Bldg Type' , 'House Style' , 'Kitchen Qual'\ncheck for high correlations to price for each\n\n\nExplore numeric features\n-run several heatmaps pulling out features that are highly correlated to price\nFrom the above exploration assign a 'predictors' list of features selected that have a strong correlation to price\n\nCheck for linearity of 'predictors'\nCheck for equal distribution of features\ncheck for high correllation between features\n\n\nConvert '1st Flr SF', log_Gr Liv Area', and 'SalePrice' to logarithmic scale\nSave 'predictors' list\n\n\nContinue to next notebook, ames_modeling\names_modeling.ipynb\n\nread in data\ncreate X and y based on the 'predictors' list and 'SalePrice\nTrain/Test Split data\nScale the data (these values will be held for models that required scaled data)\nTest OLS Linear Regression\nTest LASSO Regression (using scaled data)\nTest Elastic Net Regression\nSelect best model (OLS Linear Regression)\nLoad in Kaggle data\nClean Kaggle data\nRun OLS Regression and save predictions\nConvert results from logarithmic form back into standard form\nCreate a submission DataFrame\nExport predictions to 'Submissions' folder as attemptX.csv\n\nOutside Research\n\nOpendoor (housing company): Explored their business model and fees associated with their service. \nhttps://www.opendoor.com/\nRealist Tax (housing data agrregator): Explored the data they provide and the locations they collect data on. \nhttps://www.corelogic.com/products/realist.aspx\n\nFindings/Conclusions:\nBased on my findings, the hypothesis can be deemed correct and true. \nA predictive price model can be built based on the Ames, Iowa assessor data to predict the value and subsequent sale price of a home. The model will be accurate enough to be used in Opendoor’s business model and the model will work in new markets (provided that similar housing data is provided). \nThrough analysis of the Sci-Kit Learn linear regression models, OLS Linear Regression, LASSO Regression, and Elastic Regression it was found that OLS Linear regression had the highest r^2 scores and the lowest RMSE scores on this given data set. This is largely due to the fact that the model was never overfit to a detrimential amount therefore by applying LASSO and Elastic Net models, the bais of the model was atrificially increased unnecissarily and the prediction result accuracy suffered because of it. \nProper feature selection in this data set was more impactful on the RMSE score than changing the model type. Features were selected based on their respective correlations to the target, 'Saleprice'. During the first iteration of the model correlations above .5 and below -.25 were included as feature. In the second iteration this range was increased to include features with a correlation of .2 or greater or less than -.1, the RMSE continued to increase and there was no drop in r^2 train vs test data to indicate over fitting. On the third feature itteration all correlation features were included, the theory here would be that the model would be overfit. Upon scoring the results, again there was an increase in RMSE and no significant drop in r^2 score from the train data to the test data, this again indicated the model was still not overfit. \nThe theoretical limit to features (based on the sqrt(rows) theory) was 45 features. At the end of the analysis the model had exactly 45 features. Increased feature count testing was not conducted but the assumption is that the model would become overfit and the accuracy of the model when introduced to new data would decline. \nBeyond simple feature selection based on numerical column correlation to price, feature engineering was also included. The columns 'SalePrice' , '1st Flr SF', and 'Gr Liv Area' were all found to have a parabolic curve when tested for linearity. These features were transformed to the logarithmic scale by taking their natural log. This helped increase the accuracy of the model.  note: 'SalePrice' did need to be converted back to the standard scale before model scores could be determined.* \nDummy variables were also used on 'Kitchen Qual' and 'Bsmt Qual' features to change them from objects to numerical features. Creating dummy columns for each feature quickly grows the feature list and exceeds the theoritical 45 allowable features for this data set. Careful selection of which columns to create into dummies and therefore a feature is needed. These selection was conducted by dummifying the feature in a new DataFrame, checking its correlation to price, and then comparing how strong those correlations were compared to other dummified features. Through this process of elimination 'Kitchen Qual' and 'Bsmt Qual' were found to be the best predictive features and complied with the 45 feature limit. \nIn conclusion, the optimal model was found to be OLS linear regression. The features included in the model all had a correlation to the target 'SalePrice'. There did not seem to be a dilution of accuracy as features were added. The final feature count was 45 and the model produced a RMSE score of 22894.78 and R^2 score of 0.9.\nRecommendations/Further Steps:\nMy recomendation is that Opendoor can in fact create an accurate home price prediction model based on assessor data. There needs to be more research conducted on the optimal feature count, it may be greater than 45. The feature count will largely be determined on the data available for the target city, not all county assessors collect the same information. This issue can be somewhat resolved by using a data aggregator such as Realist Tax. The benifit of using a service such as this is that they collect data on the entire U.S. in a consistent and structured manner. This is vital to the models success as the input features to the model will need to be the same from city to city. It is advisable to take a sample data set from Realist Tax and reconduct this study, ensuring a similar model can be built.\n""], 'url_profile': 'https://github.com/scottrosengrants', 'info_list': ['1', 'R', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'JavaScript', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'MIT license', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mjRaza', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VaibhavVadia', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': '中国', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['ORF-regression-sklearn\n'], 'url_profile': 'https://github.com/tianzhaotju', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Marketing-panel-data-regression\nThe project developed various panel data regression models including cluster OLS, PSAR(1), AR(1) to quantify the effect of roadshow strategy on sales volume.\nTechnology\nThe project is created with:\n\nStata version: 14\n\nTable of Contents\n\nDataset Overview\nConclusions\nAssumptions & Models\nAssumption Illustration & Test\nModel Comparison & Results\n\nDataset Overview\nWe learn from the graph that roadshow seems to increase sales instantly and the effect lasts in the following months. The increase volumne, average sales level and volatility of sales differ in different regions.\n\n\n\nMore specifically, we find that:\n\nIncrease of sales by one roadshow is higher in first-tier region and lower in inland provinces.\nAverage sales are higher in first-tier region and lower in inland provinces.\nFluctuation of sales is greater in first-tier region and smoother in inland provinces.\n\nConclusions\n\nThe increase of sales by a roadshow is statistically significant, though the increment quantity differs in different models.\nOur best models, AR(1) and PSAR(1) estimate the increase of sales by a roadshow to be 670 - 948.\nThe effect of a roadshow lasts for 5 months.\n\nNote: We measure the last of roadshow effect in this way: Roadshows of previous 1 month, 2 months, 3 months and 4 months have significant influence on sales but roadshows of previous 5 months do not, so the effect lasts for 5 months.\nWe show the increase of sales by 1 roadshow in different models. The models here assume roadshow has the same effect on each province, e.g, 1 roadshow in Guangdong increases same amount of sales as in Hunan. We'll discuss about the validity of these assumptions later.\n\n\n\nBelow, we show the increase of sales by 1 roadshow in different provinces. The models here assume roadshows affects provinces differently. For instance, in Sichuan, 1 roadshow increases sales by 388 * 4 - 645 = 907 > 0.\n\n\n\nAssumptions and Models\nAssumptions\nFor panel data models, it's most common to consider the following 5 effects. Whether these effects exist determines the validility of models because models are based on different assumptions of these effects.\n\nSame or different province sales regardless of roadshow: Different province effect\nSame or different effect of roadshow in provinces: Different roadshow effect\nGroupwise heteroskedasticity: Different variations among provinces\nAutocorrelation within each group: Serial correlation within a province\nGroupwise contemporaneous correlation: Cross province dependence\n\nModels\nWe try the following nine panel data models and list their main assumptions out here.\n\nOLS: only considers same effect of roadshows on sales in each province\nClustering OLS: only considers different effect of roadshows on sales in each province\nPanel-Corrected Standard Error (PCSE): only considers groupwise heteroskedasticity and groupwise contemporaneous correlation\nFeasible Generalized Least Lquares AR(1) (Autoregressive Process): only considers same autocorrelation within each group\nFeasible Generalized Least Lquares PSAR(1) (Panel-Specific Autoregressive Process): only considers different autocorrelation within each group\nFeasible Generalized Least Lquares (FGLS same beta): considers groupwise heteroskedasticity, groupwise contemporaneous correlation and same autocorrelation within each group\nFeasible Generalized Least Lquares (FGLS different beta): considers groupwise heteroskedasticity, groupwise contemporaneous correlation and different autocorrelation within each group\nVarying Coefficients - constant: different but constant coefficients for each province\nVarying Coefficients - random variable: different coefficients for each province and view coefficients as random variables\n\nAssumption Illustration & Test\nIllustration\nWe give simple and clear examples of the five effects we need to consider. After interpreting these effects, we think all the effects except for the different roadshow effect exist.\n\n\n\nTest\n\nWe verify the existence of Groupwise heteroskedasticity by Greene-Wald test\nWe verify the existence of Autocorrelation within each group by Wooldridge-Wald test\nWe verify the existence of Groupwise contemporaneous correlation by Greene: Breusch-Pagan LM test\nTest codes are listed in the report.\n\nModel Comparison & Results\nBased on the test and our analysis, we think AR(1) and PSAR(1) are the best models because they consider all the three effects that are proved to exist in the model and consider different province effect. Assuming the roadshow effect on each province is the same (which should be a reasonable assumption), AR(1) and PSAR(1) estimate the increase of sales by a roadshow to be 670 - 948. Here're the assumptions and results of five regression models.\n\n\n\nNote: Tick represents the model considers the effect , ‘—’ represents the model does not. The numbers of the last column indicates the increase of sales in present month by 1 roadshow in this month (e.g 381) and by 1 roadshow in previous 4 months(e.g 1073).\nTotal increase of sales by 1 roadshow : PSAR(1): 62 + 152 * 4 = 670, AR(1): 96 + 213 * 4 = 948\n""], 'url_profile': 'https://github.com/tracyzg1818', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '334 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mahnatse-rgb', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': [""Housing-price-Advanced-regressionn\nStart here if...\nYou have some experience with R or Python and machine learning basics. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.\nCompetition Description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nPractice Skills.\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting\n""], 'url_profile': 'https://github.com/vpfahad', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '978 contributions\n        in the last year', 'description': ['Regression\nExploring Regression with Regression Trees, K Nearest Neighbor, Ridge, and Lasso using K-Fold Cross Validation\nData\nThe goal of this project is to find a mapping of these features to risk in heart disease\n \nThis dataset was provided by researchers at the Hungarian Institute of Cardiology on Kaggle\nModel Selection\nI used K-Fold Cross Validation to perform model selection\n\n\n\n\n\n\nK-Fold Cross Validation Scores\nUsing K = 5 \n\n'], 'url_profile': 'https://github.com/vee-upatising', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Logistic-Regression-deployment\n'], 'url_profile': 'https://github.com/atul70', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['DS-ML-Logistic-Regression\nMachine Learning using Logistic Regression: Whenever the variable that needs to be predicted requires a ""binary"" solution to the problem, we use Logistic Regression. This repository contains a project that will use Logistic Regression to determine whether a passenger; given their features could have survived the Titanic Crash.\nFollow the following steps to implement any ML algorithm:\n\nPerform Exploratory Analysis to understand the data given\nLook for any missing values or discrepancies in the data and address those\nUse the most optimal function to replace missing values\nClean the data further\nPerform ML algorithms\nCompare between algorithms to choose the best result\n\n'], 'url_profile': 'https://github.com/rjrahul24', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Support-Vector-Regression-SVR-\n'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'MATLAB', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 16, 2020', 'Stata', 'Updated Feb 25, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Linear-Regression-from-scratch\nIn this project I implemented linear regression from complete scratch.\nI used gradient descent to fit the regression line.\n'], 'url_profile': 'https://github.com/MaclaurinYudhisthira', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Prasanna24-max', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Thunder Bay, ON', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NLP_Assignment\nThis repository contains code for housing price prediction using 1D convolution.\n'], 'url_profile': 'https://github.com/Pathikpatel', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '145 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kraja928', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""Working-with-Linear-Regression\nIt consists of a model to predict 'Sales price of real estates in Chennai' based on the training data using the concept 'Linear Regression'\n""], 'url_profile': 'https://github.com/ShreyaSurabhi', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Random-Forest-Regression with Dataset\n'], 'url_profile': 'https://github.com/abutair', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Whitby, Ontario', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jawadm96', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['CNN-Non-Linear-Regression\n'], 'url_profile': 'https://github.com/PatKrrizh', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Barcelona', 'stats_list': [], 'contributions': '511 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jpinyot', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\ny = b0 + b1*x\n'], 'url_profile': 'https://github.com/KoushikRaghav', 'info_list': ['Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'C++', 'Updated Feb 24, 2020', 'Python', 'Updated Jul 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Support-Vector-Regression-SVR-\n'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Linear-Regression-from-scratch\nIn this project I implemented linear regression from complete scratch.\nI used gradient descent to fit the regression line.\n'], 'url_profile': 'https://github.com/MaclaurinYudhisthira', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Prasanna24-max', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'Thunder Bay, ON', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NLP_Assignment\nThis repository contains code for housing price prediction using 1D convolution.\n'], 'url_profile': 'https://github.com/Pathikpatel', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '145 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kraja928', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'Thessaloniki, Greece', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': [""visual-regression-testing\nHere is a sample for visual regression testing . We used cypress for faster results, easy-to-use tooling and headless run!\nMore info https://www.cypress.io/\nRUN TESTS\n\nRun npm install\nRun npm start to a thread to start the node server.\nRun to another thread on command line npm run test:base\nRun npm run test:actual\n\n\nAll results are under [cypress/snapshots] folder\nVideo of test run is under [cypress/video] folder\n\nNote that if you change the host variable on test:actual to 'http://localhost:8081/' all tests should pass.\nExample:\n\nAdded Applitools implementation\nOne of the most famous tools for visual regression testing that use AI is Applitools https://applitools.com/!\n\nRun npm start to a thread to run the node server.\nYou should set your apikey on applitools.config.js. Example\n\nmodule.exports = {\n  apiKey: 'YOUR_API_KEY',\n  ...\n}\n\nRun to another thread on command line npm run applitools:base\nRun npm run applitools:actual\n\nYou should see something like this on the platform :\n\n""], 'url_profile': 'https://github.com/jpourdanis', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Linear-Regression-model\n'], 'url_profile': 'https://github.com/SanjanaPrasad13', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Logistic-Regression-from-scratch\n'], 'url_profile': 'https://github.com/MaclaurinYudhisthira', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TMaccor', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Wine-SDS-Regression-Ref\n'], 'url_profile': 'https://github.com/saytosony', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'HTML', 'Updated Feb 18, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'R', 'Updated Feb 26, 2020', 'Updated Feb 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Linear_Regression_Duke_University\nProject work for Linear Regression (Coursera: Statistics with R Specialization offered by Duke University).\n'], 'url_profile': 'https://github.com/rajathada', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Quick-Linear-Regression\nNotes: Need to install numpy, scipy and matplotlib.\n'], 'url_profile': 'https://github.com/Namerlight', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Cincinnati, Ohio', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['FAA-Logistic-Regression-Project\nProject Link - https://rpubs.com/puneetbhatiadce/574496\n'], 'url_profile': 'https://github.com/puneetbhatia95', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Whitby, Ontario', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jawadm96', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '303 contributions\n        in the last year', 'description': ['Regression-Car_Sales_Prediction\nOne of the major automobile company would like to design new product which gives high sales. In order to define the product, they want to understand and identify important drivers for the sales (what are the factors driving sales) and Predict the new car sales for given car model with defined factors.\nExpectations\n\nUnderstand the data and perform the data preparation before the model building.\nPerform all the modeling steps including pre and post modeling steps like data preparation and implementation of the model.\nUnderstand output and explain the model fit.\nDetermine what is the ""best"" linear model?\nApply transformation to the given variables and find out the possible best model after transformations.\nGenerate the final equation.\nValidate the model and present the results in Excel or PPT.\n\nData Dictionary\nDescription of the variables:\n\nManufacturer - Car Manufacturer Name\nModel - Car Model Name\nSales_in_thousands - Car Sales in Thousands.\n__year_resale_value - Resale value after 4 years\nVehicle_type - Type of Car.\nPrice_in_thousands - Price of the car\nEngine_size - Car Engine Size.\nHorsepower - Car Horse Power.\nWheelbase - Car Wheel base.\nWidth - Car Width\nLength - Car Length\nCur_weight - Car curb weight\nFuel_capacity - Fuel Capacity in liters\nFuel_efficiency - Fuel efficiency (kms/liter)\nLatest_Launch - Car Model Launch Date\nPower_perf_factor - Power performance factor\n\nCredits\nThis project contains data and other related material that is developed by AnalytixLabs.\nI acknowledge and grateful to Professor ChandraMouli Kotta for his continuous support throughout the Certification (Data Science using Python that helped me learn the skills of Linear Regression and develop a Linear Regression model for predicting car sales.\nDeveloper\nBirender Singh (birender@buffalo.edu)\n\nLicense\nCopyright {2020}\n{Birender Singh birender@buffalo.edu}\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n'], 'url_profile': 'https://github.com/IamBirender', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Decision-Tree-Regression with the dataset\n'], 'url_profile': 'https://github.com/abutair', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '270 contributions\n        in the last year', 'description': ['ML5-Color-Regression\nJust a repo for my color regression with my practice for ml5. Learning this from The Coding Train!\nDemo - https://arnavsirigere.github.io/ML5-Color-Regression\n'], 'url_profile': 'https://github.com/arnavsirigere', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['K-NN Algorithm and Linear Regression Model\nGetting Started\nInstalling Python\nTo use the script, you will need to install Python 3.6.x and add to path:\n\nPython 3.6.x\n\nInstalling Dependencies\nAfter cloning the project, go to the root directory:\nInstall the dependent libraries by typing the following in cmd/terminal:\n$ pip install -r requirements.txt\n\nStarting the Script\nTo run the script, go to the root directory and run python in cmd/terminal and type the following in the python console:\n>>> from knn_main import *\n\nNote: ensure that python refers to Python 3.6.x\nRunning K-NN on Regression Dataset (Q1)\nTo run the k-nn algorithm on regression dataset [mauna_loa,rosenbrock,pumadyn32nm], type the following in the python console:\n>>> run_Q1(k_range=[1,31])\n\nNote: k_range takes a list containing lower bound and upper bound of k values\nRunning K-NN on Classification Dataset (Q2)\nTo run the k-nn algorithm on classification dataset [iris,mnist_small], type the following in the python console:\n>>> run_Q2(k_range=[1,31])\n\nNote: k_range takes a list containing lower bound and upper bound of k values\nRunning K-NN with KD Tree and Compare Performance (Q3)\nType the following in the python console:\n>>> run_Q3(d=list(range(2,10)))\n\nNote: d takes a list containing values of dimension numbers\nRunning Linear Regression with SVD on All Dataset (Q4)\nType the following in the python console:\n>>> run_Q4()\n\nBuilt With\n\n\nnumpy - all variables are numpy arrays\n\n\nsklearn - kd tree data structure\n\n\n'], 'url_profile': 'https://github.com/thomas-enxuli', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear Regression from scratch\nI implement Linear Regression from scratch, using gradient descent and normal equation to estimate the coefficients and compare it with sklearn.\n'], 'url_profile': 'https://github.com/ShijieZhang0529', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/isbhargav', 'info_list': ['HTML', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 12, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 16, 2020', 'JavaScript', 'Updated Aug 14, 2020', 'Python', 'MIT license', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['SeleniumRegressionSuite_Bamboo\nAutomation RegressionSuite in Bamboo\n'], 'url_profile': 'https://github.com/billzyteam', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Regression-Analysis-Concept\nI have explained in my own way three basic regression analysis concepts in this repository.\n'], 'url_profile': 'https://github.com/muniah', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2,199 contributions\n        in the last year', 'description': ['testcafe-visual-regression-testing\n\nMy awe-inspiring Nuxt.js project\n\nBuild Setup\n# install dependencies\n$ yarn install\n\n# serve with hot reload at localhost:3000\n$ yarn dev\n\n# build for production and launch server\n$ yarn build\n$ yarn start\n\n# generate static project\n$ yarn generate\nFor detailed explanation on how things work, check out Nuxt.js docs.\n'], 'url_profile': 'https://github.com/Kosuke0820', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Boston, MA.', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['#Personal GitHub page of Stephen Dorris\n##This Page Work in Progress collection of repositories to showcase my work over the past few years of my surrounding studies as a Masters CS student at Northeastern Univsersity\n(Currently Live as of 4/14)\n\n\nMapReduceTwitter(Java/Hadoop): This repository houses mapreduce programs surrounding the analysis of a large, yet simple data set. My goal for this repository is to showcase that interseting questions can be answered even with a small feature set.\n\n\nImageTools (Java) : Image Processing Application used to blur, filter,dither (etc) Images and save them to local filesystem. Written with Margarita Gubanova\n\n\nLogRegresssionForSkipPrediction: Work on a Spotify Data Science Challenge, in attempts to determine an accurate model to predict if a user with skip a given song in a listening session. Written with April Gustafson.\n\n\n(Currently Being Developed/Created as of 4/14)\n\n\nsfml: Currently Has skeleton version Minipaint, will have full application in a few weeks, which has a GUI, client/server networking for private IP address connection, and messaging feature.\n\n\nHeirarchical Clustering Project\n\n\n'], 'url_profile': 'https://github.com/stephen-dorris', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chinmayjoshi789', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hoffman-Chris', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['machine-learning\n'], 'url_profile': 'https://github.com/cours-lyco', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/baptistemokas', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Ranchi', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['I have integrated an ML project(Hardwork Pays off) with web\nHardworkPays off is a linerRegression model.\nThe model predicts the marks of the student depending upon the number of hours the student has studied.\n'], 'url_profile': 'https://github.com/Akansha2202', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'INDIA ', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Machine-Learning-Logistic-Regression\n'], 'url_profile': 'https://github.com/Sulabh-Sharma', 'info_list': ['C#', 'Updated Apr 3, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Vue', 'Updated Feb 15, 2020', 'Java', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Java', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GitNickProgramming', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Logistic-Regression-Using-Python\nCase study on predicting Telecom Churn\n'], 'url_profile': 'https://github.com/Abhi0926', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/paras-yadav1', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'greater noida', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Logistic_Regression-Diabetes_prediction\n'], 'url_profile': 'https://github.com/kkapasiya', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ashish-ECE', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['stat-3400-applied-regression\nClasswork from STAT 3400: Applied Regression at University of Colorado, Boulder\nSpring 2020\nProffesor Osita Onyejekwe\n'], 'url_profile': 'https://github.com/LucasLaughlin', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['Moon-Camera-Position-Regression\n'], 'url_profile': 'https://github.com/hank-kuo-cs', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Logistic Regression Cancer Dataset\nThis project is an implementation of Logistic Regression to solve a two-class classification problem, where we train a model to learn from a cancer dataset and predict whether a given tumor is Malignant or Benign.\n'], 'url_profile': 'https://github.com/sachinvgpl-5', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['DESCRIPTIVE STATISTICS & REGRESSION ANALYSIS IN R\nINTRODUCTION:\nIn the given, assignment of week 1 we have supposed to use R programming language to perform Descriptive Statistical and Regression Analytical functions with the described data set. In this we as a student has been given freedom either to pick “Trees” from inbuilt data set of R or any other data set from inbuilt sources. If we talk bit in depth about this, we have to perform multiple regression on dataset to present them numerically and graphically. The most interesting thing about this assignment is we going to use R that too on real-life example.\nThis assignment has been divided into 2 Parts. In Part A we have to perform total 6 steps that will be described under Part A section and in Part B we have perform 3 steps consist of 4 main function to learn basics like summary (), log (), lm () and ggcorrplot () for correlation graph.\nANALYSIS:\nIn the initial phase of this project to kick start we are required to install packages followed by the libraries like “MASS”, “DAAG”, “GGPLOT2” and “GGCORRPLOT”. All the mentioned libraries have different functionalities and roles which will be going to use in further programming for analysis.\nPART A\nTASK 1: In this task 1 of Part A, we have invoked the dataset “Trees” from the inbuilt library of R. Here, we have used attach () so that we don’t have to specify data set tree in the further code of R.\nattach(trees)\nTASK 2: In this task 2 we have found 5 summary points of the data set trees for example for the column Girth of trees data set we have used summary(Trees $ Girth) and resulted the values in minimum number as  8.30, maximum value as 20.60, mean as 13.25, median as 12.90 and the inter quartile range as 4.20 which is calculated by the difference of 1st quartile and 3rd quartile.\nTASK 3: In this given task we have found the correlation between the different factors and columns of the data set trees. In this task we have performed regression function with the help of lm () and along with that we have plotted a straight line of regression which tell us the regression between the two variables. We have used abline to draw that line on the graph. Furthermore, we have used title () to put the title on the graph, used summary () to find the minimum, maximum, inter quartile, mean and median values of the trees.lm data frame. Then we have use coef () and residuals () to find the coefficient and residuals of trees.lm. Below is the graph for a Straight-line regression in correlation of Girth and Height from data set of trees.\nTASK 4: In this task we have to construct histogram and density plot for the columns of the data set.\nTASK 5: In the above graph we have presented a histogram shown in the above graph of girth and frequency, after putting the data into histogram we can see that it is right or positively skewed which also tell that the mean of this data would be greater than the median. . Furthermore, we have used density plot function to graph to find the density of height of trees.\nGraph 2: Histogram\nplot(density(trees$Girth), ylab = ""Density of Girth"", xlab = ""Number of Girth"")\nGraph 3: Density Graph\nTASK 6: Above data set we have presented a histogram shown in the above graph for the frequency of height of trees it seems to be left skewed after getting presented on the graph. On the other hand, the mean of the height is somewhere equal to the median of the height of the trees as pe the pattern of the graph. Furthermore, we have used density plot function to graph to find the density of height of trees.\nTASK 7: In this task we have built a histogram shown in the above graph of volume with the frequency of trees, the graph is positively skewed, and the mean volume of trees is higher the median volume of trees. We have used hist () function to construct the graph. Furthermore, have used plot density function to plot density plot graph for volume density.\nTASK 8: In this task we have performed the graphical representation to find the interquartile range of all the three factors of trees Girth, Height and volume. We have used single graph for all the three columns to keep it compact. As in the graph we can see the first quartile, 3rd quartile and can find the values of interquartile range. The dots outside the quartile boxplot graph are known as outliers.\n#Boxplots for Trees dataset\nboxplot(trees$Girth, horizontal = TRUE, main = ""BoxPlot for Girth"")\nboxplot(trees$Height, horizontal = TRUE, main = ""BoxPlot for Height"")\nboxplot(trees$Volume, horizontal = TRUE, main = ""BoxPlot for Volume"")\nGraph 8: Boxplot for Girth, Height and Volume\nBelow is the boxplot graph to find the interquartile range of volume of the trees and the outliers in the data.\nBelow is the boxplot graph to find the interquartile range of height of the trees. We can find the 1st quartile range, 3rd quartile range and the inter quartile range.\nTASK 9: In this task we are supposed to make normal probability distribution graph which shows us that the distribution of data of height of trees is normally distributed and showing us 2 outliers on head and tail of the distribution.\nIn this task we are supposed to make normal probability distribution graph which shows us that the distribution of data of Volume of trees is normally distributed and showing us 1 outlier on head and 3 on the tail of the distribution.\nqqnorm(trees$Volume, xlab = ""TREES"", ylab = ""VOLUME"",main = ""Normal Probability Plot"")\nIn this task we are supposed to make normal probability distribution graph which shows us that the distribution of data of Girth of trees is normally distributed and showing us 1 outlier on head and 3 on tail of the distribution.\nPART B\nWe are moving on the second part of assignment in which we are required to work on two different data set i.e. “Rubber” & “Oddbooks”. In this part we have to apply 4 different function to get the target output the functions are as follows summary (), log (), lm () and ggcorrplot ().\nTASK 1: In this task we have invoked data set from R directory i.e. Rubber, here we have first applied the log () on data set and stored it into logRubber. Then we applied multi regression function keeping multi variable with the function which is inversely proportional to other variable results are stored in logRubber.lm. Further we applied summary () function to find the residuals with minimum values, maximum value, first quartile, third quartile and median values.\nIn the result Residual standard error: 0.35 on 27 degrees of freedom Multiple R-squared:0.678, Adjusted R-squared:  0.655, F-statistic: 28.5 on 2 and 27 DF, p-value: 2.24e-07.\nThis part consists of constructing the correlation matrix graph with all the factor so rubber which tell how much one variable is dependent on the other variable depending the correlation. As we can see the in all the factor od rubber all have very all weak correlation between each other. The red color shows that the correlation is high between the variable whereas purple and dark tell us the correlation is very weak. Here we have used round () and cor () to put all the data in the matrix then we have installed the ggplot library and ggcorrplot library to get the desired output.\nTASK 2: In this task we have invoked data set from R directory i.e. Oddbooks, here we have first applied the log () on data set and stored it into logOddbook. Then we applied multi regression function keeping multi variable with the function which is inversely proportional to other variable results are stored in logOddbook.lm. Further we applied summary () function to find the residuals with minimum values, maximum value, first quartile, third quartile and median values.\nThis part consists of constructing the correlation matrix graph with all the factor so oddbooks which tell how much one variable is dependent on the other variable depending the correlation. As we can see the in all the factor of oddbooks correlation between height, breadth and weight is very high whereas talking about the correlation in thickness and with other variable is week. The red color shows that the correlation is high between the variable whereas purple and dark tell us the correlation is very weak. Here we have used round () and cor () to put all the data in the matrix then we have installed the ggplot library and ggcorrplot library to get the desired output.\nGraph 10: Correlation Matrix for Oddbooks\nCONCULSION:\nIn conclusion, I would like to discuss how we have found the values of 3 different data set we found by invoking the data set from the inbuilt datasets available. In the whole process we have calculated different types of function for different purpose. We calculated the summary of all the number of data set for the respective columns. In the whole assignment we have learned how to find the summary, Straight line regression, creating the histogram and density plot, boxplot and normal probability plot.\nIn the second part of the assignment we have invoked to different data set Rubber and oddbooks from the library MASS, DAAG and ggplot2. Here, we have performed different functions summary (), log (), lm () and ggcorrplot ().  We also constructed the matrix plot for find the correlation between the different variables of the data set.\nREFRENCE:\nhttps://cran.r-project.org/doc/contrib/usingR.pdf\nhttps://www.statmethods.net/r-tutorial/index.html\n'], 'url_profile': 'https://github.com/ayj22', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Ranchi', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['I have integrated an ML project(Hardwork Pays off) with web\nHardworkPays off is a linerRegression model.\nThe model predicts the marks of the student depending upon the number of hours the student has studied.\n'], 'url_profile': 'https://github.com/Akansha2202', 'info_list': ['Python', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'R', 'Updated Feb 26, 2021', 'Jupyter Notebook', 'Updated Feb 10, 2020']}"
"{'location': 'INDIA ', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Machine-Learning-Logistic-Regression\n'], 'url_profile': 'https://github.com/Sulabh-Sharma', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jaynariya605', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': [""Non-Linear-Regression\nIn this project, we fit a non-linear model to the data points corresponding to China's GDP from 1960 to 2014\n""], 'url_profile': 'https://github.com/behnazhkh', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Banda, Uttar Pradesh', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""Non_Linear_Regression\nAbout the Dataset I am going to try and fit a non-linear model to the datapoints corrensponding to China's GDP from 1960 to 2014. We download a dataset with two columns, the first, a year between 1960 and 2014, the second, China's corresponding annual gross domestic income in US dollars for that year.\n""], 'url_profile': 'https://github.com/praphullmaurya', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ilzkl1991', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sonalihanamantache', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': [""Dog-Cat-Analysis\nIn this I've used Logistic Regression to predict cat's gender on the basis of height and weight.\nSame can be done on Dogs as well.\n""], 'url_profile': 'https://github.com/Usharbudha', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/eniola-oladele', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Atlanda', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Spotify-Network-Analysis-\nAnalyzed artists’ social networks and implemented regression model to discover factors contributing to the song popularity\nBackground\nAccording to the Economist, a weekly ranking of the most popular singles on Billboard Hot 100 in U.S., one-third of hit songs are made by artists’ collaborations. Therefore, we would like to analyze how the collaboration among artists would affect the success of an artist / a song. We focus on Spotify rather than traditional music media, such as radios, because Spotify is a more mature social network. It fulfills the definition of social network feature by allowing users to follow artists and make friends. More importantly, it has large user base (more than 250 million users) to develop a network. Moreover, with its analytics service for music publishers, we could gather organized and credible data from Spotify API easily\nIntroduction and Research Questions\nDoes collaboration with other artists necessary bring more streams to an artist on Spotify? Does more collaboration necessarily bring more streams? Or is it whom you collaborate with that matters more for an artist’s popularity on platform? Our study aims to answer the questions above through a combination of network analysis and statistical analysis.\nThere are four major assumptions of our study:\nThe artist collaboration network has a core-periphery structure.\nArtists who collaborate more with other artists (high degree) have higher popularity.\nMore centralized artists in the network (higher coreness) have higher popularity.\nArtists’ genre attributes to his success, and similarity (Jaccard similarity) between artists in a track collaboration is related to the popularity as well.\nIn this report we argue that artist collaboration network indeed has a core-periphery structure. It may be surprising to some of the audiences, but we do not find collaboration with other artists alone bring any positive value to the stream or popularity increase. However, if artists do collaborate with each other, the more they deviate from the general type of artists in the network (general being defined using Jaccard distance), the more popular the collaborated track seems to be. In addition to verifying the assumptions above, we also find during our study that in terms of getting higher popularity, artists with similar fan base (as defined by the number of followers on platform) seems to have more synergy, and this observation holds true for both famous and less popular artists.\nData Preparation\nThe dataset we use contains 10-year worth of artist, album, and track data for 4,862 artists on Spotify. (10,000 observations per year as limited by Spotify, for 10 years). We used Spotipy-- a lightweight Python library for the Spotify Web API to get the data needed. We have track, artist, as well as album level information such as duration of the track, artists who made the track, popularity of the track, which album it belongs to, type of album it belongs to, number of followers the artist have, popularity of an artist, etc. One of the most important variables we used in the research is the popularity of an artist, and the popularity of an artist is given as a number between 0-100. The artist Ed Sheeran has the popularity 100 across the board since he has the highest number of total streams on Spotify. Other artists’ popularity is calculated as the faction of their total stream compared with that of Ed, Sheeran’s, rounded to an integer value. Popularity equals the artist total streams divided by Ed Sheeran’s Total Streams. As pointed out by Spotify, “The popularity of a track is a value between 0 and 100, with 100 being the most popular. Popularity is based mainly on the total number of playbacks. Duplicate tracks, such as both in a single and in an album, are popularity rated differently.”\nExploratory Data Analysis\nWe observed a very interesting pattern on the number of different track types being produced. In Exhibition1 we observe that the number of tracks released as single has been increasing since 2009, while the number of compilation album and normal albums shows a decreasing trend. We suspect that this is due to number of independent artists who have limited production capabilities joining Spotify has been increasing and diluted the statistics, but the real reason is both unknown and out of the scope of our research project.\nIn Exhibition2, we observed a non-linear relationship between the artist popularity and number of followers an artist has.  Since by the definition of artist popularity, we had an assumption that there is a positive relationship between artist popularity and the number of followers, through EDA we are able to confirm that the relationship is indeed positive and non-linear. In later part of the research we would digger further into this relationship and see how the relationship compares with other centrality measures and Jaccard distance of an artist.\nNext, we looked at the distribution of average number of tracks produced by an artist in Exhibition3. It seems most artists produce 25 songs per year or less. The distribution is right skewed, and we find from the original data that the unrealistically large number such as 100 and more songs produced are actually related to movie original soundtracks, not the typical three to four minutes song that we listen to.\nSpotify has its own internal algorithms to classify music into different genres, however, that information was never released. What we do have is the genre that an artist in classified into. We can see In Exhibition4 that after grouping (for example, some variations of rock music are all grouped as rock music), we observe the majority of artists on Spotify are classified into Pop (26%), Rap (17%) and Hip Pop (12%). Whether the number of genres an artist is classified into also play a part in an artist popularity will also be studied in our research.\nNetwork Analysis\n\n\nThe collaboration among musical artists has an upward trend\nWe assumed that the collaboration level among musical artist would increase over time with the boom of the Spotify network. However, from both Network-level Centrality Concentration Measure (Exhibit5) and Distribution Table of Edge-level Eigenvector Centrality (Exhibit6), we see that all centrality measurements remain in certain range and have irregular increasing or decreasing trends. Therefore, we will calculate the centrality measure on a yearly basis for our further analysis in order to gain more insight.\nTo have a deeper understanding of the Spotify Collaboration network, we chose to look down into the network structure in 2012 (highest degree concentration and betweenness) and 2016 (lowest closeness and highest eigenvector). From 2012 and 2016 Network colored by coreless, although it’s hard to say the network is getting denser, we could see that nodes with similar eigenvector seem to get closer. This implies that more-central artists tend to collaborate more with more-central artist and less-central artists tend to collaborate with less central artists.\n\n\nThe collaboration network of musical artists has a core-periphery structure\nFrom Exhibit 5 we could see that the closeness is extremely low, and the eigenvector is extremely high throughout the years. In the three graphs above, most of the nodes are marked as red (extreme low eigenvector), and very few is marked as blue and green (high eigenvector). From Exhibit7, we could observe the right skewness of eigenvector centrality and the two peaks of closeness centrality. These three evidence all imply that the overall network is not centralized when each node is given equal weights, and the network would become highly centralized when we use the importance of neighbor as weight. That is, important artists connect mostly to other important artists and also connect unimportant artists, while unimportant artists do not connect altogether themselves; Therefore, we conclude that the Spotify collaboration network has a core-periphery structure. For detailed yearly network stats, please refer to Exhibit8.\n\n\nPositions of artists in the network has no significant correlation with artist’s popularity\nThree graphs colored by coreness and sized by popularity show that Popularity has no distinct relationship with Coreness or the position of a node. Thus, our assumption could not be hold by the network outlook, but we will further test it in our following regression model.\n\n\nRegression Analysis\n\nRegression variables\nTo find factors influencing on artists and tracks popularity, we run regressions using artists and tracks popularities as dependent variables respectively, which all have a normal distribution.\nFor artists data, the predictors include Top Track, Compilation Number, Single Number, Genre Number, Star and Average Track Duration. The variables’ annotations are in Exhibition9.\nSocial media exposure reflects popularity. We choose number of spotify followers to evaluate artists. Since the follower number is right skewed, we define the artists with top quantile number of followers as “star”, which is around one million followers.\nSince part of the artists don’t have collaboration given a year, when researching on networks influence on artists popularities, we focus on artists who have had collaboration experience in the past ten years. The collaborative artists popularities are basically normally distributed. Based on our model above, predictors also include artists’ Jaccard similarity, degree and coreness.\nFor tracks data, we focus on collaborative tracks. The predictors include, Star Collaboration (0,1), Non-Star Collaboration (0,1), Compilation (0,1), Single (0,1), Duration and Artist Average Popularity. The variables’ annotations are in Exhibition10.\nThe collaborative tracks popularities are basically normally distributed. Based on the model above, we also add Jaccard similarity as another predictor to account for the influence of collaboration.\nRegression Models\nFor yearly data on each artists’ attributes, we first treat it as panel data and apply fixed effect regression model controlling fixed effects for year and artists. To test if fixed effect model is significantly better than mixed effect model and significant differs from random effect model, we run other two model and apply F-test and Hausman Test separately. However, it turns out fixed effect regression doesn’t show edges. (Exhibition11)\nWe also find in the data that most of the artists don’t release songs every year. Every year, artist popularities are reevaluated so there is no effect from previous popularities.\nWe take artist data as section data and run regression with controlling the year factor:\nArtist Popularity= Jaccard Similarity+Degree+Coreness+Top Track(0,1)+Compliation+Single+Genre Number+Duration+Star(0,1)+ factor(year)+ Constant Term\nThe track data is also section data, thus we decide to run regression with controlling for the year factor:\nTrack Popularity= Jaccard Similarity+Star Collaboration(0,1)+NonStar Collaboration(0,1)+Compliation(0,1)+Single(0,1)+Duration+Artist Popularity+ factor(year)+ Constant Term\nRegression Results\nThe regression shows that track popularity, number of singles, and number of followers positively impact popularity while compilation number, genre number and track duration have negative impact. The outcomes are consistent. Then we focus on collaboration artists: after adding Jaccard similarity, degree and coreness into the regression, it turns out that Jaccard similarity has negative relationship with popularity, which means artists are more popular with fewer genres and of more minor types; both degree and coreness don’t have significant influence on popularity.\nDigging into social network statistics, we can find there is non-linear relationship between Jaccard similarity and popularity. Looking into the losses graph between these two variables (Exhibition12), we apply quadratic polynomial transformation on Jaccard similarity, and add them into regression. It turns that the model outcomes improve marginally, and to keep models simple, it seems reasonable to run without polynomial terms.\n\nThe regression on collaborative tracks popularity shows that Star, Non-Star Collaboration, number of singles, duration and artist popularity have positive impact on track popularity. Adding Jaccard similarity we find it has positive relationship with popularity, which means tracks from majority style collaboration leads to more stream.\nWe can find Jaccard similarity and popularity have non-linear relationship. Looking into the losses graph (Exhibition13), we apply cubic polynomial transformation on Jaccard similarity, and add them into regression. It turns that the model outcomes again improves marginally.\nConclusion and Implications\nBased on social network analysis and regression analysis above, we refer to our assumptions and draw our conclusions.\nConclusion1:  Over the years, the networks of artists collaboration shows a core-periphery structure.\nConclusion2: More collaborations in one year not necessarily increase artists’ popularity. It’s more important to identify influence from collaboration type instead of number of collaborations.\nConclusion3: Core artists in a given year are not necessarily more popular. The central location of artists in a network do not benefit them in terms of popularity and streams.\nConclusion4: Artists of fewer genres or minority genres can be more popular. On track level, collaboration between artists with more similar genres is more likely to success.\nThese conclusions bring up some implications on artists and tracks success. Well-connected artists tend to collaborate with each other but that doesn’t promise their success. Reversely, people can get bored of constant collaboration from more-central artists while brand new collaboration can help add to tracks popularity.\nTracks rise to fame mostly based on artist himself, and it’s not the best choice to collaborate intensively in one album with others. That is also why we can find most albums consist of mostly singles and observe fewer collaborations. However, it is not to say that collaboration is trivial, it is rather that artists, especially less famous artists need to more focus on developing his own style and not rush to broaden music style or seek collaboration with artists who has different style or more followers.\n'], 'url_profile': 'https://github.com/KailangWei', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['Predicting lung cancer survival time\nChallenge by Owkin\nMain discussions and estimations in main_report.ipynb.\n'], 'url_profile': 'https://github.com/AntGro', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'MATLAB', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', '1', 'Updated Feb 10, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '614 contributions\n        in the last year', 'description': ['Titanic-Survival\nUsed Logistic Regression to predict the survival of passengers on Titanic\n'], 'url_profile': 'https://github.com/V2dha', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'Tema/Ghana', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Transport Fare Prices Prediction System\nA machine learning model using regression analysis - Built with ML.NET Framework\n\n/**------------------------------------------------------------------------------------\n\nThis machine learning program was written by Solomon Yaw Adeklo,                   *\nA final year computer science student of Valley View University,Ghana.             *\nThis software is open sourced and anyone is free to use it for educational purpose *\n*----------------------------------------------------------------------------------**/\n\nThis is a machine learning model for predicting the price of transport fair using regression analysis algorithm.The system was trained with some dataset which have been labeled for supervised learning.\nDataset\nThe data in the data fields contain the following fields:\nvendor_id: Unique ID of the taxi vendor.\nrate_code: rate type of the taxi trip.\npassenger_count: number of passengers for the trip.\ntrip_time_in_secs: The amount of time the trip took.\ntrip_distance: The distance of the trip.\npayment_type: The payment method  – either credit card or cash\nfare_amount: The total taxi fare paid is the label to be predicted\nClasses\nTwo classes were created, one class to hold features and the other class to hold the prediction\nBelow is the code for the TaxiTrip Class\n\nBelow is the code for the TransportFarePrediction Class\n\nFunctions\nThere were three functions that were deployed. They are:\nThe Train() function which The Train() function is a function that build and trains the model by executing the following:\n\nload the data from disk into memory\nextract the data and perform some transformation\ntrain the model\nfinally return the trained model\nThe train function returns an model/ITransformer object.\n\nThe Evaluate() function is used to assess the model’s performance. Accuracy for  example.\nFinally the TestSinglePrediction() method makes a prediction based on a single input record\nBelow are the snapshots of the program\n\n\n\n'], 'url_profile': 'https://github.com/solomonyaw', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ka1shi-medium', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""ProactiveAttritionManagementCaseStudyInR\nAIM\nThis case requires us to develop a model for predicting customer churn at “Cell2Cell,” a\nfictitious wireless telecom company, and use insights from the model to develop an incentive plan\nfor enticing would-be churners to remain with Cell2Cell.\nThe data are a scaled down version of the full database generously donated by an anonymous wireless\ntelephone company.There are still 71,047 customers the database, and 75 potential predictors.\nImplementation\nUnderstanding the problem statement\nFirstly we understand the problem statement and the data dictionary (i.e. what each variable means)\nand what is the question we are trying to answer.\nLoading and munging the dataset\nThen we load the dataset into R using readxl package's read_excel()\nfunction. And then doing some explicit datatype changes that have been wrongly interpreted by read_excel().\nThen removing variables with 0 standard deviation. And the redundant variables.\nWe are told that the values are missing at random so no need to worry much just remove the variables which\nhave high percentage of missing values and for other variables imputing them with mean.So I created a user\ndefined function to check the descriptive statistics, different quariles, number of missing values and an\nindicator if a variable has outliers using a formula.\nFitting a logistic regression model.\nThen we do Factor Analysis to select numerical data and anova for categorical variables to select important variables.\nThen we fit the step-wise Logistic regression model and use Vif on subsequent models to check for Multicollinearity.\nThen we use different methods to check model accuracy like Lift chart, Concordance, confusion matrix, decile analysis,etc.\nThe process of model fitting one is an Iterative process.\nPrerequisites\n\nR Installed\nKnowledge of Data Munging.\nKnowledge of Logistic Regression.\n\nAuthor\nAbhinav Gera\n""], 'url_profile': 'https://github.com/AbhinavGera', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Triple-S-India', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['sds291-workshops\nSlides and resources for workshops for SDS 291 Multiple Regression, Spring 2020.\n'], 'url_profile': 'https://github.com/emmal73', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'Sunnyvale, California', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Agenda:\nFit a model using StatModels api\nFit a model using Sci-Kit Learn : LinearRegression\nCompare the metrics of two models and choose the best one.\nWe are not doing prediction in this excercise but we will understand which is the best model for prediction based on F-Stat, T-stat, Adj. R2, co efficients etc.\n'], 'url_profile': 'https://github.com/bhaskarnn9', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['sparc_public\nA Python implementation of Sparse Regression Codes (SPARCs)/Sparse Superposition Codes for communication over the AWGN channel.\nIncludes code to implement power allocated, spatially coupled, PSK modulated and complex SPARCs. For an example of how to run SPARC encoding/decoding simulations or state evolution simulations, please see sparc_demo.ipynb.\nRelevant papers:\nC. Rush, A. Greig, and R. Venkataramanan, “Capacity-achieving sparse superposition codes via approximate message passing decoding,” IEEE Trans. Inf. Theory, vol. 63, pp. 1476–1500, March 2017.\nA. Greig and R. Venkataramanan, “Techniques for improving the finite length performance of sparse superposition codes,” IEEE Trans. Commun., vol. 66, pp. 905–917, March 2018.\nC. Rush, K. Hsieh and R. Venkataramanan, “Capacity-achieving Spatially Coupled Sparse Superposition Codes with AMP Decoding,” ArXiv.\n'], 'url_profile': 'https://github.com/kuanhsieh', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Telecom-churn-case-study-\nThis project is a very good explanation for Logistic Regression algorithm.\n\nDatasets are avilable in the Datasets folder\n\nIt contains 4 files,including data dictionary\n\n\n\n'], 'url_profile': 'https://github.com/vpfahad', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'United Kingdom', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bf108', 'info_list': ['Jupyter Notebook', 'Updated Oct 2, 2020', 'C#', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'R', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 12, 2020', 'JavaScript', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayushiagarwal10', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['sparc_public\nA Python implementation of Sparse Regression Codes (SPARCs)/Sparse Superposition Codes for communication over the AWGN channel.\nIncludes code to implement power allocated, spatially coupled, PSK modulated and complex SPARCs. For an example of how to run SPARC encoding/decoding simulations or state evolution simulations, please see sparc_demo.ipynb.\nRelevant papers:\nC. Rush, A. Greig, and R. Venkataramanan, “Capacity-achieving sparse superposition codes via approximate message passing decoding,” IEEE Trans. Inf. Theory, vol. 63, pp. 1476–1500, March 2017.\nA. Greig and R. Venkataramanan, “Techniques for improving the finite length performance of sparse superposition codes,” IEEE Trans. Commun., vol. 66, pp. 905–917, March 2018.\nC. Rush, K. Hsieh and R. Venkataramanan, “Capacity-achieving Spatially Coupled Sparse Superposition Codes with AMP Decoding,” ArXiv.\n'], 'url_profile': 'https://github.com/kuanhsieh', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['PQR\ncode for Shrinkage Quantile Regression for Panel Data with Multiple Structural Breaks\nHere we provide a copy of code for Shrinkage Quantile Regression for Panel Data with Multiple Structural Breaks.\nThe most important function file is mult-quantile.R, which contains three functions, rq.fit.lasso2，PQRcpt.func，IC.func.\nPQRcpt.func is used to estimate regression coefficients, individual effects and structural breaks.\nThe input variable X of PQRcpt.func is designed as $$ (x_{1,1}^\\top,x_{1,2}^\\top,\\cdots,x_{1,T}^\\top, \\cdots, x_{N,T}^\\top)^\\top $$.\nThe input variable y is the corresponding variable, which is a $$ NT \\times 1 $$ vector.\nThe input variable taus denotes the quantile level(s).\nThe output of PQRcpt.func contains coef, error, cpt and fix, which denotes the estimation of regression coefficients,\nvalue of check loss function, position of structural break(s) and estimation of fixed effects, respectively.\nIC.func is used to choose tuning parameter from the input.\nDGP.R, hausdorff.R and nobreak.sim.func are designed to generate data, compute hausdorff distance and percentage of correctly detect\nstructural break.\nb0.R is main file to show our simulation results of Scenario 2 in the paper.\n'], 'url_profile': 'https://github.com/tylerzzf', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Oladotun', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Hi all,\nThis is a linear regression model.Please note the following points before having a look at it.\n1)Demand  is a dependent variable and it is time series dependent.Hence,extra features are added as an effort for improving model\nperformance.\n2)Demand is not normally distributed,so,it is converted into normal distribution by taking log.\n3)Root Mean squared error=0.38\n4)RMSLE=0.3560\n5)I have tried my best to provide comments in the code for better understanding.\nPlease note that,This project is developed as a practice by me.Hence,I have referred various resources.\n'], 'url_profile': 'https://github.com/riteshpatil310197', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ocarian', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Boston_Housing\nRegression model with genetic algorithm implementation for the Boston housing dataset\n'], 'url_profile': 'https://github.com/jayr1125', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '518 contributions\n        in the last year', 'description': ['Learning by Building RM (Regression Model)\nThis repo consists of all folders and files needed for the project of ""Learning by Building"" of Regression Model course at Algoritma.\nThe programming language used in this repo is R. I wrote an R markdown file of this repo. If you prefer to read the notebook in Python language, you could see it here.\nI published the Rmarkdown on RPubs. You can view it here.\nFeel free if you\'d like to clone or download the repo.\nThanks for visiting my repo!\n'], 'url_profile': 'https://github.com/utomoreza', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Gradient-Descent\nNotebook demonstrating gradient descent and the maths behind Linear and Logistic Regression algorithms.\nAlso contains code showing how to write basic regression for Coninuous and Categorical variables.\n'], 'url_profile': 'https://github.com/Greg-S-12', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Krylov subspace method for nonconvex optimization.\nTraining neural network using Krylov subspace method on a regression problem. See train.py for detail.\nUnlike first-order optimizers, to adapt the optimizer for other problems, one needs to change the inference function and network structure\ndue to the fact that evaluating the Hessian-vector product requires that one store all the trainable parameters in one giant vector (then slice and distribute them to appropriate\nlayers when constructing the network).\n'], 'url_profile': 'https://github.com/ChengTang', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 9, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 22, 2020', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Jupyter Notebook', 'Updated Apr 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Student-Admission\nPredicting the admission of the student using Logistic Regression based on two exams\n'], 'url_profile': 'https://github.com/PaarthParekh', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['UCI-Car-Safety-Prediction\nPerforms a basic prediction of car safety evaluation label using logistic regression\n'], 'url_profile': 'https://github.com/hackintoday', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NarmadhaMM', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Neural-Net-Price-Prediction\nDataset\nDiamond dataset from Kaggle(https://www.kaggle.com/shivam2503/diamonds) was used to predict prices using Neural Network Regression.\nThe dataset contains:\nprice price in US dollars ($326--$18,823)\ncarat weight of the diamond (0.2--5.01)\ncut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\ncolor diamond colour, from J (worst) to D (best)\nclarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\nx length in mm (0--10.74)\ny width in mm (0--58.9)\nz depth in mm (0--31.8)\ndepth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\nThe column with string entry was changed into one-hot encoded columns based number of unique entries.\nNeural Network\nPackage used: PyTorch\nA simple Feed Forward Neural Network (3-hidden layers) was used.\nActivation Function: LeakyReLU\nXavier initialisation is used to initialise the weights.\nData was sent in mini-batches of 128.\nConvergence is easily achieved after few thousands of epochs.\nResults\nTraining Accuracy: 98.21%\nTest Accuracy: 98.06%\n'], 'url_profile': 'https://github.com/sri1299', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-With-VIF-Technique\nMultivariate analysis of linear regression with dimension reduction using VIF and P value technique\n'], 'url_profile': 'https://github.com/lohith0501', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Bhopal Madhya Pradesh', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hitechgaurav', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['predicting gpa using linear regression manually with gradient descent\nhere we performed linear regression manually with gradient descent to predict score\n'], 'url_profile': 'https://github.com/rishisankhla', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Washington DC', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Housing Price Prediction\n--Data Science project at UC Berkeley--\n--Team: Sharad Varadarajan, Akhil Patel, Mark Barnett, Nathan Nam--\nKaggle Competition - https://www.kaggle.com/c/house-prices-advanced-regression-techniques\nPlease see the kaggle page for full problem details\nFor a comprehensive look at our analysis, please see our jupyter notebook\nBrief Description\nIn this kaggle project, our team performed creative feature engineering and advanced regression techniques to predict housing prices in Ames, Iowa.\nTech Stack/Methods\n\nPython (w/ scikit-learn)\nJupyter Notebook\nUnivariate/Bivariate analysis\nData Cleansing/Feature Engineering\nData Imputation\nData Transformations\nLinear Regression\nRegularization (Ridge and Lasso)\n\n'], 'url_profile': 'https://github.com/sharadv99', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Stats-tool-research-paper\nRegression form that estimates individual x values opposed to best fit line\n'], 'url_profile': 'https://github.com/dylan518', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Triple-S-India', 'info_list': ['MATLAB', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dphelan61', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['1D-CNN\nApplication of 1D convolutional neural networks using non linear regression model\n'], 'url_profile': 'https://github.com/nagateja98', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Decision Tree from scratch\nThe goal of this project is to create a decision tree from scratch i.e. without using the built-in scikit-learn functions but instead using just pandas and numpy.\nThe decision tree can be used either for regression or classification.\nCredits\nThe whole exercise is inspired and follows closely the Git tutorial by Sebastian Mantey, see below references:\nhttps://github.com/SebastianMantey/Decision-Tree-from-Scratch\n'], 'url_profile': 'https://github.com/Rosacroce', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Zenetor', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EprimePRO', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Montreal, QC, Canada', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/talha-riaz', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'St Louis,MO,USA', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Linear-and-Logistic-Regression-Using-Gradient-Descent\nLinear and Logistic Regression using Matrix Multiplication and Gradient Descent(Python)\n'], 'url_profile': 'https://github.com/nishchaymisra13', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'Des Moines, IA', 'stats_list': [], 'contributions': '281 contributions\n        in the last year', 'description': ['Shop\nTo start your Phoenix server:\n\nInstall dependencies with mix deps.get\nCreate and migrate your database with mix ecto.setup\nInstall Node.js dependencies with cd assets && npm install\nStart Phoenix endpoint with mix phx.server\n\nNow you can visit localhost:4000 from your browser.\nReady to run in production? Please check our deployment guides.\nLearn more\n\nOfficial website: https://www.phoenixframework.org/\nGuides: https://hexdocs.pm/phoenix/overview.html\nDocs: https://hexdocs.pm/phoenix\nForum: https://elixirforum.com/c/phoenix-forum\nSource: https://github.com/phoenixframework/phoenix\n\n'], 'url_profile': 'https://github.com/toranb', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': ['logreg-NB\nlogistic regression and naive Bayes classification algorithms from scratch using c++ and R\nThis project show what R does under the hood to optimize its itself when performing machine learning algorithms. Compare the two R and cpp implementaions for both logistic regression and naive bayes to see for yourself. Hint: It does a lot\nAlso shows how fast C++ and R will run each algorithm. Hint: C is much faster\n'], 'url_profile': 'https://github.com/evanslyke31', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['SR_line_weld_element\nProject using symbolic regression to relate stresses found in line weld elements to SIFs in cracks.\n'], 'url_profile': 'https://github.com/kevenrc', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'C++', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Elixir', 'Updated Feb 16, 2020', 'C++', 'Updated Apr 21, 2020', 'Python', 'Updated Feb 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': [""1D-CNN-Nonliner-Regression\nThis is a 1D-CNN model built with Keras to solve regression problems (e.g. CA Housing).\nThe data set in the experiment is the 'California house price.csv', with 0.7 training/testing ratio. The model could be tuned by changing the first layer input shape to fit different data set.\n""], 'url_profile': 'https://github.com/asdlishoui', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'Halifax, Canada', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JeelGondaliya183', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""HousePrice-Prediction-kaggle\nCompetition Rank: 34 / 5019\nHouse Prices: Advanced Regression Techniques Predict sales prices and practice feature engineering, RFs, and gradient boosting\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nModels used Ridge,Lasso, SVR, Elasticnet, GradientBoosting,LightGradientBoosting, and Stacking.\n""], 'url_profile': 'https://github.com/princekaushik', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '1,864 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Monarene', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Thananjaya', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['housing-price\nRegression using CNN 1D for House price prediction on California Housing Dataset\nInput Layer: The input data has been preprocessed and doesn’t contain any missing values that can affect the prediction model. Every instance of the dataset has eight features, hence the input is 8 to provide a holistic impression per instance. The first layer in the network must reshape it to a single dimension.\nBatch Normalisation: Batch normalization is a technique for training very deep neural networks that stan- dardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochsrequired to train deep networks.\n1D CNN layer: The first layer defines a kernel of height 1. One filter would allow the neural network to learn a single feature in the first layer which is not adequate, hence 64 filters are defined to detect 64 features.\nPooling layer: A pooling layer is usually used after a CNN layer in order to subsample the input to provide a simpler output that helps prevent overfitting of the data. The proposed model uses max-pooling layer after two consecutive CNN layers followed by an average-pooling layer after two consecutive CNN layers.\nFully Connected Layers: The FC layers use the flat- tened output from the CNN layers in the vector of height 64. The proposed model has three FC layers and the last FC layer is responsible for providing a single output since we have a single value to predict, the house price (i.e. regression).\n'], 'url_profile': 'https://github.com/NTsering', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abubakarwaris1', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'Sunnyvale, California', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhaskarnn9', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alanrpfeil', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '474 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Austinstevesk', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'HTML', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', '1', 'C', 'Updated Feb 11, 2020', '1', 'Python', 'Updated Feb 15, 2020']}"
"{'location': 'United States', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Linear Regression With Multi Variable\nSupervised Learning Classification Algorithm\n'], 'url_profile': 'https://github.com/DanthuluriSatya', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Learning RESTful Simple ML Prediction\nflask-restplus based salary predictor\nThis repo example predicts the salary given number of years of experience.\nModel\nlinear_model.py trains and saves the model to the disk.\nFlask Server\napp.py manage the external facing APIs.\nMake Requests\nmake_requests.py has example code to test the end points.\n'], 'url_profile': 'https://github.com/vilvainc', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sivakrishna96', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Roorkee', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\nPredict sales prices and practice using feature engineering, RFs, and gradient boosting\nCompetition Description from Kaggle\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nMy Approach\nFollowing approaches and techniques i applied:\n-EDA with Pandas and Seaborn.\n-Find features with strong correlation to target.\n-Data Wrangling, convert categorical to numerical.\n-Apply the basic Regression models of sklearn.\n-Use gridsearchCV to find the best parameters for each mode.\n-Compare the performance of the Regressors and choose best one.\nMy notebook is organized as follows:\nPart 0: Imports, Settings and switches, Global functions\n-import libraries\n-settings for number of cross validations\n-define functions that are used often.\nPart 1: Exploratory Data Analysis\n1.1 Get an overview of the features (numerical and categorical) and first look on the target variable SalePrice\n-shape, info, head and describe\n-Distribution of the target variable SalePrice\n-Numerical and Categorical features\n-List of features with missing values and Filling missing values\n-log transform.\n1.2 Relation of all features to target SalePrice\n-Seaborn regression plots for numerical features\n-List of numerical features and their correlation coefficient to target\n-Seaborn boxplots for categorical features\n-List of categorical features and their unique values.\n1.3 Determine the columns that show strong correlation to target\n-Correlation matrix 1 : all numerical features\n-Determine features with largest correlation to SalePrice_Log.\nPart 2: Data wrangling\n-Dropping all columns with weak correlation to SalePrice\n-Convert categorical columns to numerical\n-Checking correlation to SalePrice for the new numerical columns\n-use only features with strong correlation to target\n-Correlation Matrix 2 (including converted categorical columns)\n-create datasets for ML algorithms\n-One Hot Encoder\n-StandardScaler.\nPart 3: Scikit-learn basic regression models and comparison of results\n-implement GridsearchCV with RMSE metric for Hyperparameter tuning\n-for these models from sklearn:\n-Linear Regression\n-Ridge\n-Lasso\n-Elastic Net\n-Stochastic Gradient Descent\n-DecisionTreeRegressor\n-Random Forest Regressor\n-KNN Regressor\n-baed on RMSE metric, compare performance of the regressors with their optimized parameters,\nthen explore correlation of the predictions and make submission with mean of best models.\n-Comparison plot: RMSE of all models\n-Correlation of model results\n-Mean of best models.\n'], 'url_profile': 'https://github.com/Manishk12', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['multiple-logistical-regression-aus-rain\n'], 'url_profile': 'https://github.com/rithikaadiga', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Perú', 'stats_list': [], 'contributions': '384 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexliqu09', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Prishtina', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ksylejmani', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': [""Linear Regression with R Assignment\nObjective: To predict the Housing prices in the suburbs of Boston as a function of the different attributes\nFollowing are some steps to be followed\nData Preperation :Understand summaries of the data, clean the data - missing value treatment, outlier treatment\nData Profiling : Understand the relationship between every IDV and DV (Correlations,visualisations,etc)\nBuild a Linear Regression model using IDV's that have good correlation with the DV\nEvaluate the model performance : Using p values and adjusted R square values\nValidate the model on the test data\n""], 'url_profile': 'https://github.com/sanchita21', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saik2121', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 17, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Feb 15, 2020', '1', 'R', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Energy-Efficiency-Regression-Predictive-Exp.-\n'], 'url_profile': 'https://github.com/younggee11', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'United Kingdom', 'stats_list': [], 'contributions': '725 contributions\n        in the last year', 'description': ['Predicting Superconductors\' Critical Temperatures using Regression  \nThis project covers the exploration of various ML techniques used for predicting the critical temperatures required for different superconductors to conduct electrical current with no resistance [1]. Various techniques and decisions are considered and discussed in the report for visualising & analysing the data, extracting its features, training multiple regression models to evaluate which one is suits the data best and make final predictions compared to Hamidieh\'s findings in the papaer ""A data-driven statistical model for predicting the critical temperature of a superconductor"".\nYou can read the full report here.\nResults\nAn improvement is noticed between the linear regression\'s final results and Hamidieh’s results [1], as the the RMSE is improved by 0.22K, while the R2 score remained the same:\n\nRMSE: 17.38K\nR2: 0.74\n\n\n\n\nUsage\nCreate a new virtual environment and install the Python libraries used in the code by running the following command:\npip install -r requirements.txt\n\nTo run the program, move to the “src” directory and run the following command:\npython main.py -s <section> [-m <model>] [-g] [-d]\n\nwhere:\n\n\n""-s section"": is a setting that executes different parts of the program. It must be one of the following: data_vis, train or test.\n\n\n""-m model"": is an optional setting that selects the regression model to use for training. It must be one of the following: linear, ridge, lasso, elastic_net, decision_tree, mlp, svm or random_forest_generator.\n\n\n""-g"": is an optional flag the grid search algorithm to determine the optimal hyperparameters for the selected regression model. The flag only takes effect when using linear regression with either Ridge or Lasso regularisation.\n\n\n""-d"": is an optional flag that enters debugging mode, printing additional statements on the command line.\n\n\nLicense\n\nsee LICENSE file.\n\nContact\n\nEmail: adam@jaamour.com\nWebsite: www.adam.jaamour.com\nLinkedIn: linkedin.com/in/adamjaamour\nTwitter: @Adamouization\n\nReferences\n[1] K. Hamidieh, “A data-driven statistical model for predicting the critical temperature of a superconductor,” Computational Materials Science, vol. 154, pp. 346–354, Nov. 2018.\n'], 'url_profile': 'https://github.com/Adamouization', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['LogisticRegression-and-NaiveBayes\nDownload the mnist.dat digit data from MNIST website\n'], 'url_profile': 'https://github.com/roongtarohit', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Campina Grande - PB - Brazil', 'stats_list': [], 'contributions': '408 contributions\n        in the last year', 'description': [""Multiple linear regression with python\nFirst, we need to load in our dataset. We're using the Scikit-Learn library, and it comes prepackaged with some sample datasets. The dataset we'll be using is the Boston Housing Dataset. The dataset has many different features about homes in the Boston area, like house size, crime rate, building age, etc. The goal is to predict the price of the house based on these features.\nHere are all the imports we need:\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\n\nbh_data = load_boston()\nprint(bh_data.keys())\n\nboston = pd.DataFrame(bh_data.data, columns=bh_data.feature_names)\nprint(bh_data.DESCR)\n\nboston['MEDV'] = bh_data.target\n\nX = pd.DataFrame(np.c_[boston['LSTAT'], boston['RM']], columns=['LSTAT','RM'])\ny = boston['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=9)\n\nlin_reg_mod = LinearRegression()\nlin_reg_mod.fit(X_train, y_train)\n\npred = lin_reg_mod.predict(X_test)\n\ntest_set_rmse = (np.sqrt(mean_squared_error(y_test, pred)))\ntest_set_r2 = r2_score(y_test, pred)\n\nprint(test_set_rmse)\nprint(test_set_r2)\nTo Dummy variables\nX = pd.get_dummies(data=X)\n""], 'url_profile': 'https://github.com/RonnyldoSilva', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['LASSO-and-Boosting-for-Regression\nMany variables are included so that algorithms that select or learn weights for attributes could be tested. However, clearly unrelated attributes were not included; attributes were picked if there was any plausible connection to crime (N=122), plus the attribute to be predicted (Per Capita Violent Crimes). The variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units.\nThe per capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the United States: murder, rape, robbery, and assault. There was apparently some controversy in some states concerning the counting of rapes. These resulted in missing values for rape, which resulted in incorrect values for per capita violent crime. These cities are not included in the dataset. Many of these omitted communities were from the midwestern USA.\n\n\nPre-processed the Communities and Crime Dataset to deal with missing values by using the “Mean Substitution” technique of data imputation.\n\n\nPlotted a covariance matrix heatmap to identify the strongest correlation between the predictors (population, number of homeless people, number of immigrants, number of people living in urban areas, etc.) and the label (Violent Crimes per population).\n\n\nSelected top 11 features responsible for violent crimes in a community by looking at their coefficient of variation values. Created scatter plots and box plots to give more insight about the dataset using matplotlib and seaborn libraries.\n\n\nCompared the performance of Linear regression, Ridge regression, LASSO regression and Principle component regression models on the dataset to find the best fit.\n\n\nAchieved a lowest test error rate of 1.69% in the XGBoost Model using L1 penalised linear regression.\n\n\n'], 'url_profile': 'https://github.com/pratishtha9', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mrinal-pandey', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/piyushdjr', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regression-with-more-than-2-features\n'], 'url_profile': 'https://github.com/seniorfoua', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Logistic Regression Graded Assignment with R\nThis assignment is a part of graded assignments from Jigsaw Academy-Online school of Analytics\nThis assignment is related to building a logistic regression model on sample survey data.\nThis data set has been collected as a part of a survey exercise done by a leading snacks manufacturer.\nThis snacks manufacturer produces 6 different brands.\nThe actual brand names have been masked and are present in the dataset as Brand A, Brand B, Brand C, Brand D, Brand E and Brand F. There is also a data dictionary named GoodForUVar.pdf\nA leading snacks manufacturer (A)wanted to understand the relationship between overall brand perceptions and the drivers of respondent’s decision on a product to determine the most important factors impacting A’s brands. This manufacturer is interested in finding out what factors affect the perception for Brand A. Can you help?\n'], 'url_profile': 'https://github.com/sanchita21', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 15, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 25, 2020', 'Python', 'Updated Feb 15, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'Python', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 13, 2020', 'R', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Ames Housing Data and Kaggle Challenge\n\nProblem Statement\nHomeowners, leasing agents and developers are all interested in knowing where to best invest their money and resources in order to get the greatest return when selling a house.\nInvestors and realtors are also interested in knowing which houses would be most financially profitable, especially if these evaluations can be discovered first.\nBased on the Ames Housing Dataset, I will create a regression model to predict the price of a house at sale.\nExploratory Data Analysis\nNull Values\nFirst, I evaluated the null values in the training data. I found that many features in the data dictionary (relating to Bsmt, Fireplace, Garage, Pool, Misc) had possible values of \'NA\' that seemed to be showing up in the dataset as null instead of \'NA\'. This was also verified via a value of zero in a related feature. Alley and Fence could not be verified by any other feature. Nonetheless, their missing values were replaced with \'NA\', keeping in mind that these could either be unlisted or not existing properties of any single house.\nThe Lot Frontage features also had a large number of null values that could not be imputed since it is a continuous variable of ""Linear feet of street connected to property"". After replacing and dropping the columns with too many null values, the remaining rows with null values were dropped from the training data so that they did not prevent the model from running (which was less than two percent of the rows in the dataset).\nData Types\nIn reference to the data dictionary, the data types were evaluated and adjustd as needed. This included dropping the observational identifiers, converting a nominal feature from integers to strings, and assigning numerical values to ordinal features. I then created dummy variables for the remaining nominal object columns.\nI also altered the year-related columns to reflect the relative values compared to the time of the sale.\nOutliers\nAfter reviewing the distributions of the features relative to the Sale Price, I removed a limited number of extreme outliers which seemed to reflect very unusual, atypical sales.\nCorrelations\nI then evaluated the correlations of the remaining features with Sales Price and among each other. In order to address the issue of multicollinearity, I created many interaction terms. Afterwards, I found the highest correlated features of all the existing features to the Sale Price\nModeling the Data\nPrep the Test Data\nSince I changed many of the features in the training data, I also had to manipulate the test data to display identical changes. This included eliminating null values, changing dtypes, getting dummy variables, and adding interaction terms.\nModeling & Evaluation\nI used linear regression to model the data, and I evaluated the results based on RMSE and the r2 score using and comparing across train-test split, cross validation, and the whole training set. I also evaluated the model when using a log of the y-variable and iterating through a variety or parameters within the Lasso and RidgeCV functions.\nConclusions\nIn conclusion, I found that creating an expansive list of interaction terms improved my model significantly and resulted in the highest correlations with the Sale Price. It seemed that the interaction between the overall quality of a house and many other features was most impactful in evaluating the Sale Price. It was also not necessary to log the Sale Price when using all of the interaction terms.\nIn the end, I was able to explain 90% of the variance in the Sales Prices by the model.\nFurthermore, in combining the highest correlations with the highest coefficients (all else held equal), it is likely that the best investment to increase the value of a home may be to increase the capacity of the garage to include another car. Although the features in the model were not expicitly relative to a location, in order to generalize across more than just the town of Ames, it would be best to receive similar data from other cities/towns to evaluate if the model predicts well elsewhere.\n'], 'url_profile': 'https://github.com/megganlanpher', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lurui0421', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Seattle, WA, USA', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': [""regression-on-an-open-ended-problem\nMy approach to solving an open ended problem provided to me by the Adobe Data Science team.\nThis problem was a part the hiring process for a Data Science intern. Here, all that was given to me was a training dataset and a testing dataset. My task was to perform regression on that testing data using the training data for training the model. This is by no means a perfect approach to the question, especially since it didn't get me through the next round. My reason for uploading this approach to Github is to get feedback from the peers on how I can improve my approach.\n""], 'url_profile': 'https://github.com/prathameshmahankal', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Maverik95', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/p4prabhav', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Boston_House_Price_Prediction_Regression_Model\nThis is a predictive regression model for estimating the Boston house prices in thousands of $ given some housing factors such as crime rate in neighborhood, number of schools % lower status of the population etc.\n'], 'url_profile': 'https://github.com/satinfocom20', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'R', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sachin10397', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhimithra02', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Linear-Regression-Analysis-on-Advertising-Data-set\n'], 'url_profile': 'https://github.com/PushpanathanPerumal90', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TzavarasIoannis', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ZipCodeROIByLinearRegressionModel\nObjective: Identify the top 3 zip codes to invest in for the best ROI using Zillow data. Data used is from 1996-2017.\nCSV used is a pre-cleaned CSV, normalized within Excel. The file is too big to upload unfortunately. Other normalization was done through the package PandasSQL. PandasSQL allows you to write T/SQL within Python.\n'], 'url_profile': 'https://github.com/bschneider29', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Build-a-Regression-Model-in-Keras\n'], 'url_profile': 'https://github.com/Ahmedtem', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['6-Implement-SGD-for-linear-regression\n'], 'url_profile': 'https://github.com/skshiraj', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marwa87', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NLP-Nonlinear-Regression-using-1D-CNN\nThis repository include a one dimensional Convolutional Neural network. The used dataset is California Housing dataset.The Solution is nonlinear regression model\n'], 'url_profile': 'https://github.com/karanshah10', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Updated Feb 12, 2020', 'MIT license', 'Updated Feb 16, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['5-Logistic-Regression-on-Donors-Choose-dataset\n'], 'url_profile': 'https://github.com/skshiraj', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Credit-card-fraud-analysis--LogisticRegression\nMachine learning  LogisticRegression\n项目背景\n信用卡欺诈属于二分类问题，欺诈交易在所有交易中的比例很小。通过以往的交易数据分析出每笔交易是否正常，是否存在盗刷风险是这次项目的目标。\n本项目包括：\n\n了解逻辑回归分类，以及如何在 sklearn 中使用它\n通过数据可视化对数据探索和模型结果评估进一步加强了解\n\n安装\n\n功能特性\n环境依赖\n部署步骤\n\n使用\n主要项目负责人\n在使用中有任何问题，欢迎反馈给我，可以用以下联系方式跟我交流：fennyh@csu.edu.cn\n参与贡献方式\n开源协议\n'], 'url_profile': 'https://github.com/Fennyhhh', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'Istanbul, Turkey', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tolunaykatirci', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'Whitby, Ontario', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jawadm96', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'Shenzhen', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['BigDataProject1--LogisticRegressi\n大数据导论Project，极大似然估计方法实现逻辑回归算法\n'], 'url_profile': 'https://github.com/KinloongTiao', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abubakarwaris1', 'info_list': ['Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abubakarwaris1', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'Thunderbay Ontario', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NonLinearRegression_ConvID_NN\nImplemented on Google Colab\nImplement a one dimensional convolution (Conv1D)-based neural network using housing dataset\n'], 'url_profile': 'https://github.com/mohiuddin02', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'Istanbul, Turkey', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tolunaykatirci', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yagnad', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'Copenhagen ', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Meghdad-DTU', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""This project was created using jupyter notebook and the O'Reilly Hands-on Machine Learning with Sckit-learn, Keras & TensorFlow book by Aurelien Geron.\n""], 'url_profile': 'https://github.com/gsantiago1618', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Natural Language Processing\nAdjective Classifier\n\nWord tokenizer\nPig latinizer\nFeature extractor\nLogistic Regression Adjective Classifier\n- Uses L1 regularization and dynamically finds optimal penalty C value.\n\nTested for Python 3.8.1\n'], 'url_profile': 'https://github.com/khan-ibrahim', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['SPSS-Project\nBackground:\nDATA COLLECTION\nChoose 3 lists of 36 stocks on the TSX from http://clouddc.chass.utoronto.ca.ezproxy.library.yorku.ca/ds/cfmrc/\nPick 36 stocks based on\n• 36 stocks whose symbols begin with Y or A (call this the 1st group)\n• 36 stocks whose symbols begin with J or U (call this the 2nd group)\n• 36 stocks whose symbols begin with B or A (call this the 3rd group)\nand get an end of day price for ALL of them on the same date (April 6).\nRULE:\nONLY CHOOSE STOCKS that have both 2015 & 2016 prices.\nTry to get a range of prices. This improves the “appearance” of the distribution. Do not use stocks that have a negative price or a zero price in either year (may select more than your total of 108 stocks because some might need to be rejected if they have zero or negative prices – end up with 108 stocks with non-zero and non-negative prices).\nGet an end of day price for approximately 1 year later in 2016 (± 1-5 days) FOR FIRST & SECOND BASKETS of stocks.\nComparing two means:\nWe are performing Two-Sample T-Test in SPSS.\nTwo-Sample T-Test is also known as independent T-Test or between-subjects T-test. We perform this test when we want to compare the mean of two different samples.\nBy running Analyze/CompareMeans/ Independent-Samples T-Test, we can decide if there is a significant difference in the mean value of the sets of stocks between two group1 and group2 in 2015, at the α = 0.05 level of significance.\nComparing 3 means:\nPerforming one-way ANOVA in SPSS.\nANOVA stands for Analysis Of VAriance. We run ANOVA when we want to compare the mean scores of three or more groups.\nBy running Analyze/General Linear Model/Univariate in SPSS to analyze if there is a significant difference in the mean value of the sets of stocks in each of the three groups in 2015, at the α =0.05 level of significance.\nRegression:\nPerforming simple linear regression in SPSS\nWe run simple linear regression when we want to access the relationship between two continuous variables.\nWe are running Analyze/Regression/Linear in SPSS to analyze the if the price of stocks in year 2015 were to be significant to determine the price of the stocks in year 2016.\nThe stock price in 2015 is called dependent variable and the stock price that we are going to predict in 2016 is called dependent variable.\n'], 'url_profile': 'https://github.com/AUSB', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}","{'location': 'PVD, USA', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['logit-intro\nThis Rmarkdown document is a brief description on Logistical Regression using R and Rmarkdown.\nLogistic Regression PDF\n'], 'url_profile': 'https://github.com/mccurcio', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Feb 11, 2020', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Feb 19, 2020', 'Updated Feb 16, 2020', 'Updated May 26, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['Diabetes-prediction-using-logistic-regression-Random-forest\nThis effort was made to understand how logistic regression/Random forest will classify diabetes prediction along with its decision boundary\n'], 'url_profile': 'https://github.com/lohith0501', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['Bootstrap-Sampling-\nThis effort was to check how bootstrap sampling can work on smaller data set for Regression model\n'], 'url_profile': 'https://github.com/lohith0501', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Washington DC', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Click-Through-Rate Prediction\n--Data Science project at UC Berkeley--\n--Team: Sharad Varadarajan, Cameron Kennedy, Julia Buffinton, Ram Iyer--\nDescription\nGiven a large dataset containing clicks on advertisements served by Criteo from Kaggle, we sought to use our knowledge of large scale ML to build a scalable machine learning model to predict whether a user would click on a certain advertisement. The data contained 46 million records with an extremely sparse 33 million features. We developed a ""homegrown"" implementation  of logistic regression in Python using Spark (without MLlib) and evaluated its performance using log loss and accuracy. Our best model achieved a log loss of 0.650 and accuracy of 0.645.\nThe files in this directory contain the presentation notebook and code for this project.\nPlease see the full codebook here.\nTech Stack/Methods\n\nPyspark\nBig Data analysis\nFeature Engineering/EDA for sparse dataset\nGoogle Cloud\nLogistic Regression (w/ Log Loss)\nGradient Descent\nRidge and Lasso Regularization\n\n'], 'url_profile': 'https://github.com/sharadv99', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Turkey / Istanbul', 'stats_list': [], 'contributions': '610 contributions\n        in the last year', 'description': [""Numerica\n\nMy own experimental implementations of numerical methods as homework.\nUse documentation to see how to use, and check test.py for real examples.\nTable of Contents\n\nUsage\n\nImporting\nFunction Definition\nMatrix Definition\n\n\nDocumentation\n\n1- Solving Nonlinear Equations\n\nRoot Bracketing Methods\n\nGraph\nBisection\nRegula-Falsi\n\n\nIterative Methods\n\nFixed-Point Iteration\nNewton-Raphson\nSecant\n\n\n\n\n2- Matrix Operations\n\nBasic Operations\n\nDefinition\nCreating an Identity Matrix by n\nGetting Dimensions of a Matrix\nTranspose of a Matrix\n\n\nFinding Inverse of a Matrix\n\nGauss-Jordan Method\n\n\nMatrix Utils\n\nConcat Matrices by Row (Horizontal)\nConcat Matrices by Column (Vertical)\nMap a Row of Matrix\nMap all Matrix Cells\nIs Matrix Check\nSlice Matrix Vertically\n\n\n\n\n3- Solving Systems of Linear Equations\n\nGauss Elimination\nJacobi\nGauss-Seidel\n\n\n4- Solving Systems of Nonlinear Equations\n5- Numerical Integration\n\nTrapezoidal\nSimpson\n\n\n6- Numerical Differentiation\n\nEuler Methods\n\nBackward\nForward\nMidpoint\n\n\n\n\n7- Finite Differences\n\nDetermine  Degree of a Polynomial\n\n\n8- Interpolation\n\nLagrange\n\n\n9- Regression\n\nLeast Squares\n\n\n\n\nResources\nTesting Package\nUploading to PyPI\n\nUsage\npython >= 3.8 is required\nImporting\nimport numerica as n\nfrom numerica import f // function definition\nfrom numerica import m // matrix definition\n\nFunction Definition\nf('expression')\n\nfx = f('3x^2 + 2x + 3')\nfx(2)\n\nMatrix Definition\nm(\n    a11, a12, a13;\n    a21, a22, a23;\n    a31, a32, a33\n)\n\nmatrix = m('1,2,3; 4,5,6; 7,8,9');\n\nDocumentation\n1- Solving Nonlinear Equations\nRoot Bracketing Methods\nGraph\nn.nl_graph(fx, dx, epsilon, x)\n\nBisection\nn.nl_bisection(fx, epsilon, a, b)\n\nRegula-Falsi\nn.nl_regulafalsi(fx, epsilon, a, b)\n\nIterative Methods\nFixed-Point Iteration\nn.nl_fixedpoint(hx, epsilon, x)\n\nNewton-Raphson\nn.nl_newtonraphson(fx, epsilon, x)\n\nSecant\nn.nl_secant(fx, epsilon, x0, x1)\n\n2- Matrix Operations\nBasic Operations\nMatrix Definition\nm(\n    a11, a12, a13;\n    a21, a22, a23;\n    a31, a32, a33\n)\n\nIdentity Matrix\nn.m_id(n)\n\nSize of Matrix\n(m, n) = n.m_size(A)\n\nTranspose of a Matrix\nn.m_transpose(A)\n\nFinding Inverse of a Matrix\nGauss-Jordan Method\nn.mi_gaussjordan(A)\n\nMatrix Utils\nConcat Matrices by Row (Horizontal)\nn.m_rowconcat(A, B)\n\nConcat Matrices by Column (Vertical)\nn.m_colconcat(A, B)\n\nMap a Row of Matrix\nn.m_rowmap(A, i, iteratee)\n\nMap all Matrix Cells\nn.m_cellmap(A, iteratee)\n\nIs Matrix Check\nn.is_matrix(A)\n\nSlice Matrix Vertically\nn.m_rowslice(A, start, stop, step)\n\n3- Solving Systems of Linear Equations\nGauss Elimination\nn.ls_gauss(A, C)\n\nJacobi\nn.ls_jacobi(A, C, X, epsilon=0.001)\n\nGauss-Seidel\nn.ls_gaussseidel(A, C, X, epsilon=0.001)\n\n4- Solving Systems of Nonlinear Equations\n5- Numerical Integration\nTrapezoidal\nn.itg_trapezoidal(fx, x0, xn, n)\n\nSimpson\nn.itg_simpson(fx, x0, xn, n)\n\n6- Numerical Differentiation\nEuler Methods\nBackward\nn.diff_backward(fx, x)\n\nForward\nn.diff_forward(fx, x)\n\nMidpoint\nn.diff_midpoint(fx, x)\n\n7- Finite Differences\nDetermine Degree of a Polynomial\nn.fd_degree(pair_tuples)\nn.fd_degree([(x0,y0), (x1,y1), (x2,y3), ...])\n\n8- Interpolation\nLagrange\nn.itp_lagrange(pair_tuples)\nn.itp_lagrange([(x0,y0), (x1,y1), (x2,y3), ...], x)\n\n9- Regression\nLeast Squares\nn.reg_leastsquares(pair_tuples, degree) // returns polynomial\nn.reg_leastsquares_solve(pair_tuples, x, degree) // solves polynomial \nn.reg_leastsquares_solve([(x0,y0), (x1,y1), (x2,y3), ...], x, deg)\n\nResources\n\nYTU Numerical Analysis Lecture Notes\nhttps://mat.iitm.ac.in/home/sryedida/public_html/caimna/index1.html\n\nTesting Package\nTest Directly as Script\npython3.8 -m numerica\n\nor Install Package Locally (from repo root dir)\npip3.8 install .\n\nand Test It from REPL\nimport numerica as n\n# ...\n\nor Use test.py Interactively\npython3.8 -i test.py\n# ...\n\nor Just Test and Exit\npython3.8 test.py\n\nUploading to PyPI\nInstall Twine\npip3.8 install twine\n\nBuild\nrm -rf build & rm -rf dist & rm -rf numerica.egg-info\npython3.8 setup.py sdist bdist_wheel\n\nUpload\ntwine upload dist/*\n\n""], 'url_profile': 'https://github.com/ramesaliyev', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'San jose, california', 'stats_list': [], 'contributions': '295 contributions\n        in the last year', 'description': ['homework_0-\nML assignment\n'], 'url_profile': 'https://github.com/SruthiChilukuri', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Gandhinagar, Gujarat, India.', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""Ridge-From-Scratch-Using-Sklearn-s-Optimizer-SAGA\nScratch Ridge Regressor Using Sklearn's SAGA Optimizer\nThis is the model of well known regressor called ridge. Which includes regularization methods(l2). Using sklearn's optimizer you can get best results, and in this method I've simplified the code further so that it is well understandable for ML beginners.\nUse: If you want to inherite the model into a different system, you can easily refer this code and make optimization further.\nFor example if you have faster validation techniques then you can remove all the validation steps in this code and use it only for getting trained weights using SGD.\nSame condition for preprocessing and normalization.\nThis code is for study purpose. Anyone can refer this as base model.\n""], 'url_profile': 'https://github.com/Naman1233', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': [""Quality Prediction in a Mining Process Using Regressor Model\nFroth flotation is a process for selectively separating hydrophobic materials from hydrophilic.\nHistorically this was first used in the mining industry and is very common in a mining plant, where it was one of the great enabling technologies of the 20th century.\nThe development of froth flotation has improved the recovery of valuable minerals, such as copper- and lead-bearing minerals.\nAlong with mechanized mining, it has allowed the economic recovery of valuable metals from much lower grade ore than previously.\nThis dataset I'll be using is about a flotation plant which is a process used to concentrate the iron ore.\nThe main goal is to use this data to predict how much impurity is in the ore concentrate.\nAs this impurity is measured every hour, if I can predict how much silica (impurity) is in the ore concentrate, this can help the engineers, giving them early information to take actions (empowering!).\nHence, they will be able to take corrective actions in advance (reduce impurity, if it is the case) and also help the environment (reducing the amount of ore that goes to tailings as you reduce silica in the ore concentrate).\nIn this kernel, I'll:\n\n\nPrepare the data for machine learning\n\n\nTrain a model using a Regressor Model\n\n\nMeasure & optimize the accuracy of your model\n\n\n""], 'url_profile': 'https://github.com/zuruoke', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '10', 'Python', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Pretoria, South Africa', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MasiMasinga', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ratan-Debnath', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'Dallas, Texas, USA', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Implementation-of-Gradient-Descent-Algorithm-for-Linear-and-Logistic-Regressions\nsgemm_product.csv is the dataset used for this project. This can also be downloaded from the UCI Repository mentioned in the project report.\nThe project has been carried out in python using Jupyter notebook, and the full code is available in ""Gradient Descent Implementation Code.ipynb"" file.\nThe ""Project Report"" file contains the Machine Learning experimentations that were performed on the data set, provides results for the experimentations, and offers a small discussion section.\n'], 'url_profile': 'https://github.com/shrinithiraghavan', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'Dallas, Texas, USA', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Implementation-of-Gradient-Descent-Algorithm-for-Linear-and-Logistic-Regressions\nsgemm_product.csv is the dataset used for this project. This can also be downloaded from the UCI Repository mentioned in the project report.\nThe project has been carried out in python using Jupyter notebook, and the full code is available in ""Gradient Descent Implementation Code.ipynb"" file.\nThe ""Project Report"" file contains the Machine Learning experimentations that were performed on the data set, provides results for the experimentations, and offers a small discussion section.\n'], 'url_profile': 'https://github.com/shrinithiraghavan', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Non-linear-regression-with-1D-convolution-neural-network\nThe repository contains Dataset, python notebook, and the trained model\nA different code to run with trained model is also uploaded. other wise main code will train the model if you run all the block right now.\n'], 'url_profile': 'https://github.com/Paavanpatel', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Non-linear-regression-with-1D-convolution-neural-network\nThe repository contains Dataset, python notebook, and the trained model\nA different code to run with trained model is also uploaded. other wise main code will train the model if you run all the block right now.\n'], 'url_profile': 'https://github.com/Paavanpatel', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets. Key takeaways include:\n\nThe Pearson Correlation (range: -1 -> 1) is a standard way to describe the correlation between two variables\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'Australia', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Time-series-prediction-of-country-s-GDP-using-Linear-Regression\nThe objective is to predict the GDP of country using time series data ranging from year (1850-2005).\nSource: http://dataservices.gfz-potsdam.de/pik/showshort.php?id=escidoc:3155894\n'], 'url_profile': 'https://github.com/ArchaDoriya', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""Regression_Insurance_Analysis\nAuthors: Finn Tan\nWe are a healthcare consultancy company hired by an European Life Insurance firm who is looking to expand into the US market. As such, we have been tasked to provide some statisical insights into the main drivers of life expectancy which will be a crucial input in driving our client's pricing strategies.\nCoupled with the above, we were also required to suggest 5 US states that would be best to break into, which were ultimately conducted by finding a subset of US states with high life expectancy but with low life insurance coverage.\nIn order to accomplish this, we have used the 2019 County Health Rankings National Data which provides a comprehensive numerical data on counties':\n- Health Outcome\n- Health Behviours\n- Clinical Care\n- Social & Economic Factors\n- Physical Environment\n- Demographics\n\nFor better readability, we have split our findings into several parts including:\n- 'index - Data Cleaning.ipynb'\n- 'index - EDA & Visualization.ipynb' \n- 'index - Regression Modelling, Model Evaluation & Conclusion.ipynb'\n- 'functions.py' - include customized functions\n\nProcess\nGiven the sheer size of the features involved and for better interpretation for our final user ('Life Insurer'), it is crucial to narrow down the number of variables. There are several ways to do this but the ones we are about to list below are definitely not exhaustive.\n- Baseline Method: Using all available variables\n- Naive Selection: using top features that are correlated to Life Expectancy\n- Filter Method: dropping low variance features followed by removing highly correlated features\n- Stepwise Selection: Adding features with p-values below certain threshold and dropping those\n- Recursive Feature Elimination: sklearn's function of greedily choosing\n- Lasso: use GridSearch to find the best penalizing parameter 'alpha' for the Lasso algo. We will then select features that have not been shrinked to 0\n\nOnce we get all the features selected by each method above, we pass those into Statsmodel's OLS function. Subsequently, we will select our most prefferred model by comparing their R2 scores, AIC (model complexity) and also consider the number of features included, which is the primary consideration here.\nPost model selection, we will then check if the chosen model satisfies the assumptions of a regression; no multicollinearity between selected features, homosceasticity and normality of errors. In the end, we shall evaluate the model if it fits the purpose for our final user.\nMethod Evaluations\n\nBaseline\nFor the baseline model, we used all 60 features which managed to generate train R2 scores in the 72 - 75% region. However the large number of features coupled with the high non-signficant (p-values) is not very appealing, especially for our end user.\nNaive Selection\nThe Naive Selection method uses features with high correlation (set at 0.65) with the 'life expectancy' variable. Whilst the technique might be useful for reducing the number of features, one of the pitfalls of this is that the method might ignore other useful features.\nFilter Method\nCoupled with the above, the previously contemplated models might include various highly correlated features, violating the assumptions of a regression model. As such, we tried using a middle approach by:\n- dropping features with no or low variance. These features typically do not add much predictive value in a model\n- dropping features which are highly correlated\n\nThis method however still results in a high number of features / non-significant features.\nRecursive Feature Elimination\nUsing sklearn's RFECV function returns the best number of features to use according to the R2 scoring criteria. With this we obtained:\n\nThis looks slightly better than the filter method however the number of features still remains an issue here.\nStepwise ['Excerpt from the Learn.co']\n'In stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.'\nWith this model, we achieved a fairly reasonable R2 and a manageable number of features.\nLasso\nUsing GridSearch CV to identify the best alpha hyperparameter / penalizer. We then took the non-zero coefficients from the model and fitted that into an OLS model which resulted in outcomes similar to the above stepwise model.\nModel Selection\nLooking purely by R2 scores, model complexity and optimum number of features, we have decided to go for the Stepwise model. However we will still need to check if the model fits the regression assumptions. First let check for multicollinearity using the VIF metric.\n\nSince no features are more than 5, multicollinearity is not a big concern here. Next we checked for homoscedasticity by plotting the model's residuals.\n\nThis looks fine with no concerning trends. No we look at the normality of the errors using the QQ plot before and after removing outliers:\nBefore:\n\nAfter:\n\nThis looks better now. Finally we can explore the coefficients and finally deploy our chosen & updated model on the test data.\n\nUsing the coefficients above, we ended up with a:\nTest R2 score of 68%\nSummary map:\n\nConclusion, Limitations and Future Work\nTo conclude, it is important to emphasize that whilst we have chosen the model above, there are many other methods out there which may result in a better model. That said, the above model should at least provide our end user with the crucial indicators for estimating life expectancy.\nFor future work, we will look to explore:\n- investigate performance of forward / backward selection, Ridge regression, interactions and polynomials\n- user other models apart from OLS, ie RF, CART, etc\n- investigate indirect correlations between features\n\n""], 'url_profile': 'https://github.com/Ftan91', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['R-Linear-Regression-with-more-than-2-inputs\n'], 'url_profile': 'https://github.com/seniorfoua', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bmagnificent86', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'HTML', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '404 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rubyruins', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fijeskp', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PankajLal99', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/divya21raj', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'New York, NY ', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jingkunzler211', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Machine-Learning\nAll my Machine Learning work using MATLAB divided in different areas of application:\nPractica 1 - Linear Regression\nPractica 2 - Regularization & Cross validation\nPractica 3 - Logistic Regression (Binary)\nPractica 4 - Logistic Regression (Multiclass) in the MNIST problem\nPractica 5 - Naive Bayes classifier\nPractica 6 - PCA\nPractica 7 - Clustering\nPractica 8 - Recommender System\n'], 'url_profile': 'https://github.com/jesusaguas', 'info_list': ['Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'HTML', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Feb 11, 2020', '1', 'MATLAB', 'Updated Feb 14, 2020']}"
"{'location': 'Pomona, California', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bapurandare', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['GradeWeightSolver\nA short tutorial of performing Linear Regression in R by determining the weights of grade requirements of students towards the overall grade\nThe Scenario\nI have thought of beginning a series, writing blogs for my Tech-Blogz, particularly on anything Machine Learning, Data Science, Analytics, etc. It could be tutorials, tricks, tips, thoughts and experiences or basically anything in between. So today I have thought of writing a basic tutorial for performing simple linear regression in R.\nAs a TA, I easily have connections with different professors and faculty members in our Computer Science department at our university at York. I was able to connect with one of my closest professors, and I asked for a little help from him, to provide his students\' grades for a particular course. I would not be mentioning which professor is this and for what course and which academic session it is, but we will be using the grades.xlsx dataset, credits to the anonymous professor who has provided the dataset (does not wish to be named). Also names, student IDs and emails of students were removed for privacy reasons.\nThe Dataset\nThe grades.xlsx dataset is an Excel file, which is composed of 636 observations (i.e. 636 students and each row correspoonds to their grades in the semester) and has 18 columns:\n\ns_id - the student ID, labelled 0001 to 0636; the original student IDs are removed\nColumns 2 to 17 are the predictors, these are the grading requirements that will make up the final grade; each component weighted differently\nfinal_grade - the response variable; basically, the percentage of the requirement is multiplied by its grade; this is done to all variables and then summed altogether\n\nIn terms of values, notice that only column 1 is an integer (iDs) but we will find out soon enough that we don\'t really care about the column. Columns 2-18 are reals with minimum value of 0 and a maximum value of 1, which represents the percentage (i.e. 0 for 0% and 1 for 100%, so 75% just means 0.75).  \nFor the predictors, there are 9 labs, 2 tests, 1 midterm, 3 quizzes and 1 final exam. Note that I do not know what the weightings are for each requirement since the grades dataset was from a very old academic session, and I have not yet been born that time yet (lol). Hence, it is those weightings that we will try to predict using linear regression. There are some things that may skew our results, such as:\n\nOutliers: there would always will be, and so we will see how many of them are in our dataset and assess whether it is worth to remove the outliers\nLet us also decided on whether or not to remove the students who have missing exam grade or have several incomplete grades\nIt is also unknown as to whether the final grades are as is or if they are curved (either bumped up or down). In certain universities, if a professor senses that the midterm grades turn out to be low, they tend to make the exam easier so that students can pull their averages up - and this can mess up our analyses. It may also be possible that midterms tend to be high and hence the professor makes exams harder to pull class averages down. Also, if by the end of the term and the professor finds out that his/her class has a low class average when it comes to final grades, they tend to curve. In that case, averages shift, and so do the student\'s grades. We do not know whether the response variable here is already adjusted or the original average.\n\nWhere to Find the Dataset:  It is in this GitHub Repo, please download grades.xlsx\nTutorial in Linear Regression\nNote that the R script can also be found in this GitHub repo.\nGetting started\n\n\n\nFirst download the dataset in this repository. Then launch R (click here if you have not installed R/RStudio yet or if you need technical help with R/RStudio) and start a new project. Once the R window opens, you would see a small square window on the upper right corner of the screen. Click on the ""Import Dataset"" button, then click on ""From Excel"" option since we know that the dataset comes from an Excel sheet. Navigate on to the downloaded grades.xlsx file. Make sure that we keep the headings on. Then keep all defaults and then import. Alternatively, you can type in this R code in the console to import the dataset:\n> library(readxl)\n> grades <- read_excel(""~/Google Drive/linear_reg_R/grades.xlsx"")\n> View(grades)\nWARNING! If you opt to use this code, ensure that the parameter inside the read_excel()  contains the correct path to where you saved your downloaded dataset file. Update the above code if necessary!\nNotice that the first line allows us to import Excel files, the second line allows us to read our Excel file and import the dataset into a variable called grades. The third line simply allows us to view the dataset.\nExamine the dataset if you will. Use the the third line of the R code above to view the dataset upon import. Alternatively, ensure that the ""Environment"" tab is enabled in the small window on the upper right hand corner and you will see a list of available variables in the current environment. Click (or double click ""grades"") to view the dataset. Compare if we have the same; below shows the first 10 observations that you would find in the dataset:\n\n\n\nPlaying Around With the Data (A Bit)\nIt is important to get to know your data, what you\'re dealing. Although I have given some preliminary information on what kind of dataset we will be working on and what attributes it has, it is also good to know what other things we can deduce from the dataset. But then for this simple demo, it is enough to know a couple of stuff, perhaps probably only the most pertinent ones. We would not want to waste too much time, it is important to get some exploratory analysis, but just a bit is fine for this case - since we are only looking into building a simple linear regression model. \nSupposedly that you do not know how many observations are there and how many attributes we are dealing with (i.e. you start fresh with the code). Thus, you need to determine the basic stuff first, such as the dimensions of the dataset - which simply refers to how many observations (denoted by rows) and how many attributes (denoted by columns) are there. We can use the R command, which you may type in the console:\n> dim(grades)\n[1] 636  18\nWhat we can see here is that the grades dataset has 636 observations and 18 attributes. Of course, we already knew that but we just did it for good analysis practice. Trust me, do it habitually. Again, 636 observations or rows means there are 636 students in the professor\'s class and with 18 attributes, the first being simply the ID (which we would not really need in the regression), the next 16 are the predictors for determining a student\'s final grade, and the last column is the response variable, which is the student\'s final grade. Just to give some context, I know I have mentioned some terms such as predictors and response variable but haven\'t really defined (which I should have). Anyways, the former just means that it is a variable that is used to determine the value of the response variable (final_grade in our case) and the other predictors are the rest of the other attributes such as the midterm and final_exam (note that s_id is not a predictor). We only consider it a predictor if it will affect our response variable. As one can think about it, there could be many multiple (but at least 1) predictors but there should only be exactly one response variable (similar to independent and dependent variables respectively).\nData Pre-Processing\nAfter getting to know your data, it is critical that data be pre-processed before analyses can be performed or before any model can be constructed. (Big) data is noisy and the data that we get is not always perfect. Scenarios such as\n\nMissing data or information \nInconsistent formatting \nErroneous data \nData mis-types\n\nare common when working with data, that in fact many data scientists and statisticians spend almost up to 80% of the time on the project just simply to do any data pre-processing tasks (not necessarily 80% but most often than not, longer time than to actually perform their main analysis and tasks). There could be various reasons for why such mishaps happens, such as human error in data input, region discrepancies, data rendering issues, etc. One may solve these issues through data pre-processing techniques such as doing some data correction (eg. filling in missing data with 0s, deleting observations that have missing or incorrect information, taking out outliers, etc.). In detail, I would like to write a separate through in-depth article about data pre-processing and its techniques. But that\'s for a future blog. At least you know the basic picture.\n> df <- grades[!(grades$final_exam==0),]\n> dim(df)\n[1] 602  18\nIt\'s a good thing that the dataset that the professor has provided is free from noise (if not perfect, then almost perfect). Since we have some students who dropped the course in the middle of the term or who have not written the exam, then that means they receive a 0 grade for their exam, and because this skews our data - let us take them out. As a side warning though, if there is a significant number of instances of such case, it is not recommended to drop them since you are losing data; you are losing information and this is a trade-off between low good quality data vs high bad quality data. We try to get the balance of having good quality data and at the same time keeping them as much as we could. The more the merrier. But if we look at our dataset, notice that if we perfom the first line, which basically creating another dataset df and store there all observations from grades whose final_exam is not 0. We again use dim() function to determine how many instances are there in df, which is supposed to hold the students who did not write the exam. Because we have 602 now, and we initially have 636, then only 34 students did not write the exam and we can take it out because that is merely a 5% of the entire student population in the class (i.e. we only consider 95% of the student population which is still good). Another thing to note that just because a student gets a 0 in the exam does not mean that he/she dropped the course or did not write it. It could be that he/she actually wrote the exam and did not really receive a score (i.e. no correct answer). But we shall not worry about this for now; I re-assured that all students that we have in df have written the exam.\nMore Exploring on the Data\nSome professors are keen at curving final grades especially when it does not meet the satisfactory C+, which is usually around 60-65%. Too low doesn\'t look good in them, nor does too high. So this average is important (that\'s why you hear the bell curve a lot!). We would like to know if the final grades here are ""curved"". Apparently if it is, then we would be expecting some errors in our prediction model. Think about it, if there is no curve and that students receive the grade according to the original requirements/weighting, then they could either score too low or too high than the average range. And there would be no error then because the model would be able to pinpoint exactly what the weightings are for each course requirement.\n> pass <- df$final_grade >= .5\n> length(pass[pass == TRUE])\n[1] 499\n> mean(df$final_grade)\n[1] 0.657233\n> median(df$final_grade)\n[1] 0.6783065\nThe first two lines here determines how many students have passed the course. Great, 499, which is around 80% of the course. And around 20% have failed. Note that a pass at our university means a final grade of 50% or above. Looking at the mean and median, we get around 65% and 67% which is somehow close to the average range mentioned earlier, little higher but still looks fine. Two interpretations: either this is the original actual average, OR this is the curved grade (could be up or down). But we will see in our analysis that if there is no error, then it\'s the former case. Else, it\'s the latter. No error in this context means a ""a very significantly little to no"" error. \nNow, let us turn our attention into the current 2D plot of one of the predictors vs the response. Suppose I select midterm. How does that correlate with the final_grade? Well, let us look into the 2D plot.\n\n\n\nYou can type the following R code in your console to display this plot.\n> plot(final_grade~midterm,data=df)\nThis means that we plot an xy-plot of the midterm vs the final_grade. Notice the tilde? To its left is the response and the predictor(s) to its right. The data is set to df. \nSo consider using a different predictor instead of midterm, say final_exam. Then your R code should look like this:\n> plot(final_grade~final_exam,data=df)\nin order to produce this plot:\n\n\n\nIn both cases, we notice the linear pattern the points are following. This suggests that a linear model is a possible candidate of the type of model to use for this particular problem. We will be applying linear regression to find the line of best fit. That is to say, find a line that will best fit all the point in a single straight line, with the least minimal amount of errors possible. We will see that upon finding the line we can add up the squares of the distances between the points and the best-fitting line (called residuals) and the reason it is squared is because we want positive values since we are talking of distances. This is called the Sum of Squared Residuals or SSE. \nBefore we begin with the modelling, I\'d like to show you another neat thing. Type this simple line of code in your RStudio console. What do you get as the result?\n> summary(df)\nFunctions to Use in Our Modelling\nWhen we do the modelling later on, we might need a couple of the functions.\n> pf <- function(model, term_grades) {\n+     y <- model$coefficients[1]\n+     for (i in 1:16) {\n+         y <- y+model$coefficients[i+1]*term_grades[i]\n+     }\n+     return(y)\n+ }\nIf you are not too familiar with functions, basically functions are simply a blocks of code that sequentially gives you some tasks. So for instance, in the above R code, pf is the name of the variable that is of type function. And this particular function we have here takes in two parameters, the model and the term_grades. These could be arbitrary names, but it is more helpful for the person reading your code if the variable names could give insight on what they actually represent. So in our case, model basically descirbes what kind of model to be used and term_grades is going to be a vector which represents the grades of some student. While we need not specify the type of these variables, other programming languages such as Java and C do. Within the method\'s (interchangeable terms with functiton) body, we set y to be equal to model$coefficients[1] (you will see later the context and how this plays out but accept it for now). The next three lines is a for loop, which is a popular control structure in programming languages that allow iterations. Here, we interpret it as i = 1, 2, 3, ..., 16, where the i in each iteration gets replaced within the for block. But basically this is just telling us that we multiply the coefficients of our model with the term grades of the students and add them all up. As a made-up example, if the model\'s coefficients are (6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 10) and the student\'s grades are (100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85), then the model should output whatever is the value of 6 * 100 + 6 * 99 + ... + 6 * 86 + 10 * 85. Click here to learn more about functions. This is in C though, but at least it teaches you the ropes on simple functions. If you wish to learn more about R syntaxes on functions, then you can click here.\n> sqe <- function(actual, pred) {\n+     squared_error <- (actual-pred)^2\n+     return (squared_error)\n+ }\nBy looking at this code, would you be able to guess what the function does and what it outputs? \nThese two functions will be essential in our later work. Type them for now in the RStudio console.\nDataset Partitioning\nIn supervised learning, such as in regression, it is important to split the dataset into what you call the training set and the testing set. The training set is used by the program to build a model, given the predictor(s) and the response. The more that you have in your training set, the better the model can be. The testing set is used by the program to make predictions as to what the response output would be and compare it with actual response, for each observation in the test set. Of course, the more matches there are the higher accuracy of the model is. And  the more incorrect predictions just leads to a higher error rate. There are times where we also have a validation set, but we will keep it simple for now and do it another time. The ratio of the number of training data vs testing data is not really set at stone. Let us use 9:1 ratio. Hence 90% of the data is for training and the remaining 10% for testing. We can either just choose the first 90% of the data to be our training data and the remaining as the testing data, but that can cause a lot of bias. Instead, we apply random sampling.\n> set.seed(1)\n> rnd_sample <- sample(1:dim(df)[1],dim(df)[1]*9/10)\n> model_matrix <- model.matrix(final_grade~., data=df)[,-1]\nWe can input any value for the seed. This just means that if we perform the same analysis again using the same seed, we will get the sample back. Using another value for the seed gives a different set of random sample. The second line just means that out of the total number of students, we take a 90% random sample. In the third line, we create a model matrix using final_grade as the response and the rest of the other variables as the predictors.\n> x_train <- model_matrix[rnd_sample,]\n> x_test <- model_matrix[-rnd_sample,]\n> y_train <- df$final_grade[rnd_sample]\n> y_test <- df$final_grade[-rnd_sample]\nWe then create our training and testing sets. The variable names should be self-explanatory of what they do. It also provides the syntax for getting the training/testing data from the datasets. That should be easy enough to work out without further detailed explanation.\nThe Linear Model\nThe finally, the moment of truth that we can make our linear model! We simply type this R code in:\n> lin_model <- lm(final_grade~., data=df)\n> summary(lin_model)\nNotice that lm() is the function used in R to create a linear model. And then the summary() outputs a summary of what the model is. You should have something like this:\n\n\n\nThe second column of the table in the above shows the different coefficients/weights of each grade component. Notice that each test seems to be weighted almost evenly, so do the labs and the quizzes. It may also be possible some grade component to weigh more depending on the professor\'s (example: test 2 has a heavier weight than test 1 because it is comprehensive). But in our case, everything seems to be fairly consistent. If you think about it, it\'s possible that there are errors after all (see our remarks in the above). And all those errors are carried over to the intercepts. But how do we actually calculate errors in models?\nThe Accuracy of the Model\nThe error rate is one way of measuring a model\'s accuracy. In linear regression, we use the statistic SSE (as mentioned in the above) in order to determine the error of the model. Let us try it in R.\n> pred_test <- predict(lin_model, newdata=data.frame(x_test), type=""response"")\n> compare_frame <- data.frame(y_test,pred_test)\n> compare_frame$sqe <- sqe(compare_frame$y_test, compare_frame$pred_test)\n> sse <- sum(compare_frame$sqe)\n> sse\n[1] 0.01772238\nThe predict() function in R is used to predict a particular value of an observation given its predictors and the model used. We then create another data frame and we would see side by side the predicted and actual values of each observation. We then use the sqe() function that we defined earlier to find the SSE. And see that our SSE happens to be 0.01772238, which we see is good because of the low error.\nGrade Prediction\nLet\'s use our model to predict a student\'s grade in the class given his/her grades. Note that the student is imaginary and isn\'t really part of the class.\n> lin_model$coefficients\n (Intercept)        lab_1        lab_2        lab_3        lab_4        lab_5        lab_6        lab_7        lab_8        lab_9 \n0.0416333113 0.0076334686 0.0214687450 0.0186076030 0.0203180532 0.0172562780 0.0184116061 0.0124774185 0.0194041968 0.0079677938 \n      quiz_1       quiz_2       quiz_3      midterm       test_1       test_2   final_exam \n0.0017523498 0.0007565712 0.0064761939 0.1897859505 0.1719335054 0.1756828784 0.2905830227 \n> my_grades = c(0.917, 0.933, 1.00, 0.941, 0.974, 0.873, 0.984, 0.94, 0.699, 1.00, 1.00, 0.786, 0.875, 0.667, 0.82, 0.708)\n> pf(lin_model, my_grades) \n[1] 0.8134929 \nIf those were the student\'s grades, then he/she gets an 81 in the course as the final grade, which is around an A. \nFinal Remarks\nI hope that you enjoyed going through and reading over my tutorials. And I do hope you learned something today on your read. I am still planning on the Machine Learning/Data Science series tutorials/articles. I plan on releasing at least 1 or 2 on a monthly basis, due to my heavy and busy schedule. My next one (still unplanned) but it should be released sometime in March. Stay tuned! \nOn other note, check out some of my other links:\n\nMy Website - http://galix.me\nTech Blogz: Tips, Tricks and Tutorials on anything Tech-y - http://galix.me/tech-blogz/tech.html \nLiving the G Life: I Now Have A Personal Blog, new updates every 2 weeks or so - http://galix.me/pb/pb.html\nMy CV by the way (just flexin\'): http://galix.me/CV_Updated_January2020.pdf\n\nDisclaimer\nAs a short disclaimer, it may be possible that I have made some mistakes as well. So I shall not be liable for anything and you may use this for reference only and at your own risk! Contact me through email at gian.alix@gmail.com for any corrections found.\n'], 'url_profile': 'https://github.com/techGIAN', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': ['\n\n\nVersion\nStatus\nCoverage\nDownloads\n\n\n\n\n\n\n\n\n\n\n\nR-package: mmb \nThis is the repository of the R-package mmb for arbitrary dependency mixed multivariate bayesian models for inferencing, regression and neighborhood search using joint probabilities and Kernel Density Estimation.\nInstall using CRAN:\ninstall.packages(""mmb"")\n\nInstallation of latest release\nThe master-branch always contains a release-version. Sometimes there may be a short delay between the master-version and what is on CRAN. In this case, you can still install the latest release using the package devtools (mind the subdir):\nR> devtools::install_github(""https://github.com/MrShoenel/R-mmb"", subdir = ""pkg/mmb"", ref = ""master"")\n\nDocumentation and function reference\nThe latest documentation can be found at https://mrshoenel.github.io/R-mmb/.\nCiting\nPlease use the following BibTeX to cite the package mmb:\n@article{honel2020rmmb,\n\ttitle={mmb: Arbitrary Dependency Mixed Multivariate Bayesian Models},\n\tDOI={10.5281/zenodo.4046002},\n\turl={https://doi.org/10.5281/zenodo.4046002},\n\tnote={Install this package from CRAN: install.packages(""mmb"")},\n\tpublisher={Zenodo},\n\tauthor={Sebastian Hönel},\n\tyear={2020},\n\tmonth={Sep}\n}\n\nBuilding the package\nThat\'s easy! Just run:\n> Rscript build.R all\n\nThis builds everything, generates manuals (PDF and HTML) and packages the archive.\n'], 'url_profile': 'https://github.com/MrShoenel', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'Lodz/Poland', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['InfectedEstimation\n'], 'url_profile': 'https://github.com/MichaelWojda', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'Athens Greece', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xkapotis', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'Greece', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Project description and motivation\nHallo and welcome to the Seattle Airbnb project. This project is part of the ""Data Scientist nanodegree"" by Udacity.\nThe purpose of the project is to analyze a real world dataset and communicate findings in an understandable manner.\nThe dataset that was chosen is the Seattle Airbnb homes that contain data from Airbnb listings in the Seattle area.\nThe main goal of the project is to answer the three following questions:\n\nWhat are the characteristics of a home that influence its price most and what is the importance of each charcteristic?\nWhat is the average price of a home based on the amount of people it can accommodate?\nWhat are the Top10 neighborhoods in regards with the mean price a home can have?\n\nLibraries required to run the code\n•\tNumpy\n•\tPandas\n•\tMatplotlib\n•\tSeaborn\n•\tSklearn\nFiles included\n•\tREADME document\n•\tairbnb.ipynb:  Jupyter notebook where the code used to produce the results is written.\n•\tlistings.csv:  CSV file that contains Airbnb home data for Seattle – provided by Udacity.\nSummary of results\nIn this short analysis of the dataset, a ML linear regression model has been built to predict home prices based on other features a potential home can have. Due to the very large amount of independent variables,  I used the ‘ExtraTreesClassifier’ to select the most important features when it comes to predicting the price. Then, I insert those features in a linear regression model and derive the coefficients. This analysis showed that that what affects the price most, is the number of people that a listing can accommodate.  This has a strong positive correlation with the price. Similar trend was observed also with the “review_scores_location” feature, meaning that the location of the listing plays a key role to the price of the listing.\nIt is also notable, that the “review_scores_value”  has a negative correlation with price. Although at first this can sound strange, it reveals that people tend to give high scores in listings that have a low price. These are the most important features that influence the price.\nThe analysis also illustrates the price level a home can reach based on the approximate amount of people it can accommodate.  Starting from 0-4 persons, the average price is a little bit above 100$ and it can go up to 200$ and 375$ for accommodating 4-8 and 8-16 persons respectively.\nFinally, the Top10 of the high price rated neighborhoods of Seattle  is illustrated. These include Fairmount Park with an average price of 375$ followed by Industrial District and Portage Bay with approximately 240$. The other neighborhoods show quite little difference in average home price.\nThat was a very short analysis of Seattle based Airbnb listings. To gain better knowledge and make safer conclusions its essential to dive deeper into the data and build more sophisticated models that would show a higher evaluation metric.\nAcknowledgment\nI would hereby like to thank the Udacity team not only for providing the data necessary to finalize this project, but also for all the support and lessons in the past and in the future.\n'], 'url_profile': 'https://github.com/FotisKyr', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'Hanoi - Rochester', 'stats_list': [], 'contributions': '807 contributions\n        in the last year', 'description': ['Hand-X-Rays-Rotation-Prediction\nIn this project Resnet-18 is used as a regression model to predict the degree of rotations of hand x-rays images.\nFor more details about the dataset used, access the following link: https://drive.google.com/open?id=1IKYXWO5gkulN9ZWuJ1sSAeoNl-R1_kT7\n'], 'url_profile': 'https://github.com/thuphuong1401', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '170 contributions\n        in the last year', 'description': ['Machine_Learning\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables)\n'], 'url_profile': 'https://github.com/KarinkiManikanta', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VaishnavBipin', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}","{'location': 'Portland, Oregon', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['671-Statistical-Learning\nBayesian theory of classification, the bias/variance trade-off, linear and quadratic discriminant analysis, Bayesian logistic regression, neural networks, Gaussian processes and structured learning.\n'], 'url_profile': 'https://github.com/aaadriano', 'info_list': ['Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Feb 17, 2020', 'HTML', 'GPL-3.0 license', 'Updated Nov 13, 2020', 'Java', 'Updated Feb 15, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2021', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'R', 'Updated Feb 18, 2020']}"
"{'location': 'New York', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['BreastCancerDetector\n-- an API that predicts malignancy based on features of biopsied breast cells.\nIntroduction\nDescriptions of the Data Challenge \nYou belong to the data team at a local research hospital. You\'ve been tasked with developing a means to help doctors diagnose breast cancer. You\'ve been given data about biopsied breast cells; where it is benign (not harmful) or malignant (cancerous).\n\nWhat features of a cell are the largest drivers of malignancy? Build a model that predicts whether a given biopsied breast cell is benign or malignant.\nWhat features drive your false positive rate for your model you derived above, what features drive your false negative rate?\nHow would a physician use your product?\nThere is a non-zero cost in time and money to collect each feature about a given cell. How would you go about determining the most cost-effective method of detecting malignancy?\n\nThe final product:\nBreastCancerDetector is an API that will return the prediction based on the input cell parameters.\nExample input:\ncurl -d \'{ ""ID"": 752904, ""Clump Thickness"": 10, ""Uniformity of Cell Size"": 1, ""Uniformity of Cell Shape"": 1, ""Marginal Adhesion"": 1, ""Single Epithelial Cell Size"": 2, ""Bare Nuclei"": 10, ""Bland Chromatin"": 5, ""Normal Nucleoli"": 4, ""Mitoses"": 1 }\' http://127.0.0.1:5000\n\nExample output:\n{ ""Prediction"": ""Benign"", ""Confidence"": 0.98 }\n\nThe predictions are saved in a timestamped json file under the folder ""Output"".\nExample file name: ""predictions_03182020_1616.json"". Timestamp: MonthDayYear_HourMinute\nInstructions to run the API\n\nclone the repo to local machine\ncd to the folder\nIn the terminal, run: python api.py\n\ncan define the port by: python api.py -p xxxx (default is 5000)\n\n\nIn a separate terminal, test with sample input as described above\n4 out of the 10 values are required:\n\n""ID""\n""Bland Chromatin""\n""Bare Nuclei""\neither ""Uniformity of Cell Size"" or ""Uniformity of Cell Shape""\n\n\n\nInstructions to run unit testing\n\ncd to the folder\nIn the terminal, run: python tests.py OR python -m unittest\n\nThe Data\n9 features (all ranged 1-10) + Class (4 for benign, 2 for malignant)\nFeatures: Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses\nThe Model\nTrain a Logistic Regression model based on 8 out of the 9 parameters. \nAs Uniformity of Cell Size & Uniformity of Cell Shape are highly correlated (as shown below), the model is trained based on their averaged values. In the API, the model only requires one of the two parameters to be present.\n\n\n\nBy analyzing parameter importance, 3 parameters capture most of the predictive power of the model:\n\nBland Chromatin\nUniformity (the average between Uniformity of Cell Size & Uniformity of Cell Shape)\nBare Nuclei\n\n'], 'url_profile': 'https://github.com/huayicodes', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'Edison, New Jersey', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Top-50-Spotify-songs-2019-Analysis\nPerformed EDA and Linear Regression on the data set from Kaggle for top 50 Spotify Songs in year 2019 using Python\n'], 'url_profile': 'https://github.com/Rutujacw', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/louislau66', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'Piscataway, NJ', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Blood Pressure\n'], 'url_profile': 'https://github.com/peter-ehmann', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tarang0305', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shrutig5g', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Python', 'Updated Feb 25, 2020', 'R', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'Boston, MA, US', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/drshah96', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '879 contributions\n        in the last year', 'description': [""This project was bootstrapped with Create React App. As its data endpoint, it's running Strapi CMS with a Graphql interface (we're running a contanerised version of Strapi on an EC2 instance on AWS). UI-wise, the app is relying on the components from Material UI and uses Nivo to render the chart.\nThe app is currently deployed to Heroku, but since the EC2 instance is running on HTTP and Heroku is running on HTTPS, your browser will most likely block the mixed content :)\nHow to run this thing locally, then\nTo start in dev mode, run\nnpm run start\nTo start the production build, run\nnpm run build\nnpx serve -s build\nKnown bugs\n\nWhen clicking on measurements in the table to update them, it is possible to change the date to that of another already existing measurement and thus run into an error when trying to save (updating the measurement with its orignal date or a new date that doesn't clash works as expected, however).\nWhen the measurements' table content overflows, the header srolls with the rest of the table instead of staying put (oops).\n\n""], 'url_profile': 'https://github.com/bapjiws', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['AI\nLearning AI ,it contains ml codes like Regressing classification Kmean etc\n'], 'url_profile': 'https://github.com/Rahuly-adav', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KatharinaGruber', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['AirQulaityIndex_XGboost\nPrediction of air quality index using Xgboost regressor with hyper parameter tuning using random grid.\n'], 'url_profile': 'https://github.com/chaudhary-vivek', 'info_list': ['Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Feb 17, 2020', 'JavaScript', 'Updated Feb 21, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['AirQualityIndex_Randomforest\nPrediction of air quality index using random forest regressor with hyper parameter tuning using random grid.\n'], 'url_profile': 'https://github.com/chaudhary-vivek', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mayne941', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vaishnavi0002', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': [""Data Programming Project - Turkey Political Opinions Dataset\nAn exploratory and a binary classification project utilizing logistic regression. This project was conducted to practice basics of a data science project (e.g. feature selection, binary classification, exploratory analysis).\nIn scope of Data Programming course assignment, a dataset is acquired through Kaggle. After examination of survey data named “Turkey Political Opinions”, it is concluded that the dataset is suitable for predicting participant’s political party preference with respect to their given answers to number of political orientation questions. Hence, scope of the project will be prediction of the political view (conservative/left-wing) feature through Logistic Regression.\nLink for the dataset and other related projects: https://www.kaggle.com/yemregundogmus/turkey-political-opinions\nSurvey questions consist of demographic questions such as gender, age, region, qualification as well as 10 polar opinion questions and participant’s choice of political party. Translation of survey questions are as follows:\n\nPlease specify your gender.\nPlease state your age. (5 categories)\nPlease state the region you are living in.\nPlease state your qualification.\nQ1: Do you think Turkey's economy is strong?\nQ2: Do you think Turkey needs reforms in education?\nQ3: Are you oppose to privatization?\nQ4: Should death penalty be allowed for certain crimes?\nQ5: Do you believe Turkey's journalists are neutral enough?\nQ6: Do you support the law that bans sale of alcohol after 22:00?\nQ7: Do you prefer living in a secular state?\nQ8: Do you support the abortion ban?\nQ9: Do you think state of emergency restricts freedoms in Turkey?\nQ10: Would you like to see a new party in the parliament?\nPolitical View: Which party do you support?\n\n""], 'url_profile': 'https://github.com/gozdeorhan', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""Predicting-the-salaries-for-Police-Department-of-Baltimore-City-government\nBackground\nIn this work, we conduct linear regression analysis in Excel with Baltimore City government employee salaries open data to help the Police Department make better decisions about future budget.\nWe’re going to see if we can create a model to help us determine an employee’s salary based on the number of years that they’ve worked in a specific department in Baltimore City government.\nData Analytics Outline\n1. Industry Questions\nWhether we can make predictions about the salaries for a specific agency or type of job within city government using available dataset? What kind of additional information can be explored based on existing data? How can we build explainable and applicable models for city government to design a future budget plan?\n2. Data Questions\nIn this study, we work with Baltimore City government open salary data to perform linear regression analysis. How can we evaluate the effectiveness and robustness of our models? What are the metrics that we care about for interpreting the result of our prediction models? More specifically, how to interpret the results of linear regression statistics in order to get some insights for Baltimore City government current operations?\nThe original data were exported from the Baltimore City Open Data portal as CSV documents by fiscal years, and the processed version Excel document in this repository is aggregated by department in Baltimore City government.\n3. Data Analysis Tools\nA linear regression analysis can facilitate us make predictions about variables based on existing data and further precisely describe specific trends and potential relationships between phenomena.\nTo evaluate the quality of our regression models, below metrics are developed by statisticians, which enable us to quantify model effectiveness.\n\n\nRegression Coefficient\nIn linear regression, coefficients are the values that multiply the predictor values. The sign of each coefficient indicates the direction of the relationship between a independent variable and the dependent variable.\n\n\nR-Squared value\nR-Squared value shows how well our selected independent variable(s) explain the variability in your dependent variable(s). This metric is always between [0, 1], snd higher R-squared value generally indicates a better model.\n\n\nP-value\nThe p-value for each independent variable tests the null hypothesis that the coefficient is equal to zero (no effect). Simply put, a predictor that has a low p-value is likely to be a meaningful addition to the model.\n\n\n4. Data Answers\nIntuitively, length of employment in Police Department of Baltimore City government might be a reliable indicator for employees' annual salary. To verify this idea, simple Linear regression model is conducted, and the chart below makes this relationship convincingly. This model fit our data with an R2 value of 0.761 and a standard error of residual of $5,757.\n\nIn addition to the annual salary as determined on their contracts, we are also interested in what the Baltimore Police Department actually pays their employees and how they might want to think through budgeting for salaries in the department. However, if we perform the same analysis with the GROSS column data (the actual payout for the employee’s salary from the department), we see a very different output. This model fit our data with an R2 value of only 0.007 and a untolerable large standard error of residual.\n\nIt turns out that the years work for Police Department is not a good predictor of Gross salary, and that there might be other independent variables that actually determine how much an employee gets paid in a given fiscal year.\nHow can we better determine a model for predicting the employee’s gross salary? More specifically, the overpaid percent of salary (overpaid_PCT = (gross - annual_rt / annual_rt)*100%) is introduced as a new independent variable, and together with employment length to help predict the Gross salary.\nNext, we model a multiple linear regression equation to explore other factors related to Gross salary. Campared the R squared value of the result with the previous simple linear regression analysis, it is clear that our new model is more robust. Now, the R2 value is 0.893 and the standard error of residual of approximately $11,170.\n\n5. Industry Answers\nWith simple linear regression model, we notice that the employment length performs relatively well in the prediction of annual salary of police officers in Baltimore City government. However, this predictor doesn’t give us much information about the trend of gross salary.\nTo solve this problem, we introduce aother independent variable and perform multiple linear regression analysis and finally derive a robust model for gross salary prediction. In this condition, our least squares line accounts for approximately 90% of the data.\nTherefore, we recognize that there might be other independent variables that actually determine the final salary payment from the City of Baltimore. To better interpret the relationship and help future budget in specific department, it is necessary to exlore and create additional data points such as their department subcategory or certain types of jobs.\nData Manipulation Instructions\nWant to learn more details about data manipulation in Excel to clean complex dataset and conduct linear regression analysis? Click here for a simple step-by-step instructions!\nData Sources\n\nBaltimore City open salary data\nAggregated Baltimore City salary data for fiscal years 2011-2019\nIntegrated data of the Baltimore Police Department employees\nDescription of Baltimore City governmeny salary data\n\n""], 'url_profile': 'https://github.com/YilunCai627', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['AIN4002 Assignment 1 2019\nYour task for this assignment is to train a model to perform linear regression, using Python. The purpose of this model is to predict the Percentage achieved in a subject when a student spends a specific amount of minutes studying. To complete this task, you need to do the following:\n\nConsult the DataSetStudentNumber.pdf file to determine which Data Set and set of Extra Instructions you need to use to complete your assignment.\nDownload the DataSets.zip file and find the file number associated with your student number.\nDownload the Extras.zip file and find the file number associated with your student number.\nUse the entire dataset to train the linear regression model. Name your Python file as your student number, 11111111.py.\nUse the instructions from the Extra instructions file to determine what other outputs are required for your assignment.\nDownload the Submission Template.csv file and fill in the required values. Ensure that the Submission Template file is delimited by “;”. Name your Submission Template file as your student number before submission, e.g. 11111111.csv. Compare your .csv file to the Example Submission.csv file to make sure your submission is in the correct format.\nSubmit both your .py and .csv files on the Moodle site in the (2 separate) upload slots provided.\nYour Python script must clearly show how all the values in the submission .csv file have been calculated. Use comments to indicate where important values are calculated.\n\n'], 'url_profile': 'https://github.com/neeravpanchal26', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cohej640', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Spam-or-Ham\nIn scope of NLP, this project tackles SMS spam detection problem utilizing various ML algorithms such as Logistic Regression, Multinomial Naïve Bayes, K-Nearest Neighbour, Decision Tree, Random Forest, AdaBoost, Gradient Boosting.\nThe dataset contains 4,825 legitimate (ham) messages and 747 spam messages in English with a total number of 5,572 short messages. This indicates that the major percentage of the data with the percentage of 86.6 are labelled as ham where only 13.4% of them labelled as spam.\nThe dataset was found in a .csv format where each row corresponds to a single message and is composed of two columns: label (v1) and the raw text (v2).\nLink to dataset: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n'], 'url_profile': 'https://github.com/gozdeorhan', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Car-Price-Prediction-Project\n'], 'url_profile': 'https://github.com/rishabhm76', 'info_list': ['Python', 'Updated Feb 11, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MIT license', 'Updated Feb 15, 2020', 'Python', 'Updated Feb 10, 2020', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Predict-GPA-of-students-by-using-exam-score-and-attendance-\nTrained an Ordinary Least Square (OLS) Regression model to predict GPA of a student using their exam score and attendance as input features. It was further modified to include GPA achieved in previous semesters to improve accuracy.\n'], 'url_profile': 'https://github.com/smitalk', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Advanced Prediction of Multistage continuous flow manufacturing process\nCreated data regression models to predict 15 unknown variables within  4% error at any plant condition.  Denoised 11000 x 115 dataset, leading to 70% improvement in prediction accuracy. Used data engineering to produce features that were relevant to the target variables. Produced confusion matrices to determine relevant features for machine learning. Final prediction horizon of four times dataset timescale to within 5% error\n'], 'url_profile': 'https://github.com/hjeffreywang', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Morocco', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Raihannajib', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pyt243', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '253 contributions\n        in the last year', 'description': ['This repo contains various Machine learning, Deep Learning basics and mini projects:)\n'], 'url_profile': 'https://github.com/keshav-b', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Yunnan University', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['best-fit\nScript for a function that fits polynomial and segmented regressions and chooses the best one automatically using an Akaike or Bayesian information criterion. Please feel free to suggest any changes that you think might make this more useful to people!\n'], 'url_profile': 'https://github.com/laofei177', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sathishvp7', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Sign_to_Speech_10701_Project\nClassify sign language using compilation of 2-D video/images based on the natural statistics of relative digit-hand movements corresponding to words in ASL, and to create a regression mapping to the word bank provided for the purpose of translation.\n'], 'url_profile': 'https://github.com/GaryWilkins', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': ['Quality-Prediction-Iron-Ore-floatation\nQuality Prediction in Iron Ore Mining using the concepts of Statistical Analysis in R programming language to predict the percentage of impurity (silica) in iron ore concentrate by building a regression model and to test the process capability of the input feed in the plant using concepts of Six Sigma\n'], 'url_profile': 'https://github.com/Charan619', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Kolkata, West Bengal, India', 'stats_list': [], 'contributions': '707 contributions\n        in the last year', 'description': ['ML-Web-App-Iris-Species-Predictor\nA simple Flask application that uses Machine Learning to predict\nthe species of Iris flower based on user choices of different features,\nlike sepal length, sepal width, petal length, petal width.\nAlso allows the user to choose between Logistic Regression and K Nearest Neighbors Classifier\n\n\n\n'], 'url_profile': 'https://github.com/BALaka-18', 'info_list': ['Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Jupyter Notebook', 'Updated Jun 2, 2020', '2', 'R', 'Updated Feb 20, 2020', '1', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 25, 2020', 'Python', 'Updated Aug 26, 2019', 'Python', 'Updated Feb 13, 2020', 'GPL-3.0 license', 'Updated Feb 12, 2020', 'R', 'Updated Feb 14, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '250 contributions\n        in the last year', 'description': ['Store_Item_Demand_Forecasting\nThe goal is to predict sales for 10 stores with 50 items each. In a first step statistical models ARIMA and SARIMA are used. Second, a LSTM is used. The statistical models are easier to set up and apply, while also delivering better results in this case. Better scores with the LSTM could be achieved with further hyper parameter tuning.\nData: Store Item Demand Forecasting\nFiles\n01_Data_Exploration\nThe data is explored. We see how many samples we have, how clean the data is etc. Next, we search for patterns in the data which could be helpful for the prediction as well as which influences our model choice for it.\n02_Statistical_Models\nAfter identifying relevant patterns, auto regressive models are used to predict the future sales (demand).\n03_LSTM\nPredictions with a LSTM. This is the final model. Various network topologies and hyper parameters have been tried to come up with this configuration.\n'], 'url_profile': 'https://github.com/daved01', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10,154 contributions\n        in the last year', 'description': ['\n\n\n\n\n\nAims |\nPanel Topics |\nCourse Schedule |\nInternal Links |\nExternal Links |\nWelcome to Machine Learning in the Molecular Sciences\n\n\nAims\nThe NYU-ECNU Center for Computational Chemistry at New York University Shanghai (a.k.a, NYU Shanghai) announced a summer school dedicated to machine learning and its applications in the molecular sciences to be held June, 2017 at the NYU Shanghai Pudong Campus. Using a combination of technical lectures and hands-on exercises, the school aimed to instruct attendees in both the fundamentals of modern machine learning techniques and to demonstrate how these approaches can be applied to solve complex computational problems in chemistry, biology, and materials science. In order to promote the idea of free to code, this project is built to help you understand some basic machine learning models mentioned below.\nPanel-Topics\nFundamental topics to be covered include basic machine learning models such as kernel methods and neural networks optimization schemes, parameter learning and delta learning paradigms, clustering, and decision trees.  Application areas will feature machine learning models for representing and predicting properties of individual molecules and condensed phases, learning algorithms for bypassing explicit quantum chemical and statistical mechanical calculations, and techniques applicable to biomolecular structure prediction, bioinformatics, protein-ligand binding, materials and molecular design and various others.\nCourse-Schedule\n\n\nMonday, June 12\n8:45 - 9:00: Welcome and Introduction\n9:00 - 10:00: Introduction to Machine Learning (presented by Matthias Rupp)\n10:00 - 10:20: Coffee Break\n10:20 - 11:20: Kernel-based Regression (presented by Matthias Rupp)\n11:20 - 12:30: Dimensional Reduction, Feature Selection, and Clustering techniques (presented by Alex Rodriguez)\n12:30 - 14:00: Lunch Break\n14:00 - 15:00: Introduction to Neural Networks (presented by Mark Tuckerman)\n15:00 - 15:30: Coffee Break\n15:30 - 17:30: Practical Session: Clustering with Feature Selection and Validation (presented by Alex Rodriguez)\n\n\nTuesday, June 13\n9:00 - 10:00: Random Forests(presented by Yingkai Zhang)\n10:00 - 10:30: Coffee Break\n10:30 - 11:30: Learning Curves, Representations, and Training Sets I (presented by Anatole von Lilienfeld)\n11:30 - 12:30: Learning Curves, Representations, and Training Sets II (presented by Anatole von Lilienfeld)\n12:30 - 14:00: Lunch Break\n14:00 - 15:00: Review of Electronic Structure, Atomic, Molecular, and Crystal Representations (presented by Mark Tuckerman)\n15:00 - 15:30: Coffee Break\n15:30 - 17:30: Practical Session: Learning Curves (presented by Anatole von Lilienfeld)\n\n\nWednesday, June 14\n9:00 - 10:00: Predictiong Properties of Molecules and Materials (presented by Matthias Rupp)\n10:00 - 10:30: Coffee Break\n10:30 - 11:30: Parameter Learning and Delta Learning (presented by Anatole von Lilienfeld)\n11:30 - 12:30: Learning Electronic Densities (presented by Mark Tuckerman),ML Models of Crystal Properties (presented by Anatole von Lilienfeld)\n12:30 - 14:00: Lunch Break\n14:00 - 15:30: Practical Session: Machine Learning and Property Prediction I  (presented by Matthias Rupp)\n15:30 - 16:00: Coffee Break\n16:00 - 17:30: Practical Session: Machine Learning and Property Prediction II (presented by Matthias Rupp)\n\n\nThursday, June 15\n9:00 - 10:00: Machine Learning of Potential Enenery Surfaces (Ming Chen, California Institute Technology)\n10:00 - 10:30: Coffee Break\n10:30 - 11:30: Machine Learning Based Enhanced Sampling (Ming Chen)\n11:30 - 12:30: Machine Learning of Free Energy Surfaces (presented by Mark Tuckerman)\n12:30 - 14:00: Lunch Break\n14:00 - 15:00: Cluster-based Analysis of Molecular Simulations (presented by Alex Rodriguez)\n15:00 - 15:30: Coffee Break\n15:30 - 17:30: Practical Session: Neural Network Learning of Free Energy Surface (presented by Mark Tuckerman)\n\n\nFriday, June 16\n9:00 - 10:00: Development of Protein-ligand Scoring Functions (presented by Yingkai Zhang)\n10:00 - 10:30: Coffee Break\n10:30 - 11:30: Machine Learning in Structural Biology I (presented by Yang Zhang)\n11:30 - 12:30：Machine Learning in Structural Biology II (presented by Yang Zhang)\n12:30 - 14:00: Lunch Break\n14:00 - 15:30: Practical Session: Random Forests and Scoring Functions (presented by Yingkai Zhang)\n15:30 - 16:00: Coffee Break\n16:00 - 17:30: Practical Session: Machine Learning for Structural Bioinformatics (presented by Yang Zhang)\n\n\nCodes\n\n\nTuesday-June-13\nFor Practical Session: Learning Curves, please run these commands on Jupyterlab via huawei cloud:\n!pip install qml\n!git clone https://github.com/qmlcode/tutorial.git\nls\ncd tutorial\nls\n%load exercise_2_1.py\n%run exercise_2_1.py\n%load exercise_2_2.py\n%run exercise_2_2.py\n%load exercise_2_3.py\n%run exercise_2_3.py\n%load exercise_2_4.py\n%run exercise_2_4.py\n\n\nWednesday-June-14\nFor Practical Session: Machine Learning and Property Prediction, please run these commands on Wolfram Cloud:\n(*Please adjust the following path to where you unpacked the reference implementation code from the supplementary material.*)\n\nAppendTo[$Path,FileNameJoin[{""Path"",""to"",""library""}]]; (* Parent directory containing QMMLPack directory *)\n\n\n\nThursday-June-15\nFor Practical Session: Machine Learning of Free Energy Surfaces, please run these commands on Linux system (In order to compile the code, a C++ compiler and the mkl library will be needed):\n1. Unpack the tar file:\n                                  tar -xzvf Neural_network_practical_software.tar.gz\n\n2. Change Command-Line-Interface to the directory created by unpacking and compile the source code. At the beginning, edit \'Makefile\' and change the C and C++ compliers to\nthe corresponding ones you have available on your sytem, e.g., \'gcc\' and \'g++\' or \'icc\' if necessary. The complie the code by typing\n                                                       make\n\n3. Create a training data set from the full dataset. One of two commands is avaiable for use:\n                                   head -n ala-dip-data_all.txt > ala-dip-data.txt\n                                   tail -nl ala-dip-data_all.txt > ala-dip-data.txt\n\nHere n is the number of training points you wish to extract from the full dataset.\n4.  Edit the 2nd, 3rd, 4th, and 5th lines in the file ""INPUT.txt"" if you want to change the calculation type, number of conjugate gradient steps, checkpointing frequency of\nweights, and number of conjugate gradient line-minmization steps. As to the calculation type, \'1\' indicates caculating neural network parameters starting from scratch, and \'-1\' calculating neural network parameters starting from an old set contained in file ""weight.txt"", and \'0\' means validation calculation of the neural network.\n\n\nDeployment\nMachine Learning in Molecular Sciences\nInternal-Links\n\n\nAnnual Conference on Neural Information Processing Systems (NIPS)\n\n\nInternational Conference on Machine Learning (ICML)\n\n\nConference on Learning Theory (COLT)\n\n\nExternal-Links\nOne of the exciting aspects of Machine-Learning (ML) techniques is their possible to democratize molecular and materials modelling with\nrelatively economical computational calculations and low level entry for common folks. (Pople\'s Gassian software makes quantum chemistry\ncalculations really approachable).\nThe success of machine-learning technology relies on three contributing factors: open data, open software and open education.\nOpen data:\nPublicly accessible structure and property databases for molecules and solid materials.\nComputed structures and properties:\nAFLOWLIB  (Structure and property repository from high-throughput ab initio calculations of inorganic materials)\nComputational Materials Repository (Infrastructure to enable collection, storage, retrieval and analysis of data from electronic-structure codes)\nGDB (Databases of hypothetical small organic molecules)\nHarvard Clean Energy Project (Computed properties of candidate organic solar absorber materials)\nMaterials Project (Computed properties of known and hypothetical materials carried out using a standard calculation scheme)\nNOMAD (Input and output files from calculations using a wide variety of electronicstructure codes)\nOpen Quantum Materials Database (Computed properties of mostly hypothetical structures carried out using a standard calculation scheme)\nNREL Materials Database (Computed properties of materials for renewable-energy applications)\nTEDesignLab (Experimental and computed properties to aid the design of new thermoelectric materials)\nZINC (Commercially available organic molecules in 2D and 3D formats)\nExperimental structures and properties:\nChEMBL (Bioactive molecules with drug-like properties)\nChemSpider (Royal Society of Chemistry’s structure database, featuring calculated and experimental properties from a range of sources)\nCitrination (Computed and experimental properties of materials)\nCrystallography Open Database (Structures of organic, inorganic, metal–organic compounds and minerals )\nCSD (Repository for small-molecule organic and metal–organic crystal structures)\nICSD (Inorganic Crystal Structure Database)\nMatNavi (Multiple databases targeting properties such as superconductivity and thermal conductance)\nMatWeb (Datasheets for various engineering materials, including thermoplastics, semiconductors and fibres)\nNIST Chemistry WebBook (High-accuracy gas-phase thermochemistry and spectroscopic data)\nNIST Materials Data Repository (Repository to upload materials data associated with specifc publications)\nPubChem (Biological activities of small molecules)\nOpen Software:\nPublicly accessible learning resources and tools related to machine learning\nGeneral-purpose machine-learning frameworks:\nCaret (Package for machine learning in R)\nDeeplearning4j (Distributed deep learning for Java)\nH2O.ai (Machine-learning platform written in Java that can be imported as a Python or R library)\nKeras (High-level neural-network API written in Python)\nMlpack (Scalable machine-learning library written in C++)\nScikit-learn (Machine-learning and data-mining member of the scikit family of toolboxes built around the SciPy Python library)\nWeka (Collection of machine-learning algorithms and tasks written in Java)\nMachine-learning tools for molecules and materials:\nAmp (Package to facilitate machine learning for atomistic calculations)\nANI (Neural-network potentials for organic molecules with Python interface)\nCOMBO (Python library with emphasis on scalability and eficiency)\nDeepChem (Python library for deep learning of chemical systems)\nGAP (Gaussian approximation potentials)\nMatMiner (Python library for assisting machine learning in materials science)\nNOMAD (Collection of tools to explore correlations in materials datasets)\nPROPhet (Code to integrate machine-learning techniques with quantum-chemistry approaches)\nTensorMol (Neural-network chemistry package)\nOpen education:\n\n\nfast.ai is a course that is “making neural nets uncool again” by making them accessible to a wider community of researchers.\nOne of the advantages of this course is that users start to build working machine-learning models almost immediately. However, it is not for absolute\nbeginners, requiring a working knowledge of computer programming and high-school-level mathematics.\n\n\nDataCamp ofers an excellent introduction to coding for data-driven science and covers many practical analysis tools\nrelevant to chemical datasets. This course features interactive environments for developing and testing code and is suitable for non-coders because it\nteaches Python at the same time as machine learning.\n\n\nAcademic MOOCs are useful courses for those wishing to get more involved with the theory and principles of artifcial intelligence and machine learning,\nas well as the practice. The Stanford MOOC is popular, with excellent alternatives available from\nsources such as edx (see, for example, ‘Learning from data (introductory machine learning)’) and udemy\n(search for ‘Machine learning A–Z’). The underlying mathematics is the topic of a course from Imperial College London coursera.\n\n\nMany machine-learning professionals run informative blogs and podcasts that deal with specifc aspects of machine-learning practice. These are useful\nresources for general interest as well as for broadening and deepening knowledge. There are too many to provide an exhaustive list here, but we recommend\nmachinelearningmastery and lineardigression as a starting point.\n\n\nAbout |\nCommittee |\nSpeakers |\nSchedule |\nLocation |\nSponsor |\n'], 'url_profile': 'https://github.com/nickcafferry', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Population-Prediction\nPopulation prediction program made in matlab that uses linear regression to predict population in future and it can interpolate population for years that population is not known. It takes data from Excel file and prints graph and predicted population at a given year. This program was my ""Computational Programming for Engineers"" course bonus homework. For homework I used population data taken from TUİK(Statistics Institution of Turkey) and predicted population. The Excel file that I have used is listed in the repository.\n'], 'url_profile': 'https://github.com/gen-encrypted', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/meghs333', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['1104102_1dconv_reg\nImplementation of 1D convolution (Conv1D)-based neural network for predicting median house value using longitude, latitude, housing median age, total number of rooms, total number of bedrooms, population, number of households, and median income from California housing dataset. The solution must be a nonlinear regression model.\nIn conclusion, we see the best optimizer for our dataset is Adam with learning rate as 0.001 and epochs as 450, kernel size as 1 and batch size as 64. From the performed experiment we get to know the benefits and efficiency of 1D CNN which has a growing demand in todays industry. It excludes the scarcity of training data and high complexity which is violated by the real-time constraints.\n'], 'url_profile': 'https://github.com/shivanisingh28', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Big-Mart-Sales-\nThe aim is to build a predictive model and find out the sales of each product at a particular store. Techniques: developed intuition from hypothesis generation, applied exploratory data analysis, data preprocessing and feature engineering tasks. Built a Linear Regression and a Random Forest model. Finally, model evaluation part has been done\n'], 'url_profile': 'https://github.com/som21-star', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'India', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Telecom-Churn\nAs per a report 15 - 20%user telecom user churns/switch to other telecom organizations. I predict the possible churners using Random Forest and Logistic Regression (With PCA). I also found the important predictors which can help the business to stop the churners.\n'], 'url_profile': 'https://github.com/sanjeevy94', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'São Paulo', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['overdisp\nDetection of overdispersion in count data for multiple regression analysis. Log-linear count data regression is one of the most popular techniques for predictive modeling where there is a non-negative discrete quantitative dependent variable. In order to ensure the inferences from the use of count data models are appropriate, researchers may choose between the estimation of a Poisson model and a negative binomialmodel, and the correct decision for prediction from a count data estimation is directly linked to the existence of overdispersion of the dependent variable, conditional to the explanatory variables. Based on the studies of Cameron and Trivedi (1990) <doi:10.1016/0304-4076(90)90014-K> and Cameron and Trivedi (2013, ISBN:978-1107667273), the overdisp() command is a contribution to researchers, providing a fast and secure solution for the detection of overdispersion in count data. Another advantage is that the installation of other packages is unnecessary, since the command runs in the basic R language.\nProject name: overdisp - Overdispersion in Count Data Multiple Regression Analysis\nDescription: Log-linear count data regression is one of the most popular techniques for predictive modeling where there is a non-negative discrete quantitative dependent variable. In order to ensure the inferences from the use of count data models are appropriate, researchers may choose between the estimation of a Poisson model and a negative binomial model, and the correct decision for prediction from a count data estimation is directly linked to the existence of overdispersion of the dependent variable, conditional to the explanatory variables. Based on the studies of Cameron and Trivedi (1990, 2013), the \\code{overdisp()} command is a contribution to researchers, providing a fast and secure solution for the detection of overdispersion in count data. Another advantage is that the installation of other packages is unnecessary, since the command runs in the basic R language.\nUsage: overdisp(x, dependent.position = NULL, predictor.position = NULL)\nx - The user\'s dataset.\ndependent.position - A single number that declares the position of the dependent variable in the user dataset.\npredictor.position - A number, or a set of numbers, that declares the position of explanatory variables in the dataset.\nDetails:\nThe test for detecting overdispersion of count data proposed by Cameron and Trivedi (1990) is based on following equation, where \\eqn{H_{0}}  is the equidispersion given by \\eqn{Var(Y|X)=E(Y|X)} as follows:\n\\deqn{Var(Y|X)=E(Y|X)+\\Phi [E(Y|X)]^2}\nwhich is similar to the variance function of the negative binomial model indicated by: \\eqn{Var(Y_{i})=u+\\Phi u^2}, where \\eqn{\\Phi = 1/\\Psi} and \\eqn{u_{i} = exp(\\alpha + \\beta_{1}X_{1i}+ \\beta_{2}X_{2i})+...+\\beta_{k}X_{ki}}. For the test in\nhighlighted expression, the significance of parameter \\eqn{\\Phi} must be verified, in which \\eqn{H_{1}: \\Phi >0} e \\eqn{H_{0}: \\Phi =0}.\nFor the detection of overdispersion in the count data, at a certain level of significance, Cameron and Trivedi (1990) postulated that a Poisson model should be estimated a priori. According to the authors, after this, an auxiliary ordinary least squares (OLS) model should be estimated without the intercept, whose dependent variable \\eqn{Y*}, given by expression \\eqn{[(Y_{i}- \\lambda_{i})^2 -Y_{i}]/ \\lambda_{i}}, should be calculated using the fitted values of \\eqn{\\lambda} from the initially established Poisson model.\nThe auxiliary model given by \\eqn{Y*{i}=\\beta\\lambda{i}} should use \\eqn{\\lambda} as the sole predictor variable. After the estimation of the auxiliary model, Cameron and Trivedi (1990) recommend checking the p value from the Student\'s t-test for the predictor variable \\eqn{\\lambda}. In the cases where \\eqn{P >|t|>sig}, equidispersion at a pre-established significance level is indicated; when \\eqn{P>|t|<=sig}, overdispersion at a pre-established significance level is indicated.\nTable of contents:\nA list with class ""htest"" containing the following components:\nstatistic: the value of the Lambda t test score.\np.value: the p-value for the test.\nmethod: the character string ""Overdispersion Test - Cameron & Trivedi (1990)"".\ndata.name: a character string giving the name(s) of the data.\nalternative: the character string ""overdispersion if lambda p-value is less than or equal to the stipulated significance level"".\nCredits:\nRafael de Freitas Souza, Luiz Paulo Favero, Patricia Belfiore and Hamilton Luiz Correa.\nExamples:\noverdisp(warpbreaks, dependent.position = 1, predictor.position = 2:3)\n'], 'url_profile': 'https://github.com/fsrafael', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['data_science_msc\nCoursework from my MSc\n'], 'url_profile': 'https://github.com/fplon', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}","{'location': 'Riga, Latvia', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['ML\nAll training and testing data for Neural Networks is available in my Google Drive\nContent\n\nFeed Forward Neural Net wiht NumPy from scratch, replicating PyTorch Layer architecture. Used for non-linear multidimensional regression\nNaive Bayes with NumPy\nLinear Regression with NumpPy\nSVM with NumPy\nAdvancing Pandas skills, data cleaning and running different regression algorithms\nSentimental analysis with Tensorlow and word2vec\nDice recognition with Tensorflow CNN\n\n'], 'url_profile': 'https://github.com/HenriARM', 'info_list': ['Jupyter Notebook', 'Updated Apr 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Oct 9, 2020', 'MATLAB', 'Updated Feb 16, 2020', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'R', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2021']}"
"{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Donors-choose-analysis\nPerformed Natural Language Processing on a dataset of 109248 project proposals using models like Bag of Words, TFIDF, Average Word2Vec, performed feature reduction and visualization with PCA and t-SNE and used classification and regression algorithms like KNN, LASSO, Ridge, Naïve Bayes, SVM, Decision Trees and such by using hyperparameter tuning with performance metrics like ROC/AUC curve to predict whether a project gets chosen.\nOriginal dataset: https://www.kaggle.com/c/donorschoose-application-screening/data\n'], 'url_profile': 'https://github.com/anweshakakoty', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'Gaza Strip', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['DataCamp\nDataCamp is great platform to learn and master data science.\nHere All the courses that I have completed.\nAll the texts from DataCamp, and the codes also, but with some modifications by Me because Some of these are set up to actually run locally (mapped to the datasets)\nCareer and Skill Tracks\nSupervised Learning with scikit-learn\nIndividual Courses\n'], 'url_profile': 'https://github.com/MohamedAbuAmira', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Urban-Spatial-Analysis\nData pre-processing using pandas and numpy for data clean and aggregation, then geocode crime data into point and spatially join census tract and crime point using ArcMap; Used Seaborn and Matplotlib in Python for visualization and Explore Data Analysis; Built multi-layer maps using CARTO combined with Stepwise regression model to verify certain type of  crimes that significantly affects the price of short-term rental(Airbnb) and long-term housing respectively in Boston area\n'], 'url_profile': 'https://github.com/Yujia-Jacqueline-He', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'Surabaya, East Java, Indonesia', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['Orange_Tutorial\n\nBahasa\nTutorial Orange Data Mining dalam Bahasa Indonesia untuk pengguna ahli ataupun pengguna awam\nUntuk lebih lanjut bisa dilihat di Wiki\nLanguage\nOrange Data Mining Tutorial in bahasa for a expert or a layman\nFor further information you could see at Wiki\n'], 'url_profile': 'https://github.com/ranggakd', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bedilkarimov', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '399 contributions\n        in the last year', 'description': ['Practice_Projects\nData Science Practice Projects\n\nFile-Tree Structure-\n📦Practice_Projects\n ┣ 📂Ads_CTR_Optimisation\n ┃ ┣ 📜Ads_CTR_Optimisation.csv\n ┃ ┣ 📜Ads_CTR_Optimisation.html\n ┃ ┣ 📜Ads_CTR_Optimisation.ipynb\n ┃ ┣ 📜README.md\n ┃ ┣ 📜Thompson_Sampling_Slide_1.png\n ┃ ┣ 📜Thompson_Sampling_Slide_2.png\n ┃ ┣ 📜Thompson_Sampling_Slide_3.png\n ┃ ┣ 📜Thompson_Sampling_Slide_4.png\n ┃ ┣ 📜UCB_Algorithm_Slide_1.png\n ┃ ┗ 📜UCB_Algorithm_Slide_2.png\n ┣ 📂ANN\n ┃ ┣ 📜README.md\n ┃ ┣ 📜RedWineQuality.html\n ┃ ┣ 📜RedWineQuality.ipynb\n ┃ ┗ 📜winequality-red.csv\n ┣ 📂AutoEncoder\n ┃ ┣ 📜AutoEncoder.html\n ┃ ┣ 📜AutoEncoder.ipynb\n ┃ ┣ 📜ImageColorization.html\n ┃ ┣ 📜ImageColorization.ipynb\n ┃ ┣ 📜ImageColorizationV0.ipynb\n ┃ ┣ 📜README.md\n ┃ ┣ 📜RecommenderSystem.html\n ┃ ┣ 📜RecommenderSystem.ipynb\n ┃ ┣ 📜ReverseImageSearch.html\n ┃ ┗ 📜ReverseImageSearch.ipynb\n ┣ 📂BoltzmannMachines\n ┃ ┣ 📜RBM.html\n ┃ ┣ 📜RBM.ipynb\n ┃ ┗ 📜README.md\n ┣ 📂CNN\n ┃ ┣ 📂ColorClassification\n ┃ ┃ ┣ 📜ColorClassification.html\n ┃ ┃ ┣ 📜ColorClassification.ipynb\n ┃ ┃ ┣ 📜red.jpg\n ┃ ┃ ┗ 📜wikipedia_color_names.csv\n ┃ ┣ 📂DogsVsCats\n ┃ ┃ ┣ 📜cat1.jpg\n ┃ ┃ ┣ 📜DogsVsCats.html\n ┃ ┃ ┗ 📜DogsVsCats.ipynb\n ┃ ┣ 📂FaceDetection\n ┃ ┃ ┣ 📜embedding_model.h5\n ┃ ┃ ┣ 📜FaceDetection.html\n ┃ ┃ ┣ 📜FaceDetection.ipynb\n ┃ ┃ ┣ 📜siamese.png\n ┃ ┃ ┣ 📜SiameseNetworkWithTripletLoss.html\n ┃ ┃ ┣ 📜SiameseNetworkWithTripletLoss.ipynb\n ┃ ┃ ┗ 📜siamese_model.h5\n ┃ ┣ 📂FashionMNIST\n ┃ ┃ ┣ 📜cp.h5\n ┃ ┃ ┣ 📜FashionMNIST.html\n ┃ ┃ ┣ 📜FashionMNIST.ipynb\n ┃ ┃ ┗ 📜model.h5\n ┃ ┣ 📂HorsesVsHumans\n ┃ ┃ ┣ 📜horse1.jpg\n ┃ ┃ ┣ 📜HorsesVsHumans.html\n ┃ ┃ ┗ 📜HorsesVsHumans.ipynb\n ┃ ┣ 📂IntelImageClassification\n ┃ ┃ ┣ 📜img1.jpg\n ┃ ┃ ┣ 📜IntelImageClassification.html\n ┃ ┃ ┗ 📜IntelImageClassification.ipynb\n ┃ ┣ 📂NeuralStyleTransfer\n ┃ ┃ ┣ 📜NeuralStyleTransfer.html\n ┃ ┃ ┣ 📜NeuralStyleTransfer.ipynb\n ┃ ┃ ┣ 📜NeuralStyleTransfer1.png\n ┃ ┃ ┣ 📜NeuralStyleTransfer2.png\n ┃ ┃ ┣ 📜NeuralStyleTransfer3.png\n ┃ ┃ ┣ 📜VassilyKandinsky.jpg\n ┃ ┃ ┗ 📜YellowLabrador.jpg\n ┃ ┣ 📂RCNN\n ┃ ┃ ┣ 📜R_CNN.html\n ┃ ┃ ┣ 📜R_CNN.ipynb\n ┃ ┃ ┗ 📜volvo_cars_employees_2019_sub_hero.webp\n ┃ ┣ 📂SignLanguageMNIST\n ┃ ┃ ┣ 📜SignLanguageMNIST.html\n ┃ ┃ ┗ 📜SignLanguageMNIST.ipynb\n ┃ ┣ 📂TensorFlowForPoets\n ┃ ┃ ┣ 📜Flowers.html\n ┃ ┃ ┣ 📜Flowers.ipynb\n ┃ ┃ ┗ 📜roses1.jpg\n ┃ ┣ 📂YOLO\n ┃ ┃ ┣ 📜coco.names\n ┃ ┃ ┣ 📜test.jpg\n ┃ ┃ ┣ 📜YOLO.html\n ┃ ┃ ┣ 📜YOLO.ipynb\n ┃ ┃ ┣ 📜yolov3-tiny.cfg\n ┃ ┃ ┗ 📜yolov3.cfg\n ┃ ┗ 📜README.md\n ┣ 📂Credit_Card_Analysis\n ┃ ┣ 📜CC GENERAL.csv\n ┃ ┣ 📜Credit_Card_Analysis_HC.html\n ┃ ┣ 📜Credit_Card_Analysis_HC.ipynb\n ┃ ┣ 📜Credit_Card_Analysis_KMeans.html\n ┃ ┣ 📜Credit_Card_Analysis_KMeans.ipynb\n ┃ ┗ 📜README.md\n ┣ 📂Credit_Card_Fraud_Detection\n ┃ ┣ 📜creditcardfraud.zip\n ┃ ┣ 📜Credit_Card_Fraud_Detection.html\n ┃ ┣ 📜Credit_Card_Fraud_Detection.ipynb\n ┃ ┗ 📜README.md\n ┣ 📂Doctor_Fees_Prediction\n ┃ ┣ 📜Doctor_Fees_Prediction.html\n ┃ ┣ 📜Doctor_Fees_Prediction.ipynb\n ┃ ┣ 📜Final_Test.xlsx\n ┃ ┣ 📜Final_Train.xlsx\n ┃ ┗ 📜README.md\n ┣ 📂Market_Basket_Optimisation\n ┃ ┣ 📜Apriori Algorithm in R.pdf\n ┃ ┣ 📜Market_Basket_Analysis_(Kaggle_Solution).ipynb\n ┃ ┣ 📜Market_Basket_Optimisation.csv\n ┃ ┣ 📜Market_Basket_Optimisation.html\n ┃ ┣ 📜Market_Basket_Optimisation.ipynb\n ┃ ┗ 📜README.md\n ┣ 📂NLP\n ┃ ┣ 📜ConvSentimentAnalysis.html\n ┃ ┣ 📜ConvSentimentAnalysis.ipynb\n ┃ ┣ 📜Dataset.tsv\n ┃ ┣ 📜README.md\n ┃ ┣ 📜Sentiment_Analysis.html\n ┃ ┗ 📜Sentiment_Analysis.ipynb\n ┣ 📂OpenCV\n ┃ ┣ 📂Haarcascades\n ┃ ┃ ┣ 📜haarcascade_car.xml\n ┃ ┃ ┣ 📜haarcascade_eye.xml\n ┃ ┃ ┣ 📜haarcascade_frontalface_default.xml\n ┃ ┃ ┗ 📜haarcascade_fullbody.xml\n ┃ ┣ 📂images\n ┃ ┃ ┣ 📂exposures\n ┃ ┃ ┃ ┣ 📜list.txt\n ┃ ┃ ┃ ┣ 📜memorial00.png\n ┃ ┃ ┃ ┣ 📜memorial01.png\n ┃ ┃ ┃ ┣ 📜memorial02.png\n ┃ ┃ ┃ ┣ 📜memorial03.png\n ┃ ┃ ┃ ┣ 📜memorial04.png\n ┃ ┃ ┃ ┣ 📜memorial05.png\n ┃ ┃ ┃ ┣ 📜memorial06.png\n ┃ ┃ ┃ ┣ 📜memorial07.png\n ┃ ┃ ┃ ┣ 📜memorial08.png\n ┃ ┃ ┃ ┣ 📜memorial09.png\n ┃ ┃ ┃ ┣ 📜memorial10.png\n ┃ ┃ ┃ ┣ 📜memorial11.png\n ┃ ┃ ┃ ┣ 📜memorial12.png\n ┃ ┃ ┃ ┣ 📜memorial13.png\n ┃ ┃ ┃ ┣ 📜memorial14.png\n ┃ ┃ ┃ ┗ 📜memorial15.png\n ┃ ┃ ┣ 📂faces\n ┃ ┃ ┣ 📜4star.jpg\n ┃ ┃ ┣ 📜abraham.jpg\n ┃ ┃ ┣ 📜abraham_mask.png\n ┃ ┃ ┣ 📜apple.jpg\n ┃ ┃ ┣ 📜blob.jpg\n ┃ ┃ ┣ 📜blobs.jpg\n ┃ ┃ ┣ 📜bottlecaps.jpg\n ┃ ┃ ┣ 📜bunchofshapes.jpg\n ┃ ┃ ┣ 📜cars.avi\n ┃ ┃ ┣ 📜chess.jpg\n ┃ ┃ ┣ 📜digits.png\n ┃ ┃ ┣ 📜elephant.jpg\n ┃ ┃ ┣ 📜gradient.jpg\n ┃ ┃ ┣ 📜hand.jpg\n ┃ ┃ ┣ 📜Hillary.jpg\n ┃ ┃ ┣ 📜hough-transform.jpg\n ┃ ┃ ┣ 📜house.jpg\n ┃ ┃ ┣ 📜input.jpg\n ┃ ┃ ┣ 📜lena.jpg\n ┃ ┃ ┣ 📜mask.jpg\n ┃ ┃ ┣ 📜numbers.jpg\n ┃ ┃ ┣ 📜obama.jpg\n ┃ ┃ ┣ 📜orange.jpg\n ┃ ┃ ┣ 📜Rohit_Photo.jpg\n ┃ ┃ ┣ 📜scan.jpg\n ┃ ┃ ┣ 📜shapes.jpg\n ┃ ┃ ┣ 📜shapestomatch.jpg\n ┃ ┃ ┣ 📜shapes_donut.jpg\n ┃ ┃ ┣ 📜soduku.jpg\n ┃ ┃ ┣ 📜someshapes.jpg\n ┃ ┃ ┣ 📜Sunflowers.jpg\n ┃ ┃ ┣ 📜thunder.jpg\n ┃ ┃ ┣ 📜Trump.jpg\n ┃ ┃ ┣ 📜waldo.jpg\n ┃ ┃ ┣ 📜WaldoBeach.jpg\n ┃ ┃ ┗ 📜walking.avi\n ┃ ┣ 📜Basics (Matplotlib).html\n ┃ ┣ 📜Basics (Matplotlib).ipynb\n ┃ ┣ 📜Basics.html\n ┃ ┣ 📜Basics.ipynb\n ┃ ┣ 📜BLOB.html\n ┃ ┣ 📜BLOB.ipynb\n ┃ ┣ 📜Computational_Photography.html\n ┃ ┣ 📜Computational_Photography.ipynb\n ┃ ┣ 📜Contours.html\n ┃ ┣ 📜Contours.ipynb\n ┃ ┣ 📜HAAR Cascade Classifiers.html\n ┃ ┣ 📜HAAR Cascade Classifiers.ipynb\n ┃ ┣ 📜Hough.html\n ┃ ┣ 📜Hough.ipynb\n ┃ ┣ 📜LandMarks.html\n ┃ ┣ 📜LandMarks.ipynb\n ┃ ┣ 📜letter-recognition.data\n ┃ ┣ 📜Object Tracking.html\n ┃ ┣ 📜Object Tracking.ipynb\n ┃ ┣ 📜Object_Detection.html\n ┃ ┣ 📜Object_Detection.ipynb\n ┃ ┣ 📜OCR using ML.html\n ┃ ┣ 📜OCR using ML.ipynb\n ┃ ┣ 📜README.md\n ┃ ┗ 📜shape_predictor_68_face_landmarks.dat\n ┣ 📂Placement_Predictor\n ┃ ┣ 📜PlacementPredictor.html\n ┃ ┣ 📜PlacementPredictor.ipynb\n ┃ ┣ 📜PlacementPredictor.pdf\n ┃ ┣ 📜Placement_Data_Full_Class.csv\n ┃ ┗ 📜README.md\n ┣ 📂Recommender_System\n ┃ ┣ 📜Cold_Start.JPG\n ┃ ┣ 📜Correlation.JPG\n ┃ ┣ 📜cosine.png\n ┃ ┣ 📜movie_id_titles.csv\n ┃ ┣ 📜README.md\n ┃ ┣ 📜Recommendation_System_(Kaggle_Solution).ipynb\n ┃ ┣ 📜Recommendation_System_1.html\n ┃ ┣ 📜Recommendation_System_1.ipynb\n ┃ ┣ 📜Recommendation_System_2.html\n ┃ ┣ 📜Recommendation_System_2.ipynb\n ┃ ┣ 📜tmdb_5000_credits.csv\n ┃ ┣ 📜tmdb_5000_movies.csv\n ┃ ┣ 📜user_rating.tsv\n ┃ ┗ 📜Wikipedia_Example.JPG\n ┣ 📂RL_Intuition\n ┃ ┣ 📜CartPole-v0.html\n ┃ ┣ 📜CartPole-v0.ipynb\n ┃ ┣ 📜Deep_Q-Network.jpg\n ┃ ┣ 📜edX - Reinforcement Learning Explained.zip\n ┃ ┣ 📜Escape_Room.py\n ┃ ┣ 📜FrozenLake-v0.html\n ┃ ┣ 📜FrozenLake-v0.ipynb\n ┃ ┗ 📜README.md\n ┣ 📂Sequence_Models\n ┃ ┣ 📂Emojify\n ┃ ┃ ┣ 📜Emojify.html\n ┃ ┃ ┣ 📜Emojify.ipynb\n ┃ ┃ ┣ 📜emojify_data.csv\n ┃ ┃ ┣ 📜test_emoji.csv\n ┃ ┃ ┗ 📜train_emoji.csv\n ┃ ┣ 📂IMDB_Reviews\n ┃ ┃ ┣ 📜IMDB_Reviews.html\n ┃ ┃ ┗ 📜IMDB_Reviews.ipynb\n ┃ ┣ 📂NeuralMachineTraslation\n ┃ ┃ ┣ 📜Seq_to_Seq.html\n ┃ ┃ ┗ 📜Seq_to_Seq.ipynb\n ┃ ┣ 📂ShakespeareSonnets\n ┃ ┃ ┣ 📜ShakespeareSonnets.html\n ┃ ┃ ┣ 📜ShakespeareSonnets.ipynb\n ┃ ┃ ┗ 📜sonnet.txt\n ┃ ┣ 📂TextProcessing\n ┃ ┃ ┣ 📜SarcasmHeadlinesDataset.zip\n ┃ ┃ ┣ 📜TextProcessing.html\n ┃ ┃ ┗ 📜TextProcessing.ipynb\n ┃ ┣ 📂TransferLearning\n ┃ ┃ ┣ 📜GloVe.html\n ┃ ┃ ┗ 📜GloVe.ipynb\n ┃ ┗ 📜README.md\n ┣ 📂SOM\n ┃ ┣ 📜Credit_Card_Applications.csv\n ┃ ┣ 📜README.md\n ┃ ┣ 📜SelfOrganizingMap.html\n ┃ ┗ 📜SelfOrganizingMap.ipynb\n ┣ 📂TensorFlow\n ┃ ┣ 📂models\n ┃ ┃ ┣ 📜checkpoint\n ┃ ┃ ┣ 📜first_model.ckpt.data-00000-of-00001\n ┃ ┃ ┣ 📜first_model.ckpt.index\n ┃ ┃ ┗ 📜first_model.ckpt.meta\n ┃ ┣ 📜auto-mpg.data\n ┃ ┣ 📜Classification_Diabetes.html\n ┃ ┣ 📜Classification_Diabetes.ipynb\n ┃ ┣ 📜Manual_Neural_Network.html\n ┃ ┣ 📜Manual_Neural_Network.ipynb\n ┃ ┣ 📜pima_indians_diabetes.csv\n ┃ ┣ 📜README.md\n ┃ ┣ 📜Regression_AutoMPG.html\n ┃ ┣ 📜Regression_AutoMPG.ipynb\n ┃ ┣ 📜TensorFlow_Basics.html\n ┃ ┗ 📜TensorFlow_Basics.ipynb\n ┗ 📜README.md\n\n'], 'url_profile': 'https://github.com/Rohit-Jain-2801', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nourshosharah', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'Bengaluru, Karnataka ', 'stats_list': [], 'contributions': '1,285 contributions\n        in the last year', 'description': [""Fintech Project on Increasing-Subscription-Rate\nDeveloping machine learning model to predict a user who is most unlikely to subscribe for the paid membership of the app, used Logistic Regression to classify the users based on the app behavior usage and was able to predict with an accuracy of 77%, Overall this can be helpful for marketing team to target the ads for the user who are less likely to subscribe for paid version, this also helps to give the promotional offers only to specific set of customers there by reducing the marketing cost.\nInstallation\nUse the package manager pip to install foobar.\npip install numpy\npip install pandas,Pandas-profiling\npip install matplotlib,seaborn\npip install scikit-learn\n\nUsage\nimport pandas as pd\ndataset = pd.read_csv('Increasing_Subscription_Rate.csv')\n\nOutputs:\n\nLicense\nMIT License Copyright (c) 2020 REDDY PRASAD\n""], 'url_profile': 'https://github.com/reddyprasade', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/louislau66', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['DataScience2Proj1\nModeling Techniques: Regression, RidgeRegression, LassoRegression, QuadRegression, QuadXRegression, CubicRegression, CubicXRegression, ANCOVA. Datasets: AutoMPG, Concrete, and 3 more from UCI Machine Learning Repository. Do data-preprocessing (section 3.4) and exploratory data analysis (section 4.4) for each dataset. Use forward selection to add one feature/parameter at a time and record rSq, rSqBar and rSqCV and plot them versus the number of parameters (n). Do this for every modeling technique. Determine the best value n* for rSqBar and rSqCV. How similar are they? Call summary for the best model and explain its output. Read section 1.3.\n'], 'url_profile': 'https://github.com/Scare983', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Updated Oct 14, 2020', 'R', 'Updated Feb 13, 2020', '1', 'HTML', 'Updated Feb 24, 2021', 'Jupyter Notebook', 'Updated May 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 25, 2020', 'Updated Feb 10, 2020']}"
"{'location': 'Melbourne, VIC, Australia', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Forecasting Bankruptcy across Differing Time Horizons\nIn this paper, the objective is to predict corporate bankruptcy for the manufacturing sector in North America across differing time horizons i.e. one year ahead, two years ahead, three years ahead, four years ahead and five years ahead of the actual event of default. A logistic regression model is used for forecasting bankruptcy. SAS is used for programming. This code is the proprietary work of University of Cincinnati, Ohio, USA. This can be used only for academic purposes. Reproduction of any part of this course will require permission from the author.\n'], 'url_profile': 'https://github.com/lbose', 'info_list': ['SAS', 'Updated Feb 11, 2020', 'HTML', 'MIT license', 'Updated Apr 26, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': [""Pseudodoxia !!!WIP!!!\n\nI am currently developing a really real 'fake' data generator python/django/data engineering project. I have been looking everywhere for specific datasets and I cannot find any! So I am going to create my own. This is to assist anyone who is looking to get into data science/analytics/predictive analysis and needs quick data that caters to your specific needs. Generate unlimited realistic pseudo retail data to perform one of the following:\n\nVariance, Correlation, Time Series, Covariance, Mean, Median, Mode, Linear Regression, Percentiles, Moments, Normal (gaussian) distribution, Heat maps, Standard deviation, Etc...\n\nCreate your set, your way so you can practice and learn more about gaining insight into data... not THIS data. Use THIS data as a learning tool.\nData is distributed in a weighted manner although some data may be normally distributed ultimately leading into:\n-Time/Date/Location/Price/Combined product grouping specific sales data\nEX: more sales in december at 11am or 5pm in NYC.\n\nHigher priced items are less weighted than less priced items.\n\nThe API will serve requests while the module allows for customized data sets\n\n***You can generate any kind of retail transactions just think it through and create the set.***\n© 2020 Quaxis Corporation for Research & Innovation\n""], 'url_profile': 'https://github.com/sysad-aldama', 'info_list': ['SAS', 'Updated Feb 11, 2020', 'HTML', 'MIT license', 'Updated Apr 26, 2020']}",,,,,,,,
