"{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution [arXiv]\nPytorch implementation for ""Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution"".\n\n\n\nDependencies\nPython>=3.7, PyTorch>=1.1, numpy, skimage, imageio, matplotlib, tqdm\n\nQuickstart (Model Testing)\nResults of our pretrained models:\n\n\n\nModel\nScale\n#Params (M)\nPSNR on Set5 (dB)\n\n\n\n\nDRN-S\n4\n4.8\n32.68\n\n\n\n8\n5.4\n27.41\n\n\nDRN-L\n4\n9.8\n32.74\n\n\n\n8\n10.0\n27.43\n\n\n\nYou can evaluate our models on several widely used benchmark datasets, including Set5, Set14, B100, Urban100, Manga109. Note that using an old PyTorch version (earlier than 1.1) would yield wrong results.\npython main.py --data_dir $DATA_DIR$ \\\n--save $SAVE_DIR$ --data_test $DATA_TEST$ \\\n--scale $SCALE$ --model $MODEL$ \\\n--pre_train $PRETRAINED_MODEL$ \\\n--test_only --save_results\n\nDATA_DIR: path to save data\nSAVE_DIR: path to save experiment results\nDATA_TEST: the data to be tested, such as Set5, Set14, B100, Urban100, and Manga109\nSCALE: super resolution scale, such as 4 and 8\nMODEL: model type, such as DRN-S and DRN-L\nPRETRAINED_MODEL: path of the pretrained model\n\nFor example, you can use the following command to test our DRN-S model for 4x SR.\npython main.py --data_dir ~/srdata \\\n--save ../experiments --data_test Set5 \\\n--scale 4 --model DRN-S \\\n--pre_train ../pretrained_models/DRNS4x.pt \\\n--test_only --save_results\nIf you want to load the pretrained dual model, you can add the following option into the command.\n--pre_train_dual ../pretrained_models/DRNS4x_dual_model.pt\n\nTraining Method\nWe use DF2K dataset (the combination of DIV2K and Flickr2K datasets) to train DRN-S and DRN-L.\npython main.py --data_dir $DATA_DIR$ \\\n--scale $SCALE$ --model $MODEL$ \\\n--save $SAVE_DIR$\n\nDATA_DIR: path to save data\nSCALE: super resolution scale, such as 4 and 8\nMODEL: model type, such as DRN-S and DRN-L\nSAVE_DIR: path to save experiment results\n\nFor example, you can use the following command to train the DRN-S model for 4x SR.\npython main.py --data_dir ~/srdata \\\n--scale 4 --model DRN-S \\\n--save ../experiments \nCitation\nIf you use any part of this code in your research, please cite our paper:\n@inproceedings{guo2020closed,\n  title={Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution},\n  author={Guo, Yong and Chen, Jian and Wang, Jingdong and Chen, Qi and Cao, Jiezhang and Deng, Zeshuai and Xu, Yanwu and Tan, Mingkui},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2020}\n}\n\n'], 'url_profile': 'https://github.com/guoyongcs', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Hierarchical Scene Coordinate Classification and Regression for Visual Localization\nThis is the PyTorch implementation of our paper, a hierarchical scene coordinate prediction approach for one-shot RGB camera relocalization:\nHierarchical Scene Coordinate Classification and Regression for Visual Localization, CVPR 2020\nXiaotian Li, Shuzhe Wang, Yi Zhao, Jakob Verbeek, Juho Kannala\nSetup\nPython3 and the following packages are required:\ncython\nnumpy\npytorch\nopencv\ntqdm\nimgaug\n\nIt is recommended to use a conda environment:\n\nInstall anaconda or miniconda.\nCreate the environment: conda env create -f environment.yml.\nActivate the environment: conda activate hscnet.\n\nTo run the evaluation script, you will need to build the cython module:\ncd ./pnpransac\npython setup.py build_ext --inplace\nData\nWe currently support 7-Scenes, 12-Scenes, Cambridge Landmarks, and the three combined scenes which have been used in the paper. We will upload the code for the Aachen Day-Night dataset experiments.\nYou will need to download the datasets from the websites, and we provide a data package which contains other necessary files for reproducing our results. Note that for the Cambridge Landmarks dataset, you will also need to rename the files according to the train/test.txt files and put them in the train/test folders. And the depth maps we used for this dataset are from DSAC++. The provided label maps are obtained by running  k-means hierarchically on the 3D points.\nEvaluation\nThe trained models for the main experiments in the paper can be downloaded here.\nTo evaluate on a scene from a dataset:\npython eval.py \\\n        --model [hscnet|scrnet] \\\n        --dataset [7S|12S|Cambridge|i7S|i12S|i19S] \\\n        --scene scene_name \\\n        --checkpoint /path/to/saved/model/ \\\n        --data_path /path/to/data/\nTraining\nYou can train the hierarchical scene coordinate network or the baseline regression network by running the following command:\npython train.py \\\n        --model [hscnet|scrnet] \\\n        --dataset [7S|12S|Cambridge|i7S|i12S|i19S] \\\n        --scene scene_name \\ # not required for the combined scenes\n        --n_iter number_of_training_iterations \\\n        --data_path /path/to/data/\nLicense\nCopyright (c) 2020 AaltoVision.\nThis code is released under the MIT License.\nAcknowledgements\nThe PnP-RANSAC pose solver builds on DSAC++. The sensor calibration file and the normalization translation files for the 7-Scenes dataset are from DSAC. The rendered depth images for the Cambridge Landmarks dataset are from DSAC++.\nCitation\nPlease consider citing our paper if you find this code useful for your research:\n@inproceedings{li2020hscnet,\n    title = {Hierarchical Scene Coordinate Classification and Regression for Visual Localization},\n    author = {Li, Xiaotian and Wang, Shuzhe and Zhao, Yi and Verbeek, Jakob and Kannala, Juho},\n    booktitle = {CVPR},\n    year = {2020}\n}\n\n'], 'url_profile': 'https://github.com/AaltoVision', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Understanding-Regression-Techniques\nUnderstanding Regression Techniques by Packt Publishing\n'], 'url_profile': 'https://github.com/PacktPublishing', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""Machine Learning on ServiceNow\nPerform your Machine Learning on any table inside ServiceNow and store the model for prediction at later stages.\nGo to portal page - https://{instance-name}.service-now.com/machine_learning/?id=ml_template_page\nStep 1 : Choose table\n\nStep 2 : Select Training Data and Output Data\n\nStep3 : Select No. of Test Records to calculate accuracy. Also if you want to convert any labeled column to numerical one, use converters field. Use JavaScript in converters in the shown format.\n\nStep 4 : Use charts to get insights from data\n\nStep 5 : Select Algorithm, set learning rate, iterations and batch size and click train.\n\nStep 6 : Wait for model to get trained and see the accuracy.\n\nStep 7 : Then save the model using Save Model button.\n\nStep 8 : See the saved model inside the servicenow application menu and click 'preview usage script' to get the code for use inside other scripts like Script Include, Business Rules etc.\n\nGenerated files\nThis repository contains generated files and a checksum.\nDo not edit the files in this repository outside of an instance of ServiceNow.\nIf you find yourself unable to import your repository due to the presence of files edited outside an instance of ServiceNow, merge commits that mix files from different revisions, or other data that does not match the checksum, you may recover using either of the following techniques:\n\n\nRemove the problem commits:\n\nClone your repository to a personal computer with the git command line tools installed and open a git command prompt in the repository root\nRun git log and take note of the SHA1s of the problem commits\nBuild revert commits using git revert SHA1 repeatedly, working backward in time, for each commit that introduced changes not generated by a ServiceNow instance\nRun git push\n\n\n\nOverwrite the problem code snapshot with a known good one:\n\nClone your repository to a personal computer with the git command line tools installed and open a git command prompt in the repository root,\nLocate a known good code snapshot and record its SHA1. For this step, git log can be useful.\nRun git reset --hard SHA1 to a commit that was generated by a ServiceNow instance\nRun git reset HEAD{1}\nRun git add -A\nRun git commit\nRun git push\n\n\n\n""], 'url_profile': 'https://github.com/amolsangar', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['323', 'Python', 'MIT license', 'Updated May 7, 2020', '29', 'Python', 'MIT license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'MIT license', 'Updated Jan 21, 2021', '3', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['From data to wealth: textual information extraction and numeric processing\nThis is our solution to MCM-2020 Problem C:\nSunshine Company wants to get advice on the sale strategies on their three\nproducts based on data from Amazon.com, especially the time-based patterns.\nIn doing so, we did the following work and provided substantial and detailed\nconclusions.\n\nProblem description:  2020_MCM_Problem_C.pdf\nOur solution:  Solution.pdf\nMain code:  Textual information extraction and numeric processing.ipynb\nOriginal data:  Data / original-provided\nProcessing data:  Data / data-processing\nTex files:   Tex\n\n'], 'url_profile': 'https://github.com/chunwangpro', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['DATA.GOV.UK - Visual Regression Testing\nThis repo contains the BackstopJS configuration in order to run visual regression tests for the data.gov.uk admin panel.\nCredits\nThis repo is a fork of the work done by the Digital Marketplace team\nSetup\nEnsure that you have the latest versions of Node js and NPM installed locally.\nClone this repo and run:\nnpm install\n\nClone the .env.example file and create a .env file at the root of your local repo. Fill in the environment variables in accordance with their descriptions in .env.example. The variables STANDARD_RESOURCE, ORGANOGRAM_RESOURCE_NO_SOURCE and ORGANOGRAM_RESOURCE_SOURCE_SPECIFIED will be auto generated by the setup script (see below) if they are left blank or overwritten if they are filled in and setup is re-run.\nIt\'s recommended that you run the VRT against a blank environment besides the standard DGU test data. See the data setup section below for more details including instructions on how to setup test data locally. If you run this against a populated environment, filling in the necessary env variables, then the VRT will still run as expected but it will ""fail"" as it isn\'t running against the data that it has been approved against and is therefore expecting. This isn\'t a problem if you just want to check specific features manually but can be an issue if you want to update the tests.\nAdditionally, it\'s recommended you set the docker ckan environment variable CKAN_DEBUG to false for your local instance. This will turn off the debug console and other debug features for the ckan admin panel. Whilst this repo includes code to handle said debug features, it slows down local instances and can make the VRT sluggish and difficult to run.\nHow it works\nBackstopJS compares an instance of ckan that you specify in the .env file, uses puppeteer to run that instance through a set of scenarios aka: views within the ckan user journey and variations of those views, and compares screenshots taken from those scenarios to existing screenshots stored in source control. Backstop will then produce a html report for interrogation, making use of puppeteer\'s rich diff functionality. You as the dev working on a feature or bugfix can then assess if any changes produced are acceptable or not and approve these changes, updating the screenshots in source control.\nThe original intent of these tests was to assess changes between versions of ckan to ensure that the admin frontend hadn\'t broken when upgrading. This however has evolved as we start to think about ways to integrate this testing suite into a consistent CI flow.\nIf these admin journeys change at any point, the scenarios in this tool will need to be reviewed.\nRunning tests locally\nQuick start\nnpm run setup OR npm run update-resource-ids\n\nEither of these scripts will ensure that your local VRT is synced up with the data in your local ckan instance so that tests have al lthe data they need to run properly. Run one of these scripts before running test.\nnpm run test\n\nWith the basic setup as supplied, this will run an initial test on the domain specified in your .env file. By default, this will run against screenshots stored for ckan version 2.8. You can see the report by opening backstop_data/html_report/index.html in your browser. If there are any changes between the instance you are running and the stored screenshots then these tests will fail, showing diffs in the produced report. If you want to update these screenshots to the most recently run test screenshots, you can run:\nnpm run approve\n\nThis will update your saved screenshots to the most recently run tests. Remember you still have to commit these changes to source control. Every time you make a positive change, simply run npm run test to ensure visual fidelity and then npm run approve to update your reference screenshots.\nDifferent ckan versions and view filters\nThis repo includes multiple scenario ""sets"" for different version of ckan. Currently, this includes a set of screenshots for ckan 2.9, 2.8 and 2.7. By default, running test will use the config for 2.8 as this is the version currently running in production. To use a different config, for example 2.9, just pass this as an argument:\nnpm run test 2.9\n\nThis will run your local instance against ckan 2.9 instead of 2.8. Remember that your .env file needs to reflect the scenario set that you are testing eg: Don\'t run a local version of 2.9 against the 2.8 scenarios (unless this is explicitly what you want to do, in which case do not approve these changes as they will override the existing 2.8 scenario set).\nRunning the default test against any ckan config currently command will trigger 40 scenarios, each one with 3 views (desktop, tablet and phone), totalling to 120 tests. This will take a while to run all of these and may start eating into your machine\'s memory in a non-trivial way. To subvert this, scenarios are broken down into the following sections:\n\nhomepage - 1 scenario\ndataset - 11 scenarios\npublisher - 5 scenarios\nharvest - 9 scenarios\nuser - 8 scenarios\ndashboard - 3 scenarios\nadmin - 3 scenarios\n\nYou can use these section keywords to only target specific scenarios using the command npm run test [config] [keyword]. For example, to only test the harvest section for ckan 2.7, simply run npm run test 2.7 harvest to only run those 9 scenarios for 2.7. You can similarly target a single scenario or however many you want by passing the filter flag with some targeting regex for the scenario label(s) directly to the test script, like so:\nFor running ckan locally, it is recommended you use the docker-ckan project. Please see the documentation for that project for details on how to run ckan locally via docker.\nUpdating tests\nTo add new test scenarios, you can update configs/ckan-config.js under the scenarios key and follow the format of other entries in the list. Each scenario expects the following attributes:\n\nlabel: The name of the scenario. Please be descriptive and use a prefix where appropriate eg: if your new test is within the publisher views, the label should be ""Publisher - [your test]"".\nurl: The url of the scenario. Remember to make use of process.env.DOMAIN as necessary.\n\nIn addition to the above, you can add the following additional attributes depending on what you need for your scenario:\n\nskipLogin: This will skip the login step. For instances where you are testing a view that relies on not being logged in.\nsubmitForm: Assumes there is a single form on your view and submits it. This is principally for instances where you want to test how form errors look on forms. Please see the Gotchas section below for details on a minor quirk within ckan forms.\nuploadField: Waits for elements specific to the image upload functionality to ensure that views using this pattern run properly for tests.\nselect2: Waits for elements specific to the select2 custom dropdown vendor within to ensure that views using this pattern run properly for tests.\nslugPreview: Waits for elements specific to the slug preview functionality custom dropdown vendor within to ensure that views using this pattern run properly for tests.\ntextarea: For views that use a textarea field, this attribute explicitly specifies the heigh of these fields. This is to fix an issue where the resize arrow would shift unpredictably on tests that included textareas.\ntableRows: Sets the content of data table row data to zero for views using the data table pattern to ensure that dynamic data doesn\'t interfere with tests.\ndashboard: Removes some specific elements related to dashboard views (dashboard default and user activity) to ensure that dynamic data doesn\'t interfere with tests.\nharvestJobList: An attribute for a specific view to clear dynamic data for the harvest job list view.\ntableShowMore: Removes the show more pattern from data tables to combat an unpredictable bug where the VRT would sometimes beat the js that rendered table hiding and showing on long data tables.\n\nMake sure to review the test report before approving to check that no unwanted changes have slipped in.\nIf you want to add a new scenario set, you can add a new config root under the configs directory. If you are adding a new ckan version, it is recommended that you follow the setup in the other ckan config roots (currently 2.7.js, 2.8.js and 2.9.js) and reference ckan-config.js as they do. If you are adding a completely new set of scenarios relating to datagovuk but unrelated to the ckan admin panel, you will need to create an entirely new config. You can use ckan-config.js as a basis for your config. If you want to use filters in your new scenario, it is recommended that you edit test.js and amend the code to properly catch your filter.\nCkan data setup\nIn order to ensure that tests are based on consistent data and therefore consistent views, the tests in this repo are based on datagovuk\'s test data setup. You can run the commands specified in the documentation for ckanext-datagovuk from within the ckan image of the docker-ckan stack.\nYou can verify this within the VRT after adding the test data, as well as add an organogram dataset, a necessary dataset for the VRT tests, by running npm run setup. This will check that each piece of test data exists and add the organogram dataset via puppeteer. As part of this script, the env variables STANDARD_RESOURCE, ORGANOGRAM_RESOURCE_NO_SOURCE and ORGANOGRAM_RESOURCE_SOURCE_SPECIFIED will be auto generated. You can also run this function as a standalone command by running:\nnpm run update-resource-id\n\nYou can pass either organogram or default to run only the organogram or the standard resource generations respectively. Please note that if you have already populated these env variables, that they will be overwritten by this script.\nDebugging issues in the code\nBecause a lot of the code in this repo is abstracted by puppeteer running a headless browser instance, as well as relying on an instance of docker-ckan running alongside, debugging issues can be difficult. If you encounter issues whilst running or developing upon this repo, it is recommended that you try the following:\n\nLook at the Gotchas section below.\n\nIt could be that the issue you\'re encountering has already been captured and explored.\n\nUse puppeteer\'s screenshot functionality.\n\nawait page.screenshot({path: \'screenshot.png\'});\n\nIn the above example, puppeteer will take a screenshot of the current instance of puppeteer and output it to screenshot.png at the root of your local repo. You can change the output location to whatever you want and put this line virtually anywhere in this code and it\'ll be valid. This will give you a representation of puppeteer\'s state at any one time compared to what you\'re expecting.\nIf you do find an issue, please create a pull request to either fix the issue or add it to the gotchas below.\nGotchas\nIssues with ckan\n\nIn ckan scenarios that involve submitting a form to test for form errors, the ideal would be to use form.submit() in line with the browser API. Unfortunately, ckan\'s basic-form js module hijacks the form submission flow, meaning that the submit button for that form has to be explicitly clicked. This can still be done via puppeteer, however it\'s not especially neat.\nOn any view with an upload field, notably the organogram resource views, ckan\'s image-upload js module takes a little while to load and puppeteer manages to beat it 9 times out of 10. The upload-field.js script has therefore been added to wait for the js to load.\nSimilar to the above around having to wait for js to load in a view, some forms have an autofill slug field, managed by ckan\'s slug-preview module. This both hides one field and generates markup for a non-input field which auto-updates based on an associated input field. Like above, puppeteer beats the rendering work for this 9 times out of 10, creating inconsistent view tests. The slug-preview.js script exists to account for this.\n\nBackstop limitations\n\nBackstop doesn\'t have a testing library for IE or Edge. The reason for this is that backstop relies on the headless infrastructure provided by engines like puppeteer or caspar, something that old microsoft browsers don\'t support. We have a need to support IE11 at least for this project, so you may need to do some additional manual testing on top of running these tests.\nBackstop can only run against a very specific stack. If any additional data is added, because Backstop explicitly tests against visual discrepancy, then tests will fail. This means that if any additional data is added to a local stack or anything is changed, either said data will need to be removed or you will have to reset your local stack.\nBackstop storing screenshots in source control is potentially problematic. A single set of ckan scenarios (120 screenshots) comes to approximately 15MB. A long term solution needs ot be considered for how we store scenarios to avoid taking up obscene amounts of space in source control.\n\n'], 'url_profile': 'https://github.com/alphagov', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'Thessaloniki, Greece', 'stats_list': [], 'contributions': '2,251 contributions\n        in the last year', 'description': ['Covid19 Greece\nOpen interactive notebook in Binder:\n\n'], 'url_profile': 'https://github.com/gakonst', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'Florence / Turin', 'stats_list': [], 'contributions': '339 contributions\n        in the last year', 'description': ['COVID-19-forecasting\nA linear regression model to forecasting the Italian new COVID-19 cases of next days.\nThe data are provided by the Italian Civil Protection and can be consulted here: https://github.com/pcm-dpc/COVID-19.\nNew daily data are available from 18.00.\nDescription\nI predict the new COVID-19 italian cases for:\n\nTotal cases\nNew positive cases (hospitalized + home isolation)\nDaily intensive Care patients\n\nThe predictions are for the next 3 days.\nNote\n\nI fixed the polynomial degree, depending on the tasks mentioned above.\nAdded daily intensive care patients to the task (from 30-03).\nUpdated number of days to forecast.\n\nResults\nEvery day I will update the table with the number of cases actually verified and the model predictions for the next day.\n\n\n\nData\nTotal cases\nTotal cases - prediction\nNew positive cases\nNew positive cases - prediction\n\n\n\n\n16-03\n27980\n29070\n2470\n3329\n\n\n17-03\n31506\n32202\n2989\n3107\n\n\n18-03\n35713\n35365\n2648\n3256\n\n\n19-03\n41035\n39514\n4480\n3054\n\n\n20-03\n47021\n45447\n4670\n4042\n\n\n21-03\n53578\n52559\n4821\n4739\n\n\n22-03\n59138\n60380\n3957\n5215\n\n\n23-03\n63927\n66679\n3780\n4961\n\n\n24-03\n69176\n71134\n3780\n4614\n\n\n25-03\n74386\n75374\n3491\n4224\n\n\n26-03\n80539\n79570\n4492\n3842\n\n\n27-03\n86498\n84926\n4401\n4037\n\n\n28-03\n92472\n90641\n3651\n4106\n\n\n29-03\n97689\n96544\n3815\n3778\n\n\n30-03\n101739\n101754\n1648\n3592\n\n\n31-03\n105792\n105501\n2107\n2477\n\n\n01-04\n110574\n108659\n2937\n1860\n\n\n02-04\n115242\n112405\n2477\n1736\n\n\n03-04\n119827\n116523\n2339\n1469\n\n\n04-04\n124632\n120889\n2886\n1227\n\n\n05-04\n128948\n125679\n2972\n1275\n\n\n06-04\n132547\n130346\n1941\n1367\n\n\n07-04\n135586\n134393\n880\n1076\n\n\n08-04\n139422\n137693\n1195\n482\n\n\n09-04\n143626\n137693\n1615\n1727\n\n\n10-04\n147577\n145435\n1396\n1514\n\n\n11-04\n152271\n149761\n1996\n1287\n\n\n12-04\n156363\n154779\n1984\n1203\n\n\n13-04\n159516\n159775\n1363\n1128\n\n\n14-04\n162488\n164062\n675\n953\n\n\n15-04\n165155\n167776\n1127\n680\n\n\n16-04\n168941\n170933\n1189\n519\n\n\n17-04\n172434\n174528\n355\n389\n\n\n18-04\n175925\n178234\n809\n138\n\n\n19-04\n178972\n182022\n486\n-8\n\n\n20-04\n181228\n185582\n-20\n-187\n\n\n21-04\n183957\n188495\n-528\n-423\n\n\n22-04\n187327\n191288\n-10\n-709\n\n\n23-04\n189973\n194413\n-851\n-884\n\n\n24-04\n192994\n197343\n-321\n-1163\n\n\n25-04\n195351\n200370\n-680\n-1331\n\n\n26-04\n197675\n203088\n256\n-1531\n\n\n27-04\n199414\n205583\n-290\n-1574\n\n\n28-04\n201505\n207607\n-608\n-1684\n\n\n29-04\n203591\n209516\n-548\n-1824\n\n\n30-04\n205463\n211367\n-3106\n-1938\n\n\n01-05\n207428\n213084\n-608\n-2380\n\n\n02-05\n209328\n214777\n-239\n-2449\n\n\n03-05\n210717\n216441\n-525\n-2458\n\n\n04-05\n211938\n217840\n-199\n-2499\n\n\n05-05\n213013\n218984\n-1513\n-2490\n\n\n06-05\n214457\n219892\n-6939\n-2641\n\n\n07-05\n215858\n220837\n-1904\n-3444\n\n\n08-05\n217185\n221812\n-1663\n-3566\n\n\n09-05\n218268\n222796\n-3119\n-3644\n\n\n10-05\n219814\n223690\n-1518\n-3883\n\n\n11-05\n219814\n220508\n-836\n-3911\n\n\n12-05\n221216\n221479\n-1222\n-3853\n\n\n13-05\n222104\n222567\n-2809\n-3839\n\n\n14-05\n223096\n223603\n-2017\n-4000\n\n\n15-05\n223885\n224637\n-4370\n-4057\n\n\n16-05\n224760\n225618\n-1883\n-4363\n\n\n17-05\n225435\n226591\n-1836\n-4373\n\n\n18-05\n225886\n227505\n-1798\n-4372\n\n\n19-05\n226699\n228313\n-1424\n-4364\n\n\n20-05\nUpdate at 18.00\n229146\nUpdate at 18.00\n-1480\n\n\n\n\n\nUpdate for daily intensive care patients\n\n\n\nData\nDaily intensive care\nDaily intensive care - prediction\n\n\n\n\n30-03\n75\n108\n\n\n31-03\n42\n90\n\n\n01-04\n12\n66\n\n\n02-04\n20\n40\n\n\n03-04\n15\n19\n\n\n04-04\n-74\n1\n\n\n05-04\n-17\n-34\n\n\n06-04\n-79\n-51\n\n\n07-04\n-106\n-78\n\n\n08-04\n-99\n-107\n\n\n09-04\n-88\n-130\n\n\n10-04\n-108\n-147\n\n\n11-04\n-116\n-165\n\n\n12-04\n-38\n-182\n\n\n13-04\n-83\n-182\n\n\n14-04\n-74\n-189\n\n\n15-04\n-107\n-194\n\n\n16-04\n-43\n-203\n\n\n17-04\n-79\n-216\n\n\n18-04\n-124\n-225\n\n\n19-04\n-98\n-224\n\n\n20-04\n-62\n- 226\n\n\n21-04\n-102\n-222\n\n\n22-04\n-87\n-224\n\n\n23-04\n-117\n-71\n\n\n24-04\n-94\n-78\n\n\n25-04\n-71\n-83\n\n\n26-04\n-93\n-86\n\n\n27-04\n-53\n-90\n\n\n28-04\n-93\n-92\n\n\n29-04\n-68\n-96\n\n\n30-04\n-101\n-98\n\n\n01-05\n-116\n-102\n\n\n02-05\n-39\n-106\n\n\n03-05\n-38\n-105\n\n\n04-05\n-22\n-106\n\n\n05-05\n-52\n-105\n\n\n06-05\n-94\n-105\n\n\n07-05\n-22\n-108\n\n\n08-05\n-143\n-107\n\n\n09-05\n-134\n-112\n\n\n10-05\n-7\n-117\n\n\n11-05\n-28\n-114\n\n\n12-05\n-47\n-113\n\n\n13-05\n-59\n-113\n\n\n14-05\n-38\n-114\n\n\n15-05\n-47\n-113\n\n\n16-05\n-33\n-113\n\n\n17-05\n-13\n-112\n\n\n18-05\n-13\n-110\n\n\n19-05\n-33\n-108\n\n\n20-05\nUpdate at 18.00\n-107\n\n\n\n\n'], 'url_profile': 'https://github.com/AlessandroMinervini', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}","{'location': 'Eugene, Oregon', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': [""R.Housing.Prices\nKNN Regression, Regression Trees, & GLM\nPredictive Analytics - Housing Prices King County, WA\nUnited States housing prices have fluctuated over the past century. Different parts of the country have gradually become more expensive to live in as a result of changes in economic conditions.\nHousing prices reflect the estimated values of homes in relation to their physical features and geographical locations.\nThe goal of the analysis was to use regression analysis in helping to identify which characteristics have the greatest effects on home value and provide this information to prospective homeowners.\nGetting Started\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\nPrerequisites\nWhat things you need to install the software and how to install them\nR Studio\nLibraries: 'tidyverse','rpart','rpart.plot','Metrics','forecast','caret',\n       'ggplot2', 'FNN', 'fastDummies','dataPreparation','reshape2','corrplot'\n\nData Exploration\nThe data was sourced from Kaggle, consisting of 21 variables and 21,613 observations.\nThe dataset describes features and prices of homes within the area of King County, Washington during the time period of May 2014-2015, which includes Seattle and the surrounding cities.\nAs the most populous county in Washington at 2.2 million, and the 12th most populated county in the United States, King County offers diverse housing to its many inhabitants.\nThe analysis aims to examine the relationship between the housing features and the sales price within the area.\nAuthors\n\nAdrian Leung - Initial work - (https://github.com/leungad)\n\n""], 'url_profile': 'https://github.com/leungad', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'JavaScript', 'MIT license', 'Updated Dec 23, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '6', 'Jupyter Notebook', 'Updated Mar 22, 2020', '12', 'Python', 'MIT license', 'Updated May 20, 2020', '2', 'R', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'Belo Horizonte, Minas Gerais. Brasil', 'stats_list': [], 'contributions': '307 contributions\n        in the last year', 'description': [""Regresser - Visual Regression Library\nWhat is it?\nA visual regression tool for comparing web apps and websites to visually find differences between versions.\nUsage:\nWebdriver\nFirst we need to create a baseline, your main version. Any other version will be compared to this one.\nfrom regresser import webdriver\n\nwebdriver.Driver(url='https://www.nytimes.com/', background=True, timeout=60)\nNow we're able to create another version. Eg.:\nfrom regresser import webdriver\n\nwebdriver.Driver(url='https://www.nytimes.com/', background=True, timeout=60)\nwebdriver.Driver(url='https://www.nytimes.com/', version='stg', background=False, timeout=60)```\nOptions:\nversion='staging' #version name \ndriver='firefox' #defaults to firefox. Use 'chrome' to enable chromedriver\ntimeout=10 #timeout loading a page\nbackground=True or False #Loads the website in background (headless)\nwidth=None #force a width, otherwise will be automatic.\nheight=None #force a height, otherwise will be automatic.\nAnalyzer\nfrom regresser import analyzer\n\nanalyzer.Compare(version='stg')\nResults\nThere are currently two outputs: alpha_diff.png and highlights.png.\nThe alpha_diff will hide everything except differences, and highlights will show differences in red color after comparing\nversions.\n""], 'url_profile': 'https://github.com/jersobh', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['BackstopJS: Avoid misery with Visual Regression Testing\nThis is a demo into how to use BackstopJS Visual Regression Testing in many different ways.\nRequirements\n\nDocker can be ran separately from Lando\nBackstopJS install\nnpm install -g backstopjs\n\nSetup Backstop with Demo Repository\n\nMake sure to install Lando\nClone the repository\nRun the following command\n\nlando start\nlando composer install\nlando db-import drupal8.2020-03-12-1584018477.sql.gz - To import the database\nOpen the files.zip and add the files folder to the following directory /web/sites/default/\nIf you need the URL use either of the following commands:\n\nlando info (This will give you info about the container)\nlando rebuild (This will rebuild lando this should always be used if any changes to lando.yml are made.)\n\n\nHomepage should appear if you need to login lando drush uli to get a login\nIf you need cache rebuild lando drush cr\n\n\n\nSetting Up for Local Development Options\nBackstopJS with Lando\nYou want to use backstop-docker in current project. Copy the directory into your repository and follow these instruction. and use this part of the lando.yml file.\n$services:\n$    backstop-docker:\n$        type: node:custom\n$        overrides:\n$          image:  backstopjs/backstopjs:4.4.2\n$\n$tooling:\n$    backstop-docker:\n$        service: backstop-docker\n$        description: Runs ""lando backstop-docker"" to use backstop with docker\n$        cmd: ""backstop --config=backstop-docker/backstop.js""\n$\n\nBackstopJS with docker-compose\nYou want to use backstop-docker in current project. Copy the directory into your repository and follow these instruction. and add local environment to the environment.json. Add the following to docker-compose.yml.\n$  backstop:\n$    image: ""backstopjs/backstopjs:4.4.2""\n$    environment:\n$      BASE_URL: ""http://drupal:80""\n$    volumes:\n$      - ./backstop:/src\n$    shm_size: 1gb\n$   entrypoint: [backstop, --config=/src/backstop.js]\n\nNote the command will change a bit when only using BackstopJS with Docker instead of lando backstop-docker use docker-compose run backstop\nBackstopJS with NPM\nYou want to use BackstopJS with NPM here is example and documentation on how to run the BackstopJS this way.\nGetting Started using BackstopJS\nBackstop-Docker\nTo use this just simply update the following files if needed:\n\n\nbackstop-docker/environment.json\n\nThis will include any environments you want to run BackstopJS against such as Production, Stage/Test, or Dev.\nLocal is setup to run with lando.yml within the backstop.js. If you are not using Lando you can add local to\nthe environment.json file\n\n\n\nbackstop-docker/page.json\n\n\npage.json includes all pages that will be used to test with BackstopJS.\n{""label"":  ""Namepage"", ""url"": ""/en/contact""}\nYou can include selectors in this file as well. The backstopSelectors can be used to pick a region or section of the page. The "".class"" is div class to pick in the page and the ""document"" will take a screen capture of the whole page as well.\n{""label"":  ""NameFeedback"", ""url"":  ""/node/1234"",\n       ""backstopSelectors"": [\n       "".class"",\n       ""document""\n       ]\n     },\n\n\n\nOnce these files have been updated go to the following documentation to see the commands you can use to run to compare pages.\n\n\nCI setup\nTo add BackstopJS to the CircleCI 2.0 make sure to change the backstopjs here ""report"": [""browser"", ""CI""],. Add the ""CI"" to the report section.\n  backstop:\n     parameters:\n       target:\n         type: string\n     working_directory: /home/circleci/code\n     docker:\n       # Need to use docker image for these steps.\n       - image: circleci/python:2.7.14\n     steps:\n       - checkout\n       - setup_remote_docker\n       - run: docker-compose up --no-start backstop\n       - run: docker cp ./backstop/. ""$(docker-compose ps -q backstop)"":/src/\n       - run: docker-compose run backstop reference --target=prod\n       - run: docker-compose run backstop test --target=test\n       - run:\n           command: docker cp ""$(docker-compose ps -q backstop)"":/src/. ./backstop/\n           when: always\n       - store_test_results:\n           path: /home/circleci/code/backstop/report\n       - store_artifacts:\n           path: /home/circleci/code/backstop\n\nTroubleshooting\nBackstop-docker\nIf you update the lando.yml file and you have trouble bring the backstop-docker container back up. Lando does not remove old containers so the following needs to be done:\n $ docker stop backstopjs_backstop_1\n $ docker rm backstopjs_backstop_1\n $ lando start\n\nAttribution\nBackstopJS was created and is maintained by Garris Shipon\n'], 'url_profile': 'https://github.com/LastCallMedia', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['InstandDL: An easy and convenient deep learning pipeline for image segmentation and classification\n\nInstantDL enables experts and non-experts to use state-of-the art deep learning methods on biomedical image data. InstantDL offers the four most common tasks in medical image processing: Semantic segmentation, instance segmentation, pixel-wise regression and classification. For more in depth discussion on the methods, as well as comparing the results and bechmarks using this package, please refer to our preprint on bioRxiv here\n\n\n\n\nDocumentation\nFor documentation please refere to docs\nFor a short video introducing InstantDL please see:\n\n\n\n\n\nContributing\nWe are happy about any contributions. For any suggested changes, please send a pull request to the develop branch.\nCitation\nIf you use InstantDL in a project, you can cite the preprint on bioRxiv\n@article {Waibel2020.06.22.164103,\nauthor = {Waibel, Dominik Jens Elias and Shetab Boushehri, Sayedali and Marr, Carsten},\ntitle = {InstantDL - An easy-to-use deep learning pipeline for image segmentation and classification},\nelocation-id = {2020.06.22.164103},\nyear = {2020},\ndoi = {10.1101/2020.06.22.164103},\npublisher = {Cold Spring Harbor Laboratory},\nURL = {https://www.biorxiv.org/content/early/2020/06/23/2020.06.22.164103},\neprint = {https://www.biorxiv.org/content/early/2020/06/23/2020.06.22.164103.full.pdf},\njournal = {bioRxiv}\n}\n\nThe main publication will be added soon.\n'], 'url_profile': 'https://github.com/marrlab', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'San Jose, CA', 'stats_list': [], 'contributions': '367 contributions\n        in the last year', 'description': [""COVID-19-model\nMachine learning-based logistic regression for confirmed COVID-19 cases that predicts carrying capacity of coronavirus (when coronavirus infections will stop) in a given region\n• blue bar graph represents actual cases \n• red line graph represents logistic regression model (predicts coronavirus' carrying capacity) \nModel of Hubei, China's COVID-19 Cases\n\nModel of Thailand's COVID-19 Cases\n\nConfirmed COVID-19 Cases Dataset Source\nhttps://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series\n""], 'url_profile': 'https://github.com/AaditT', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '848 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sihan-tawsik', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""README #\nSelenium WebDriver\nWhat is this repository for?\n\nFor Selenium Automation Testing\nOxygen.3a Release (4.7.3a)\n\nHow do I get set up?\n\nDownload Selenium Jar Files\nConfigure Eclipse With Jar Files\nTestNG\nManually\n\nContribution guidelines\n\nWrite Test Cases In Java\nBitBucket\n\nHow to push the code and commit code\n\nFrom the command line, enter cd <path_to_local_repo> so that you can enter commands for your repository.(If you are in another repo)\nEnter git add --all at the command line to add the files or changes to the repository.\nEnter git commit -m '<commit_message>' at the command line to commit new files/changes to the local repository.\nIn Commit message write any message\nEnter git push  at the command line to copy your files from your local repository to Bitbucket.\nIf prompted for authentication, enter your Bitbucket password\n\nHow To Install Maven In Windows\n\nDownload Maven Zip File Extract It\nSet Enivroment Variables (M2_Home , MAVEN_Home)\ni)Right Click MyComputer\nii)Propterties\niv)Advance System Setting\nv)Envir Variable\nSet Path Variable (%MAVEN_HOME%)\n\nHow To Run Maven Project From Terminal\nPrerequisites\ni)TestNG Plugin In Pom.xml\nii)TestNG Dependency In Pom.xml\niii)Should Be TestNG Tests\niv)Must Have TestNG.xml\n*Add maven-surefire-plugin In Pom.xml\nWho do I talk to?\n\nAdmin\nMitch\n\n""], 'url_profile': 'https://github.com/Usman-tech655', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['Multiple Linear Regression\n'], 'url_profile': 'https://github.com/palak9', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'Iran', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/clearscreen-hub', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}","{'location': 'Yogyakarta', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': [""Regression\nRegression is a method to form relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables and usually denoted by X).\nDependencies\n\nNumPy\nMatplotlib\nPyTorch\n\nHow to Use\n$ python3 regression.py # default linear_regression\n$ python3 regression.py exponential_regression #choose spesific regression type\n\nCitation\nIf you found it useful don't forget to cite:\n@misc{Al-Fahsi2020,\n  author = {Al-Fahsi, R.D.H.},\n  title = {Regression},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/reshalfahsi/regression}},\n  commit = { *fill it with which version of commit you use e.g. 33a2c2b* }\n}\n\nor\nR.D.H. Al-Fahsi, Regression, (2020), GitHub repository, https://github.com/reshalfahsi/regression\n\nLicense\nMIT License\nMaintainer\nResha Dwika Hefni Al-Fahsi\n""], 'url_profile': 'https://github.com/reshalfahsi', 'info_list': ['Jupyter Notebook', 'Updated Dec 24, 2020', '3', 'Python', 'MIT license', 'Updated Mar 15, 2020', '3', 'JavaScript', 'GPL-2.0 license', 'Updated Aug 4, 2020', '18', 'Python', 'MIT license', 'Updated Dec 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'HTML', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Apr 8, 2020']}"
"{'location': 'Waterloo, Ontario, Canada', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['regression\nThis is my machine learning homework: Implementation of linear and ridge regression\n'], 'url_profile': 'https://github.com/parisadj72', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/Machine-Learning-at-FMI-VU', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jmarkovitsr', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benjones781', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rscheze', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Zürich, Switzerland', 'stats_list': [], 'contributions': '437 contributions\n        in the last year', 'description': ['Two-step penalised logistic regression for multi-omic data with an application to cardiometabolic syndrome\nThis is the accompanying code for Cabassi et al. (2020). It is based on the code Zhao and Zucknick (2020) available at https://github.com/zhizuio/IPFStructPenalty/ . Part of this code was used for the multi-omic analysis of Seyres et al. (2020).\nReferences\nCabassi, A. Seyres, D., Frontini, M., and Kirk, P.D.W. (2020). Two-step penalised logistic regression for multi-omic data with an application to cardiometabolic syndrome. arXiv preprint. arXiv:2008.00235.\nSeyres, D., et al. (2020). Transcriptional, epigenetic and metabolic signatures in cardiometabolic syndrome defined by extreme phenotypes. bioRxiv preprint. bioRxiv:2020.03.06.961805.\nZhao, Z. and Zucknick, M. (2020). Structured penalized regression for drug sensitivity prediction. Journal of the Royal Statistical Society: Series C (Applied Statistics).\nContact\nPlease feel free to contact me (Alessandra) should you need any help using this code. You can find my up-to-date email address in my GitHub profile.\n'], 'url_profile': 'https://github.com/acabassi', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['LendingClubROI\nUsing SciKitLean regression to improve ROI of LendingClub loans\n'], 'url_profile': 'https://github.com/tommybox3377', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['RReliefF\nPython Implementation of RReliefF - a feature selection tool for regression problems\nCreated by Amrit Sethi\nRReliefF is a a feature selection tool for regression problems that helps in determining the predictive performance of the different features in a data set.\nIn addition to RReliefF, implementations of Relief and ReliefF - feature selection algorithms for classification problems, are also available in relieff.py\nAlthough the function is python based, the function interface is designed to mimic the Matlab implementation.\nImplementation of Relief based algorithms\nThis code follows the algorithm for the Relief based algorithms as described in ""An adaptation of Relief for attribute estimation in regression"" by M. Robnik-Sikonja and I. Kononenko\nEquation References used in the comments of the main file are based on the aforementioned article\nTo work with RReliefF specifically, use W = RReliefF(X, y, opt)\nopt can be replaced with the following optional arguments:\n\nupdates - This can be \'all\' (default) or a positive integer depending\nk - The number of neighbours to look at. Default is 10.\nsigma - Distance scaling factor. Default is 50.\nweight_track - Returns a matrix which tracks the weight changes at each iteration. False by default\n\nExamples\nExamples involving implementations for the 3 primary Relief based algorithms are included in examples.py. The variable regressionProblem can be set to True for trying out RReliefF and False for Relief and ReliefF. The examples included deliberately include a feature with random values to demonstrate how the Relief based algorithms can weed out irrelevant features in both, regression and classification problems.\nRReliefF was also used in the following publication I co-authored (and serves as a good example of the algorithm\'s efficacy):\nA Comparative Study of Wavelet-based Descriptors for Fault Diagnosis of Self-Humidified Proton Exchange Membrane Fuel Cells\n'], 'url_profile': 'https://github.com/AmritSe', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['PyTorch-Logistic-Regression-Tutorial\n'], 'url_profile': 'https://github.com/Laxmidhar-ml', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'R', 'Updated Mar 13, 2020', '1', 'Updated Mar 12, 2020', 'Updated Mar 14, 2020', '2', 'R', 'MIT license', 'Updated Aug 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 16, 2020', '3', 'Python', 'Updated Mar 15, 2020', '3', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""huawei_credit_scoring\nThis is a simple logistic regression implemented machine learning project.\nIt didn't import extra lib such as scikit-learn or Pytorch\n""], 'url_profile': 'https://github.com/LeonardoLiberty', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'Jalandhar, India', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['MLproject\n'], 'url_profile': 'https://github.com/guptasahil2002', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'San Francisco, United States', 'stats_list': [], 'contributions': '704 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prabhatsharma', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Regression-Algorithms\nRegression Algorithms\n'], 'url_profile': 'https://github.com/kar6270', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'gurgaon', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ujjwalvarshney', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'www.zerihunassociates.com ', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['This is a very quick run-through of some basic statistical concepts, adapted from Lab 4 in Harvard\'s CS109 course. Please feel free to try the original lab if you\'re feeling ambitious :-) The CS109 git repository also has the solutions if you\'re stuck.\nLinear Regression Models\nPrediction using linear regression\nLinear regression is used to model and predict continuous outcomes with normal random errors. There are nearly an infinite number of different types of regression models and each regression model is typically defined by the distribution of the prediction errors (called ""residuals"") of the type of data. Logistic regression is used to model binary outcomes whereas Poisson regression is used to predict counts. In this exercise, we\'ll see some examples of linear regression as well as Train-test splits.\nThe packages we\'ll cover are: statsmodels, seaborn, and scikit-learn. While we don\'t explicitly teach statsmodels and seaborn in the Springboard workshop, those are great libraries to know.\n'], 'url_profile': 'https://github.com/abebual', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'Guatemala', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['Logistic_R\nLogistic Regression lab.\n'], 'url_profile': 'https://github.com/Fernando0107', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'Việt Nam', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NCKH_LR\nNCKH - logistic regression\n1. Set enviroment:\n\npython minimum version: >=3.6\ninstall virtual enviroment: pip3 install virtualenv\ncreate virtual enviroment: virtualenv $ENV_NAME\nactivate vitualenv (linux): source $ENV_NAME/bin/activate\ninstall list package : pip3 install -r requirements.txt\n\n\nENV_NAME=envLR\n\noptions\n\nadd virtual enviroment to jupyter notebook:  python3 -m ipykernel install --user --name=$ENV_NAME\ndeactivate (linux): deactivate\n\n2. Run:\npython3 index.py\n\nall model will be saved in result/model\naccuracy result will be wrote in file log\nROC training result saved in result/ROC\n\n'], 'url_profile': 'https://github.com/nhoxnho1212', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Logistic-Regression-1\nLogistic Regression 1\n'], 'url_profile': 'https://github.com/kar6270', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Deployment-Demo\nDeployment of Regression Model\n'], 'url_profile': 'https://github.com/priyakatrapally', 'info_list': ['Python', 'Updated May 24, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Jul 18, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 10, 2020']}"
"{'location': 'Mumbai / India', 'stats_list': [], 'contributions': '1,429 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mrpandey1', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Linear-Regression-1\nUnderstanding Linear Regression\n'], 'url_profile': 'https://github.com/RicardoFloresH', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hajalibayram', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['WIll edit it later\n'], 'url_profile': 'https://github.com/waleedsial', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'Faculty of Economics, CMU', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Kinkreg\n#To install this package, you have to install devtools package\nlibrary(devtools)\ninstall_github(""woraphonyamaka/kinkreg"", force=TRUE)\nlibrary(kinkreg)\n'], 'url_profile': 'https://github.com/woraphonyamaka', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'Milan, Italy', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['IBM Workshop: Future Sales Prediction\nThis challenge was done in occasion of a workshop held by IBM at MIP Politecnico di Milano and is taken from Kaggle challenge (https://www.kaggle.com/c/competitive-data-science-predict-future-sales).\nIn this competition, I worked with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. The task is to predict total sales for every product and store in the next month.\n'], 'url_profile': 'https://github.com/giumanto', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear_regression_basic\nLinear Regression using formula\n'], 'url_profile': 'https://github.com/sanjaycae', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'Cambridge, UK', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['QAFFP regression\nThis repository contains the 43 data sets extracted from ChEMBL database, code and scripts needed to reproduce the results reported in:\nQSAR-derived Affinity Fingerprints (part 2): modeling performance for cytotoxicity prediction\nIsidro Cortés-Ciriano, Ctibor Škuta, Andreas Bender, and Daniel Svozil\nRunning the code\nTo generate the results in our paper run run_QAFFP.py as indicated in the file launch_all.sh.\nAll the data sets used in the publications are in the folder: sdf_files.\n'], 'url_profile': 'https://github.com/isidroc', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/josejohny68', 'info_list': ['Python', 'Updated Sep 13, 2020', 'R', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Nov 13, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020']}"
"{'location': 'CA, Los Angles ', 'stats_list': [], 'contributions': '425 contributions\n        in the last year', 'description': ['Locally weighted Regression\nhttp-localhost-8888-notebooks-Locally-20weighted-20Regression.ipynb-\n'], 'url_profile': 'https://github.com/Mahrokh-Eb', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'Indonesia', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['pythonDS-Predict-Diamond-Price-\n'], 'url_profile': 'https://github.com/MuhammadHasbiAshshiddieqy', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'San Francisco Bay Area, California', 'stats_list': [], 'contributions': '624 contributions\n        in the last year', 'description': ['Predicting House Prices\nIn this project I focused on Exploratory data analysis (EDA) and advanced regression techniques. The dataset used can be found here: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n\nThere are five steps in the EDA:\n\nBasic Cleaning- handling missing data, outliers and categorical variables\nUnderstanding the problem and variables\nUnivariate study- about the dependent variable\nMultivariate study- relation between dependent and independent variables\nTest assumptions for multivariate techniques\n\n\nRegression techniques used are:\n\nLinear Regression with Ridge Regularization(L2 Penalty)\nLinear Regression with Lasso regularization (L1 penalty)\nLinear Regression with ElasticNet regularization (L1 and L2 penalty)\nStacked Linear Regression with LGBM, XGBoost, Gradient Boost and Kernel Ridge.\n\n\n\n'], 'url_profile': 'https://github.com/shweta-yadav15', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'India,Mumbai', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Temperature-Prediction\nCompare accuracy and error rate of different models (Linear Regression, Polynomial Regression, Decision tree, Random Forest Regression)\n'], 'url_profile': 'https://github.com/desaiankit911', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'Tempe, Arizona', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\nHouse Prices: Advanced Regression Techniques(A Regression Problem with Python) — Understand Regression Methods in Python with Scikit-Learn. Feel free to check out my article on Medium.\n'], 'url_profile': 'https://github.com/shirley0823', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/josejohny68', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'Meerut, Uttar Pradesh, India', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['Linear-Regression-Python\nMultivariate Linear Regression through Python\n'], 'url_profile': 'https://github.com/prinzz1208', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'Albany, NY', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Avocado-Regression\nRegression Analysis on Avocado Dataset\n'], 'url_profile': 'https://github.com/jpellitteri', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'Sydney', 'stats_list': [], 'contributions': '161 contributions\n        in the last year', 'description': [""Predict-Boston-House-Prices\nThis project enlists a list of basic and ensemble learning models used to predict Boston House Prices | Advanced\nRegression Techniques.\nBackground\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling\nor the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences\nprice negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition\nchallenges you to predict the final price of each home.\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible\nalternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\n""], 'url_profile': 'https://github.com/ucdcsl55', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}","{'location': 'Charlotte ', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Predictive-Analytics\nRegression and Ckustering\n'], 'url_profile': 'https://github.com/Venkatapathi-Sagi', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 9, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2021', 'Updated Mar 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aksinghal5590', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['com.evocomp.symbolicregression\nSymbolic Regression test using Jenetics\n'], 'url_profile': 'https://github.com/jamhb91', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['boston-housing-regression\nLinear Regression for Boston housing\n'], 'url_profile': 'https://github.com/richardyavits', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Pipeline for regression of Tumor Cell Percentages from mass-spectrometry data\nThis project uses drake library as a foundation of the project for regression analysis.\nRequirements\nThis project make use of the following packages:\n\n\n\npackage\nloadedversion\ndate\nsource\n\n\n\n\ndrake\n7.10.0\n2020-02-01\nCRAN (R 3.5.2)\n\n\nggplot2\n3.2.1\n2019-08-10\nCRAN (R 3.5.2)\n\n\ndplyr\n0.8.3\n2019-07-04\nCRAN (R 3.5.2)\n\n\nplyr\n1.8.4\n2016-06-08\nCRAN (R 3.5.0)\n\n\nggpmisc\n0.3.3\n2019-12-01\nCRAN (R 3.5.2)\n\n\ndata.table\n1.12.2\n2019-04-07\nCRAN (R 3.5.2)\n\n\nxgboost\n0.90.0.2\n2019-08-01\nCRAN (R 3.5.2)\n\n\nxtable\n1.8-4\n2019-04-21\nCRAN (R 3.5.2)\n\n\nMALDIquant\n1.19.3\n2019-05-12\nCRAN (R 3.5.2)\n\n\niml\n0.9.0\n2019-02-05\nCRAN (R 3.5.2)\n\n\ncaret\n6.0-84\n2019-04-27\nCRAN (R 3.5.2)\n\n\nrandomForest\n4.6-14\n2018-03-25\nCRAN (R 3.5.0)\n\n\ndoParallel\n1.0.15\n2019-08-02\nCRAN (R 3.5.2)\n\n\nSHAPforxgboost\n0.0.4\n2020-05-14\nCRAN (R 3.5.1)\n\n\n\nRun\nBefor run the project make sure that dpath variable in the function.R point to the directory where data files are. To run just execute the following command in the project directory:\nRscript make_static.R\n\n'], 'url_profile': 'https://github.com/smartscalpel', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Portugal', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['ml_linear_regression\nFirst Machine Learning Project: Linear Regression\nDataset used:\nhttps://www.kaggle.com/nnqkfdjq/statistics-observation-of-random-youtube-video\n'], 'url_profile': 'https://github.com/tomasfj', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/daryagah', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['lassocode\nbasic code for lasso regression\n'], 'url_profile': 'https://github.com/zenosyne1128', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Speed and Object Detection from Visual Feed (Video)\nTeam: Akash, Bonga & Kartik\nIntro\nCurrently, basic object detection enables users to understand higher-level abstractions\nof images but however limits the user’s perception. In contrast to just displaying labeled\ninfo and not other relevant details, our project will explored a scenario based on a\ndashcam video, for use-case in the field of self-driving cars and navigation.\nOur submisison expands to three sections and has worked on a speed detection challenge, attainign promising results with enough scope to expand beyond the videos, towards real-time use and for working more on advanced DNN models.\n\nDownload the files for training and testing (data folder) from the comma.ai GitHub repository.\n\nPipeline and architecture:\nMain Tools: Caffe, PyTorch, CNNs, Single-shot MultiBox detectors, OpenCV\n\n\nDL model - initial architecture & training\n\n\nTo begin our exploration, we decided to start with a basic 8 layer convulational neural net. This net structure is an adaption of tensor flows implementation of a convolution net. However, instead of using the exact net structure, we altered its input dimension and output dimensions as well as its convolution layers in order to successful run our model on our video dataset.\nThe results of the model can be found at the bottom of the the ""Speed Prediction DL.ipnyb"" in the file ""DLNotebook"". The overall performance of this model indicated that there was not a significant predictive aspect of the model.\n\n\n\nUsing Optical Flow (MSE ~5)\n\n\nThis approach has expanded upon the baseline model and incorporated the techniques from Nvidia\'s paper for object detection, mainly capitalizing on Optical Flow and image transformations to train the Neural Net.\n\ni. Harnessing optical flow and training from consecutive image captures from video\nii. Incorporation of Multi-box detection & future work towards Real time detection\n \n\nAfter initial training and preprocessing, we observe that consectuive and multiple images are better than single images, pulled from the mp4 file.\n\n\nLinear Regression Model\n\n\nApart from DL curiosity and above models, we explored a bare-bones architecture built for object detection, and these files are available in the \'linearRegression\' folder.\nThis model has achieved an MSE value close to 6, but has used significantly lesser compute power. However, we see merit in expanding more for stage 2 model towards real-time detection with Caffe support.\n\n \nImplementation\n\n\nExplore the ipynb and supported .py files\n\n\nEnsure the supported libraries are installed (argparse, OpenCV/cv2, numpy)\n\n\n\nKindly refer to final-rerun.py file for looking at our model. All the comments illsutrate each section from preprocessing, training and testing, with results saved in the \'dl-opt-flow\' folder.\nFor explanding and studying real-time detection, please see:\n\n\npython comma_play.py --video train.mp4\n\nFor this case, the pre-trained model and weidhts are needed, from MobileNetSSD, available online.\n\nPlease refer to solvers.py file and the generated graphs in lin-reg directory\n\nFuture\nThe scope of this project and our curriculum has allowed us to deliver on the two models above. Beyond this, it can be expanded to Sensor Fusion, LIDAR or image processing problems and for domain of autonomous driving.\n'], 'url_profile': 'https://github.com/bongam12', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/josejohny68', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Housing sale price prediction on Ames Dataset\nHouse Prices: Advanced Regression Techniques\nParticipated in Kaggle Competition - https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/ovsienkobohdan', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'Updated Mar 9, 2020', 'R', 'MIT license', 'Updated Jan 15, 2021', 'Python', 'Updated Mar 16, 2020', 'HTML', 'Updated Mar 10, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Jun 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}"
"{'location': 'Houghton, USA', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['RegressionProject\nThis is linear regression project with cross-validation implemented.\n'], 'url_profile': 'https://github.com/VishalDevnale', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nihachay', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'Duke University', 'stats_list': [], 'contributions': '320 contributions\n        in the last year', 'description': ['\nExample ""broken-linear"" model distribution fit to interstellar extinction model parameter data (see Konz 2020, in preparation) using the TRK statistic. Shaded regions indicate the 1σ, 2σ and 3σ confidence regions of the model distribution.\nWhat is the TRK Statistic?\nRobustly fitting a statistical model to data is a task ubiquitous to practically all data-driven fields, but the more nonlinear, uncertain and/or scattered the dataset is, the more diffcult this task becomes. In the common case of two dimensional models (i.e. one independent variable x and one dependent variable y(x)), datasets with intrinsic uncertainties, or error bars, along both x and y prove diffcult to \x0cfit to in general, and if the dataset has some extrinsic uncertainty/scatter (i.e., sample variance) that cannot be accounted for solely by the error bars, the difficulty increases still.\nHere, we introduce a novel statistic (the Trotter, Reichart, Konz statistic, or TRK) developed that is advantageous towards model-fi\x0ctting in this ""worst-case data"" scenario, especially when compared to other methods. Models predicted by maximizing the TRK statistic\'s likelihood function are geometrically equivalent to models that minimize the sum of the squares of the radial distances of each datapoint centroid from the model curve.\n\nIllustration of the basic geometry of the TRK statistic given a single\ndatapoint and model curve/distribution, from Trotter 2011. The datapoint is centered at (xn; yn) (point O), with error\nellipse described by the widths (Σx,n,Σy,n) that combines the intrinsic 2D uncertainty of the datapoint with the extrinsic 2D uncertainty of the model/dataset in general. The model curve yc(x; θ) is tangent to the error ellipse at tangent point (xt,n, yt,n) (point T), and the red line is the linear approximation of the model curve. The blue line indicates the rotated coordinate axis un for the TRK statistic (see Konz 2020, in this repo).)\nHow to Use the TRK Statistical Suite\nThis statistic, originally introduced in Trotter 2011 is now implemented as a suite of fitting algorithms in C++ that comes equipped with many capabilities, including:\n1. Support for any nonlinear model;\n2. Probability distribution generation, correlation removal and custom priors for model parameters;\n3. asymmetric 2D uncertainties in the data and/or model, and more.\nWe also have built a web-based fitting calculator (here) through which the algorithm can be used easily, but generally, with a high degree of customizability.\nThe most recent/current documentation and rigorous/thorough introduction of the statistic/the TRK suite is given in Konz 2020, (in this repo), in preparation. A separate, standalone documentation for the source code itself is also given in the documentation folder.\n\nProbability distributions of model and extrinsic variance parameters describing the example ""broken-linear"" model fit shown in the first figure, generated using Markov Chain-Monte Carlo methods and the TRK statistic. Distributions of individual parameters are along outside columns, while 2D joint distributions are given as confidence ellipses along the center column, with 1σ, 2σ and 3σ confidence regions.\nLicensing and Citation\nThe TRK suite is free to use for academic and non-commercial applications (see license in this repository). We only ask that you cite Trotter, Reichart and Konz 2020, in preparation.\nFor commercial applications, or consultation, feel free to contact us.\nNick Konz, Dan Reichart, Adam Trotter\nDepartment of Physics and Astronomy\nUniversity of North Carolina at Chapel Hill\n'], 'url_profile': 'https://github.com/nickk124', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'Somewhere Remote on Mars ', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Allaye', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['LendingClub-Loan-Defult-Rate-Prediction\n'], 'url_profile': 'https://github.com/yyyoment', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['Machine-learning-simple-model\nLinear and Logistic regression from scratch\nTwo jupiter notebook to create machine learning model from scratch.\nPlan the demand of rental bike demand with linear regression and Analyse Titanic data with logistic regression model.\n'], 'url_profile': 'https://github.com/seim13', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Ames House Prices\n\nIntroduction\nYou have just joined a new ""full stack"" real estate company in Ames, Iowa. The strategy of the firm is two-fold:\n\nOwn the entire process from the purchase of the land all the way to sale of the house, and anything in between.\nUse statistical analysis to optimize investment and maximize return.\nThe company is still small, and though investment is substantial the short-term goals of the company are more oriented towards purchasing existing houses and flipping them as opposed to constructing entirely new houses. That being said, the company has access to a large construction workforce operating at rock-bottom prices.\n\n1. Estimating the value of homes from fixed characteristics\nDevelop an algorithm to reliably estimate the value of residential houses based on fixed characteristics.\n2. Determine any value of changeable property characteristics unexplained by the fixed ones\nIdentify characteristics of houses that the company can cost-effectively change/renovate with their construction team.\n3. What property characteristics predict an ""abnormal"" sale?\nEvaluate the mean dollar value of different renovations.\nOverall Approach\nData Pre-processing\n\nCleaning and Feature-Engineering of data\nIdentify fixed vs changeable features\nFor the 3rd step, my first use of SMOTE to address class imbalance.\n\nModelling\n\nLinear Regression used to regress vs fixed characteristics, with Regularizaion to effectively remove unimportant features and colinearity\nIsolate residuals and model those separately as being influence by changeable features\n\nEvaluation\n\nEvaluate coefficients for reasonableness\nTrain a model on pre-2010 data and evaluate its performance on the 2010 houses.\n\n'], 'url_profile': 'https://github.com/noahberhe', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'Bengaluru, KA', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kaiwalya2502', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'www.zerihunassociates.com ', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Classification\nNote: We\'ve adapted this Mini Project from Lab 5 in the CS109 course. Please feel free to check out the original lab, both for more exercises, as well as solutions.\nWe turn our attention to classification. Classification tries to predict, which of a small set of classes, an observation belongs to. Mathematically, the aim is to find 𝑦, a label based on knowing a feature vector 𝐱. For instance, consider predicting gender from seeing a person\'s face, something we do fairly well as humans. To have a machine do this well, we would typically feed the machine a bunch of images of people which have been labelled ""male"" or ""female"" (the training set), and have it learn the gender of the person in the image from the labels and the features used to determine gender. Then, given a new photo, the trained algorithm returns us the gender of the person in the photo.\nThere are different ways of making classifications. One idea is shown schematically in the image below, where we find a line that divides ""things"" of two different types in a 2-dimensional feature space. The classification show in the figure below is an example of a maximum-margin classifier where construct a decision boundary that is far as possible away from both classes of points. The fact that a line can be drawn to separate the two classes makes the problem linearly separable. Support Vector Machines (SVM) are an example of a maximum-margin classifier.\nTuning the Model\nThe model has some hyperparameters we can tune for hopefully better performance. For tuning the parameters of your model, you will use a mix of cross-validation and grid search. In Logistic Regression, the most important parameter to tune is the regularization parameter C. Note that the regularization parameter is not always part of the logistic regression model.\nThe regularization parameter is used to control for unlikely high regression coefficients, and in other cases can be used when data is sparse, as a method of feature selection.\nYou will now implement some code to perform model tuning and selecting the regularization parameter 𝐶.\nWe use the following cv_score function to perform K-fold cross-validation and apply a scoring function to each test fold. In this incarnation we use accuracy score as the default scoring function.\n'], 'url_profile': 'https://github.com/abebual', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}","{'location': 'West Lafayette, IN', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['This was a group project completed in a graduate regression course. Our goal was to identify which of the 79 variables had an impact on house prices and to examine their effect sizes. In this project, there is:\n\nThe R code to perform the data cleaning, analysis, and generate the plots.\nA slide deck that summarizes the main points of the project.\nThe full report complete with all details and discussion of the methodology.\n\n'], 'url_profile': 'https://github.com/loganbradley', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 9, 2020', '1', 'C++', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'R', 'Updated Mar 15, 2020']}"
"{'location': 'West Lafayette, IN', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['This was a group project completed in a graduate regression course. Our goal was to identify which of the 79 variables had an impact on house prices and to examine their effect sizes. In this project, there is:\n\nThe R code to perform the data cleaning, analysis, and generate the plots.\nA slide deck that summarizes the main points of the project.\nThe full report complete with all details and discussion of the methodology.\n\n'], 'url_profile': 'https://github.com/loganbradley', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chaunguyen812', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic-Regression-using-Stata\nLogistic Regression using Stata by Packt Publishing\n'], 'url_profile': 'https://github.com/PacktPublishing', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'KUET', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Logistic Regression Classification\nLoan Status Prediction Using Logistic Regression Classification.\n'], 'url_profile': 'https://github.com/Asraf047', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Industry_relevance_linearRegression\nIndustry relevance project using Linear Regression\n'], 'url_profile': 'https://github.com/chandan54', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kashten', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Logistic_Regression_Python\nImplementation of Logisitic Regression Algorithm using Python.\n'], 'url_profile': 'https://github.com/gauthampkrishnan', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khanjandharaiya', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/siglezmus', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Credit-Card-Approvals\nLogistic regression based Credit Card Applications\n'], 'url_profile': 'https://github.com/mehmath38', 'info_list': ['R', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 10, 2020', 'TeX', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'CSS', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}"
"{'location': 'Cairo', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['A-Deep-Regression-Model-in-Keras\nA Deep Regression Model in Keras\nBuild a regression model using the deep learning Keras library with increasing the number of training epochs and changing number of hidden layers.\nthese experiments show how changing these parameters impacts the performance of the model.\n'], 'url_profile': 'https://github.com/Moustafazn', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Vellore', 'stats_list': [], 'contributions': '548 contributions\n        in the last year', 'description': ['Introduction to Machine Learning\nThis is a collection of the regression algorithms used and implemented in training ML models\n\nMultiple Linear Regression\nPolynomial Regression\nSupport Vector Regression\nDecision Tree Regression\nRandom Forest Regression\nLogistic Regression\n\n'], 'url_profile': 'https://github.com/VaishnaviNandakumar', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/apag101', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Mohali, India', 'stats_list': [], 'contributions': '167 contributions\n        in the last year', 'description': ['Multivariate-LinearRegression-Implementation\nImplementation of Multi-variate Linear Regression from scratch on self generated data, using NumPy. Generation of data is done in the jupyter notebook itself.\n'], 'url_profile': 'https://github.com/KaranjotSV', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Boulder, CO', 'stats_list': [], 'contributions': '730 contributions\n        in the last year', 'description': ['Three Ways to Perform Linear Regression\nIn this repository I demonstrate three main ways to perform univariate linear regression in Python.\n'], 'url_profile': 'https://github.com/BigBangData', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AakankshaBaid', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pragyabhandari', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['simpleLinearRegression\nMy first project on Simple Linear Regression\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/thareddy', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '601 contributions\n        in the last year', 'description': ['###############################################################\n###### Auther: Asiri Amal K                                 ###\n###### Description : Car Price Prediction model in ML       ###\n###############################################################\n#Imports\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\ncar = pd.read_csv(""DatasetADs.csv"")\nData Mining and Preprocessing\ncar.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMileage_Min\nMileage_Max\nprice\nYear\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\ncount\n32395.000000\n32395.000000\n3.239500e+04\n32395.000000\n3.239500e+04\n3.239500e+04\n32395.000000\n\n\nmean\n116592.066677\n131076.174070\n1.054981e+05\n2010.798796\n2.157886e+05\n2.907133e+05\n9.201204\n\n\nstd\n75722.653802\n92740.841965\n8.690894e+04\n4.373741\n1.449547e+05\n2.233345e+05\n4.373741\n\n\nmin\n0.000000\n4999.000000\n1.050000e+04\n2001.000000\n9.790000e+04\n9.990000e+04\n1.000000\n\n\n25%\n60000.000000\n64999.000000\n6.200000e+04\n2008.000000\n1.247000e+05\n1.711000e+05\n6.000000\n\n\n50%\n110000.000000\n119999.000000\n8.600000e+04\n2011.000000\n1.610000e+05\n2.199000e+05\n9.000000\n\n\n75%\n170000.000000\n179999.000000\n1.200000e+05\n2014.000000\n2.700000e+05\n3.589000e+05\n12.000000\n\n\nmax\n500000.000000\n1000000.000000\n9.100000e+06\n2019.000000\n1.304000e+06\n2.660000e+06\n19.000000\n\n\n\n\ncar.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nMileage_Min\nMileage_Max\nprice\nserie\nMake\nDate_ads\nFuel\nYear\nCountry_Make\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\n0\nCasablanca\n35000\n39999\n295000\nGIULIA\nAlfa Romeo\n4/3/2019\nDiesel\n2017\nItalie\n369900\n494900\n3\n\n\n1\nCasablanca\n10000\n14999\n450000\nGIULIA\nAlfa Romeo\n7/7/2019\nDiesel\n2019\nItalie\n369900\n494900\n1\n\n\n2\nMohammedia\n130000\n139999\n102000\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010\nItalie\n229900\n279900\n10\n\n\n3\nMohammedia\n130000\n139999\n102000\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010\nItalie\n229900\n279900\n10\n\n\n4\nCasablanca\n200000\n249999\n100000\nGiulietta\nAlfa Romeo\n7/8/2019\nDiesel\n2011\nItalie\n229900\n279900\n9\n\n\n\n\nDataset has 13 classes\nprint(len(car.columns))\ncar.columns\n13\n\n\n\n\n\nIndex([\'City\', \'Mileage_Min\', \'Mileage_Max\', \'price\', \'serie\', \'Make\',\n       \'Date_ads\', \'Fuel\', \'Year\', \'Country_Make\', \'min_prix_neuf\',\n       \'max_prix_neuf\', \'Age\'],\n      dtype=\'object\')\n\ncar.dtypes\nCity             object\nMileage_Min       int64\nMileage_Max       int64\nprice             int64\nserie            object\nMake             object\nDate_ads         object\nFuel             object\nYear              int64\nCountry_Make     object\nmin_prix_neuf     int64\nmax_prix_neuf     int64\nAge               int64\ndtype: object\n\nNo missing values in the dataset\ncar.isnull().sum()\nCity             0\nMileage_Min      0\nMileage_Max      0\nprice            0\nserie            0\nMake             0\nDate_ads         0\nFuel             0\nYear             0\nCountry_Make     0\nmin_prix_neuf    0\nmax_prix_neuf    0\nAge              0\ndtype: int64\n\nCategorical classes - 6\n\nCity\nserie\nMake\nDate_ads\nFuel\nCountry_Make\n\ncat_classes = car.select_dtypes(include=object).columns\nnum_classes = car.select_dtypes(include=np.int64).columns\nprint(len(cat_classes))\nprint(cat_classes)\nprint(num_classes)\n\n#Sample Categorical values\ncar.select_dtypes(include=object).head()\n6\nIndex([\'City\', \'serie\', \'Make\', \'Date_ads\', \'Fuel\', \'Country_Make\'], dtype=\'object\')\nIndex([\'Mileage_Min\', \'Mileage_Max\', \'price\', \'Year\', \'min_prix_neuf\',\n       \'max_prix_neuf\', \'Age\'],\n      dtype=\'object\')\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nserie\nMake\nDate_ads\nFuel\nCountry_Make\n\n\n\n\n0\nCasablanca\nGIULIA\nAlfa Romeo\n4/3/2019\nDiesel\nItalie\n\n\n1\nCasablanca\nGIULIA\nAlfa Romeo\n7/7/2019\nDiesel\nItalie\n\n\n2\nMohammedia\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\nItalie\n\n\n3\nMohammedia\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\nItalie\n\n\n4\nCasablanca\nGiulietta\nAlfa Romeo\n7/8/2019\nDiesel\nItalie\n\n\n\n\nCIty - Class\n\nThere are 249 Cities in this category,\nMaximum number of vehicles can be found in \'Casablanca\' and it is 10168\nThe second one is \'Rabat\' but it does have lesser than half of vehicles in \'Casablanca\' and it is 3141\n\nClass Category will not give valuable information for the machine learning predictions  because of the ""Curse of Cardinality"" (high dimentionality in feature space).\nprint(car[\'City\'].value_counts().shape)\ncar[\'City\'].value_counts()\n(249,)\n\n\n\n\n\nCasablanca                 10168\nRabat                       3141\nMarrakech                   2268\nTanger                      2217\nFÃ¨s                         1666\nAgadir                      1657\nSalÃ©                        1111\nMeknÃ¨s                      1057\nKÃ©nitra                     1000\nEl Jadida                    673\nTÃ©touan                      670\nMohammedia                   568\nOujda                        489\nTemara                       416\nSafi                         366\nBÃ©ni Mellal                  361\nNador                        314\nKhouribga                    255\nSettat                       225\nTaza                         211\nBerrechid                    197\nLaÃ¢youne                     172\nOuarzazate                   152\nLarache                      115\nKhÃ©nifra                     110\nBerkane                      107\nErrachidia                   105\nKhemisset                     98\nGuelmim                       77\nDakhla                        74\n                           ...  \nHad Kourt                      1\nAgdz                           1\nSidi Smail                     1\nBeni Malek                     1\nGuigou                         1\nOulad Abbou                    1\nSouk Tlet El Gharb             1\nAdar                           1\nDcheira                        1\nTahla                          1\nLkhaloua                       1\nBoufkrane                      1\nSidi Slimane Echcharraa        1\nAkhfennir                      1\nFreija                         1\nOunagha                        1\nN\'khila                        1\nSid L\'mokhtar                  1\nFarkhana                       1\nSidi Yahya                     1\nBouarfa                        1\nBÃ©ni Yakhlef                   1\nSouihla                        1\nBoudinar                       1\nEl Mansouria                   1\nBouzemmour                     1\nBoumalne                       1\nM\'haya                         1\nFiguig                         1\nAoulouz                        1\nName: City, Length: 249, dtype: int64\n\n# Dropping city\ncars = car.drop(""City"", axis=1)\nserie - Class\n\nHighly Skewed\n\nprint(car[\'serie\'].value_counts().shape)\ncar[\'serie\'].value_counts()\n(181,)\n\n\n\n\n\nLogan                2858\nKangoo               1962\nClio                 1796\nPartner              1074\nFiesta               1055\nBerlingo              977\nPicanto               822\nSandero               802\nMegane                781\nPolo                  744\ni 10                  727\nDuster                689\nDokker                674\nPassat                661\nAccent                598\nCorolla               538\nClasse C              533\nPunto                 504\n308                   424\nTouran                422\nAstra                 420\nTiguan                384\nSanta Fe              363\nGOLF 7                361\nDoblo                 350\nAccord                349\nCaddy                 346\nTouareg               345\n208                   330\nC3                    312\n                     ... \nClasse GLC              8\nVelar                   8\nRenegade                7\nXJ                      7\n911 Carrera             6\nClasse CL               6\nTT                      6\nX4                      5\ncabrio                  5\nCLASSE C COUPE          5\nCayman                  4\nXC90                    4\nMegane Estate           3\nCOMBO                   3\nV40 Cross Country       3\nMOKKA                   3\nXE                      3\nHR-V                    2\nBoxster                 2\nRX                      2\nCLUBMAN                 2\nPrius                   2\nGIULIA                  2\nClasse GLS              1\nYaris Verso             1\nF-Type                  1\nSERIE 3 GT              1\nCX-3                    1\nLS                      1\nF-PACE                  1\nName: serie, Length: 181, dtype: int64\n\nMake Class\n\n29 Categories\nHighly Skewed dataset\n\nSince the dataset is highly skewed we cannot most probably high count values like ""Rrenoult"", ""Dacia"", ""Volkswagon"" will be give more accurate results but ""Lexus"", ""Mini"", ""Jaguar"" like lesser counts will not.\nIn general make will be a huge impact for a car second hand market so it is better to use this category for the predictions, The accuracy will increase with the size of the dataset\nprint(car[\'Make\'].value_counts().shape)\ncar[\'Make\'].value_counts()\n(29,)\n\n\n\n\n\nRenault          5239\nDacia            5094\nVolkswagen       3610\nPeugeot          2662\nHyundai          2246\nCitroen          1748\nFord             1371\nFiat             1217\nKia              1161\nMercedes-Benz    1111\nToyota           1091\nAudi             1034\nBMW               876\nHonda             876\nOpel              712\nNissan            501\nSeat              431\nLand Rover        306\nSkoda             243\nJeep              151\nSsangyong         136\nMitsubishi        123\nVolvo             119\nAlfa Romeo         99\nPorsche            94\nMazda              70\nJaguar             44\nmini               27\nLexus               3\nName: Make, dtype: int64\n\nDate_ads - Class\n\n218 Occurences\nNo value at all for the prediction\n\nprint(car[\'Date_ads\'].value_counts().shape)\ncar[\'Date_ads\'].value_counts()\n(218,)\n\n\n\n\n\n9/15/2019     1256\n9/14/2019      922\n9/13/2019      786\n9/12/2019      688\n9/10/2019      644\n9/11/2019      638\n9/16/2019      613\n9/9/2019       569\n9/8/2019       488\n9/2/2019       485\n9/7/2019       475\n9/3/2019       474\n9/5/2019       444\n7/30/2019      441\n9/1/2019       426\n9/6/2019       419\n8/27/2019      414\n6/21/2019      409\n9/4/2019       409\n8/26/2019      406\n7/10/2019      397\n8/28/2019      394\n6/25/2019      392\n8/29/2019      391\n6/26/2019      385\n7/15/2019      381\n8/31/2019      371\n7/3/2019       371\n7/9/2019       371\n7/11/2019      365\n              ... \n11/15/2018       1\n3/6/2019         1\n1/17/2019        1\n2/19/2019        1\n4/22/2019        1\n3/30/2019        1\n2/21/2019        1\n3/13/2019        1\n3/8/2019         1\n3/4/2019         1\n4/1/2019         1\n11/21/2018       1\n11/18/2018       1\n2/14/2019        1\n2/11/2019        1\n12/25/2018       1\n1/18/2019        1\n2/16/2019        1\n3/19/2019        1\n3/10/2019        1\n1/26/2019        1\n5/7/2019         1\n12/8/2018        1\n12/24/2018       1\n1/10/2019        1\n1/31/2019        1\n3/5/2019         1\n4/10/2019        1\n3/16/2019        1\n2/27/2019        1\nName: Date_ads, Length: 218, dtype: int64\n\n# Drop Date_ads class\ncars = cars.drop(""Date_ads"", axis=1)\nFuel - Class\n\nThis will be a valuable information\n\nprint(car[\'Fuel\'].value_counts().shape)\ncar[\'Fuel\'].value_counts()\n(5,)\n\n\n\n\n\nDiesel        26033\nEssence        6321\nHybride          24\nElectrique       14\nLPG               3\nName: Fuel, dtype: int64\n\nCountry_Make - Class\n\nValuable Information\n\nprint(car[\'Country_Make\'].value_counts().shape)\ncar[\'Country_Make\'].value_counts()\n(11,)\n\n\n\n\n\nFrance          9649\nAllemagne       7464\nRoumanie        5094\nCorÃ©e du sud    3543\nJapon           2664\nUSA             1522\nItalie          1316\nEspagne          431\nRoyaume-Uni      350\nTchÃ©quie         243\nSuÃ¨de            119\nName: Country_Make, dtype: int64\n\nDroped Features : Date_ads, City\nUseful Features: serie, Make, Fuel, Country_Make\nNumaric Classes - 7\n\nAge\nYear\nMileage_Min\nMileage_Max\nmin_prix_neuf\nmax_prix_neuf\nPrice\n\ncar.hist(bins=50, figsize=(20,15))\nplt.show()\n\ncar[\'price\'].hist(bins=500, figsize=(10,5))\nplt.xlim(0, 350000)\nplt.show()\n\nmax(car[\'price\'])\n9100000\n\nYear Represents the same data of Age\nplt.plot(car[\'Age\'].value_counts(ascending=True), car[\'Year\'].value_counts(ascending=True))\nplt.plot()\n# print(car[\'Age\'].value_counts(ascending=True))\n# print(car[\'Year\'].value_counts(ascending=True))\n[]\n\n\nNotice a few things in these histograms:\n\nYear and Age represents the same information so further calculation it will only use the Age\nMin-Max Milage, Min-Max Price, and Price has Outliers that should be removed\nThese attributes have very different scales.\nFinally, many histograms are tail heavy: they extend much farther to the right of\nthe median than to the left. This may make it a bit harder for some Machine\nLearning algorithms to detect patterns. We will try transforming these attributes\nlater on to have more bell-shaped distributions\n\nCar Price has huge outliers\nplt.boxplot(cars[\'price\'], vert = False)\nplt.show()\n\nValues Greater than 500,000\ncar.where(car[\'price\'] > 500000).dropna()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nMileage_Min\nMileage_Max\nprice\nserie\nMake\nDate_ads\nFuel\nYear\nCountry_Make\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\n397\nRabat\n5000.0\n9999.0\n510000.0\nA3\nAudi\n7/13/2019\nDiesel\n2019.0\nAllemagne\n305000.0\n445000.0\n1.0\n\n\n828\nCasablanca\n40000.0\n44999.0\n890000.0\nA8\nAudi\n7/10/2019\nDiesel\n2018.0\nAllemagne\n1250000.0\n1450000.0\n2.0\n\n\n829\nBÃ©ni Mellal\n0.0\n4999.0\n1000000.0\nA8\nAudi\n7/9/2019\nDiesel\n2019.0\nAllemagne\n1250000.0\n1450000.0\n1.0\n\n\n1024\nRabat\n60000.0\n64999.0\n510000.0\nQ5\nAudi\n9/12/2019\nDiesel\n2016.0\nAllemagne\n499000.0\n659000.0\n4.0\n\n\n1041\nMarrakech\n5000.0\n9999.0\n620000.0\nQ5\nAudi\n9/13/2019\nDiesel\n2018.0\nAllemagne\n499000.0\n659000.0\n2.0\n\n\n1116\nCasablanca\n35000.0\n39999.0\n630000.0\nQ7\nAudi\n9/16/2019\nDiesel\n2016.0\nAllemagne\n678000.0\n918000.0\n4.0\n\n\n1117\nCasablanca\n35000.0\n39999.0\n630000.0\nQ7\nAudi\n9/16/2019\nDiesel\n2016.0\nAllemagne\n678000.0\n918000.0\n4.0\n\n\n1118\nCasablanca\n40000.0\n44999.0\n620000.0\nQ7\nAudi\n9/15/2019\nDiesel\n2016.0\nAllemagne\n678000.0\n918000.0\n4.0\n\n\n1119\nCasablanca\n110000.0\n119999.0\n660000.0\nQ7\nAudi\n7/15/2019\nDiesel\n2016.0\nAllemagne\n678000.0\n918000.0\n4.0\n\n\n1120\nRabat\n30000.0\n34999.0\n600000.0\nQ7\nAudi\n7/23/2019\nDiesel\n2016.0\nAllemagne\n678000.0\n918000.0\n4.0\n\n\n1122\nCasablanca\n75000.0\n79999.0\n520000.0\nQ7\nAudi\n9/7/2019\nDiesel\n2016.0\nAllemagne\n678000.0\n918000.0\n4.0\n\n\n1125\nRabat\n70000.0\n74999.0\n670000.0\nQ7\nAudi\n7/2/2019\nDiesel\n2017.0\nAllemagne\n678000.0\n918000.0\n3.0\n\n\n1126\nAgadir\n40000.0\n44999.0\n800000.0\nQ7\nAudi\n9/15/2019\nDiesel\n2018.0\nAllemagne\n678000.0\n918000.0\n2.0\n\n\n1140\nCasablanca\n130000.0\n139999.0\n610000.0\nSerie 1\nBMW\n9/8/2019\nEssence\n2005.0\nAllemagne\n338000.0\n420000.0\n15.0\n\n\n1572\nRabat\n25000.0\n29999.0\n520000.0\nSerie 4\nBMW\n8/21/2019\nDiesel\n2017.0\nAllemagne\n592000.0\n771000.0\n3.0\n\n\n1578\nCasablanca\n35000.0\n39999.0\n740000.0\nSerie 4\nBMW\n6/21/2019\nDiesel\n2017.0\nAllemagne\n592000.0\n771000.0\n3.0\n\n\n1579\nCasablanca\n40000.0\n44999.0\n740000.0\nSerie 4\nBMW\n6/21/2019\nDiesel\n2017.0\nAllemagne\n592000.0\n771000.0\n3.0\n\n\n1584\nCasablanca\n35000.0\n39999.0\n555000.0\nSerie 4\nBMW\n9/10/2019\nDiesel\n2019.0\nAllemagne\n592000.0\n771000.0\n1.0\n\n\n1791\nCasablanca\n20000.0\n24999.0\n620000.0\nSerie 5\nBMW\n8/31/2019\nDiesel\n2018.0\nAllemagne\n505000.0\n771000.0\n2.0\n\n\n1820\nRabat\n110000.0\n119999.0\n760000.0\nSerie 7\nBMW\n8/29/2019\nDiesel\n2016.0\nAllemagne\n1003000.0\n1355000.0\n4.0\n\n\n1821\nCasablanca\n5000.0\n9999.0\n810000.0\nSerie 7\nBMW\n8/22/2019\nDiesel\n2018.0\nAllemagne\n1003000.0\n1355000.0\n2.0\n\n\n1822\nCasablanca\n0.0\n4999.0\n995000.0\nSerie 7\nBMW\n7/30/2019\nDiesel\n2018.0\nAllemagne\n1003000.0\n1355000.0\n2.0\n\n\n1884\nCasablanca\n75000.0\n79999.0\n510000.0\nX4\nBMW\n8/26/2019\nDiesel\n2014.0\nAllemagne\n580000.0\n740000.0\n6.0\n\n\n3447\nMeknÃ¨s\n200000.0\n249999.0\n700000.0\nC5\nCitroen\n6/21/2019\nDiesel\n2001.0\nFrance\n339900.0\n389500.0\n19.0\n\n\n9518\nTanger\n35000.0\n39999.0\n800000.0\nPanda\nFiat\n6/25/2019\nEssence\n2017.0\nItalie\n103900.0\n125900.0\n3.0\n\n\n14561\nCasablanca\n0.0\n4999.0\n550000.0\nF-PACE\nJaguar\n8/4/2019\nDiesel\n2017.0\nRoyaume-Uni\n512800.0\n913400.0\n3.0\n\n\n14620\nKÃ©nitra\n25000.0\n29999.0\n560000.0\nCherokee\nJeep\n6/28/2019\nDiesel\n2008.0\nUSA\n406500.0\n449000.0\n12.0\n\n\n14728\nMeknÃ¨s\n0.0\n4999.0\n600000.0\nGrand Cherokee\nJeep\n7/2/2019\nDiesel\n2019.0\nUSA\n489000.0\n669000.0\n1.0\n\n\n14755\nCasablanca\n0.0\n4999.0\n650000.0\nWrangler\nJeep\n8/20/2019\nEssence\n2019.0\nUSA\n381350.0\n514000.0\n1.0\n\n\n15987\nCasablanca\n70000.0\n74999.0\n530000.0\nRange Rover\nLand Rover\n9/14/2019\nDiesel\n2013.0\nRoyaume-Uni\n994800.0\n1675600.0\n7.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17396\nOujda\n5000.0\n9999.0\n920000.0\nClasse GLC\nMercedes-Benz\n6/26/2019\nEssence\n2019.0\nAllemagne\n499000.0\n754000.0\n1.0\n\n\n17399\nCasablanca\n30000.0\n34999.0\n660000.0\nClasse GLE\nMercedes-Benz\n9/14/2019\nDiesel\n2016.0\nAllemagne\n669000.0\n999000.0\n4.0\n\n\n17400\nTÃ©touan\n5000.0\n9999.0\n590000.0\nClasse GLE\nMercedes-Benz\n7/29/2019\nDiesel\n2016.0\nAllemagne\n669000.0\n999000.0\n4.0\n\n\n17401\nTanger\n70000.0\n74999.0\n640000.0\nClasse GLE\nMercedes-Benz\n7/27/2019\nDiesel\n2016.0\nAllemagne\n669000.0\n999000.0\n4.0\n\n\n17402\nOujda\n45000.0\n49999.0\n650000.0\nClasse GLE\nMercedes-Benz\n7/11/2019\nDiesel\n2016.0\nAllemagne\n669000.0\n999000.0\n4.0\n\n\n17403\nTÃ©touan\n110000.0\n119999.0\n900000.0\nClasse GLE\nMercedes-Benz\n7/25/2019\nEssence\n2016.0\nAllemagne\n669000.0\n999000.0\n4.0\n\n\n17404\nRabat\n70000.0\n74999.0\n560000.0\nClasse GLE\nMercedes-Benz\n7/28/2019\nDiesel\n2017.0\nAllemagne\n669000.0\n999000.0\n3.0\n\n\n18058\nCasablanca\n10000.0\n14999.0\n750000.0\nADAM\nOpel\n9/10/2019\nDiesel\n2013.0\nAllemagne\n132900.0\n175000.0\n7.0\n\n\n21436\nCasablanca\n30000.0\n34999.0\n570000.0\nCayman\nPorsche\n8/2/2019\nDiesel\n2017.0\nAllemagne\n650000.0\n810000.0\n3.0\n\n\n21437\nCasablanca\n30000.0\n34999.0\n570000.0\nCayman\nPorsche\n8/2/2019\nDiesel\n2017.0\nAllemagne\n650000.0\n810000.0\n3.0\n\n\n21438\nCasablanca\n40000.0\n44999.0\n530000.0\n911 Carrera\nPorsche\n9/2/2019\nEssence\n2008.0\nAllemagne\n1200000.0\n2660000.0\n12.0\n\n\n21439\nCasablanca\n60000.0\n64999.0\n890000.0\n911 Carrera\nPorsche\n8/29/2019\nEssence\n2012.0\nAllemagne\n1200000.0\n2660000.0\n8.0\n\n\n21440\nKÃ©nitra\n55000.0\n59999.0\n9100000.0\n911 Carrera\nPorsche\n9/11/2019\nEssence\n2013.0\nAllemagne\n1200000.0\n2660000.0\n7.0\n\n\n21443\nCasablanca\n10000.0\n14999.0\n1200000.0\n911 Carrera\nPorsche\n9/7/2019\nEssence\n2017.0\nAllemagne\n1200000.0\n2660000.0\n3.0\n\n\n21495\nRabat\n90000.0\n94999.0\n640000.0\nCayenne\nPorsche\n7/2/2019\nDiesel\n2015.0\nAllemagne\n890000.0\n2110000.0\n5.0\n\n\n21496\nCasablanca\n65000.0\n69999.0\n620000.0\nCayenne\nPorsche\n9/15/2019\nDiesel\n2015.0\nAllemagne\n890000.0\n2110000.0\n5.0\n\n\n21497\nCasablanca\n140000.0\n149999.0\n600000.0\nCayenne\nPorsche\n6/10/2019\nDiesel\n2015.0\nAllemagne\n890000.0\n2110000.0\n5.0\n\n\n21498\nRabat\n100000.0\n109999.0\n800000.0\nCayenne\nPorsche\n7/14/2019\nDiesel\n2016.0\nAllemagne\n890000.0\n2110000.0\n4.0\n\n\n21500\nCasablanca\n45000.0\n49999.0\n700000.0\nCayenne\nPorsche\n8/8/2019\nDiesel\n2016.0\nAllemagne\n890000.0\n2110000.0\n4.0\n\n\n21501\nCasablanca\n90000.0\n94999.0\n560000.0\nCayenne\nPorsche\n9/7/2019\nDiesel\n2016.0\nAllemagne\n890000.0\n2110000.0\n4.0\n\n\n21502\nCasablanca\n90000.0\n94999.0\n560000.0\nCayenne\nPorsche\n9/7/2019\nDiesel\n2016.0\nAllemagne\n890000.0\n2110000.0\n4.0\n\n\n21504\nTanger\n20000.0\n24999.0\n720000.0\nCayenne\nPorsche\n9/8/2019\nDiesel\n2017.0\nAllemagne\n890000.0\n2110000.0\n3.0\n\n\n21505\nTanger\n20000.0\n24999.0\n720000.0\nCayenne\nPorsche\n9/8/2019\nDiesel\n2017.0\nAllemagne\n890000.0\n2110000.0\n3.0\n\n\n21506\nMarrakech\n75000.0\n79999.0\n580000.0\nCayenne\nPorsche\n9/12/2019\nDiesel\n2017.0\nAllemagne\n890000.0\n2110000.0\n3.0\n\n\n21507\nCasablanca\n80000.0\n84999.0\n600000.0\nCayenne\nPorsche\n6/27/2019\nDiesel\n2017.0\nAllemagne\n890000.0\n2110000.0\n3.0\n\n\n21508\nMarrakech\n15000.0\n19999.0\n1050000.0\nCayenne\nPorsche\n7/30/2019\nEssence\n2017.0\nAllemagne\n890000.0\n2110000.0\n3.0\n\n\n21509\nMarrakech\n15000.0\n19999.0\n1050000.0\nCayenne\nPorsche\n7/30/2019\nEssence\n2017.0\nAllemagne\n890000.0\n2110000.0\n3.0\n\n\n21525\nCasablanca\n70000.0\n74999.0\n570000.0\nPanamera\nPorsche\n6/27/2019\nDiesel\n2015.0\nAllemagne\n1010000.0\n1880000.0\n5.0\n\n\n28301\nTanger\n60000.0\n64999.0\n550000.0\nLand Cruiser\nToyota\n7/26/2019\nDiesel\n2015.0\nJapon\n960000.0\n960000.0\n5.0\n\n\n31853\nCasablanca\n10000.0\n14999.0\n525000.0\nTouareg\nVolkswagen\n8/29/2019\nDiesel\n2019.0\nAllemagne\n510000.0\n745000.0\n1.0\n\n\n\n109 rows Ã— 13 columns\n\nm_price = car.where(car[\'price\'] > 900000).dropna()\nm_price[\'price\'].hist()\nplt.show()\n\nCheck the correlations\nfrom pandas.plotting import scatter_matrix\nattributes = [\'Mileage_Min\', \'Mileage_Max\', \'Year\', \'min_prix_neuf\',\'max_prix_neuf\', \'Age\']\nscatter_matrix(car[attributes], figsize=(12, 8))\nplt.show()\n\nnp.array(num_classes)\narray([\'Mileage_Min\', \'Mileage_Max\', \'price\', \'Year\', \'min_prix_neuf\',\n       \'max_prix_neuf\', \'Age\'], dtype=object)\n\nNote:\n\nAs I mentioned above age and year has a 1 correlation magnitude so year can be removed\nmin_prix_neuf(min price) and max_prix_neuf(max_price) has a closely 1 magnitude correlation so it can be change to\naverage price\nmileage min and max has a closely 1 magnitude correlation so it can be change to average mileage\n\n## Dropping year\ncars = cars.drop(""Year"", axis=1)\ncars.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMileage_Min\nMileage_Max\nprice\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\ncount\n32395.000000\n32395.000000\n3.239500e+04\n3.239500e+04\n3.239500e+04\n32395.000000\n\n\nmean\n116592.066677\n131076.174070\n1.054981e+05\n2.157886e+05\n2.907133e+05\n9.201204\n\n\nstd\n75722.653802\n92740.841965\n8.690894e+04\n1.449547e+05\n2.233345e+05\n4.373741\n\n\nmin\n0.000000\n4999.000000\n1.050000e+04\n9.790000e+04\n9.990000e+04\n1.000000\n\n\n25%\n60000.000000\n64999.000000\n6.200000e+04\n1.247000e+05\n1.711000e+05\n6.000000\n\n\n50%\n110000.000000\n119999.000000\n8.600000e+04\n1.610000e+05\n2.199000e+05\n9.000000\n\n\n75%\n170000.000000\n179999.000000\n1.200000e+05\n2.700000e+05\n3.589000e+05\n12.000000\n\n\nmax\n500000.000000\n1000000.000000\n9.100000e+06\n1.304000e+06\n2.660000e+06\n19.000000\n\n\n\n\n# Min, Max price -> Average Price\ncars[""avg_prix""] = (cars[\'min_prix_neuf\']+cars[""max_prix_neuf""])/2\n#Dropping min, max\ncars = cars.drop(""max_prix_neuf"", axis=1)\ncars = cars.drop(""min_prix_neuf"", axis=1)\ncars.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMileage_Min\nMileage_Max\nprice\nAge\navg_prix\n\n\n\n\ncount\n32395.000000\n32395.000000\n3.239500e+04\n32395.000000\n3.239500e+04\n\n\nmean\n116592.066677\n131076.174070\n1.054981e+05\n9.201204\n2.532509e+05\n\n\nstd\n75722.653802\n92740.841965\n8.690894e+04\n4.373741\n1.822976e+05\n\n\nmin\n0.000000\n4999.000000\n1.050000e+04\n1.000000\n9.990000e+04\n\n\n25%\n60000.000000\n64999.000000\n6.200000e+04\n6.000000\n1.526500e+05\n\n\n50%\n110000.000000\n119999.000000\n8.600000e+04\n9.000000\n1.884000e+05\n\n\n75%\n170000.000000\n179999.000000\n1.200000e+05\n12.000000\n3.125000e+05\n\n\nmax\n500000.000000\n1000000.000000\n9.100000e+06\n19.000000\n1.930000e+06\n\n\n\n\n# Min, Max mileage -> Average mileage\ncars[""avg_mileage""] = (cars[\'Mileage_Min\']+cars[""Mileage_Max""])/2\n#Dropping min, max\ncars = cars.drop(""Mileage_Min"", axis=1)\ncars = cars.drop(""Mileage_Max"", axis=1)\ncars.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nprice\nAge\navg_prix\navg_mileage\n\n\n\n\ncount\n3.239500e+04\n32395.000000\n3.239500e+04\n32395.000000\n\n\nmean\n1.054981e+05\n9.201204\n2.532509e+05\n123834.120374\n\n\nstd\n8.690894e+04\n4.373741\n1.822976e+05\n83740.941893\n\n\nmin\n1.050000e+04\n1.000000\n9.990000e+04\n2499.500000\n\n\n25%\n6.200000e+04\n6.000000\n1.526500e+05\n62499.500000\n\n\n50%\n8.600000e+04\n9.000000\n1.884000e+05\n114999.500000\n\n\n75%\n1.200000e+05\n12.000000\n3.125000e+05\n174999.500000\n\n\nmax\n9.100000e+06\n19.000000\n1.930000e+06\n750000.000000\n\n\n\n\ncorr_matrix = cars.corr()\ncorr_matrix[""price""].sort_values(ascending=False)\nprice          1.000000\navg_prix       0.485551\navg_mileage   -0.135952\nAge           -0.409049\nName: price, dtype: float64\n\nCorrelation Conclusion\nWhen correlation coefficient is close to 0 it means it has very valuable information when predictions\n\nrelevent data for the price is mileage\nAge and average price is other numric values that can be used\n\nattributes = [\'Age\', \'avg_prix\', \'avg_mileage\']\nscatter_matrix(cars[attributes], figsize=(12, 8))\nplt.show()\n\ncars.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nprice\nserie\nMake\nFuel\nCountry_Make\nAge\navg_prix\navg_mileage\n\n\n\n\n0\n295000\nGIULIA\nAlfa Romeo\nDiesel\nItalie\n3\n432400.0\n37499.5\n\n\n1\n450000\nGIULIA\nAlfa Romeo\nDiesel\nItalie\n1\n432400.0\n12499.5\n\n\n2\n102000\nGiulietta\nAlfa Romeo\nEssence\nItalie\n10\n254900.0\n134999.5\n\n\n3\n102000\nGiulietta\nAlfa Romeo\nEssence\nItalie\n10\n254900.0\n134999.5\n\n\n4\n100000\nGiulietta\nAlfa Romeo\nDiesel\nItalie\n9\n254900.0\n224999.5\n\n\n\n\nFeatures and Labels\ncars_featurs = cars.drop(""price"", axis=1)\ncars_labels = cars[""price""]\ncars_labels =  pd.DataFrame(cars_labels, columns=[\'price\'])\ncars_labels.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nprice\n\n\n\n\n0\n295000\n\n\n1\n450000\n\n\n2\n102000\n\n\n3\n102000\n\n\n4\n100000\n\n\n\n\nScaling the Features\ncars_cat = cars_featurs.drop([\'Age\', \'avg_prix\', \'avg_mileage\'], axis=1)\ncars_num = cars_featurs.drop([\'serie\' ,\'Make\', \'Fuel\', \'Country_Make\'], axis=1)\ncars_num.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\navg_prix\navg_mileage\n\n\n\n\n0\n3\n432400.0\n37499.5\n\n\n1\n1\n432400.0\n12499.5\n\n\n2\n10\n254900.0\n134999.5\n\n\n3\n10\n254900.0\n134999.5\n\n\n4\n9\n254900.0\n224999.5\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncars_scaled = scaler.fit_transform(cars_num)\ncars_scaled\narray([[-1.41784798,  0.9827435 , -1.03098856],\n       [-1.8751295 ,  0.9827435 , -1.32953292],\n       [ 0.18263735,  0.00904609,  0.13333444],\n       ...,\n       [ 0.18263735,  2.96634342,  0.61100542],\n       [-0.04600341,  2.96634342,  0.13333444],\n       [-0.73192569,  2.96634342,  0.49158768]])\n\ncars_scale = pd.DataFrame(cars_scaled, columns=cars_num.columns)\ncars_scale.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\navg_prix\navg_mileage\n\n\n\n\n0\n-1.417848\n0.982744\n-1.030989\n\n\n1\n-1.875129\n0.982744\n-1.329533\n\n\n2\n0.182637\n0.009046\n0.133334\n\n\n3\n0.182637\n0.009046\n0.133334\n\n\n4\n-0.046003\n0.009046\n1.208094\n\n\n\n\ncars_scale = cars_scale.drop(""avg_prix"", axis=1)\ncat_classes = [\'serie\' ,\'Make\', \'Fuel\', \'Country_Make\']\nCategorical Feature Encode\nfrom sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\ncars_cat_1hot = encoder.fit_transform(cars_cat[""Fuel""])\ncars_cat_1hot\narray([[1, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       ...,\n       [1, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0]])\n\nlen(encoder.classes_)\n5\n\n# fuel_df = pd.DataFrame(cars_cat_1hot, columns=encoder.classes_)\n# res = pd.concat([fuel_df, cars_scale], axis=1)\n# res = res.drop(""avg_prix"", axis=1)\n# res.head()\n# encoder = LabelBinarizer()\n# cars_cat_1hot = encoder.fit_transform(cars_cat[""Country_Make""])\n# cars_cat_1hot\n# make_df = pd.DataFrame(cars_cat_1hot, columns=encoder.classes_)\n# res = pd.concat([res, make_df], axis=1)\nOutlier Detection\n# housing[""income_cat""] = np.ceil(housing[""median_income""] / 1.5)\n# housing[""income_cat""].where(housing[""income_cat""] < 5, 5.0, inplace=True)\np1, p2 = np.percentile(car[\'price\'], [.1, 95])\nprint(p1, p2)\n16018.2 240000.0\n\ncar[\'price\'].quantile([0.5, 0.95])\n0.50     86000.0\n0.95    240000.0\nName: price, dtype: float64\n\n# cars_labels\nRecords greater than 90% of data\ncars_labels[""price""].where(cars_labels[""price""] > p2).describe()\ncount    1.556000e+03\nmean     3.405981e+05\nstd      2.498299e+05\nmin      2.407000e+05\n25%      2.650000e+05\n50%      2.950000e+05\n75%      3.600000e+05\nmax      9.100000e+06\nName: price, dtype: float64\n\ncars_labels[""price""].where(cars_labels[""price""] < p2, p2, inplace=True)\nplt.boxplot(cars_labels[\'price\'], vert = False)\nplt.show()\n\nTraining The model\nCar price lies on $$$86000.0 and $240000.0 so the values must be compar with them\nCategorical Features \'serie\' ,\'Make\', and \'Country_Make\' is not giving accurate results for this dataset because comparing the size of the dataset the features in each category is high\nfrom sklearn.model_selection import cross_val_score\ndef display_scores(scores):\n    print(""Scores:"", scores)\n    print(""Mean:"", scores.mean())\n    print(""Standard deviation:"", scores.std())\n# Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\n# lin_reg.fit(res, cars_labels)\nscores = cross_val_score(lin_reg, cars_scale, cars_labels,\nscoring=""neg_mean_squared_error"", cv=10)\nrmse_scores = np.sqrt(-scores)\ndisplay_scores(rmse_scores)\nScores: [60889.48587997 31140.85857855 48932.28349657 31881.45628462\n 52359.93315081 56249.49626565 31043.7528518  26573.40120975\n 29640.21531866 58082.10940469]\nMean: 42679.299244107955\nStandard deviation: 13039.765399661634\n\nres.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nDiesel\nElectrique\nEssence\nHybride\nLPG\nAge\navg_mileage\n\n\n\n\n0\n1\n0\n0\n0\n0\n-1.417848\n-1.030989\n\n\n1\n1\n0\n0\n0\n0\n-1.875129\n-1.329533\n\n\n2\n0\n0\n1\n0\n0\n0.182637\n0.133334\n\n\n3\n0\n0\n1\n0\n0\n0.182637\n0.133334\n\n\n4\n1\n0\n0\n0\n0\n-0.046003\n1.208094\n\n\n\n\n#Random Forest Regressior\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(bootstrap=True, criterion=\'mse\', max_depth=None,\n                      max_features=2, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=30,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\n\nscores = cross_val_score(forest_reg, cars_scale, cars_labels.values.ravel(), scoring=""neg_mean_squared_error"", cv=10)\nrdf_scores = np.sqrt(-scores)\ndisplay_scores(rdf_scores)\nScores: [60311.43322993 32041.8324361  51467.68178886 32604.83096828\n 51738.57735542 56422.07577381 31150.61212777 27464.72827134\n 29965.1026229  58084.74369348]\nMean: 43125.161826787415\nStandard deviation: 12787.092049851562\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [\n{\'n_estimators\': [3, 10, 30], \'max_features\': [2]},\n{\'bootstrap\': [False], \'n_estimators\': [3, 10], \'max_features\': [2]},\n]\n\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring=\'neg_mean_squared_error\')\ngrid_search.fit(cars_scale, cars_labels.values.ravel())\nGridSearchCV(cv=5, error_score=\'raise-deprecating\',\n             estimator=RandomForestRegressor(bootstrap=True, criterion=\'mse\',\n                                             max_depth=None,\n                                             max_features=\'auto\',\n                                             max_leaf_nodes=None,\n                                             min_impurity_decrease=0.0,\n                                             min_impurity_split=None,\n                                             min_samples_leaf=1,\n                                             min_samples_split=2,\n                                             min_weight_fraction_leaf=0.0,\n                                             n_estimators=\'warn\', n_jobs=None,\n                                             oob_score=False, random_state=None,\n                                             verbose=0, warm_start=False),\n             iid=\'warn\', n_jobs=None,\n             param_grid=[{\'max_features\': [2], \'n_estimators\': [3, 10, 30]},\n                         {\'bootstrap\': [False], \'max_features\': [2],\n                          \'n_estimators\': [3, 10]}],\n             pre_dispatch=\'2*n_jobs\', refit=True, return_train_score=False,\n             scoring=\'neg_mean_squared_error\', verbose=0)\n\ngrid_search.best_params_\n{\'max_features\': 2, \'n_estimators\': 30}\n\ngrid_search.best_estimator_\nRandomForestRegressor(bootstrap=True, criterion=\'mse\', max_depth=None,\n                      max_features=2, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=30,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\n\ncars_labels.values.ravel().shape\n(32395,)\n\nres.shape\n(32395, 8)\n\n# SVM\nfrom sklearn.svm import SVR\n\nsvm_reg = SVR(gamma=\'auto\', kernel=\'linear\')\n# lin_reg.fit(res, cars_labels)\n\nscores = cross_val_score(svm_reg, cars_scale, cars_labels.values.ravel(),scoring=""neg_mean_squared_error"", cv=10)\nrmse_scores = np.sqrt(-scores)\ndisplay_scores(rmse_scores)\nScores: [74785.36882906 24003.38381518 25375.92678051 31241.06902799\n 58945.32060029 69215.02878381 32584.40777247 21540.83602805\n 42608.41465624 75451.21644538]\nMean: 45575.097273898034\nStandard deviation: 20778.465544155144\n\n# SVM\nfrom sklearn.svm import SVR\n\nsvm_reg = SVR(gamma=\'auto\', kernel=\'rbf\')\n# lin_reg.fit(res, cars_labels)\n\nscores = cross_val_score(svm_reg, cars_scale, cars_labels.values.ravel(),scoring=""neg_mean_squared_error"", cv=10)\nrmse_scores = np.sqrt(-scores)\ndisplay_scores(rmse_scores)\nScores: [77920.13934259 28944.65872308 24659.88609403 35874.7827764\n 61930.10329196 72599.33277686 38516.34553221 27882.15727647\n 46813.60303312 79269.98611537]\nMean: 49441.09949620892\nStandard deviation: 20499.748095685445\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb_reg = GaussianNB()\n# lin_reg.fit(res, cars_labels)\n\nscores = cross_val_score(nb_reg, cars_scale, cars_labels.values.ravel(),scoring=""neg_mean_squared_error"", cv=10)\nrmse_scores = np.sqrt(-scores)\ndisplay_scores(rmse_scores)\nC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n  % (min_groups, self.n_splits)), Warning)\n\n\nScores: [73632.91493647 80705.00746683 90063.3385716  82508.59314805\n 78030.85013666 74887.66547342 73851.05060456 69171.65184274\n 64841.43740987 55163.93832834]\nMean: 74285.64479185516\nStandard deviation: 9239.860784725739\n\nUse Following to get random forest results for the best model from above\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVR\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n#put your absolute location of the csv file like ""C:/Users/..../DatasetADs.csv""\ndef process_data(file):\n    car = pd.read_csv(file)\n    cars = car.drop(""City"", axis=1)\n    cars = cars.drop(""Date_ads"", axis=1)\n    cars = cars.drop(""Year"", axis=1)\n    # Min, Max price -> Average Price\n    cars[""avg_prix""] = (cars[\'min_prix_neuf\']+cars[""max_prix_neuf""])/2\n    #Dropping min, max\n    cars = cars.drop(""max_prix_neuf"", axis=1)\n    cars = cars.drop(""min_prix_neuf"", axis=1)\n    cars[""avg_mileage""] = (cars[\'Mileage_Min\']+cars[""Mileage_Max""])/2\n    #Dropping min, max\n    cars = cars.drop(""Mileage_Min"", axis=1)\n    cars = cars.drop(""Mileage_Max"", axis=1)\n    cars_featurs = cars.drop(""price"", axis=1)\n    cars_labels = cars[""price""]\n    cars_labels =  pd.DataFrame(cars_labels, columns=[\'price\'])\n    cars_cat = cars_featurs.drop([\'Age\', \'avg_prix\', \'avg_mileage\'], axis=1)\n    cars_num = cars_featurs.drop([\'serie\' ,\'Make\', \'Fuel\', \'Country_Make\'], axis=1)\n    scaler = StandardScaler()\n    cars_scaled = scaler.fit_transform(cars_num)\n    cars_scale = pd.DataFrame(cars_scaled, columns=cars_num.columns)\n    encoder = LabelBinarizer()\n    cars_cat_1hot = encoder.fit_transform(cars_cat[""Fuel""])\n    fuel_df = pd.DataFrame(cars_cat_1hot, columns=encoder.classes_)\n    res = pd.concat([fuel_df, cars_scale], axis=1)\n    p1, p2 = np.percentile(car[\'price\'], [.1, 95])\n    cars_labels[""price""].where(cars_labels[""price""] < p2, p2, inplace=True)\n    \n    return res, cars_labels\n\n\ndef display_scores(scores):\n    print(""Scores:"", scores)\n    print(""Mean:"", scores.mean())\n    print(""Standard deviation:"", scores.std())\n\ndef random_forest_values(res, cars_labels):\n    forest_reg = RandomForestRegressor(bootstrap=True, criterion=\'mse\', max_depth=None,\n                      max_features=2, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=30,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\n\n    scores = cross_val_score(forest_reg, res, cars_labels.values.ravel(), scoring=""neg_mean_squared_error"", cv=10)\n    rdf_scores = np.sqrt(-scores)\n    display_scores(rdf_scores)\n    \n# Example\n# use absolute path like C:/Users/PC/Desktop/.../DatasetADs.csv if you have dataset in another directory\nres, cars_labels = process_data(""DatasetADs.csv"")\nrandom_forest_values(res, cars_labels)\nScores: [27537.73120752 16974.27887871 15696.82374726 20268.84884715\n 22368.53838188 27971.21473105 21694.65256948 18410.07085884\n 23771.19720919 37938.99064566]\nMean: 23263.234707672458\nStandard deviation: 6234.21408188423\n\nExponential (Log) regression\nAs we know make has 11 values so first it should be encoded\nExponential Function\n\n# calculate geometric mean\nfrom scipy import stats\ndef resiprocal(y, x, a, b):\n    s = (y - (a*np.exp(b*x)))**2\n# Lets take the first initial dataset\ncar.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nMileage_Min\nMileage_Max\nprice\nserie\nMake\nDate_ads\nFuel\nYear\nCountry_Make\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\n0\nCasablanca\n35000\n39999\n295000\nGIULIA\nAlfa Romeo\n4/3/2019\nDiesel\n2017\nItalie\n369900\n494900\n3\n\n\n1\nCasablanca\n10000\n14999\n450000\nGIULIA\nAlfa Romeo\n7/7/2019\nDiesel\n2019\nItalie\n369900\n494900\n1\n\n\n2\nMohammedia\n130000\n139999\n102000\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010\nItalie\n229900\n279900\n10\n\n\n3\nMohammedia\n130000\n139999\n102000\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010\nItalie\n229900\n279900\n10\n\n\n4\nCasablanca\n200000\n249999\n100000\nGiulietta\nAlfa Romeo\n7/8/2019\nDiesel\n2011\nItalie\n229900\n279900\n9\n\n\n\n\ndf = car.where(car[\'price\'] <  500000).dropna()\n# remove missing values\ndf = df.dropna()\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nMileage_Min\nMileage_Max\nprice\nserie\nMake\nDate_ads\nFuel\nYear\nCountry_Make\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\n0\nCasablanca\n35000.0\n39999.0\n295000.0\nGIULIA\nAlfa Romeo\n4/3/2019\nDiesel\n2017.0\nItalie\n369900.0\n494900.0\n3.0\n\n\n1\nCasablanca\n10000.0\n14999.0\n450000.0\nGIULIA\nAlfa Romeo\n7/7/2019\nDiesel\n2019.0\nItalie\n369900.0\n494900.0\n1.0\n\n\n2\nMohammedia\n130000.0\n139999.0\n102000.0\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010.0\nItalie\n229900.0\n279900.0\n10.0\n\n\n3\nMohammedia\n130000.0\n139999.0\n102000.0\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010.0\nItalie\n229900.0\n279900.0\n10.0\n\n\n4\nCasablanca\n200000.0\n249999.0\n100000.0\nGiulietta\nAlfa Romeo\n7/8/2019\nDiesel\n2011.0\nItalie\n229900.0\n279900.0\n9.0\n\n\n\n\ndf.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMileage_Min\nMileage_Max\nprice\nYear\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\ncount\n32278.000000\n32278.000000\n32278.000000\n32278.000000\n3.227800e+04\n3.227800e+04\n32278.000000\n\n\nmean\n116837.164632\n131350.851230\n103249.288835\n2010.780222\n2.140060e+05\n2.875323e+05\n9.219778\n\n\nstd\n75711.004580\n92757.711069\n62545.513175\n4.366724\n1.412870e+05\n2.145169e+05\n4.366724\n\n\nmin\n0.000000\n4999.000000\n10500.000000\n2001.000000\n9.790000e+04\n9.990000e+04\n1.000000\n\n\n25%\n60000.000000\n64999.000000\n61000.000000\n2008.000000\n1.247000e+05\n1.711000e+05\n6.000000\n\n\n50%\n110000.000000\n119999.000000\n86000.000000\n2011.000000\n1.610000e+05\n2.199000e+05\n9.000000\n\n\n75%\n170000.000000\n179999.000000\n120000.000000\n2014.000000\n2.700000e+05\n3.290000e+05\n12.000000\n\n\nmax\n500000.000000\n1000000.000000\n495000.000000\n2019.000000\n1.304000e+06\n2.660000e+06\n19.000000\n\n\n\n\n# stats.gmean(df[""price_lg""])\nimport numpy as np\n\nimport statsmodels.api as sm\n\nimport statsmodels.formula.api as smf\ndfs = df.filter([\'Make\', \'serie\', \'Age\', \'Mileage_Min\', \'Mileage_Max\', \'Fuel\', ""price""], axis=1)\n# med_num = med.filter([\'IPSI\',\'Contra\'], axis=1)\nModel by make\nresults = smf.ols(\'np.log(price) ~ Make + Fuel + np.log(Mileage_Max) + np.log(Age) \', data= dfs).fit()\nresults.summary()\n\nOLS Regression Results\n\nDep. Variable: np.log(price)   R-squared:             0.715\n\n\nModel: OLS   Adj. R-squared:        0.714\n\n\nMethod: Least Squares   F-statistic:           2375.\n\n\nDate: Tue, 17 Dec 2019   Prob (F-statistic):   0.00\n\n\nTime: 13:16:39   Log-Likelihood:      -3384.8\n\n\nNo. Observations:  32278   AIC:                   6840.\n\n\nDf Residuals:  32243   BIC:                   7133.\n\n\nDf Model:     34    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    12.5365     0.032   388.500  0.000    12.473    12.600\n\n\nMake[T.Audi]     0.3160     0.028    11.157  0.000     0.260     0.372\n\n\nMake[T.BMW]     0.2572     0.029     9.006  0.000     0.201     0.313\n\n\nMake[T.Citroen]    -0.3980     0.028   -14.309  0.000    -0.452    -0.343\n\n\nMake[T.Dacia]    -0.5263     0.027   -19.285  0.000    -0.580    -0.473\n\n\nMake[T.Fiat]    -0.4766     0.028   -16.937  0.000    -0.532    -0.421\n\n\nMake[T.Ford]    -0.2493     0.028    -8.904  0.000    -0.304    -0.194\n\n\nMake[T.Honda]    -0.1078     0.029    -3.759  0.000    -0.164    -0.052\n\n\nMake[T.Hyundai]    -0.2819     0.028   -10.195  0.000    -0.336    -0.228\n\n\nMake[T.Jaguar]     0.5018     0.049    10.218  0.000     0.406     0.598\n\n\nMake[T.Jeep]     0.1433     0.035     4.103  0.000     0.075     0.212\n\n\nMake[T.Kia]    -0.3229     0.028   -11.407  0.000    -0.378    -0.267\n\n\nMake[T.Land Rover]     0.5394     0.031    17.166  0.000     0.478     0.601\n\n\nMake[T.Lexus]     0.2652     0.192     1.381  0.167    -0.111     0.642\n\n\nMake[T.Mazda]    -0.1833     0.042    -4.336  0.000    -0.266    -0.100\n\n\nMake[T.Mercedes-Benz]     0.3354     0.028    11.868  0.000     0.280     0.391\n\n\nMake[T.Mitsubishi]    -0.2288     0.036    -6.295  0.000    -0.300    -0.158\n\n\nMake[T.Nissan]    -0.0757     0.030    -2.558  0.011    -0.134    -0.018\n\n\nMake[T.Opel]    -0.1986     0.029    -6.877  0.000    -0.255    -0.142\n\n\nMake[T.Peugeot]    -0.4225     0.028   -15.343  0.000    -0.477    -0.369\n\n\nMake[T.Porsche]     0.7878     0.042    18.885  0.000     0.706     0.870\n\n\nMake[T.Renault]    -0.3463     0.027   -12.683  0.000    -0.400    -0.293\n\n\nMake[T.Seat]    -0.2190     0.030    -7.304  0.000    -0.278    -0.160\n\n\nMake[T.Skoda]    -0.1151     0.032    -3.588  0.000    -0.178    -0.052\n\n\nMake[T.Ssangyong]    -0.3930     0.036   -11.051  0.000    -0.463    -0.323\n\n\nMake[T.Toyota]    -0.1959     0.028    -6.925  0.000    -0.251    -0.140\n\n\nMake[T.Volkswagen]     0.0402     0.027     1.467  0.142    -0.014     0.094\n\n\nMake[T.Volvo]     0.1179     0.037     3.223  0.001     0.046     0.190\n\n\nMake[T.mini]     0.2052     0.058     3.515  0.000     0.091     0.320\n\n\nFuel[T.Electrique]    -0.6238     0.072    -8.672  0.000    -0.765    -0.483\n\n\nFuel[T.Essence]    -0.3551     0.004   -83.737  0.000    -0.363    -0.347\n\n\nFuel[T.Hybride]    -0.1038     0.057    -1.836  0.066    -0.215     0.007\n\n\nFuel[T.LPG]    -0.1219     0.155    -0.785  0.432    -0.426     0.182\n\n\nnp.log(Mileage_Max)     0.0194     0.002    11.964  0.000     0.016     0.023\n\n\nnp.log(Age)    -0.5027     0.003  -176.881  0.000    -0.508    -0.497\n\n\n\n\nOmnibus: 9607.656   Durbin-Watson:         0.824\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):   115790.640\n\n\nSkew: -1.087   Prob(JB):               0.00\n\n\nKurtosis: 12.020   Cond. No.           1.52e+03\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.52e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\nEquation\npred(price) = exp(6.1602+[Audi]-0.0347+[BMW]-0.1915+[Citroen]-0.2914+[Dacia]-0.1787+[Fiat]-0.2738+[Ford]-0.1831+[Honda]-0.3703+[Hyundai]-0.2666+[Jaguar]-0.0616+[Jeep]-0.1848+[Kia]-0.3395+[LandRover]-0.0912+[Lexus]-0.7343+[Mazda]-0.4781+[Mercedes-Benz]-0.0579+[Mitsubishi]-0.2992+[Nissan]-0.1592+[Opel]-0.0803+[Peugeot]-0.2907+[Porsche]-0.2965+[Renault]-0.1572+[Seat]-0.0616+[Skoda]-0.1024+[Ssangyong]-0.4025+[Toyota]-0.1741+[Volkswagen]-0.0333+[Volvo]-0.1694+[mini]-0.1087+[min_prix_neuf]*0.1140+[Mileage_Max]*0.0182+[max_prix_neuf]0.4003+[Age]-0.5408)\nnp.exp(results.predict(car.iloc[3:4])).values[0]\n104700.72620675308\n\ncar.iloc[3:4]\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nMileage_Min\nMileage_Max\nprice\nserie\nMake\nDate_ads\nFuel\nYear\nCountry_Make\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\n3\nMohammedia\n130000\n139999\n102000\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010\nItalie\n229900\n279900\n10\n\n\n\n\npred_df_make = pd.DataFrame(np.exp(results.predict(df)), columns=[""pred_price""])\npred_price_df = pd.concat([pred_df_make, df[""price""]], axis=1)\npred_price_df.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\npred_price\nprice\n\n\n\n\n0\n260264.105589\n295000.0\n\n\n1\n463091.549046\n450000.0\n\n\n2\n104700.726207\n102000.0\n\n\n3\n104700.726207\n102000.0\n\n\n4\n112018.237466\n100000.0\n\n\n\n\nMAE\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(df[""price""], pred_df_make)\n20510.763083448437\n\nFor prediction use np.exp(results.predict(data) formula\n# example\n# if you want to predict cars 3 rd record\nnp.exp(results.predict(car.iloc[3:4]))\n3    104700.726207\ndtype: float64\n\n# if you want to predict cars 100 - 110 records\nnp.exp(results.predict(car.iloc[100:110]))\n100    86641.614503\n101    87207.214743\n102    86852.932711\n103    90668.173960\n104    93039.576876\n105    88666.404510\n106    88306.186418\n107    95960.419719\n108    95690.947047\n109    95960.419719\ndtype: float64\n\ndef predict_by_make(data):\n    print(""Make: Pred(price): $ {:.2f}"".format(np.exp(results.predict(data)).values[0]))\nGive the dataframe record as data to predict new price\npredict_by_make(car.iloc[3:4])\nMake: Pred(price): $ 104700.73\n\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nMileage_Min\nMileage_Max\nprice\nserie\nMake\nDate_ads\nFuel\nYear\nCountry_Make\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\n0\nCasablanca\n35000.0\n39999.0\n295000.0\nGIULIA\nAlfa Romeo\n4/3/2019\nDiesel\n2017.0\nItalie\n369900.0\n494900.0\n3.0\n\n\n1\nCasablanca\n10000.0\n14999.0\n450000.0\nGIULIA\nAlfa Romeo\n7/7/2019\nDiesel\n2019.0\nItalie\n369900.0\n494900.0\n1.0\n\n\n2\nMohammedia\n130000.0\n139999.0\n102000.0\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010.0\nItalie\n229900.0\n279900.0\n10.0\n\n\n3\nMohammedia\n130000.0\n139999.0\n102000.0\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010.0\nItalie\n229900.0\n279900.0\n10.0\n\n\n4\nCasablanca\n200000.0\n249999.0\n100000.0\nGiulietta\nAlfa Romeo\n7/8/2019\nDiesel\n2011.0\nItalie\n229900.0\n279900.0\n9.0\n\n\n\n\ndfs = df.filter([\'Make\', \'serie\', \'Age\', \'Mileage_Min\', \'Mileage_Max\', \'Fuel\', ""price""], axis=1)\n# med_num = med.filter([\'IPSI\',\'Contra\'], axis=1)\ndfs.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMake\nserie\nAge\nMileage_Min\nMileage_Max\nFuel\nprice\n\n\n\n\n0\nAlfa Romeo\nGIULIA\n3.0\n35000.0\n39999.0\nDiesel\n295000.0\n\n\n1\nAlfa Romeo\nGIULIA\n1.0\n10000.0\n14999.0\nDiesel\n450000.0\n\n\n2\nAlfa Romeo\nGiulietta\n10.0\n130000.0\n139999.0\nEssence\n102000.0\n\n\n3\nAlfa Romeo\nGiulietta\n10.0\n130000.0\n139999.0\nEssence\n102000.0\n\n\n4\nAlfa Romeo\nGiulietta\n9.0\n200000.0\n249999.0\nDiesel\n100000.0\n\n\n\n\nmarque\tSerie\tage\tmileage_min\tmileage_Max\ttype\nModel by model\nresults_model = smf.ols(\'np.log(price) ~ serie +  Fuel +np.log(Mileage_Max)  + np.log(Age) \', data=dfs).fit()\nresults_model.summary()\n\nOLS Regression Results\n\nDep. Variable: np.log(price)   R-squared:             0.806\n\n\nModel: OLS   Adj. R-squared:        0.805\n\n\nMethod: Least Squares   F-statistic:           723.0\n\n\nDate: Tue, 17 Dec 2019   Prob (F-statistic):   0.00\n\n\nTime: 13:19:02   Log-Likelihood:       2882.9\n\n\nNo. Observations:  32278   AIC:                  -5394.\n\n\nDf Residuals:  32092   BIC:                  -3835.\n\n\nDf Model:    185    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    11.9681     0.052   231.980  0.000    11.867    12.069\n\n\nserie[T.2008]     0.2685     0.056     4.828  0.000     0.159     0.377\n\n\nserie[T.208]     0.0942     0.051     1.840  0.066    -0.006     0.195\n\n\nserie[T.3]     0.2893     0.062     4.668  0.000     0.168     0.411\n\n\nserie[T.3008]     0.5858     0.057    10.317  0.000     0.474     0.697\n\n\nserie[T.301]     0.0774     0.052     1.500  0.134    -0.024     0.179\n\n\nserie[T.308]     0.3730     0.051     7.322  0.000     0.273     0.473\n\n\nserie[T.500]     0.3404     0.052     6.509  0.000     0.238     0.443\n\n\nserie[T.5008]     0.3568     0.068     5.249  0.000     0.224     0.490\n\n\nserie[T.500C]     0.2153     0.083     2.584  0.010     0.052     0.379\n\n\nserie[T.500X]     0.3874     0.075     5.200  0.000     0.241     0.533\n\n\nserie[T.508]     0.5002     0.053     9.479  0.000     0.397     0.604\n\n\nserie[T.6]     0.3888     0.063     6.139  0.000     0.265     0.513\n\n\nserie[T.911 Carrera]     0.8445     0.165     5.131  0.000     0.522     1.167\n\n\nserie[T.A3]     0.7741     0.051    15.056  0.000     0.673     0.875\n\n\nserie[T.A4]     0.7275     0.052    14.003  0.000     0.626     0.829\n\n\nserie[T.A5]     1.1540     0.055    21.029  0.000     1.046     1.262\n\n\nserie[T.A6]     0.8528     0.055    15.572  0.000     0.745     0.960\n\n\nserie[T.A8]     0.7316     0.077     9.452  0.000     0.580     0.883\n\n\nserie[T.ADAM]     0.4002     0.077     5.175  0.000     0.249     0.552\n\n\nserie[T.Accent]     0.1224     0.051     2.419  0.016     0.023     0.222\n\n\nserie[T.Accord]     0.4199     0.051     8.211  0.000     0.320     0.520\n\n\nserie[T.Astra]     0.4129     0.051     8.100  0.000     0.313     0.513\n\n\nserie[T.Auris]     0.4420     0.055     8.019  0.000     0.334     0.550\n\n\nserie[T.Berlingo]     0.1631     0.050     3.240  0.001     0.064     0.262\n\n\nserie[T.Boxster]     1.9069     0.165    11.583  0.000     1.584     2.230\n\n\nserie[T.C-ELYSEE]     0.0585     0.069     0.853  0.394    -0.076     0.193\n\n\nserie[T.C-ElysÃ©e]     0.0654     0.058     1.132  0.258    -0.048     0.179\n\n\nserie[T.C-Max]     0.5555     0.055    10.068  0.000     0.447     0.664\n\n\nserie[T.C1]     0.1735     0.058     3.013  0.003     0.061     0.286\n\n\nserie[T.C3]     0.1323     0.051     2.579  0.010     0.032     0.233\n\n\nserie[T.C4 CACTUS]     0.4759     0.072     6.593  0.000     0.334     0.617\n\n\nserie[T.C4 Picasso]     0.4315     0.056     7.680  0.000     0.321     0.542\n\n\nserie[T.C5]     0.2750     0.052     5.248  0.000     0.172     0.378\n\n\nserie[T.CC]     0.8698     0.055    15.862  0.000     0.762     0.977\n\n\nserie[T.CLASSE C COUPE]     1.2515     0.122    10.290  0.000     1.013     1.490\n\n\nserie[T.CLUBMAN]     0.2234     0.165     1.357  0.175    -0.099     0.546\n\n\nserie[T.COMBO]     0.5706     0.138     4.149  0.000     0.301     0.840\n\n\nserie[T.CR-V]     0.5432     0.052    10.373  0.000     0.441     0.646\n\n\nserie[T.CRETA]     0.5518     0.067     8.278  0.000     0.421     0.683\n\n\nserie[T.CX-3]     0.7046     0.227     3.098  0.002     0.259     1.150\n\n\nserie[T.Caddy]     0.6477     0.051    12.649  0.000     0.547     0.748\n\n\nserie[T.Captur]     0.4497     0.055     8.181  0.000     0.342     0.558\n\n\nserie[T.Cayenne]     1.1891     0.059    20.275  0.000     1.074     1.304\n\n\nserie[T.Cayman]     2.0467     0.165    12.434  0.000     1.724     2.369\n\n\nserie[T.Ceed]     0.3258     0.059     5.504  0.000     0.210     0.442\n\n\nserie[T.Cherokee]     0.5583     0.064     8.692  0.000     0.432     0.684\n\n\nserie[T.Civic]     0.4115     0.053     7.813  0.000     0.308     0.515\n\n\nserie[T.Classe A]     0.6637     0.053    12.426  0.000     0.559     0.768\n\n\nserie[T.Classe B]     0.6570     0.055    11.849  0.000     0.548     0.766\n\n\nserie[T.Classe C]     1.0023     0.051    19.749  0.000     0.903     1.102\n\n\nserie[T.Classe CL]     1.3343     0.103    12.908  0.000     1.132     1.537\n\n\nserie[T.Classe CLA]     1.2574     0.060    21.043  0.000     1.140     1.374\n\n\nserie[T.Classe E]     0.9336     0.052    17.912  0.000     0.831     1.036\n\n\nserie[T.Classe GLA]     1.2474     0.067    18.537  0.000     1.116     1.379\n\n\nserie[T.Classe GLC]     1.2061     0.122     9.918  0.000     0.968     1.444\n\n\nserie[T.Classe GLE]     1.4551     0.137    10.586  0.000     1.186     1.725\n\n\nserie[T.Classe GLS]     1.4483     0.227     6.367  0.000     1.002     1.894\n\n\nserie[T.Classe S]     0.8161     0.061    13.278  0.000     0.696     0.937\n\n\nserie[T.Clio]     0.1645     0.050     3.291  0.001     0.067     0.263\n\n\nserie[T.Compass]     0.2599     0.081     3.203  0.001     0.101     0.419\n\n\nserie[T.Corolla]     0.3033     0.051     5.976  0.000     0.204     0.403\n\n\nserie[T.Corolla verso]     0.3533     0.058     6.117  0.000     0.240     0.467\n\n\nserie[T.Corsa]     0.2264     0.053     4.302  0.000     0.123     0.330\n\n\nserie[T.DS3]     0.4562     0.093     4.911  0.000     0.274     0.638\n\n\nserie[T.DS4]     0.5026     0.086     5.842  0.000     0.334     0.671\n\n\nserie[T.DS5]     0.7888     0.068    11.607  0.000     0.656     0.922\n\n\nserie[T.Discovery]     0.7808     0.089     8.756  0.000     0.606     0.956\n\n\nserie[T.Discovery Sport]     0.9422     0.083    11.298  0.000     0.779     1.106\n\n\nserie[T.Doblo]     0.0277     0.051     0.542  0.588    -0.073     0.128\n\n\nserie[T.Dokker]     0.0559     0.050     1.107  0.268    -0.043     0.155\n\n\nserie[T.Dokker Van]     0.0083     0.067     0.124  0.901    -0.124     0.140\n\n\nserie[T.Duster]     0.3845     0.050     7.616  0.000     0.286     0.483\n\n\nserie[T.Elantra]     0.3731     0.056     6.605  0.000     0.262     0.484\n\n\nserie[T.F-Type]     0.3619     0.227     1.591  0.112    -0.084     0.808\n\n\nserie[T.FIORINO]    -0.2063     0.066    -3.122  0.002    -0.336    -0.077\n\n\nserie[T.Fabia]     0.2153     0.060     3.569  0.000     0.097     0.333\n\n\nserie[T.Fiesta]     0.2359     0.050     4.703  0.000     0.138     0.334\n\n\nserie[T.Fusion]     0.3931     0.056     6.974  0.000     0.283     0.504\n\n\nserie[T.GIULIA]     1.0045     0.165     6.102  0.000     0.682     1.327\n\n\nserie[T.GOLF 7]     0.8375     0.051    16.390  0.000     0.737     0.938\n\n\nserie[T.Giulietta]     0.6068     0.055    11.109  0.000     0.500     0.714\n\n\nserie[T.Golf]     0.4066     0.054     7.576  0.000     0.301     0.512\n\n\nserie[T.Grand Cherokee]     0.8196     0.056    14.729  0.000     0.710     0.929\n\n\nserie[T.HR-V]    -0.3045     0.165    -1.850  0.064    -0.627     0.018\n\n\nserie[T.Ibiza]     0.2190     0.052     4.220  0.000     0.117     0.321\n\n\nserie[T.Insignia]     0.5281     0.054     9.723  0.000     0.422     0.635\n\n\nserie[T.Jazz]     0.2802     0.052     5.339  0.000     0.177     0.383\n\n\nserie[T.Jetta]     0.4965     0.053     9.290  0.000     0.392     0.601\n\n\nserie[T.Juke]     0.5303     0.055     9.664  0.000     0.423     0.638\n\n\nserie[T.Kadjar]     0.5622     0.057     9.853  0.000     0.450     0.674\n\n\nserie[T.Kangoo]     0.2234     0.050     4.460  0.000     0.125     0.322\n\n\nserie[T.Kangoo Express]     0.2076     0.069     2.987  0.003     0.071     0.344\n\n\nserie[T.Koleos]     0.5031     0.059     8.504  0.000     0.387     0.619\n\n\nserie[T.Korando]     0.4342     0.062     6.964  0.000     0.312     0.556\n\n\nserie[T.Kuga]     0.7647     0.053    14.393  0.000     0.661     0.869\n\n\nserie[T.LS]     0.6233     0.227     2.741  0.006     0.178     1.069\n\n\nserie[T.Land Cruiser]     0.8165     0.061    13.491  0.000     0.698     0.935\n\n\nserie[T.Leon]     0.5302     0.052    10.178  0.000     0.428     0.632\n\n\nserie[T.Lodgy]     0.2842     0.062     4.581  0.000     0.163     0.406\n\n\nserie[T.Logan]    -0.0038     0.050    -0.075  0.940    -0.102     0.094\n\n\nserie[T.Logan Mcv]     0.1680     0.083     2.013  0.044     0.004     0.331\n\n\nserie[T.MOKKA]     0.4374     0.137     3.182  0.001     0.168     0.707\n\n\nserie[T.Megane]     0.2828     0.050     5.604  0.000     0.184     0.382\n\n\nserie[T.Megane 3]     0.4559     0.051     8.867  0.000     0.355     0.557\n\n\nserie[T.Megane CC]     0.3597     0.063     5.668  0.000     0.235     0.484\n\n\nserie[T.Megane Cabriolet]     0.4546     0.093     4.889  0.000     0.272     0.637\n\n\nserie[T.Megane Coupe]     0.3106     0.055     5.652  0.000     0.203     0.418\n\n\nserie[T.Megane Estate]     0.2580     0.138     1.876  0.061    -0.011     0.528\n\n\nserie[T.Megane Sedan]     0.3480     0.069     5.014  0.000     0.212     0.484\n\n\nserie[T.Micra]     0.0865     0.058     1.495  0.135    -0.027     0.200\n\n\nserie[T.Mustang]     1.3166     0.074    17.684  0.000     1.171     1.463\n\n\nserie[T.Octavia]     0.5173     0.053     9.834  0.000     0.414     0.620\n\n\nserie[T.Panamera]     1.7032     0.076    22.450  0.000     1.555     1.852\n\n\nserie[T.Panda]     0.0448     0.055     0.812  0.417    -0.063     0.153\n\n\nserie[T.Partner]     0.0408     0.050     0.810  0.418    -0.058     0.139\n\n\nserie[T.Passat]     0.5691     0.051    11.244  0.000     0.470     0.668\n\n\nserie[T.Picanto]     0.1276     0.050     2.537  0.011     0.029     0.226\n\n\nserie[T.Pinto]    -0.0571     0.064    -0.890  0.373    -0.183     0.069\n\n\nserie[T.Polo]     0.2942     0.050     5.834  0.000     0.195     0.393\n\n\nserie[T.Prius]     0.7217     0.166     4.338  0.000     0.396     1.048\n\n\nserie[T.Punto]     0.0723     0.051     1.428  0.153    -0.027     0.172\n\n\nserie[T.Q3]     1.1148     0.060    18.453  0.000     0.996     1.233\n\n\nserie[T.Q5]     1.2937     0.053    24.604  0.000     1.191     1.397\n\n\nserie[T.Q7]     0.9243     0.056    16.365  0.000     0.814     1.035\n\n\nserie[T.Qashqai]     0.6772     0.052    13.090  0.000     0.576     0.779\n\n\nserie[T.RAV 4]     0.6011     0.052    11.507  0.000     0.499     0.703\n\n\nserie[T.RX]     0.8821     0.227     3.879  0.000     0.436     1.328\n\n\nserie[T.Range Rover]     1.0888     0.057    19.055  0.000     0.977     1.201\n\n\nserie[T.Range Rover Evoque]     1.2721     0.054    23.452  0.000     1.166     1.378\n\n\nserie[T.Range Rover Sport]     1.1143     0.055    20.279  0.000     1.007     1.222\n\n\nserie[T.Renegade]     0.7585     0.098     7.777  0.000     0.567     0.950\n\n\nserie[T.Rexton]     0.0286     0.056     0.512  0.609    -0.081     0.138\n\n\nserie[T.S60]     0.6022     0.060    10.111  0.000     0.485     0.719\n\n\nserie[T.SERIE 2]     0.9517     0.069    13.711  0.000     0.816     1.088\n\n\nserie[T.SERIE 3 GT]     0.4840     0.227     2.127  0.033     0.038     0.930\n\n\nserie[T.STAVIC]     0.4709     0.069     6.852  0.000     0.336     0.606\n\n\nserie[T.Sandero]     0.0845     0.050     1.679  0.093    -0.014     0.183\n\n\nserie[T.Santa Fe]     0.6697     0.051    13.083  0.000     0.569     0.770\n\n\nserie[T.Serie 1]     0.7512     0.053    14.225  0.000     0.648     0.855\n\n\nserie[T.Serie 3]     0.7254     0.052    13.962  0.000     0.624     0.827\n\n\nserie[T.Serie 4]     1.2394     0.059    21.009  0.000     1.124     1.355\n\n\nserie[T.Serie 5]     0.8718     0.052    16.723  0.000     0.770     0.974\n\n\nserie[T.Serie 7]     0.8851     0.065    13.578  0.000     0.757     1.013\n\n\nserie[T.Sorento]     0.3261     0.054     6.086  0.000     0.221     0.431\n\n\nserie[T.Sportage]     0.4503     0.053     8.530  0.000     0.347     0.554\n\n\nserie[T.Superb]     0.6450     0.065     9.978  0.000     0.518     0.772\n\n\nserie[T.TIPO]     0.0559     0.083     0.670  0.503    -0.108     0.219\n\n\nserie[T.TT]     0.4795     0.104     4.618  0.000     0.276     0.683\n\n\nserie[T.Talisman]     0.6353     0.071     8.925  0.000     0.496     0.775\n\n\nserie[T.Tepee]     0.3099     0.052     5.967  0.000     0.208     0.412\n\n\nserie[T.Tiguan]     1.0092     0.051    19.770  0.000     0.909     1.109\n\n\nserie[T.Touareg]     0.7944     0.051    15.503  0.000     0.694     0.895\n\n\nserie[T.Touran]     0.7872     0.051    15.445  0.000     0.687     0.887\n\n\nserie[T.Tucson]     0.4904     0.053     9.304  0.000     0.387     0.594\n\n\nserie[T.V40]     0.6625     0.070     9.427  0.000     0.525     0.800\n\n\nserie[T.V40 Cross Country]     0.4850     0.137     3.528  0.000     0.216     0.754\n\n\nserie[T.Velar]     0.3948     0.228     1.735  0.083    -0.051     0.841\n\n\nserie[T.Wrangler]     1.0813     0.071    15.190  0.000     0.942     1.221\n\n\nserie[T.X-Trail]     0.4112     0.054     7.572  0.000     0.305     0.518\n\n\nserie[T.X1]     0.9885     0.062    16.020  0.000     0.868     1.109\n\n\nserie[T.X3]     0.9206     0.067    13.675  0.000     0.789     1.053\n\n\nserie[T.X4]     1.5271     0.165     9.276  0.000     1.204     1.850\n\n\nserie[T.X5]     0.7563     0.055    13.847  0.000     0.649     0.863\n\n\nserie[T.X6]     1.2093     0.069    17.400  0.000     1.073     1.346\n\n\nserie[T.XC60]     0.9110     0.060    15.290  0.000     0.794     1.028\n\n\nserie[T.XC90]     0.8312     0.122     6.833  0.000     0.593     1.070\n\n\nserie[T.XE]     0.9756     0.137     7.098  0.000     0.706     1.245\n\n\nserie[T.XF]     1.0702     0.063    16.889  0.000     0.946     1.194\n\n\nserie[T.XJ]     1.4274     0.098    14.634  0.000     1.236     1.619\n\n\nserie[T.Yaris]     0.2218     0.053     4.197  0.000     0.118     0.325\n\n\nserie[T.Yaris Verso]     0.2603     0.227     1.144  0.252    -0.186     0.706\n\n\nserie[T.cabrio]     0.3579     0.111     3.225  0.001     0.140     0.576\n\n\nserie[T.country man]     0.9394     0.070    13.369  0.000     0.802     1.077\n\n\nserie[T.i 10]     0.1011     0.050     2.008  0.045     0.002     0.200\n\n\nserie[T.i 20]     0.1971     0.058     3.391  0.001     0.083     0.311\n\n\nserie[T.i 30]     0.5001     0.052     9.659  0.000     0.399     0.602\n\n\nserie[T.outlander]     0.6254     0.083     7.496  0.000     0.462     0.789\n\n\nserie[T.pajero]     0.2481     0.057     4.386  0.000     0.137     0.359\n\n\nserie[T.pajero sport]     0.4896     0.060     8.130  0.000     0.372     0.608\n\n\nFuel[T.Electrique]    -0.5452     0.060    -9.099  0.000    -0.663    -0.428\n\n\nFuel[T.Essence]    -0.2537     0.004   -59.874  0.000    -0.262    -0.245\n\n\nFuel[T.Hybride]    -0.0078     0.048    -0.162  0.871    -0.102     0.086\n\n\nFuel[T.LPG]    -0.1943     0.129    -1.510  0.131    -0.447     0.058\n\n\nnp.log(Mileage_Max)     0.0086     0.001     6.327  0.000     0.006     0.011\n\n\nnp.log(Age)    -0.4612     0.003  -176.155  0.000    -0.466    -0.456\n\n\n\n\nOmnibus: 17453.007   Durbin-Watson:         1.167\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):   444864.534\n\n\nSkew: -2.090   Prob(JB):               0.00\n\n\nKurtosis: 20.700   Cond. No.           6.37e+03\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.37e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\nnp.exp(results_model.predict(car.iloc[3:4])).values[0]\n85880.66479234594\n\npred_df_model = pd.DataFrame(np.exp(results_model.predict(df)), columns=[""pred_price""])\npred_price_model_df = pd.concat([pred_df_make, df[""price""]], axis=1)\npred_price_model_df.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\npred_price\nprice\n\n\n\n\n0\n260264.105589\n295000.0\n\n\n1\n463091.549046\n450000.0\n\n\n2\n104700.726207\n102000.0\n\n\n3\n104700.726207\n102000.0\n\n\n4\n112018.237466\n100000.0\n\n\n\n\nMAE\nmean_absolute_error(df[""price""], pred_df_model)\n17557.510666383598\n\ndef predict_by_model(data):\n    \n    print(""Model: Pred(price): $ {:.2f}"".format(np.exp(results.predict(data)).values[0]))\nGive the dataframe record as data to predict new price\n# data = input just like original datframe record\ncar.iloc[3:4]\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nMileage_Min\nMileage_Max\nprice\nserie\nMake\nDate_ads\nFuel\nYear\nCountry_Make\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\n3\nMohammedia\n130000\n139999\n102000\nGiulietta\nAlfa Romeo\n7/27/2019\nEssence\n2010\nItalie\n229900\n279900\n10\n\n\n\n\npredict_by_model(car.iloc[3:4])\nModel: Pred(price): $ 104700.73\n\nExamples\npredict_by_model(car.iloc[3:4])\npredict_by_make(car.iloc[3:4])\nModel: Pred(price): $ 104700.73\nMake: Pred(price): $ 104700.73\n\n # actual price \ncar.iloc[3:4][""price""]\n3    102000\nName: price, dtype: int64\n\npredict_by_model(car.iloc[567:568])\npredict_by_make(car.iloc[567:568])\nModel: Pred(price): $ 155664.75\nMake: Pred(price): $ 155664.75\n\n # actual price \ncar.iloc[567:568][""price""]\n567    180000\nName: price, dtype: int64\n\npredict_by_model(car.iloc[15000:15001])\npredict_by_make(car.iloc[15000:15001])\nModel: Pred(price): $ 46380.49\nMake: Pred(price): $ 46380.49\n\ncar.iloc[15000:15001]\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCity\nMileage_Min\nMileage_Max\nprice\nserie\nMake\nDate_ads\nFuel\nYear\nCountry_Make\nmin_prix_neuf\nmax_prix_neuf\nAge\n\n\n\n\n15000\nMarrakech\n140000\n149999\n47000\nPicanto\nKia\n1/11/2019\nEssence\n2007\nCorÃ©e du sud\n109900\n149900\n13\n\n\n\n\n # actual price \ncar.iloc[15000:15001][""price""]\n15000    47000\nName: price, dtype: int64\n\n########################################################\nout_df = pd.read_csv(""Out_Data.csv"")\nres.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nDiesel\nElectrique\nEssence\nHybride\nLPG\nAge\navg_mileage\n\n\n\n\n0\n1\n0\n0\n0\n0\n-1.417848\n-1.030989\n\n\n1\n1\n0\n0\n0\n0\n-1.875129\n-1.329533\n\n\n2\n0\n0\n1\n0\n0\n0.182637\n0.133334\n\n\n3\n0\n0\n1\n0\n0\n0.182637\n0.133334\n\n\n4\n1\n0\n0\n0\n0\n-0.046003\n1.208094\n\n\n\n\nout_df.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmarque\nSerie\nage\nmileage_min\nmileage_Max\ntype\n\n\n\n\n0\nAlfa Romeo\nGIULIA\n1\n0\n5000\nDiesel\n\n\n1\nAlfa Romeo\nGIULIA\n2\n0\n5000\nDiesel\n\n\n2\nAlfa Romeo\nGIULIA\n3\n0\n5000\nDiesel\n\n\n3\nAlfa Romeo\nGIULIA\n4\n0\n5000\nDiesel\n\n\n4\nAlfa Romeo\nGIULIA\n5\n0\n5000\nDiesel\n\n\n\n\nX = out_df.drop(""marque"", axis=1)\nX = X.drop(""Serie"", axis=1)\nX = X.drop(""type"", axis=1)\nX[""avg_mileage""] = (X[""mileage_Max""]+ X[""mileage_min""])/2\nX = X.drop(""mileage_min"", axis=1)\nX = X.drop(""mileage_Max"", axis=1)\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nage\navg_mileage\n\n\n\n\n0\n1\n2500.0\n\n\n1\n2\n2500.0\n\n\n2\n3\n2500.0\n\n\n3\n4\n2500.0\n\n\n4\n5\n2500.0\n\n\n\n\nscaler = StandardScaler()\ncars_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(cars_scaled, columns=X.columns)\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nage\navg_mileage\n\n\n\n\n0\n-1.643168\n-0.927967\n\n\n1\n-1.460593\n-0.927967\n\n\n2\n-1.278019\n-0.927967\n\n\n3\n-1.095445\n-0.927967\n\n\n4\n-0.912871\n-0.927967\n\n\n\n\ncars_scale.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\navg_mileage\n\n\n\n\n0\n-1.417848\n-1.030989\n\n\n1\n-1.875129\n-1.329533\n\n\n2\n0.182637\n0.133334\n\n\n3\n0.182637\n0.133334\n\n\n4\n-0.046003\n1.208094\n\n\n\n\nlin_reg.fit(cars_scale, cars_labels)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nlin_pred = lin_reg.predict(X)\nforest_reg.fit(cars_scale, cars_labels)\nforest_pred  = forest_reg.predict(X)\nC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  ""10 in version 0.20 to 100 in 0.22."", FutureWarning)\nC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  """"""Entry point for launching an IPython kernel.\n\nsvm_reg.fit(cars_scale, cars_labels)\nsvm_pred  = svm_reg.predict(X)\nC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\nnb_reg.fit(cars_scale, cars_labels)\nnb_pred  = nb_reg.predict(X)\nC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\nlin_pred_df = pd.DataFrame(lin_pred, columns=[""Linear Regression""])\nforest_pred_df = pd.DataFrame(forest_pred, columns=[""Random Forest Regressior""])\nsvm_pred_df = pd.DataFrame(svm_pred, columns=[""SVM""])\nnb_pred_df = pd.DataFrame(nb_pred, columns=[""naive_bayes""])\npred_df = pd.concat([lin_pred_df, forest_pred_df, svm_pred_df, nb_pred_df], axis=1)\npred_df.to_csv(\'pred.csv\')\nout_df = out_df.rename(columns={""marque"" : ""Make"", ""Serie"": ""serie"", ""age"": ""Age"", ""mileage_min"": ""Mileage_Min"", ""mileage_Max"": ""Mileage_Max"", ""type"": ""Fuel""})\nout_df.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMake\nserie\nAge\nMileage_Min\nMileage_Max\nFuel\n\n\n\n\n0\nAlfa Romeo\nGIULIA\n1\n0\n5000\nDiesel\n\n\n1\nAlfa Romeo\nGIULIA\n2\n0\n5000\nDiesel\n\n\n2\nAlfa Romeo\nGIULIA\n3\n0\n5000\nDiesel\n\n\n3\nAlfa Romeo\nGIULIA\n4\n0\n5000\nDiesel\n\n\n4\nAlfa Romeo\nGIULIA\n5\n0\n5000\nDiesel\n\n\n\n\ndfs.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMake\nserie\nAge\nMileage_Min\nMileage_Max\nFuel\nprice\n\n\n\n\n0\nAlfa Romeo\nGIULIA\n3.0\n35000.0\n39999.0\nDiesel\n295000.0\n\n\n1\nAlfa Romeo\nGIULIA\n1.0\n10000.0\n14999.0\nDiesel\n450000.0\n\n\n2\nAlfa Romeo\nGiulietta\n10.0\n130000.0\n139999.0\nEssence\n102000.0\n\n\n3\nAlfa Romeo\nGiulietta\n10.0\n130000.0\n139999.0\nEssence\n102000.0\n\n\n4\nAlfa Romeo\nGiulietta\n9.0\n200000.0\n249999.0\nDiesel\n100000.0\n\n\n\n\nout_df[\'serie\'] = out_df[\'serie\'].replace([\'F-PACE\'],\'F-Type\')\nout_df[\'Make\'] = out_df[\'Make\'].replace([\'Chery\'],\'Citroen\')\npred_model_df = pd.DataFrame(np.exp(results_model.predict(out_df)), columns=[""Log-linear By Model""])\npred_series_df =  pd.DataFrame(np.exp(results.predict(out_df)), columns=[""Log-linear By Make""])\npred_df = pd.concat([lin_pred_df, forest_pred_df, svm_pred_df, nb_pred_df, pred_series_df, pred_model_df], axis=1)\npred_df.to_csv(\'pred.csv\')\ndfs[\'serie\'].unique()\narray([\'GIULIA\', \'Giulietta\', \'A3\', \'A4\', \'A5\', \'A6\', \'A8\', \'Q3\', \'Q5\',\n       \'Q7\', \'TT\', \'Serie 1\', \'SERIE 2\', \'Serie 3\', \'SERIE 3 GT\',\n       \'Serie 4\', \'Serie 5\', \'Serie 7\', \'X1\', \'X3\', \'X4\', \'X5\', \'X6\',\n       \'Berlingo\', \'C1\', \'C3\', \'C4 CACTUS\', \'C4 Picasso\', \'C5\',\n       \'C-ELYSEE\', \'C-ElysÃ©e\', \'Dokker\', \'Dokker Van\', \'Duster\', \'Lodgy\',\n       \'Logan\', \'Logan Mcv\', \'Sandero\', \'DS3\', \'DS4\', \'DS5\', \'500\',\n       \'500C\', \'500X\', \'Doblo\', \'FIORINO\', \'Panda\', \'Pinto\', \'Punto\',\n       \'TIPO\', \'C-Max\', \'Fiesta\', \'Fusion\', \'Kuga\', \'Mustang\', \'Accord\',\n       \'Civic\', \'CR-V\', \'HR-V\', \'Jazz\', \'Accent\', \'CRETA\', \'Elantra\',\n       \'i 10\', \'i 20\', \'i 30\', \'Santa Fe\', \'Tucson\', \'F-Type\', \'XE\', \'XF\',\n       \'XJ\', \'Cherokee\', \'Compass\', \'Grand Cherokee\', \'Renegade\',\n       \'Wrangler\', \'Ceed\', \'Picanto\', \'Sorento\', \'Sportage\', \'Discovery\',\n       \'Discovery Sport\', \'Range Rover\', \'Range Rover Evoque\',\n       \'Range Rover Sport\', \'Velar\', \'LS\', \'RX\', \'3\', \'6\', \'CX-3\',\n       \'Classe CL\', \'Classe CLA\', \'Classe A\', \'Classe B\', \'Classe C\',\n       \'CLASSE C COUPE\', \'Classe E\', \'Classe S\', \'Classe GLA\',\n       \'Classe GLC\', \'Classe GLE\', \'Classe GLS\', \'cabrio\', \'CLUBMAN\',\n       \'country man\', \'outlander\', \'pajero\', \'pajero sport\', \'Juke\',\n       \'Micra\', \'Qashqai\', \'X-Trail\', \'ADAM\', \'Astra\', \'COMBO\', \'Corsa\',\n       \'Insignia\', \'MOKKA\', \'108\', \'2008\', \'208\', \'3008\', \'301\', \'308\',\n       \'5008\', \'508\', \'Tepee\', \'Partner\', \'Boxster\', \'Cayman\',\n       \'911 Carrera\', \'Cayenne\', \'Panamera\', \'Captur\', \'Clio\', \'Kadjar\',\n       \'Kangoo\', \'Kangoo Express\', \'Koleos\', \'Megane\', \'Megane 3\',\n       \'Megane Cabriolet\', \'Megane CC\', \'Megane Coupe\', \'Megane Estate\',\n       \'Megane Sedan\', \'Talisman\', \'Ibiza\', \'Leon\', \'Fabia\', \'Octavia\',\n       \'Superb\', \'Korando\', \'Rexton\', \'STAVIC\', \'Auris\', \'Corolla\',\n       \'Corolla verso\', \'Land Cruiser\', \'Prius\', \'RAV 4\', \'Yaris\',\n       \'Yaris Verso\', \'Caddy\', \'CC\', \'Golf\', \'GOLF 7\', \'Jetta\', \'Passat\',\n       \'Polo\', \'Tiguan\', \'Touareg\', \'Touran\', \'S60\', \'V40\',\n       \'V40 Cross Country\', \'XC60\', \'XC90\'], dtype=object)\n\nout_df[\'serie\'].unique()\narray([\'GIULIA\', \'Giulietta\', \'A3\', \'A4\', \'A5\', \'A6\', \'A8\', \'Q3\', \'Q5\',\n       \'Q7\', \'TT\', \'Serie 1\', \'SERIE 2\', \'Serie 3\', \'SERIE 3 GT\',\n       \'Serie 4\', \'Serie 5\', \'Serie 7\', \'X1\', \'X3\', \'X4\', \'X5\', \'X6\',\n       \'Berlingo\', \'C1\', \'C3\', \'C4 CACTUS\', \'C4 Picasso\', \'C5\',\n       \'C-ELYSEE\', \'C-ElysÃ©e\', \'DS3\', \'DS4\', \'DS5\', \'Dokker\',\n       \'Dokker Van\', \'Duster\', \'Lodgy\', \'Logan\', \'Logan Mcv\', \'Sandero\',\n       \'500\', \'500C\', \'500X\', \'Doblo\', \'FIORINO\', \'Panda\', \'Pinto\',\n       \'Punto\', \'TIPO\', \'C-Max\', \'Fiesta\', \'Fusion\', \'Kuga\', \'Mustang\',\n       \'Accent\', \'Accord\', \'Civic\', \'CR-V\', \'HR-V\', \'Jazz\', \'CRETA\',\n       \'Elantra\', \'i 10\', \'i 20\', \'i 30\', \'Santa Fe\', \'Tucson\', \'F-PACE\',\n       \'F-Type\', \'XE\', \'XF\', \'XJ\', \'Cherokee\', \'Compass\',\n       \'Grand Cherokee\', \'Renegade\', \'Wrangler\', \'Ceed\', \'Picanto\',\n       \'Sorento\', \'Sportage\', \'Discovery\', \'Discovery Sport\',\n       \'Range Rover\', \'Range Rover Evoque\', \'Range Rover Sport\', \'Velar\',\n       \'LS\', \'RX\', \'3\', \'6\', \'CX-3\', \'Classe A\', \'Classe B\', \'Classe C\',\n       \'CLASSE C COUPE\', \'Classe CL\', \'Classe CLA\', \'Classe E\',\n       \'Classe GLA\', \'Classe GLC\', \'Classe GLE\', \'Classe GLS\', \'Classe S\',\n       \'cabrio\', \'CLUBMAN\', \'country man\', \'outlander\', \'pajero\',\n       \'pajero sport\', \'Juke\', \'Micra\', \'Qashqai\', \'X-Trail\', \'ADAM\',\n       \'Astra\', \'COMBO\', \'Corsa\', \'Insignia\', \'MOKKA\', \'108\', \'2008\',\n       \'208\', \'3008\', \'301\', \'308\', \'5008\', \'508\', \'Partner\', \'Tepee\',\n       \'911 Carrera\', \'Boxster\', \'Cayenne\', \'Cayman\', \'Panamera\',\n       \'Captur\', \'Clio\', \'Kadjar\', \'Kangoo\', \'Kangoo Express\', \'Koleos\',\n       \'Megane\', \'Megane 3\', \'Megane Cabriolet\', \'Megane CC\',\n       \'Megane Coupe\', \'Megane Estate\', \'Megane Sedan\', \'Talisman\',\n       \'Ibiza\', \'Leon\', \'Fabia\', \'Octavia\', \'Superb\', \'Korando\', \'Rexton\',\n       \'STAVIC\', \'Auris\', \'Corolla\', \'Corolla verso\', \'Land Cruiser\',\n       \'Prius\', \'RAV 4\', \'Yaris\', \'Yaris Verso\', \'Caddy\', \'CC\', \'Golf\',\n       \'GOLF 7\', \'Jetta\', \'Passat\', \'Polo\', \'Tiguan\', \'Touareg\', \'Touran\',\n       \'S60\', \'V40\', \'V40 Cross Country\', \'XC60\', \'XC90\'], dtype=object)\n\na = out_df[\'serie\'].unique()\nb = dfs[\'serie\'].unique()\nb = np.append(b, ""z"")\nb.shape\n(181,)\n\nfor i in range(len(a)):\n    if a[i] not in b:\n        print(a[i])\nout_df[\'Make\'].unique()\narray([\'Alfa Romeo\', \'Audi\', \'BMW\', \'Chery\', \'Citroen\', \'Dacia\', \'Fiat\',\n       \'Ford\', \'Honda\', \'Hyundai\', \'Jaguar\', \'Jeep\', \'Kia\', \'Land Rover\',\n       \'Lexus\', \'Mazda\', \'Mercedes-Benz\', \'mini\', \'Mitsubishi\', \'Nissan\',\n       \'Opel\', \'Peugeot\', \'Porsche\', \'Renault\', \'Seat\', \'Skoda\',\n       \'Ssangyong\', \'Toyota\', \'Volkswagen\', \'Volvo\'], dtype=object)\n\ndfs[\'Make\'].unique()\narray([\'Alfa Romeo\', \'Audi\', \'BMW\', \'Citroen\', \'Dacia\', \'Fiat\', \'Ford\',\n       \'Honda\', \'Hyundai\', \'Jaguar\', \'Jeep\', \'Kia\', \'Land Rover\', \'Lexus\',\n       \'Mazda\', \'Mercedes-Benz\', \'mini\', \'Mitsubishi\', \'Nissan\', \'Opel\',\n       \'Peugeot\', \'Porsche\', \'Renault\', \'Seat\', \'Skoda\', \'Ssangyong\',\n       \'Toyota\', \'Volkswagen\', \'Volvo\'], dtype=object)\n\nc = out_df[\'Make\'].unique()\nd = dfs[\'Make\'].unique()\nfor i in range(len(c)):\n    if c[i] not in d:\n        print(c[i])\nChery\n\n\n'], 'url_profile': 'https://github.com/AsiriAmalk', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Aug 8, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}"
"{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anillab', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Umernasirr', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['R-Studio\n'], 'url_profile': 'https://github.com/cjcraigportfolio', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Regression\nContains self implemented regression(linear) which can be extended to multiple and polynomial variations as well.\n'], 'url_profile': 'https://github.com/mayankA47', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Cincinnati', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Boston-Housing-Data-Analysis\nBoston {MASS}\tR Documentation\nHousing Values in Suburbs of Boston\nDescription\nThe Boston data frame has 506 rows and 14 columns.\nUsage\nBoston\nFormat\nThis data frame contains the following columns:\n\n\ncrim: per capita crime rate by town.\n\n\nzn: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus: proportion of non-retail business acres per town.\n\n\nchas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox: nitrogen oxides concentration (parts per 10 million).\n\n\nrm: average number of rooms per dwelling.\n\n\nage: proportion of owner-occupied units built prior to 1940.\n\n\ndis: weighted mean of distances to five Boston employment centres.\n\n\nrad: index of accessibility to radial highways.\n\n\ntax: full-value property-tax rate per $10,000.\n\n\nptratio: pupil-teacher ratio by town.\n\n\nblack: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n\n\nlstat: lower status of the population (percent).\n\n\nmedv: median value of owner-occupied homes in $1000s.\n\n\nSource\nHarrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.\nBelsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n[Package MASS version 7.3-51.4 Index]\n'], 'url_profile': 'https://github.com/prakhar3949', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['KNN-Regressor\nKNN Regressor\n'], 'url_profile': 'https://github.com/kar6270', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/woodytwoshoes', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Shirdi, India', 'stats_list': [], 'contributions': '198 contributions\n        in the last year', 'description': ['PredictHeartDisease\nPredicting heart attacks using predictive analysis models of machine Learning like decision tree,logistic regression, Random Forest, KNN.\n\n\nIntroduction:\nAn estimated 17 million people die of CVDs (Cardiovascular disease), particularly heart attacks\nand strokes, in the world every year. Cardiac ailments killed more Indians in 2016 (28%) than\nany other non-communicable disease, said a new study published in the September 2018 issue\nof health journal, The Lancet. These are double the numbers reported in 1990 when heart\ndisease caused 15% of deaths in India. Today we will try to build a heart attack predictor. Based\non some diagnostically measured parameters we will predict who among the subjects under\nconsideration, are on high risk of heart attack. This can revolutionize the healthcare system and\nhelp save many many lives.\n(taking help from others report)\n\n\nObjective:\nTo build a heart attack predictor based on some diagnostically measured parameters.\n\n\nProblem Definition:\nPredicting heart attacks using predictive analysis models of machine Learning like decision tree,\nlogistic regression, random Forest, KNN.\n\n\n'], 'url_profile': 'https://github.com/bhushanyadav07', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Santiago de Compostela (Spain)', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/canceleiro', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Stock-Price-Predictor\nPredicts stock prices using a linear regression ML model.\n'], 'url_profile': 'https://github.com/giobirkelund', 'info_list': ['Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', '2', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}"
"{'location': 'London', 'stats_list': [], 'contributions': '1,128 contributions\n        in the last year', 'description': ['Linear Regression\nWe will be analysing a simple dataset using Linear Regression.\nThe data has been fitted a linear regressor due to the high level of correlation between the two variables, this can be observed by the scatter plot below:\n\nThe output of the code below code (OLS - Ordinary Least Squares) \nimport statsmodels.api as sm \nX = np.append(np.ones((30,1)).astype(int), X, axis = 1) \nReg_OLS = sm.OLS(endog = y, exog = X).fit() \nsummary = Reg_OLS.summary()\nprint(summary) \n\nis as follows\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.957\nModel:                            OLS   Adj. R-squared:                  0.955\nMethod:                 Least Squares   F-statistic:                     622.5\nDate:                Mon, 09 Mar 2020   Prob (F-statistic):           1.14e-20\nTime:                        17:11:27   Log-Likelihood:                -301.44\nNo. Observations:                  30   AIC:                             606.9\nDf Residuals:                      28   BIC:                             609.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       2.579e+04   2273.053     11.347      0.000    2.11e+04    3.04e+04\nx1          9449.9623    378.755     24.950      0.000    8674.119    1.02e+04\n==============================================================================\nOmnibus:                        2.140   Durbin-Watson:                   1.648\nProb(Omnibus):                  0.343   Jarque-Bera (JB):                1.569\nSkew:                           0.363   Prob(JB):                        0.456\nKurtosis:                       2.147   Cond. No.                         13.2\n==============================================================================\n\nFrom the above output, we can state the below observations:\nThe regressor takes the following function with the coefficients from the above output:\nExpected_salary = 25790 + 9450*(Years_of_experience)\nFrom the above, we can interpret the following:\n\nThe base salary for a junior with no experience should be a minimum of £25,790\nFor every additional year of experience an individual has, their salary is expected to increase by £9,450\n\nR-squared and Adj R-Squared are both 0.96\nThe P values are much lower than 0.05, indicating that the variable X1 (Years of experience) is statistically significant.\n'], 'url_profile': 'https://github.com/MohitGoel92', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '1,236 contributions\n        in the last year', 'description': [""rqdist\nR package to create distributions from linear quantile regression\nWarning: I've deprived this package of TLC -- it's just a package wrapped around a function. For now.\n""], 'url_profile': 'https://github.com/vincenzocoia', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Warsaw', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Kardiogramy\nZbiór danych\nKardiotokografia to badanie monitorowania akcji serca płodu.\nAutomatycznie przetworzono 2126 kardiotokogramów płodowych (CTG) i zmierzono odpowiednie cechy diagnostyczne w liczbie 23. CTG zostały również sklasyfikowane przez trzech ekspertów położników i każdemu z nich przypisano etykietę konsensusu. Klasyfikacja dotyczyła zarówno wzoru morfologicznego (A, B, C...), jak i stanu płodowego (N, S, P).\nMy zajmujemy się tylko stanem płodowym.\nWśród naszych danych liczba płodów w poszczególnych grupach przedstawia się następująco:\n\n1655 w grupie „normal” (77,8%)\n295 w grupie „suspect” (13,9%)\n176 w grupie „pathologic” (8,3%)\n\nLas losowy\nParametry zostały dobrane metodą random search z walidacją k-krzyżową. Lista strojonych parametrów:\n\nn_estimators\nmax_features\nmax_depth\nmin_samples_split\nmin_samples_leaf\n\nRegresja logistyczna\nParametry zostały dobrane metodą grid search z walidacją k-krzyżową. Lista strojonych parametrów:\n\nparametr C\nsolver\nmax_iter\nclass_weight\n\nPodział danych w przypadku poszukiwań optymalnych hiperparametrów wynosił: 80% dane treningowe, 20% dane testowe.\n'], 'url_profile': 'https://github.com/mateusz-sledz', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '309 contributions\n        in the last year', 'description': ['LinearRegressionFromScratch\nA Linear Regression implementation with Gradient Descent from scratch\n'], 'url_profile': 'https://github.com/RohitNagraj', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""Bank-Marketing-Use-of-Logistic-regression\nUse of logistic regression model to predict term deposit subscription\nThe data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\nData Set Information:\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or ('no') subscribed.\nJupyter Notebook\n""], 'url_profile': 'https://github.com/sudhirtakke', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Hull', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Big-Data-Assignment-1\nData preprocessing, statistical analysis, data visualisaion and linear regression\n'], 'url_profile': 'https://github.com/Bashton19', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kartavyashankar', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear-Regression-using-Stata\nCode Repository for Linear Regression using Stata, Published by Packt\n'], 'url_profile': 'https://github.com/PacktPublishing', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['Logistic Regression: Binary and Multiclass Prediction\nCode to implement Binary and Multiclass Prediction using Logistic Regression\n'], 'url_profile': 'https://github.com/samyuktha17', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arnavgupta829', 'info_list': ['Jupyter Notebook', 'Updated May 17, 2020', 'R', 'GPL-3.0 license', 'Updated Oct 29, 2020', '1', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'MATLAB', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MIT license', 'Updated Jan 21, 2021', 'Python', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}"
"{'location': 'kafrsaqr-zagazig-egypt', 'stats_list': [], 'contributions': '643 contributions\n        in the last year', 'description': ['Kaggle_competition-\nComprehensive data exploration with Python (regression of housing price)\ndata on kaggle url :https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python/data\n'], 'url_profile': 'https://github.com/ahmed-hassan97', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Joshua-Bela', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'Portland, OR', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bounnoy', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fangwenlin', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'Gr. NOIDA', 'stats_list': [], 'contributions': '553 contributions\n        in the last year', 'description': ['K-Nearest-Neighbors\nNon-parametric method used for Classification and Regression.\n'], 'url_profile': 'https://github.com/prakharR534', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'Jersey city, NJ', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Logistic regression from Scratch\nThis repo implements Logistic regression using only numpy for numeric computing and pandas for data handling.\nIt can also be used as a tutorial to learn logistic regression and basic data cleanup using pandas and comes with relevant math references.\n'], 'url_profile': 'https://github.com/AtharvBhat', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'Norfolk, Virginia', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['Predictive Maintenance: Utilizing CRISP-DM\n\nCapstone 1 Springboard\nPredictive Maintenance Utilizing CRISP-DM & Supervised Machine Learning\nPhase I. Business Understanding.\nBackground\nNow that the hype surrounding Data Science has slightly diminished, we can affirm that this is not a drill but rather an exhilarating reality. Governments, large organizations, and start-ups alike have already seen and understood the value this discipline brings to the table and are fervently competing for talent and dominance in this space. As of 2019, we are finding ourselves at a new precipice in entering the Fourth Industrial Revolution: The Real-Time Enterprise! Just like the intersections of the past: First in 1784, Water and steam; Second in 1870, First conveyor belt; and the Third in 1969, Electronics and information technology, this pause will bring with it both challenges and new opportunities. Data stewards in the discipline have realized associative costs for data storage has expounded this issue while driving talent gaps. As their respective organizations continue to ingest voluminous amounts of data, they must become more tactical with the data they are creating and using to sustain or improve Return on Investment (ROI).\nMore so, the approach of advancing insight through analyses of mountains of data must clear and consistent so that results are both reproducible and can be made autonomous. One method in doing this is by utilizing the Cross-Industry Standard Process for Data Mining (CRISP-DM). This logical method enables data stewards and stakeholders to clearly understand what, when, where, why, and how they are mining data through six easy to follow phases:\n\n\nBusiness understanding\nData understanding\nData prep\nModeling & Application Development\nEvaluation\nModel Deployment.\n\n\n\nContraints, limitations, and assumptions\nConstraint 1. The success of the produced mathematical model will depend on how precise the prediction of material failure is. The training set comprises historical telemetry observations from only one machine in the organization\'s operational plants. For instance, after analyzing several readings (independent variables) over months of consistent machine utilization hours, the model\'s predictions will be based on future and constant occurrences similar to the recorded and analyzed observations in the past.\nConstraint 2. We do not have sufficient data to rule out time frame or seasonality significance for sensor readings in seeing how sensors perform over time and for the same period.\nLimitation 1. To predict the likelihood of a future material failure for a machine after N period of running hours, we would need to know the expected hours the device will run in the future: which we do not know ""accurately"" today.\nLimitation 2. It was also noted that in future data pulls it may be helpful to include demographic information to see how the maintainers across regions vary in skill set and maintenance practices.\nAssumption 1. The classification model for this run will address threshold levels concerning mixed readings from various sensors to establish a baseline of telemetry readings pointing to machine deterioration.\nAssumption 2. The classification model for this run will be able to incorporate new data from other machines in the organization and adjust accordingly, unearthing even more hidden dependencies not easily seen with five months of data.\nAssumption 3. Based on the limited dataset, this analysis will yield more questions. It may require more data for future analysis keeping in mind not deteriorating the return on investment (ROI) for the project, and bearing those negative/positive findings are both outputs for this project.\nBusiness Case\nThe following project will demonstrate how to utilize CRISP-DM from a practical standpoint; the next analog will use it on simulated manufacturing data predicting maintenance failures for a theoretical client\'s manufacturing operation. Predictive maintenance is an area that has a clear use-case for data-mining and primarily due to the breakthroughs of applied machine intelligence. With the continuous advancements in the Real-time enterprise fueled through the: Internet of Things (IoT), Low-Cost Telemetry Sensors/RFID tagging, Low-Cost Digital Storage, and increasing Computing Power amongst others, the capabilities of transforming voluminous data into insight in this space does not appear to be slowing. The growth in Artificial Intelligence (AI) amongst increasing levels of Automation seen in manufacturing allows firms to be more resilient in connecting fixed-assets while improving productivity through data-driven decisions and insights not possible before. As the use of automation continues to augment and takeover manufacturing, the reduced response time required in dealing with maintenance issues will outpace the speed at which humans can intervene, requiring sophisticated and automated optimization decisions, especially concerning maintenance schedules. To cope with this transition, executives must speed up organizational learning initiatives to groom new tech talent to utilize tools to assist them in managing this transition through a structured method, or else the cost of doing business will outweigh the profits of its outputs.\nSet objectives:\nIn this case, the client has furnished controlled sensor data on one machine, collected over five months (April 2018 through August 2018). The device has sensors that archive telemetry readings over time. Based on the data provided the analysis required in this set is to predict at which reading thresholds the machine would fail so that the company could optimize labor work schedules to support the maintenance operations proactively. The findings from this analysis will be applied globally throughout the organization\'s maintenance program signaling to managers when proactive maintenance is required, thus shifting labor cost to proactive sustainment to keep operations functioning.\nWho might care:\n\n• Maintenance Managers, Operations Managers, Capital Expenditure planners, and manufacturing organizations such as the Department of Defense, Exon Mobile Corporation, General Motors, Ford Motors, Apple, and Boeing, and the Department of Defense, etc. can use such a model to predict the likelihood of equipment failures to allocate resources better. They can then proactively inform their maintainers and or customers well in advance of potential disruptions in their respective operations. Understanding the probability of material failures will help sustain customer service or level of service efforts. From the customer\'s point of view, it would be very convenient in knowing if a supply, production, or any other disruption may occur so that they can in turn, proactively mitigate risk. On the manufacturer\'s hand, such a predictive model would enhance the product base and performance of the organization\'s operations. Moreover, there is a possibility of developing an app or other front-end communication effort in which customers and or internal users can consult with to understand the likelihood of issues well in advance.\n\nCost and benefits:\nEvery maintenance hour reduced in human labor will save the company and an average of 75.69 dollars which includes fringe benefits. The company\'s current budget for the machine maintenance (one machine) in this analysis consists of a staff of 3 maintainers overseeing one machine with 51 telemetry sensors each week working on average 50 hours a week at an operating expense of $11,353.50 a week or about $590,382. When factoring this number across the enterprise’s other 113 machines ($67M) the cost savings in deploying machine intelligence and optimization is diametrically required.\n'], 'url_profile': 'https://github.com/ahull002', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '319 contributions\n        in the last year', 'description': ['Analyzing What Makes an Effective Leader\nWhat-Makes-an-Effective-Leader:\nWhy are some managers better than others? Are there any general behaviors or characteristics that help a leader be perceived as more effective? The data I use in this analysis was given to me by Seth Berry and originates from a large survey of employees and their direct manager. In this dataset, each leader provided self-ratings and their direct subordinates provided rating about the leader. This is reflected by the Rater variable.  The data is structured the following way:\nThe forceful scale contains the following subscales: takes charge, declares, pushes\nThe enabling scale contains the following subscales: empowers, listens, supports\nThe strategic scale contains the following subscales: direction, growth, innovation\nThe operational scale contains the following subscales: execution, efficiency, order\nLibraries Used in This Proejct:\nlibrary(tidyverse)\nlibrary(pwr)\nlibrary(ggplot2)\nlibrary(MASS)\nload(""teamPerc.rdata"")\nteamPerc_1 <- teamPerc\n\nThree Hypotheses:\nHypotheses 1:\n\n\nLeaders with more years of experience will have a higher effect rating than leaders with less years of experience.\n\n\nIf we compare the effect ratings of leaders with more years of experience with leaders with less years of experience, then we will see that leaders with more years of experience will have a higher effect rating.\n\n\nh0:\n\nThe effect rating for leaders with more years of experience = The effect rating for leaders with less years of experience\n\n\n\nh1:\n\nThe effect rating for leaders with more years of experience > The effect rating for leaders with less years of experience\n\n\n\nHypothesis 2:\n\n\nLeaders who follow up with their employees (lvi40) will have a higher effect rating than leaders who do not follow up with their employees\n\n\nIf we compare the effect ratings of leaders who follow up with their employees with leaders who do not, then we will see that leaders who follow up with their employees have a higher effect rating.\n\n\nh0:\n\nThe effect rating for leaders who follow up with their employees = The effect rating for leaders who do not follow up with their employees\n\n\n\nh1:\n\nThe effect rating for leaders who follow up with their employees > The effect rating for leaders who do not follow up with their employees\n\n\n\nHypothesis 3:\n\n\nLeaders who are deemed forceful will have a lower effect rating than leaders who are not deemed forceful.\n\n\nIf we compare the effect ratings of leaders, who are deemed forceful with their employees with leaders who are not, then we will see that leaders who are forceful with their employees have a lower effect rating.\n\n\nh0:\n\nThe effect rating for leaders who are forceful with their employees = The effect rating for leaders who are not forceful with their employees\n\n\n\nh1:\n\nThe effect rating for leaders who are forceful with their employees < The effect rating for leaders who are not forceful with their employees\n\n\n\nPower Analysis:\nBefore I built any models, I decided to conduct an a prior power analysis and determine the sample size needed.\n\nBelow, I outputted the convential effect sizes.\npwr::cohen.ES(""f2"", ""small"")\n\n\npwr::cohen.ES(""f2"", ""medium"")\n\n\npwr::cohen.ES(""f2"", ""large"")\n\n\npwr.f2.test(u = 1, v = NULL, f2 = .02, power = .8)\n\n\npwr.f2.test(u = 1, v = NULL, f2 = .02, power = .8)\n\n\npwr.f2.test(u = 1, v = NULL, f2 = .02, power = .8)\n\n\nNotes about Power Analysis:\n\n\nPower was set to 0.8. This represents a ratio of 4:1 between Type II and Type I.\n\n\nThis data set is based on human interactions. Due to this, I expect all of the effects to be small. Thus, I set the f2 value to 0.02.\n\n\nU was set to one because for every model k = 2 a and u = df(k-1). Thus, u = 1.\n\n\nFinally, v was set to NULL because we are trying to calculate how many observations are needed for this model to be valid.\n\n\nAfter conducting your power analysis, use linear regression to test your hypotheses and produce appropriate visualizations.\nGraphs and linear Regression:\nExperience:\nexperience_model <- lm(effect ~ leader_experience, data = teamPerc_1)\nsummary(experience_model)\n\n\nggplot(teamPerc_1, aes(leader_experience, effect)) +\n  geom_point() + geom_smooth(method = ""lm"") + \n  ggtitle(""Leader Experience Graph"") + theme_classic()\n\n\nAs the graph titled ""Leader Experience Graph"" shows, as leaders gain experience, they typically have a more significant effect on their employees. This is also demonstrated by having a positive coefficient estimate for the variable leader_experience. It should be noted that the model only has an Adjusted R-squared value of 0.004406. This is an extremely low adjusted r squared, which is to be expected anytime someone is attempting to describe human behavior. However, as the graph shows, there are many data points, and this data should be investigated further. Thus, based on the model and graph, I reject my null hypothesis and accept my alternative hypothesis.\nFollow Up:\nfollow_up_model <- lm(effect ~ lvi40, data = teamPerc_1)\nsummary(follow_up_model)\n\n\nggplot(teamPerc_1, aes(lvi40, effect)) +\n  geom_point() + geom_smooth(method = ""lm"") + \n  ggtitle(""Follow Up Graph"") + theme_classic()\n\n\nThe first item that should be noted regarding this data is that the variable ""lvi40,"" which represents the amount the leader follows up, is not a continuous variable. With this being said, the estimated coefficient for lvi40 is 0.232082, which is positive and suggests that leaders have a more significant effect when they follow up with their employees. The trend line confirms this on the graph titled ""Follow Up Grap,"" which has a positive slope. Once again, the Adjusted R-squared value is low, which we expected because we are working with human behavior data. Due to this and the linear model, I reject my null hypothesis and accept my alternative hypothesis.\nForceful:\nforceful_model <- lm(effect ~ forceful, data = teamPerc_1)\nsummary(forceful_model)\n\n\nggplot(teamPerc_1, aes(forceful, effect)) +\n  geom_point() + geom_smooth(method = ""lm"") + \n  ggtitle(""Forceful Graph"") + theme_classic()\n\n\nFinally, the last model I created was forceful, and its impact on the leader\'s overall effectiveness My thinking behind my hypothesis was that leaders who were forceful with their employees were going to have a negative impact on their employees. This is demonstrated by the fact that the estimated coefficient value of -0.050922 and the graph ""Forceful Graph"" has a downward sloping trend line. As mentioned in previous model analysis, the Adjusted R-squared value was low, but we expected this. Based on this, I once again reject my null hypothesis and accept my alternative.\nSampling Data:\nThe calculations performed earlier determined that the required number of observations is 392.373. Due to this, I am going to resample the data and use 393 observations. In addition, I am going to use bootstrapping to take full samples out of the original data set.\nsilver_var <- dplyr::select(teamPerc_1, effect, leader_experience, lvi40, forceful)\nbootstrapping <- function(df) {\n  df <- df\n  \n  sampleValues <- sample(1:nrow(df), nrow(df), replace = TRUE)\n  \n  df <- df[sampleValues, ]\n  \n  bsMod <- lm(effect ~ forceful + lvi40 + leader_experience, data = df)\n  \n  results <- broom::tidy(bsMod)\n  \n  return(results)\n}\n\nbootstrapping(silver_var)\n\nbsRep <- replicate(393, bootstrapping(silver_var), simplify = FALSE)\n\nbsCombined <- do.call(""rbind"", bsRep)\n\nhist(bsCombined$statistic[bsCombined$term == ""forceful""], col = \'black\')\n\n\nPictured above is the histogram of forceful and the frequency that it occurs. The first item that should be addressed is that the absolute value of statistics form the bootstrap model is 14. This is significantly larger than 1.96, which is the generally accepted threshold for significance. In addition, the entire resultant distribution is between -10 and -20, which demonstrates that it is significant within the redistribution. This supports the conclusion to reject the null hypothesis and accept the alternative hypothesis.\nhist(bsCombined$statistic[bsCombined$term == ""lvi40""], col = \'black\')\n\n\nThis graph depicts a histogram of lvi40, which is whether leaders follow up with their employees. The distribution of the data is between 24 and 34. This is above the 1.96 thresholds for significance. Due to this, the resultant distributions support my hypothesis and conclusions I reached earlier in the bronze section.\nhist(bsCombined$statistic[bsCombined$term == ""leader_experience""], col = \'black\')\n\n\nFinally, the last histogram shows the forceful and the frequency it occurs. The distribution of the data is between 9 and 16. Thus, the model is greater than the 1.96 thresholds for statistical significance. This supports the conclusions I reached earlier in regards to my hypothesis.\nReview Models:\nFinally, I wanted to explore if there were any issues in the regression models, such as the standard errors, that I ran. I wanted to determine if there were any observations that exhibited significant leverage.\npar(mfrow = c(2,2))\nplot(experience_model)\n\n\nplot(follow_up_model)\n\n\nplot(forceful_model)\n\n\nIn all of the above models, there were observations exerting significant leverage on the residuals. This is due to some observations being outliers within the data set. Thus, the models have to redone using the MASS package and focusing on the least squares. To do this, I am going to do bisqaure weighting, which will down weight every residual that is not zero and in turn, pull the estimated coefficients in the regression down.\nforceful_model_bi <- rlm(effect ~ forceful, data = teamPerc_1, psi = psi.bisquare)\nfollow_up_model_bi <- rlm(effect ~ lvi40, data = teamPerc_1, psi = psi.bisquare)\nexperience_model_bi <- rlm(effect ~ leader_experience, data = teamPerc_1, psi = psi.bisquare)\n\nsummary(forceful_model_bi)\n\n\nsummary(follow_up_model_bi)\n\n<img src=""Visuals/Follow_Up_Model_rlm.png>""\nsummary(experience_model_bi)\n\n\nAs we expected, the new models reduced the standard error.\n'], 'url_profile': 'https://github.com/jjenki22', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'Nairobi', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dicksonoloo', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Arwindren', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Swift', 'Updated May 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/loganbussey', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Benin City', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Logistic-Regression\nHow to Build a Machine Learning Algorithm using Logistic Regression\nMy Youtube Video on this so to find out more and for explanation watch the video https://youtu.be/wSCUgLNuhF8\n'], 'url_profile': 'https://github.com/ImonEmmanuel', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Irvine, CA', 'stats_list': [], 'contributions': '354 contributions\n        in the last year', 'description': ['Estimating used car prices with Linear Regression (Craigslist Dataset)\nNearly everybody sooner or later needs to buy or sell a car. And every time people want to get a fair deal. There are actually two ways to find out a car price. First - to manually check prices on different car marketplaces like CarGurus, Autotrader, Craigslist, and then come up with a fair price based on those listings. People use this way a lot, but it has some disadvantages. For example, they can check just a small number of listings, and by not understanding the market well enough make a poor pricing decision that is biased towards very high or low price.\nAnother way to find out a car value is to use services that can predict a trade-in value. Examples of such services would be Shift, Carfax and many others. They have access to huge car databases plus well trained machine learning models that can predict a price just based on a car’s VIN number.  However, there is no information which models and algorithms they are using. One major disadvantage of those services is that they make money on trade-ins, so the predicted price will always be lower than a real market value, although it can give people at least some idea.\nThis project is located on the intersection between those two ways of pricing, so the goal is to eliminate all the disadvantages mentioned above and create a machine learning model that will predict a fair car selling price.\n'], 'url_profile': 'https://github.com/alexvznk', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Predicting-Credit-Default\nPredicting customer default on credit loans using logistic regression and KNN.\n'], 'url_profile': 'https://github.com/usman-naveed', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Ajax, Ontario', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sibin-Sibi', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Linear_Regression_Python\nImplementation of Linear Regression using Python Programming Language.\n'], 'url_profile': 'https://github.com/gauthampkrishnan', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Exploring-Marketing-Data\nExploring marketing data through statistical analysis and regression.\n'], 'url_profile': 'https://github.com/usman-naveed', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vanessazyf', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['LogisticRegressionBLB\nBag of Little Bootstrap for Logistic Regression Model. Taking in a clean binary logit dataset, this package utilizes Bag of Little Bootstraps upon a logistic model to generate various estimated intervals and errors.\nTo build an R-Package Skeleton\n\nCreate a new R package named ""LogisticRegressionBLB"", then run:\n\nlibrary(dev_tools)\nuse_readme_md()\nuse_package(""purrr"")\nuse_package(""magrittr"")\nuse_roxygen_md()\nuse_namespace()\nuse_test(""LogisticRegressionBLB"")\nuse_r(""builmodel.R"") or whatever R files we will have in here\n'], 'url_profile': 'https://github.com/ipalvis', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Peru', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Logistic-Regression\nIn this reposirtory I will use different logistic regression techniques. This type of regression is used for discrete data, usually a 1 or 0 values (Yes/No)\n'], 'url_profile': 'https://github.com/GianCarloTG', 'info_list': ['JavaScript', 'MIT license', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}"
"{'location': 'Chicago, Illinois', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['us_election_result2018\nUS Election Result 2018 (Implementing Regression, Classification, Clustering methods)\n'], 'url_profile': 'https://github.com/vidhyasagar13', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/premshan286', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['visual-regression-backstop-js-github-actions\nSimple example of a workflow with visual regression testing for a website.\nThis example assumes there is only one environment: prod\nThe outputs of the visual regression testing are available for devs to doublecheck if no failing regressions are introduced\n'], 'url_profile': 'https://github.com/dariobanfi', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['furry-winner\nThis project aims to predict the extent of pathogen damage on a field given a collection of climate factors. We use Ordinal Regression to determine how certain\nclimate factors effect a field.\n'], 'url_profile': 'https://github.com/kmcgloin1', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Hull', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Machine-Learning-Assignment-1\nPolynomial regression and k-means clustering without pre-built libraries\n'], 'url_profile': 'https://github.com/Bashton19', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Boston House Prices\nA regression analysis to predict the house prices in Boston\n'], 'url_profile': 'https://github.com/FerranDonoso', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Salt Lake City, Utah', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['pine-beetle\nFinal Project for STAT 330\nThis was the final assignment for a statistical modeling class.\nAnalysis of the regions at risk for infestation of the rocky mountain pine beetle using logistic regression.\n'], 'url_profile': 'https://github.com/lizzydrysdale', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Ghana', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['LogisticRegression\nBuilding a Logistic Regression algorithm with Diabetes dataset\nPackages required:\nPandas, NumPy, Matplotlib, Scikit-learn (sklearn)\n'], 'url_profile': 'https://github.com/Brian-code23', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['Machine-Learning-Project-Regression-to-Predict-onset-of-Diabetes\nPredicting onset of Diabetes using Linear and Polynomial Regression\nProject\nPredicting the progression of diabetes based on different demographic determinants\nThe goal of this project is to use different machine learning models to predict the progression of diabetes one year after baseline. There are 10 independent variables that are used for the prediction of the disease progression namely- age, sex, body-mass index, average blood pressure, and six blood serum measurements: low density lipoproteins (LDL), high density lipoproteins (HDL), total cholesterol (TC), triglyceride (TG), serum concentration of lamotrigine (LTG), glucose (GLU). The data was originally used in Lars Paper and has been scraped off from the following website using beautiful soup library and then it is finally read in to a text file. The text file is then converted in to excel file and is then read in to a pandas dataframe.\nThe data set has been normalized to have zero mean and variance of 1 and split in to training set and test set. The model is evaluated on the test set. Different models are evaluated that reduces the error or cost in predicting the correct output response varible (progression in diabetes in 1 year) such as:\n\nLinear Regression with single Variable\nLinear Regression with Multiple Variables\nPolynomial Regression (Regularized and Unregularized)\n\nThe training set is used for training the model. After the model is fitted on the training set, it is evaluated on both the training set and the test set. The model with the lowest cost or error in the test data set is concluded as the final model to be used for prediction of response variable.\nData\nData has been obtained from:\nhttps://web.stanford.edu/~hastie/Papers/LARS/diabetes.data\nProgramming Language\nPython 3.6 has been used for this project.\nLibraries\nFollowing libraries have been used in this project:\n\nnumpy\npandas\nmatplotlib\nseaborn\nBeautifulSoup\nsklearn\n\nStatistical Methodology\n\nData Wrangling\nData Visualization\nExploratory Analysis\nLinear Regression with single Variable\nLinear Regression with Multiple Variables\nPolynomial Regression (Regularized and Unregularized)\n\nNotebook:\nPython notebook file is available with this project and is named:\nDiabetes_ML.ipynb\nPowerpoint Summary:\ndiabetes_regression.pptx\n'], 'url_profile': 'https://github.com/GauriSaran', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Nairobi', 'stats_list': [], 'contributions': '232 contributions\n        in the last year', 'description': ['profit_predictions\nthis is a simple linear regression to predict profits maade\n'], 'url_profile': 'https://github.com/Stephen-Kamau', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'TypeScript', 'Updated Mar 13, 2020', 'R', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 11, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 16, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Seaborn_Iris\nPractising the logistic regression using Seaborn_Iris Dataset\n'], 'url_profile': 'https://github.com/dipinpdinesh', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['#Taller 6: Visual Regression Testing utilizando Resemble JS\n'], 'url_profile': 'https://github.com/haortizr', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '429 contributions\n        in the last year', 'description': [""Property Price Prediction in King County (Seattle)\nExecutive summary\nWe're taking the position of a real estate agency to advice prospective sellers in setting the right asking price, and to prospective buyers to gauge if an asking price is a good deal for a given area.\nSpecifically, we are looking to answer the following questions:\n\nWhat are the main drivers that contribute to property prices in King County?\nCould property prices in King County be reliably predicted?\nWhich areas in King County can be considered as premium locations?\n\nHigh-level overview of methodology:\n\nDiscuss the direction of project and key questions to be addressed.\nInitial data exploration, data cleansing and agree on assumptions.\nDetailed analysis of data to identify potential drivers for price.\nCreate new derived fields from raw data to further help the analysis.\nSubset data into categories where applicable.\nSelect key drivers for the model and further scrutinise them.\nRun first regression model with a single (primary) variable.\nAnalyse first regression model for improvements.\nRun second regression model with multiple variables.\nAnalyse second regression model for improvements.\nRepeat multiple variable model using different combinations of variables.\nDecide on final variable combination that gives the best r2.\nRefine model by exploring normalisation of variables and retest.\nFinalise model reliability via various statistical measures.\n\nKey findings and conclusions:\nOur model shows that the primary driver for property prices in King County is the square footage of the living space. Property prices are also highly impacted by its location in the city (derived from zipcode): in particular, properties in premium areas in and near the city have higher prices than it outer suburbs. Building grade as set by the county authority is also another factor as higher grade properties are in better condition and thus can fetch a higher price. Finally in our model, waterfront view is another factor that pushes the property price up due to the prime views.\nWith an r-squared value of 0.701 for our model, we conclude that the property price in King County can be reasonably predicted. However, we are confident that the model can be further refined if more data is available. As our model factors in the impact of the location, additional area information (for example, by zipcode) such the number of schools, public transportation and crime rate would be very useful. We believe that the high property prices in the premium areas of the city are partly driven by these factors.\nWe worked on the assumption that areas with significantly higher price per square feet (of living space) than the overall county constitute as premium locations. Cross-referencing our findings with a zipcode map (link here), we noted that Medina (zipcode 98039) is most premium location in King County, driven by its proximity to the city centre and great waterfront views facing Lake Washington. In addition, zipcodes around Belleview and downtown Seattle are also identified as premium locations.\nWith these results, we are confident that we would be able to serve our clients better in advising them on the right price for both selling and buying.\n""], 'url_profile': 'https://github.com/khairulomar', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vivek-srinivasan90', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Predicting-House-Prices\nPredicting House sale prices with 80 predictors using regression models.\nUsed Random Forest model and XGBoost model for this dataset. Feature Selection was done by Boruta Method\nand Model selection was done by cross validation.\nGot RMSLE of 0.15.\n'], 'url_profile': 'https://github.com/jishnair19', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['covid-19\nNon-linear logistic regression models for the Covid-19 outbreak\nData source is the Johns Hopkins University repo.\nJust two snippets to model the covid-19 outbreak data (see in the models folder)\nThese are definitely far from perfect and I would not use these for any official report so please use with caution. This code was written as hobby and as an opportunity to learn more about non-linear logistic regression.\nDescriptive statistics across countries. Total count and daily increase\n\n\n\nDetected cases by country\nDeath-count by country\n\n\n\n\n\n\n\n\n\nLooking at the changes in the rate for death and new cases (both plots), one could speculate that the timings of the virus (incubation time, duration of disease) are so that any counter-measure taken by a goverment today will have a delayed effect of roughly two weeks (possibly less).\n\n\n\nDaily detected cases by country\nDaily death-count by country\n\n\n\n\n\n\n\n\n\nExtrapolations from the logistic model\n[being reworked in a SIRD model] work in progress - contribute if you like\n'], 'url_profile': 'https://github.com/artoo-git', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['-Black-Friday\nPredict purchase on Black Friday with various regression models\n\n\nMultiple Linear Regression\n\n\nRidge Regression\n\n\nLasso Regression\n\n\nDecision Tree Regression\n\n\nRandom Forest Regression\n\n\nGradient Boosting Regression\n\n\nCompare all models with Cost Function (RMSE) which one is the least RSME and then\nbring that model to do hyper tuning parameter to find optimum parameters\n'], 'url_profile': 'https://github.com/PanuwatUlis', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['meta-analysis\nCode for meta-analysis and meta-regression in R.\n'], 'url_profile': 'https://github.com/dbannai', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Karachi,Pakistan', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['asymmetric-cost-functions\nDemonstration of asymmetric cost functions in Linear Regression\n'], 'url_profile': 'https://github.com/HassaanSaleem', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/raoharish02', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 10, 2020', 'R', 'Updated May 21, 2020', '1', 'R', 'GPL-3.0 license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/samip-thakkar', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': [""Starter Code for Deploying Flask\n\nThis repository outlines the basic starter code for deploying a flask application in Python, assuming you have a machine learning file you'd like to interact with.\n\n""], 'url_profile': 'https://github.com/GarrettEichhorn', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['MachineLearning\nLinear & Losso Regression\nSupport Vector Regression\nCar price prediction with Machine Learning.\nPython 3.8 & Scikit learn\n#Cleaning and preparation of data\n#One Hot encoding\n#Scaling\n#Linear and SVM regression.\n'], 'url_profile': 'https://github.com/semihyilmazz', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Decision-Trees-Regressor\nDecision Trees Regressor\n'], 'url_profile': 'https://github.com/kar6270', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['XG-Boost-Regressor\nXG Boost Regressor\n'], 'url_profile': 'https://github.com/kar6270', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Random-Forest-Regressor\nRandom Forest Regressor\n'], 'url_profile': 'https://github.com/kar6270', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Machine-Learning-Codes\nA collection of the codes implemented during my machine learning course at NYU Tandon\n'], 'url_profile': 'https://github.com/tejasrshetty', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Bengaluru, India', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Polynomial_Regression\nProblem Statement: The objective of the problem is to predict values “current price” attribute from the given features of the Test data.  Essentially, the company wants —  To identify the variables affecting cars current prices,  To create a linear model that quantitatively relates cars current prices with identified significant variables.  To know the accuracy of the model, i.e. how well these variables can predict car prices.  So interpretation is important!\n'], 'url_profile': 'https://github.com/nidhifactualai', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '347 contributions\n        in the last year', 'description': ['lienar-regression\n'], 'url_profile': 'https://github.com/NormanBenbrahim', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Regression Regularization\nRegularization is a means to reduce overfitting, reducing the degrees of freedom of the model in order to reduce the variance in the model and thereby increasing its ability to generalize.\nThis model takes a childhood respiration dataset to illustrate various techniques in regression for regularization.\nLasso implements L1 norm regularization. Lasso takes the absolute value of the weights as a penalty for the loss function.\nRidge implements L2 norm regularization Ridge takes a square of the weights as a penalty for the loss function.\nElasticNet implements a mix of Lasso and Ridge\n'], 'url_profile': 'https://github.com/billwparker', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AakankshaBaid', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': [""Linear-Regression\nProblem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Preparation\nThere is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. You need to consider only company name as the independent variable for model building.\nModel Evaluation:\nWhen you're done with model building and residual analysis, and have made predictions on the test set, just make sure you use the following two lines of code to calculate the R-squared score on the test set.\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred)\nwhere y_test is the test data set for the target variable, and y_pred is the variable containing the predicted values of the target variable on the test set.\nPlease don't forget to perform this step as the R-squared score on the test set holds some marks. The variable names inside the 'r2_score' function can be different based on the variable names you have chosen.\n""], 'url_profile': 'https://github.com/chhandita', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cumair600', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear-Regression\nBank loan classification using different algorithms\n'], 'url_profile': 'https://github.com/gopidesupavan', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/schrodinger007', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'Cambridge', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': [""Linear Regression\nAn analysis of shrinkage methods for linear regression (adapted from 'Elements of Statistical Learning')\nThe classic linear regression problem has a simple cost function\n\nwhere  represents the intercept,  represents the vector of linear coefficients, and  represent the input data matrix and response vector respectively. However how do we balance bias and variance using this approach? Below the data from the book Elements of Statistical Learning is used. 8 medically related variables are used to predict log of prostate specific antigen. The data is first augmented twice using a gaussian random distribution, and the data normalised to have mean 0 and standard deviation of 1. 4 folds were used for validation, and the error averaged over all 4 folds.\nRidge Regression\nBy adding a constraint , the parameters  are somewhat constrained. By then varying , the bias, variance trade-off can be determined and an optimum set of parameters can be found.\n \nThe parameters are seen to vary smoothly. This can be explained by the constraint function limiting the parameters describing a hypersphere in parameter space the parameters must lie within. Assuming the 'best' set of parameters lies outside of this hypersphere, the parameter values at a value of t lie on the boundary. As t is increased and this hypersphere grows in radius , the parameters are smoothly varied on the border of this hypersphere constraint until the optimal set of parameters lie within the hyperspherical contraint. Then the parameters assume a constant value when changing t as the constraint is no longer active.\nFor a two dimensional problem, function and parameter space can be visualised easily.\n\nLasso Regression\nLasso regression assumes a similar form to ridge regression however instead of a squared constraint, the constraint limits the sum of the absolute value of the parameters,  (sometimes called the Taxicab norm for reasons I won't go into). Instead of a smooth hypersphere, this results in a polyhedrally constrained area in parameter space. Due to this now linear volume, the parameter set on the edge of this constraint is more than likely to lie at a vertex. As t is then varied and the polyhedron grows towards the optimal parameter set, the parameter set on the edge of the constraint is more likely to 'snap' into a new parameter dimension, i.e. move to a different vertex. This explains the more discretized parameter values, and the behaviour that certain parameters don't appear at all in the set until t is sufficiently large.\n \nHow late on a parameter is introduced in Lasso regression relates to how 'important' this input is with respect to predicting the output. The sooner the parameter appears, the more important it's respective variable is in predicting the output (in this case amount of prostate specific antigen).\nLasso regression for a two dimensional problem.\n\n""], 'url_profile': 'https://github.com/TomRSavage', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,807 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tagty', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'Gr. NOIDA', 'stats_list': [], 'contributions': '553 contributions\n        in the last year', 'description': ['Logistic-regression\nA great algo which can predict as well as classify (regression+classification)\n'], 'url_profile': 'https://github.com/prakharR534', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sebastianoscarlopez', 'info_list': ['R', 'Updated Mar 15, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 12, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '374 contributions\n        in the last year', 'description': ['Linear_Regression\n'], 'url_profile': 'https://github.com/lathamithu', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/victoriabotis', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Memphis, TN', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['Linear-Regression\nA practice notebook to implement a linear regression model on vehicular data to predict CO2 emissions\n'], 'url_profile': 'https://github.com/zachcornelison', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Logistic-Regression\nA logistic regression model to predict whether a student gets admitted into a university.\nAlso Implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA).\n'], 'url_profile': 'https://github.com/jgmartinez90', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AndySheHoi', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['linear_regression\nScholar projet\nWritten in Golang (go1.13 darwin/amd64)\n\nUsage\nMakefile will compile both executables in the bin folder\nThe train command will read the data.csv file and export model.json. A visual of the dataset and the regression is generated on ml.png image.\nThe predict command allows to make a prediction based on the trained model\nmake\n./bin/train\n./bin/predict\nAuthor\n\nFabien Blin @ 42 Lyon\n\n'], 'url_profile': 'https://github.com/fabienblin', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NarayanaDS', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Logistic-regression\n'], 'url_profile': 'https://github.com/detached-whale', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Namibia', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jpandeinge', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Thrupthi24', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 25, 2020', 'Go', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}"
"{'location': 'Namibia', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jpandeinge', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sofia-git', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['VC-regression\n'], 'url_profile': 'https://github.com/juannanzhou', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manindersingh99', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['LogisticRegression\nLogistic Regression for Social Network Ads buying or not ( 1 or 0 )\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'Kharagpur', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/souviksamanta95', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['Linear Regression to predict house prices in bangalore\n'], 'url_profile': 'https://github.com/vhanumankumar', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['Regression-Methods (Exercise)\nDataset:\nhttps://www.superdatascience.com/pages/machine-learning\nhttps://www.kaggle.com/\nMethods:\nSimple Linear Regression\nMultiple Linear Regression\nPolynomial Regression\nLogistic Regression\n'], 'url_profile': 'https://github.com/Liinhleo', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'Cherkasy', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/hontarenkoYana', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}","{'location': 'San Jose, California', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/demigecko', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'MATLAB', 'Updated Mar 9, 2020', 'MIT license', 'Updated Mar 11, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 14, 2020', 'R', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['Linear Regression to predict house prices in bangalore\n'], 'url_profile': 'https://github.com/vhanumankumar', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['Regression-Methods (Exercise)\nDataset:\nhttps://www.superdatascience.com/pages/machine-learning\nhttps://www.kaggle.com/\nMethods:\nSimple Linear Regression\nMultiple Linear Regression\nPolynomial Regression\nLogistic Regression\n'], 'url_profile': 'https://github.com/Liinhleo', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'Cherkasy', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/hontarenkoYana', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'San Jose, California', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/demigecko', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EricGrose', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '226 contributions\n        in the last year', 'description': ['Penalized-regressions\nProject for Machine learning class\nDirections: Answer the following questions and write your answers in a word processor.\nMathematical symbols may be written by hand or using the equation editor, but standard text must\nbe typed. Necessary or requested plots or other visualizations must be included. Justify your\nconclusions and answers to receive full or partial credit. Knitted output files (e.g. html, pdf, word)\nfrom R Markdown are acceptable.\nA dataset concerning housing values in the Boston area is available in the MASS function in R.\nThis dataset is used and described in Introduction to Statistical Learning. We are interested in\npredicting the median housing value. (Note! This dataset was gathered in the 1970s, hence\nthe relatively low housing values.)\n\nEnsure that your report is clear and complete. I expect the following. (20 points)\na. Clearly commented code\nb. Easy to find answers to the questions\nc. Clearly organized output and answers to the questions including using complete\nsentences with correct spelling and grammar\nd. Figures and tables will have easy to understand labels and legends as needed\nRead in Boston data through the MASS library in R and use ?Boston to read about the\ndataset. (5 points)\nBreak the dataset into a training set a testing set. Please make sure to use your own\nrandomized split of the data (in other words, do not share your data with others). (5\npoints)\nUse 3 of the following methods to predict median housing value (15 points)\n• Elastic net (a combination of ridge and lasso)\n• Lasso regression\n• Linear regression model using variable selection\n• Partial least squares\n• Principal components regression\n• Ridge regression\nCreate a new variable called high_value that is 1 for a median home value ≥ $250,000\n(i.e. medv ≥ 25) and 0 for a median home value < $250,000 (i.e. medv < 25). (5 points)\nUse logistic regression alternatives to the 3 methods you used in part 3 to predict\nhigh_value. (Note, Partial Least Squares Discriminant Analysis, PLS-DA is a method to\napply partial least squares to a categorical outcome. plsda is a function in R and is\ndescribed here: https://rdrr.io/cran/mixOmics/man/plsda.html). (15 points)\n\n\na. Summarize and compare the training and test prediction error for the results\nfrom part 3 and part 5 in a table or a figure. (10 points)\nb. Based on these results, which model do you recommend for part 3 and why?\n(5 points)\nc. Based on these results, which model do you recommend for part 5 and why?\n(5 points)\nd. Do you prefer to treat median home value as a quantitative or dichotomous\nvariable? Why? (5 points)\n8. Calculate a 95% bootstrap confidence interval for at least one statistic from one of\nyour 6 models. Interpret this confidence interval. (10 points)\n'], 'url_profile': 'https://github.com/Andrey776', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/sunflies', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nistha-tech', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/sunflies', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}","{'location': 'Gainesville, FL', 'stats_list': [], 'contributions': '274 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/carrie-cao', 'info_list': ['Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'HTML', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/sunflies', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'Gainesville, FL', 'stats_list': [], 'contributions': '274 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/carrie-cao', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'Brooklyn, NY', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['READ THIS\nThis is the working directory for my project in my class. (SDS335)\n'], 'url_profile': 'https://github.com/leoflowers', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'Lowell, MA', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hossein-haeri', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['Linear Regression\nCode to implement linear regression model from scratch to determine the quality of wine\n'], 'url_profile': 'https://github.com/samyuktha17', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'Indore,India', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['Regression\nDifferent linear and non linear Regression models\n'], 'url_profile': 'https://github.com/parva-jain', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jasminkarki', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yelhari', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Project Name\n機器學習預測未來的用電量\n-- Project Status: [Completed]\nProject Intro/Objective\n這個Project的主要目的是,利用過去的用電量來預測用戶的未來用電量,不限於特定用戶只要資料型別符合皆可以預測。\nMethods Used\n\nMachine Learning\nData Visualization\nPredictive Modeling\netc.\n\nTechnologies\n\nPython3\nkeras, sklearn\nPandas, jupyter\nseaborn, matplotlib\netc.\n\nNeeds of this project\n\nfrontend developers\ndata exploration/descriptive statistics\ndata processing/cleaning\nstatistical modeling\nresult analysis\nwriteup/reporting\netc.\n\nFeatured Notebooks/Analysis/Deliverables\n\nlinearRegression\nLSTM_final\nPolynomial Regression1\n\n'], 'url_profile': 'https://github.com/williron3960', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amahjourwalid', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated May 4, 2020', 'C', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020']}"
"{'location': 'Chania, Greece', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Linear-Regression-FGM\nAbstract\nWe introduce  Functional Geometric Monitoring a substantial theoretical\nand practical improvement on the core ideas of Geometric Monitoring.\nInstead of a binary constraint, each site is provided with a complex, non-linear\nfunction, which, applied to its local summary vector, projects to it to a\nreal number.\n\nSimulator\nIn order to evaluate this distributed algorithm we used Distributed-Data\n-Stream-Simulator, which is implemented in python.\nRequirements\nImplemented with python: version 3.6 (or higher)\nSome not built-in python modules are required. To install run:\nconda env create --file envname.yml\n\nHow to Run\nIn order to run an example algorithm , simply run:\npython main.py\n\nHow to Test\nFor testing pytest module was used.To run all test do the following:\npytest -v\n\nIn order to run a specific group of tests you have to run the following:\npytest -k {keyword} -v\n\nDocumentation\nFor the documentation Sphinx 2.3.1. software was used.\nTo create documentation just run:\ncd docs\nmake clean html\n\nThen open the index.html with a browser.\n'], 'url_profile': 'https://github.com/eseisaki', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Amritsar, Punjab, India', 'stats_list': [], 'contributions': '257 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Komal7209', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LuisEduardoC', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [' Linear Regression\n\nUnit 3: Required\n\n\nMaterials We Provide\n\n\n\nTopic\nDescription\nLink\n\n\n\n\nLesson\nLinear Regression lesson\nHere\n\n\nSolution\nSolution code for Lesson Prompts\nHere\n\n\nSlides\nSample slides for this lesson (PPTX)\nHere\n\n\nPractice\nKobe Shot Regularization\nHere\n\n\n\nSimple Linear Regression\nHere\n\n\n\nRegression Review Lab\nHere\n\n\nDatasets\nCapital Bikeshare Rider Data, Kobe NBA data, Sacremento Real Estate Data\nHere\n\n\n\n\nThe lesson data was chosen because it generates approximate linear relationships using real-world data.\n\n\nLearning Objectives\nAfter this lesson, students will be able to:\n\nDefine data modeling and how to apply a simple linear regression.\nBuild a linear regression model using a dataset that meets the linearity assumption, using the sci-kit learn library.\nDefine and identify multicollinearity in a multiple regression.\n\n\nStudent Requirements\nBefore this lesson(s), students should already be able to:\n\nCollect data and perform basic data manipulations with Pandas\nImport libraries into Python scripts\nCreate simple data visualizations using Python\nExplain basic statistical concepts including linear algebra and descriptive statistics\n\n\nLesson Outline\n\nTOTAL (170 min)\n\n\nIntroduce the bikeshare dataset (20 min)\n\nRead in the Capital Bikeshare data (15 min)\nVisualizing the data (5 min)\n\n\nLinear regression basics (15 min)\n\nForm of linear regression\n\n\nOverview of supervised learning (25 min)\n\nBenefits and drawbacks of scikit-learn (5 min)\nRequirements for working with data in scikit-learn (5 min)\nBuilding a linear regression model in sklearn (5 min)\nscikit-learn\'s 4-step modeling pattern (10 min)\n\n\nBuild a linear regression model (10 min)\nUsing the model for prediction (15 min)\n\nDoes the scale of the features matter?\n\n\nWork with multiple features (20 min)\n\nVisualizing the data (part 2) (15 min)\nAdding more features to the model (5 min)\n\n\nWhat is Multicollinearity? (10 min)\nHow to select a model (25 min)\n\nFeature selection (5 min)\nEvaluation metrics for regression problems (10 min)\nComparing models with train/test split and RMSE (5 min)\nComparing testing RMSE with null RMSE (5 min)\n\n\nFeature engineering to improve performance (30 min)\n\nHandling categorical features (15 min)\nFeature engineering (15 min)\n\n\nBonus material: Regularization\n\nHow does regularization work?\nLasso and ridge path diagrams\nAdvice for applying regularization\nRidge regression\n\n\nComparing linear regression with other models\n\n\nAdditional Resources\nFor more information on this topic, check out the following resources:\n\nBen Lorica: Six reasons why I recommend scikit-learn\nScikit-learn examples for Lasso and Ridge Regression\nScikit-learn documentation for Lasso,  Ridge, and Elastic Net Regression\nAnalytics Vidhya\'s Compilation of Linear Regression Blogs\nData School\'s ""Friendly Introduction to Linear Regression"" using Python\n\n'], 'url_profile': 'https://github.com/mirandabenthin', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hoanglan21', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kamaleshsonu', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['PolynomialRegression\n'], 'url_profile': 'https://github.com/Soulstealer07', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '591 contributions\n        in the last year', 'description': ['Linear-Regression\nTask:\n\nGiven a dataset containing historical weather information of certain area, imple-\nment a linear regression model from scratch using gradient descent to predict the\napparent temperature. The various attributes of the data are explained in the file\ndescription.txt. Note that attributes are text, categorical as well as continuous.\nNote: Test data will have 10 columns. Apparent temperature column will be\nmissing from in between.\nCompare the performance of different error functions ( Mean square error, Mean\nAbsolute error, Mean absolute percentage error) and explain the reasons for the\nobserved behaviour.\nAnalyse and report the behaviour of the regression coefficients(for example: sign\nof coefficients, value of coefficients etc.) and support it with appropriate plots as\nnecessary.\n\n'], 'url_profile': 'https://github.com/AshishKempwad', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['testing coefficency:50 791 726 736 727 526 267 712 668 109\ntesting standard:>\u05fc0.1\nresult:50  89 109 167 267 403 526 668 712 760 791 991\nTP= 0.7\nFP= 0.3\nThe running time is relatively slow.\n'], 'url_profile': 'https://github.com/bluesun2019', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EricGrose', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'R', 'Updated Mar 11, 2020', 'Updated Mar 11, 2020', '1', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}"
"{'location': 'pune', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Logistic Regression was used in the biological sciences in early twentieth century. It was then used in many social science applications. Logistic Regression is used when the dependent variable(target) is categorical.\nFor example,\nTo predict whether an email is spam (1) or (0)\nWhether the tumor is malignant (1) or not (0)\nConsider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.\nFrom this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.\n'], 'url_profile': 'https://github.com/ShradhaBidwai', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['logistic-regression\nBasic logistic regression example with given data set. Different parameters have been used to find best accuracy.\nAll the accuracy values and visualized data have been shown in the report file including detailed explanations.\nThanks to my friend İrem Atılgan who contributed the most.\n'], 'url_profile': 'https://github.com/cavuscanay', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/amit-dipu', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['regression_qqplot\nThis is a toy model to show how we can use QQ plot to test if your residual follows normal distribution.\n'], 'url_profile': 'https://github.com/Matthew-Peng', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['Linear_Regression\nThis was done as a part of Springboard Data Science Career Track course (Unit10.1.5. Linear Regression Using Boston Housing Data Set).\n'], 'url_profile': 'https://github.com/NamikoNa', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '374 contributions\n        in the last year', 'description': ['Logistic-Regression1\n'], 'url_profile': 'https://github.com/lathamithu', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '457 contributions\n        in the last year', 'description': ['Polynomial-Regression\n'], 'url_profile': 'https://github.com/bsingh17', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '1,522 contributions\n        in the last year', 'description': ['Ridge-and-Lasso-Regression-in-R\nRidge and Lasso regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression.\nLearn more about it in detail here:\n\nhttps://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\nhttps://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n\n'], 'url_profile': 'https://github.com/tanaymukherjee', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marioeid', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aquahamzah', 'info_list': ['Updated Sep 29, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', '1', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 21, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Updated Mar 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VincentMalara', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['LinearRegression.jl\nSimple linear regression package.\n'], 'url_profile': 'https://github.com/LAMPSPUC', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['boston-housing-linear-regression\nBoston Housing: A Careful Regression Analysis\nA Jupyter notebook that explores the most important features in the boston housing dataset. Emphasis is placed on verifying the assumptions of Linear Regression, and using the fact that those assumptions hold to make precise statements about the relationship between house prices and two of the predictive variables.\n'], 'url_profile': 'https://github.com/danieltamming', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nihalika', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'Memphis, TN', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': [""China-GDP-regression\nPractice notebook to model China's GDP data from 1960 - 2014 using a non-linear regression model\n""], 'url_profile': 'https://github.com/zachcornelison', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Linear-Regression--R\nPerformed Linear regression using R on the auto dataset to predict the price of cars based on different regions throughout the country\n'], 'url_profile': 'https://github.com/HimanshuNirmal1', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChaudhariShubham', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bajajpooja85', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'Bhubaneswar', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Regression Analysis\nRegression analysis is one of the most widely used methods for prediction. Linear regression is probably the most fundamental machine learning method. A linear regression is a linear approximation of a causal relationship between two or more variables. OLS (Ordinary Least Squares) is the simplest and a sufficient method to estimate the regression line.\n'], 'url_profile': 'https://github.com/Soumya-Pathy', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['RandomForestRegression\nExample of RandomForestRegression\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['Updated Mar 9, 2020', '1', 'Julia', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['library\nPython library that implements the different Fuzzy Regression approaches.\n'], 'url_profile': 'https://github.com/TheGarkine', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MayurG25', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Ilmenau, Germany', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Regression-model-with-keras\ndeveloping a neural network for prediction using keras\n'], 'url_profile': 'https://github.com/amir78698', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Tumour regression calculation tool (TURCAT)\nUser can shape and move a box around CT, MRI or PET scans of a tumour for images pre and post treatment. A tumour regression curve is then calculated to determine how effective treatment is.\nDescription\nUsers initially specify a folder containing dicom (.dcm) images of medical scans at different stages of treatment. Each image will appear sequentially (following exiting the window of the previous image) with a red box in the centre, which can be moved and reshaped around the tumour using the mouse and arrow keys. A line graph is displayed after every image is analysed, showing the tumour regression over time, and then saved as .png file in the directory of the specified folder.\nUsage\nArrow keys to shorten/lengthen the box\n-Right arrow key makes the box 1 unit wider\n-Left arrow key makes the box 1 unit less wide\n-Up arrow key makes the box 1 unit taller\n-Down arrow key makes the box 1 unit shorter\nHold the control key with the use of any arrow key to reshape the box by 10 units (e.g. ctrl-right makes the box 10 units wider).\nClick and drag the box around to move the box around the tumour.\nNote: If the tumour regression curve is calculated in the same folder twice, the original image will be overwritten.\nInstallation\nDownload the folder with the script, test images and test output\nPre-requisite python modules\npython3.8\nmatplotlib.pyplot\nmatplotlib.patches\npydicom\nos\nDisclaimer\nPlease note this software is for research use only and not intended as a clinical device, or to aid any form of clinical judgement.\n'], 'url_profile': 'https://github.com/aaron-obrien', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lixuf', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Linear_regression_assignment\nThis assignment predicts the car models to be made to enter into US and Europe automobile industry using Linear Regression\n'], 'url_profile': 'https://github.com/chandan54', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Book-Applied-regression-analysis\nWeekly report slides update\nBook: applied regression analysis and other multivariable methods\n'], 'url_profile': 'https://github.com/yi-notes', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Edinburgh', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Quick PDF link:\nreport.pdf\n\n\n\n'], 'url_profile': 'https://github.com/samsamiczy', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Seoul', 'stats_list': [], 'contributions': '390 contributions\n        in the last year', 'description': ['Craiglist 데이터를 활용한 미국 중고차 가격 예측\n\n유호원, 조대선, 홍성현, 배준영\n\nGoals\n\n""Craiglist(미국판 중고나라)""의 약 51만건의 미국 중고차 정보를 활용한 가격 예측\n\nTechnical Skills\n\nPython, Scikit-learn, pandas, numpy\n\nWorkflow\n\nIssue\n\n데이터 특성상 허위 및 광고성 매물로 인한 주행거리, 연식등의 이상치들이 다수 존재.\n따라서, 이상치 탐색 및 제거가 중요.\n\nIssue solving\n\nvin(차대 번호) 를 활용한 이상치 탐색 작업 진행\nhttps://www.vinaudit.com/ 에서 제공하는 api를 이용하여 허위 매물 탐색 및 이력 조회\n위 api는 미국 정부기관에서 관리하는 데이터베이스를 기반으로 제작, 신뢰도가 높음\n\nResult\n\nR-square 약 0.88 달성\n\n\n\n\n가설 검증\n\n가설 1 : 미국의 중고차도 한국과 마찬가지로 약 5만km를 기준으로 가격이 급격히 떨어질 것이다.\n\n\n\n\n데이터에서 가장 많은 매물인 2012년식 포드 F-150 FX4 차량운 주행거리 3만 마일 (약 4만8천km) 지점에서 가격이 급격히 하락\n\n\n가설 2 : 미국은 각 주별로 가격의 차이가 있을 것이다.\n\n\n\n\n포드 F-150 FX4 차량의 워싱턴 주와 코네티컷 주의 차량 가격의 차는 약 1만불\n\n\n\n한계 및 개선점\n\n자동차 보증수리 여부에 대한 명확한 데이터의 부재로, Model 5의 아이디어를 좀더 발전 시키지 못함\n\n'], 'url_profile': 'https://github.com/HowardHowonYu', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Logistic-Regression-Performance\n'], 'url_profile': 'https://github.com/rahul765', 'info_list': ['1', 'Python', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'TeX', 'MIT license', 'Updated Mar 14, 2020', 'R', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['MULTIVARIATE-LINEAR-REGRESSION\nWe will predict employee salaries from different employee characteristics (or features).\nImport the data salary.csv to a Jupyter Notebook.\nA description of the variables is given in Salary metadata.csv.\nYou will need the packages matplotlib / seaborn, pandas and statsmodels.\n'], 'url_profile': 'https://github.com/jbmasemza', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/samuelcantrell', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Online Feature Selection for Regression\nWe  consider  sparse  regression  in  an  online  setting  for  problems  where  high-dimensional data arrive sequentially. We compare two recent methods: truncatedstochastic gradient descent (TSGD) and online least square with thresholding(OLST), and propose a hybrid method, OLST-then-TSGD. We implement thesealgorithms and compare their performances with each other and two offline sparseregression algorithms on synthetic and Wikiface images datasets.\n'], 'url_profile': 'https://github.com/fransiscasusan', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'CA, Los Angles ', 'stats_list': [], 'contributions': '425 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mahrokh-Eb', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Remsko', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'Pereira', 'stats_list': [], 'contributions': '640 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HectorMontillo', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Energy-Efficiency-Regression\n'], 'url_profile': 'https://github.com/jeremygithubuser', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['WorkShop-Epitech---regression\ninstallation / Setup\nmandatory : python3\npip3 install virtualenv\nvirtualenv venv\nLinux : source venv/bin/activate\nWindow : venv/Scripts/activate.bat\ninstallation modules in virtualenv\npip install scikit-learn\npip install notebook\npip install matplotlib\nlunch jupyter-notebook\njupyter-notebook\nclick on face_completion_regression and start coding\n'], 'url_profile': 'https://github.com/loisBentrari', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Advertisement-Logistic-Regression-Project\nIn this project we will be working with a fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement on a company website. We will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\n'], 'url_profile': 'https://github.com/Muk18', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nagarajpandiyan', 'info_list': ['Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 11, 2020', 'C#', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['PHM Masterclass 4\nLogistic_regression\n'], 'url_profile': 'https://github.com/andrw-jns', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kunyew', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Factor-Analysis-and-Regression\nThis project identified the factors to study satisfaction level of customer. Initially a factor analysis was done to identify the latent variables that influence customer satisfaction. Following this, a regression model was built using these factors for market segmentation. The objective of the project is to use the dataset to build a regression model to predict customer satisfaction.\n'], 'url_profile': 'https://github.com/chandrima1102', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['python_liner_regression\n使用numpy搭建线性拟合模型\nnumpy线性拟合练习\n使用numpy实现梯度下降算法,从本地csv读入数据集\n实时可视化\n基于matplotlib实时显示训练损失\n使用paddle paddle\nLiner_Regression.py 为纯numpy实现\nLiner_Regression_paddle.py 为paddle实现\ndata_generator.py 随机生成线性数据（自定维度）\n'], 'url_profile': 'https://github.com/Cerber2ol8', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Visakhapatnam', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Logistic-Regression-Donors-choose\n'], 'url_profile': 'https://github.com/Snehitha17', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kegerber', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': [""Answering Business Questions Using Linear Regression\nData exploration and application of Linear Regression ML algorithm (using Scikit-Learn) for an ECommerce dataset (fake contact details). The problem is whether the New York ECommerce company should focus their efforts on their mobile app or their website.\nA machine learning application exercise from Jose Portilla's course, Python for Data Science and Machine Learning Bootcamp.\nMore about the course here.\n""], 'url_profile': 'https://github.com/renzabergos', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aishwarya-DA', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'El Dorado Hills, California ', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Logistic_Regression_Duke_NB\n'], 'url_profile': 'https://github.com/As-gonsolin', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Syracuse, New York, USA', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rishupadhye', 'info_list': ['R', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020']}"
"{'location': 'El Dorado Hills, California ', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Logistic_Regression_Duke_NB\n'], 'url_profile': 'https://github.com/As-gonsolin', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Syracuse, New York, USA', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rishupadhye', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kunyew', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Factor-Analysis-and-Regression\nThis project identified the factors to study satisfaction level of customer. Initially a factor analysis was done to identify the latent variables that influence customer satisfaction. Following this, a regression model was built using these factors for market segmentation. The objective of the project is to use the dataset to build a regression model to predict customer satisfaction.\n'], 'url_profile': 'https://github.com/chandrima1102', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['python_liner_regression\n使用numpy搭建线性拟合模型\nnumpy线性拟合练习\n使用numpy实现梯度下降算法,从本地csv读入数据集\n实时可视化\n基于matplotlib实时显示训练损失\n使用paddle paddle\nLiner_Regression.py 为纯numpy实现\nLiner_Regression_paddle.py 为paddle实现\ndata_generator.py 随机生成线性数据（自定维度）\n'], 'url_profile': 'https://github.com/Cerber2ol8', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Visakhapatnam', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Logistic-Regression-Donors-choose\n'], 'url_profile': 'https://github.com/Snehitha17', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SandaruThilakarathne', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Oakland, CA', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/caseyp510', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Jeddah, Saudi Arabia ', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': [""Logistic-Regression-Advertisement-Data\nLogistic Regression ..... we will be working with a fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement.\nWe will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\nThis data set contains the following features,\n'Daily Time Spent on Site': consumer time on site in minutes\n'Age': cutomer age in years\n'Area Income': Avg. Income of geographical area of consumer\n'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n'Ad Topic Line': Headline of the advertisement\n'City': City of consumer\n'Male': Whether or not consumer was male\n'Country': Country of consumer\n'Timestamp': Time at which consumer clicked on Ad or closed window\n'Clicked on Ad': 0 or 1 indicated clicking on Ad\n""], 'url_profile': 'https://github.com/galsaeedi', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Support_Vector_Regression\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SandaruThilakarathne', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Oakland, CA', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/caseyp510', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Jeddah, Saudi Arabia ', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': [""Logistic-Regression-Advertisement-Data\nLogistic Regression ..... we will be working with a fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement.\nWe will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\nThis data set contains the following features,\n'Daily Time Spent on Site': consumer time on site in minutes\n'Age': cutomer age in years\n'Area Income': Avg. Income of geographical area of consumer\n'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n'Ad Topic Line': Headline of the advertisement\n'City': City of consumer\n'Male': Whether or not consumer was male\n'Country': Country of consumer\n'Timestamp': Time at which consumer clicked on Ad or closed window\n'Clicked on Ad': 0 or 1 indicated clicking on Ad\n""], 'url_profile': 'https://github.com/galsaeedi', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Support_Vector_Regression\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Tampa', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['ChicagoTaxiRegressionProj\nAim:\nTo find out which variables have a relation with chicago taxicab fare.\n'], 'url_profile': 'https://github.com/arpitsrivastavaece', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Melbourne, FL', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Excel-Regression-Analysis\nSolver Add-in must be enabled to use this file.\n'], 'url_profile': 'https://github.com/gianfelton', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mohinta2892', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayushsingh5941', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['SparseL1Regression\nA simple R Script to demonstrate how large coefficients in simple linear regression vanish when L1 Sparsity is applied to them.\n'], 'url_profile': 'https://github.com/MarkKurzeja', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': [""A sample linear regression project where we take EA Sports' FIFA 19 player dataset from Kaggle and peform linear regression to correlate a player's potential rating with other factors/parameters like age, overall rating etc..\nSteps Followed -\n\nDownload the dataset. Link - (https://www.kaggle.com/karangadiya/fifa19/data)\nplayer_data_import.py - Import all the necessary libraries and also read the dataset.\nplayer_data_preparation.py - Plot a very basic scatter plot with Age & Potential. Here, we also prepare the data and split the dataset into train and test sets.\nplayer_data_train.py - Fit the training data to our model followed by which we predict and create the validation set. In the end, we calculate the R2 score as well as the mean squared error score (MSE). The model performs better if R2 score is closer to 1 and MSE is closer to 0.\nR2 Score & MSE Score.txt - Contains R2 score and MSE score of one run of both univariate as well as multivariate linear regressions respectively.\n\n""], 'url_profile': 'https://github.com/rithwikrajendran', 'info_list': ['Python', 'Updated Mar 9, 2020', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jun 28, 2020', 'Python', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'R', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Logistic-Regression-2\n'], 'url_profile': 'https://github.com/kar6270', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Linear-Regression-Project\n\nUsing some data from Ecommerce company based in New York that sells clothing online\npredict whether they should focus more on their mobile app appearence or on their website.\n\nEcommerce csv file  has: Customer info, suchas Email, Address, and their color Avatar. Then it also has numerical value columns:\n\nAvg. Session Length: Average session of in-store style advice sessions.\nTime on App: Average time spent on App in minutes\nTime on Website: Average time spent on Website in minutes\nLength of Membership: How many years the customer has been a member.\n'], 'url_profile': 'https://github.com/levazarkh', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Linear-Regression-model\nLinear regression with one variable and multivariable to predict profits for a food truck and house prices.\n'], 'url_profile': 'https://github.com/jgmartinez90', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'Nuevo León, Mexico', 'stats_list': [], 'contributions': '338 contributions\n        in the last year', 'description': ['Multilineal Regression\nThis repo is used for Multilineal Regression Practice\nCredits\nProfesor: Dr. Andrés Hernández Gutiérrez\nStudent: Alan Rocha\nPrediction\n\nDiscussion\nTest data need to be feature scaled with the same parameters (mean, standard variance) from the training data, in order to use this data for the prediction. Also a column of 1 needs to be added as the training dataset. With this is possible to obtain the prediction just multiplying the two matrices to obtain the result.\nTimes\n\nDiscussion\nThe table shows that the larger the learning rate, the less iterations and the faster the code runs, but the w values \u200b\u200bare less precise. This is because they perform ""bigger jumps"" with the largest learning rate.\n'], 'url_profile': 'https://github.com/Poetickz', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sriramaraju423', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['BestSubsetRegression\nA BestSubsetRegression method based on Cp, AIC and cross validation.\nFolder ""prostate"" includes a dataset about prostate.\nFile ""BestSubsetReg_class.ipynb"" includes codes written by jupyter notebook.\nCreated by Tuozhenliu on 2020/3/14 for Data Mining homework.\n'], 'url_profile': 'https://github.com/TuozhenLiu', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/levazarkh', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gondhalv53', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['Proj3_PolynomialRegression\n'], 'url_profile': 'https://github.com/jmench', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bonnina', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 18, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'Updated Mar 12, 2020', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 25, 2020', 'C#', 'Updated Mar 22, 2020']}"
"{'location': 'Nuevo León, Mexico', 'stats_list': [], 'contributions': '338 contributions\n        in the last year', 'description': ['Multilineal Regression\nThis repo is used for Multilineal Regression Practice\nCredits\nProfesor: Dr. Andrés Hernández Gutiérrez\nStudent: Alan Rocha\nPrediction\n\nDiscussion\nTest data need to be feature scaled with the same parameters (mean, standard variance) from the training data, in order to use this data for the prediction. Also a column of 1 needs to be added as the training dataset. With this is possible to obtain the prediction just multiplying the two matrices to obtain the result.\nTimes\n\nDiscussion\nThe table shows that the larger the learning rate, the less iterations and the faster the code runs, but the w values \u200b\u200bare less precise. This is because they perform ""bigger jumps"" with the largest learning rate.\n'], 'url_profile': 'https://github.com/Poetickz', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sriramaraju423', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Random_Forest_Regression\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Machine Learning\n'], 'url_profile': 'https://github.com/Nihalika', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '457 contributions\n        in the last year', 'description': ['Ridge-and-Lasso-Regression\n'], 'url_profile': 'https://github.com/bsingh17', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akshatra', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Utrecht', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['Bayesian linear regression animated\nThis is a tutorial for understanding the rationale of Bayesian statistics in linear regression.\nThe purpose of the tutorial is to show the mechanisms of Bayesian statistics in an intuitive manner, mainly through general notations, graphics, and animations, without diving into the details of mathematical procedures.\n\nCredit to this work can be given as:\nJ. Wang, Bayesian linear regression animated, (2020), GitHub repository, https://github.com/wonjohn/Bayes_for_Regression\n\nAuthor\'s foreword\n\nBayesian epistemology introduces important constraints on top of rational degrees of belief and a rule of probabilistic inference--the principle of conditionalization, according to William Talbott, 2008.\nBayesian statistics forms a major branch in statistics. Bayesian statistics relies on Bayesian principle to reveal a beautiful epistemology scheme through probabilistic inference: one should rationally updates degrees of knowing or belief once new evidence is observed. Mathematically, it is denoted as:\nP(S|E) = P(E|S)P(S)/P(E)\nwhere, s can be any arbitrary statement, and E is observed evidence(s). Without observing any evidence, it is rational to stay with idealized belief denoted as the prior belief P(s). But if we have observed an evidence, there is something we can do to update our belief. One option is to utilize the measurement called the likelihood function that quantifies how our prior belief should manifest the evidence at hand. The likelihood function P(E|S) together with the prior function P(S) help to update our belief once there is more information from the reality. The updated belief is called the posterior function of S, which is P(S|E).\nIn this small snippet of tutorial, the principle of Bayesian statistics is showcased through a prevalent prediction problem: linear regression.\nRecap of linear regression in frequentist view\n\nIn the case of linear regression, without any consideration of probabilistic confidence, conventional linear regression only achieves point estimation of the model parameter through least squares method. The least squares holds a frequentist view to exclusively rely on data observation and comes back with a point estimation of the model parameters. The least squares appears to be not a bad idea in the first place as we could be able to obtain exact model form and thus predictions.\nTake the simplest case of univariate linear regression problem for example, given the noisy observations are coming from an true underlying linear model plus some noise (Fig.1), frequentists attempt to recover this underlying model by starting with an assumption that the observations suffer from Gaussian noise. It means that the noise follows a Gaussian distribution around the underlying model. In short, the noise is symmetrical with respect to the true model and should add up to zero. So frequentists believe that the least squares is a proper way as it searches the true model by minimizing the difference between its inference and the observations.\n\nFig.1 Linear regression problem setting.\nHere in Fig.1 the true linear function is intentionally revealed as a line so that we can compare how least squares help us to recover the true model from few observations. The true univariate linear model I used here is:\nM(x) = 3 + 2x\nHowever, this true underlying model is usually unknown in the form as:\nM(x) = θ1 + θ2x\nor in a vectorized form as:\nM(X) = θTX\nwhere X is referred as designed vector or matrix in the form of [1, x]T and θ is [θ1, θ2]T. In reality, what we do know is nothing but a few observations Y as (y1, y2, ... , yn) but suffering from noises ε:\nY = θTX + ε\nwhere the noises are preferably Gaussian distributed noises around the true model. Then, can we really expect conventional methods such as the least squares to find or approximate the true value of [θ1, θ2]T, which are 3 and 2 for the intercept and slope, respectively? We will stick to only three observations each time as shown in Fig.1, and intentionally create observations with Gaussian noises to test the least squares for several trials. Well, even with only three observations, the least squares works fine to find linear models by minimizing their difference from the observations as shown in Fig.2 below.\n\nFig.2 Sample least squares solutions.\nEach time, the least squares fits a linear model to three randomly drawn observations. These fitted lines or models are more or less different from the true underlying model. The difference between the fitted and true model is understandable: due to the small number of the observations, the Gaussian noise in the observations hardly manifest symmetry with respect to the true model, and sometimes all three observations fall on the same side of the true underlying model, thus fitting a model that minimizes its difference to a small number of observations almost always produce difference between the fitted model and the true model. Meantime, it is also worth noticing that most observations are close to the true model due to the Gaussian noise in them, the fitted lines are more likely to be close to the true model. In short, there are few remarks from the application of least squares to the solution of true underlying model:\n\nfrequentist approach such as the least squares is inherently uncertain and fits uncertain models, especially when observations are limited;\nfrequentist relies heavily on the number samples and time of observation--with enough trials and observations, it is likely to be more certain about where is the true model;\nbut frequentist fails to encode this process of how increasing observation times updates our confidence of finding the true model.\n\nAs long as we cannot reject other possible fitted models only except we could be 100% sure about the optimal one, it is so nice if we can quantify these uncertainties, isn\'t it? In reality, it is very important to know how certain is the model at hand as well as its prediction. In many practical cases, such as predicting housing values, stock values, pollution concentration, soil mineral distribution, etc., the confidence of our model performance help us to control the risk of making prediction and minimize economic loss. Frequentists of course are eliminated by its nature in making point estimation of the model parameters. The demand of quantifying uncertainty leads to natural transition from point estimation of the model parameter to a probabilistic perspective, and paves the way to the application of Bayesian statistics.\nBayesian inference\n\nBayesian statistics attempt to explicitly impose credibility upon the underlying model. It does favor an optimal solution to the underlying model but does not reject other possibilities. So Bayesian statistics seeks potential models along with confidence simultaneously. The credibility or probability of the underlying model is achieved through combining two important probabilities:\n\nthe probability of all potential models encoding our knowledge or belief prior to see any evidence;\nthe probability of the evidence, once observed, given by any potential model.\n\nIn the case of linear regression as shown in Fig.1, Bayesian statistics tries to figure out the probability of the unobserved linear model (linear parameters) through few point observations (points in this example).It applies Bayesian principle P(S|E) = P(E|S)P(S)/P(E) to the model parameters in M(x) = θ1 + θ2x:\nP(θ|D) = P(D|θ)P(θ)/P(D)\nwhere D, collection of noisy observations, becomes our evidence. In Fig.1, there are 3 observations available for us to find out the model parameters θ. We can denote observations points as collection of tuples {(x1, y1), (x2, y2), (x3, y3)}. P(θ|D) is called the posterior distribution of θ as it is a distribution after we updating our knowledge by seeing data as evidence. The posterior is determined by the terms on the right-hand side of the equation. P(D|θ) is the likelihood function that quantifying the probability of the observations produced by some model governed by parameters θ. P(θ) is the prior distribution of θ encoding our knowledge of parameters before making any observations. How could it be possible to know anything about θ before seeing any data? Well, in most practical cases, we do have some ideas: the relationships between precipitation and soil loss, traffic volume and road pollution, location and land price, etc...We more or less know the general range of θ, or its sign, at least. P(D) is a normalization term that makes the right-hand side of the equation a true probabilistic distribution that integrated to 1.\nIf we stay simple enough in this tutorial, we can temporarily ignore the normalization term P(D). Now,  in order to quantify the posterior P(θ|D), the problem reduces to specify the likelihood P(D|θ) and prior P(θ), which has been mentioned at the beginning of this section as important probabilities.\nBayesian function specification: likelihood\nFor any single observed data point (xk, yk), the likelihood measures the probability of the model parameter θ gives rise to this known data point. Thus, given any possible θ, how likely it is to observe this particular point of tuple (xk , yk)? Referring above to the noisy observation from the linear model, by saying we have observations with noise ε around the true model, it is most handy to impose a Gaussian distribution over the noise around the true model. In short, the likelihood of observing the tuple (xk , yk) follows a Gaussian distribution around the model specified by θ:\nP(D|θ) = P(yk|xk , θ) ~ N(yk ; θTX, ε)\nThis Gaussian form likelihood can be easily implemented as a function in python as:\ndef likeli(theta1,theta2,obs_y,obs_x):  # It is a function of theta with known observations\n    sigma = 1  # Standard deviation of the Gaussian likelihood\n    func = (1/np.sqrt(6.28*sigma**2))*np.exp((obs_y-theta1-theta2*obs_x)**2/(-2*sigma**2))\n    return func\nwhere I chose a standard deviation of 1 for this Gaussian likelihood as I assume for the noise level. It shows my confidence interval that the observations should be in around the true linear model. This likelihood is obviously a function wrt. θ as the tuple (xk , yk) is observed. More intuitively, if we observe one pair of (xk , yk) as denoted red to the left of Fig.3, the above likelihood is a function of θ or [θ1 , θ2]T, and can be plotted in a 2-dimensional space defined by θ1 and θ2 to the right of Fig.3. Here are two separate observations visualized, each of whose likelihood function is plotted and appears roughly to be a line as a function of θ. It is only roughly a line in the space of [θ1 , θ2]T, implying that there are infinite number of [θ1 , θ2]T options running all the way from positive to negative values to give rise to the observation. This is extremely reasonable as one point observation determines lines with either positive or negative interception and slope. Continue with the sample linear function above, if we can be able to make a couple of more noisy observations, we can obtain multiple likelihood for each of the observation in the [θ1 , θ2]T space.\n\nFig.3 Likelihood wrt. a single observation.\nIn this case of linear regression, isn\'t it getting clear that these line-shaped likelihood functions are potentially intersected at some relatively fixed region? That is where we can combine these likelihood function that the profile of θ can be delineated. In what way to combine? As the likelihood is a probability measurement, combining the likelihood is simply a joint probability. Observing each data point as a (xk , yk) tuple is considered to be an iid process, thus the joint likelihood of any θ gives rise to all the observations is a multiplication of all the individual likelihood:\nP(D|θ) = ∏i P((xi , yi)|θ)\nThe animation (Fig.4) below shows how this joint probability is updated with each added observation. It is quite appealing that when the second (xk , yk) tuple is observed, the joint likelihood function already started take in shape and the inference of the model parameter can be made in a well delineated subspace within the [θ1 , θ2]T space. The joint likelihood with an ellipse-shaped Gaussian centers around [3, 2] indicating a high confidence that it should be the model parameter. At the same time, the likelihood does not reject other possibilities as it is still possible, with a certain noise level in the observations, that [θ1 , θ2]T can take some other values around [3, 2]. When the third point is observed, this likelihood gives a more shrunk distribution representing the process of knowledge update wrt. [θ1 , θ2]T.\n\nFig.4 Joint likelihood wrt. to observations.\nAt this point, it is no surprising that why the Maximum Likelihood Estimation (MLE) is so frequently adopted. For linear regression, especially simple as there is only few independent variable and enough observations, and of course without too much noise, the joint likelihood function could already bring a desirable results. And it is then also quite safe to maximize the likelihood to obtain a point estimation of the model parameter.\nBayesian function specification: prior\nDifferent from that likelihood can be delineated from observed information, choosing prior function for Bayesian inference is tricky. It is called prior distribution because it requires us to configure the distribution of the model parameters prior to seeing any data, or based upon our prior knowledge with regard to the parameters. Fortunately, in several situation, we DO have such prior knowledge when building a regression model. For instance, if someone is interested in the loss of a particular type of soil (yk) due to rainfall (xk) in a region, it is already handy to know that there should be a positive relationship between (yk) and (xk). It also means that we can more or less constrain the θ to be positive. Or, maybe someone has already done similar work in other places and brought some confident results, it is even possible to further constrain the θ to be a probabilistic distribution over these available results (values).\nBut here in this tutorial, although it is a simple linear regression example, we are running into a awkward situation: nothing is availabe except the observations to make inference about the model. This is where one has to rely on improper or non-informative prior distribution for the model parameters. These keywords such as improper and non-informative indicate that the design of the prior function is entirely arbitrary. One option is to make P(θ)=1, thus it is non-informative and will have no effect over the posterior when multplies with the likelihood. Another option is to make a general assumption that P(θ) follows some well formed statistical distribution, such as Gaussian distribution shown in Fig.5 below. This kind of specification can be improper as it would potentially impose limited and unreasonable assumption that θ is normally distributed around 0.\n\nFig.5 Non-informative prior distribution for model parameters.\nThis improper Gaussian distribution within the [θ1 , θ2]T space means that if we draw points randomly from it, it is more likely to have θ with values close to zero. Visually, each random point in the [θ1 , θ2]T space determines a random line in the space of [x, y]T as shown below in Fig.6, but most lines are with interception and slope close to zero.\n\nFig.6 Randomly drawn prior functions for the model.\nSince we know that the true model parameters, plus that we also know that the true parameters are well captured by the likelihood, this improper prior appears to be way off the target. Is it possible to update this prior to a meaningful state by using the likelihood?\nBayesian posterior: combine the likelihood and prior\nThe effect of this improper Gaussian prior combined with the likelihood can be visualized as in Fig.7 below. The combination, again, follows the principle of joint statistical distribution, is achieved through multiplication. The resultant posterior distribution of the model parameter θ is compared with their likelihood distribution solely determined by observed evidences.\nIn Bayesian statistics, this multiplication is normally referred as update as mentioned at the beginning of this tutorial, where the prior knowledge is updated by the likelihood brought by observations. Equivalently, I could also say that the likelihood is being shifted or dragged by our prior belief, because our prior belief imposes a constraint even we have observed few evidence.\nAlthough it seems like the posterior distribution of θ obtained by combining its likelihood and improper prior is acceptable in the first place, not too prominently, the improperness of the improper prior in this case is still highlighted as we already know that the likelihood is perfectly centered around the true parameter values of [3, 2], and now it is shifted away. But we would never notice this in practice as we wouldn\'t know the true model parameters. One can see that the direction of the shifting likelihood is towards the prior. But as it is a multiplication, the shift is not quite intense. The multiplication combines the large value in both prior and likelihood, thus the shift is along the gentle gradient of the likelihood while moving towards the prior. The major reason that the posterior is not too far away shifted from the likelihood is that the improper prior is relatively ""flat"", whereas the distribution rendered by the likelihood is strongly centered. So, the resultant multiplication would largely driven by the likelihood.\n\nFig.7 Posterior distribution/function for model parameters.\nNow, one may start to ask: what on earth is the point to use prior functions that are improperly or non-informatively designed?! Unfortunately, there is no one-fits-all answer. It is the nature of statistic inference that we are forced to make some assumptions from scratch, just like we have to assume there is a linear relationship already before obtaining more data points than only a few of them. There are few discussions going around for choosing prior distributions scientifically. More formal research can be found in The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation (Springer Texts in Statistics, as well as Moving beyond noninformative priors: why and how to choose weakly informative priors in Bayesian analyses.\nImproper or non-informative prior shouldn\'t be the reason to be pessimistic about Bayesian principle. In most cases, we are still doing incremental science, which means there is almost always some existing information we could leverage and to be encoded as prior knowledge, like the example of soil loss prediction mentioned earlier.\nEven without the context of domain knowledge, using improper prior achieves some appealing results. A prior, along with the likelihood builds a connection to the idea of regularization, which is a technique explicitly modifies how the model parameters are sought given any specified loss function. If the likelihood is viewed as a loss function specified by observations, then the prior plays the role of a regularizer to constrain the model parameters from being solely controlled by the loss function. In practice, there are few design options for the regularizer for achieving different purposes: avoid overfitting (equivalent to Gaussian prior function), parameter selection (equivalent to Laplace prior function) and in combination. These regularizer are called in different ways, for instance, as shown in Fig.8.\n\nFig.8 Regularization equivalence of prior functions (src: http://primo.ai/index.php?title=Regularization).\nThe biggest difference is probably that Bayesian principle stays as a general statistical framework, whereas regularization is more commonly adopted from the frequentist perspective that a particular solution to the model parameter is expected.\nApart from the prior specification, it should NOT be a worse case where we have to heavily rely on observations. With the development of data acquirement approaches, we could be able to be confident about the model parameters with likelihood. The visualization of the posterior below (Fig.9) shows how increasing observations may update our knowledge even from a poor prior. The multiplication in P(D|θ)P(θ)/P(D) is explicitly two way: (1) the prior is constraining or dragging, while (2) the likelihood is updating and washing away the effect of constraining or dragging.\n\nFig.9 Change of posterior distribution/function for model parameters along with increasing observations.\nHolding the posterior at hand, we now have a more concentrated distribution of the model parameters. The final distribution of the θ in the [θ1 , θ2]T space, as shown in Fig.8 is confidently centering the point [3 , 2], while still shows possibility of parameters such as [2.5 , 2]. This probabilistic perspective is nice as the solution of the parameter is optimal at [3 , 2], we are not rejecting other possibilities without knowing how the qualities of the observations are disturbed by those random noises. We have few options to deal with this posterior. Similar to maximizing the likelihood, we can seek the maximum-a-posteriori (MAP) probability to achieve the optimal model parameters. We can also stay with the probability distribution to quantify confidence of prediction.\nSince it is a probability distribution in the posterior, it means sampling is possible from this distribution for visualization. As shown in Fig.9, drawn from the distribution, many samples are very close to [3 , 2] with few are bit far away from the center of the distribution. In the [X , Y] space, the drawn samples seems to be wobbling around a potential ""datum line"". This ""datum line"" is in fact M(x) = 3 + 2x. Apparently, the sample lines are corresponding to the probability of drawn [θ1 , θ2]T, thus we can encode this probability of θ to the lines, namely the inferred linear model. It then forms a probability distribution of potential linear models in the [X , Y] space and can be visualized as shadow to the right of Fig.10.\n\nFig.10 Model possibilities in terms of posterior distribution of model parameters.\nThe shadow defines an import confidence interval for making predictions: given any xnew, what is the probability of ynew? As the probability of giving rise to a ynew is the probability of the model, the distribution of ynew is now out-of-box given all the possible linear model. Mathematically, it is equivalent to weight all predictions made by each potential linear model by the probabilistic distribution of that model, as\nP(Dnew|D) = ʃ P(Dnew|θ,D)P(θ|D)dθ\nwhich is exactly how Fig.10 is plotted. The shadow is a combination of possible linear model weighted by their possibilities. The equation above also speaks the same idea: the posterior considers all possible θ, which means the posterior does NOT care about the exact θ! The integration plays a beautiful role to manifest such contradictory that all θ are involved (considered), but then are integrated out (does NOT care)!\nWrap-up\n\nSo far we have been stay intuitively by using general notations (such as P(θ)), graphics and animations. In order to obtain exact measurement of the distribution regarding the likelihood, prior, and posterior, we can explicitly quantify the distributions by using hyper-parameters, for instance:\n\nP(θ) = P(θ|α) ~ N(θ ; 0, α-1I) for the prior, where α is the hyper-parameter controlling the shape of probabilistic distribution;\nP(D|θ) = P(yk|xk , θ, β) ~ N(yk ; θTX, β-1I), where β is another hyper-parameter of precision (inverse variance) controlling the noise intensity as ε ~ N(ε ; 0, β-1I);\nwe can even have design function φ for φ(x)=1+x.\n\nIf we can be able to specify the hyper-parameters, the posterior is measurable, visually, the width and center of the shadow in Fig.10 is to be a function of hyper-parameters α and β. These hyper-parameters can also be obtained automatically! The approach is called maximizing the marginal likelihood through Empirical Bayes method, which will not be covered in this tutorial.\nAfter digesting the mechanisms of the Bayesian statistics in linear regression, you are ready to leverage existing packages without bothering too much to write your own code. As long as you understand what does it mean by likelihood, prior, posterior and the role of those hyper-parameters, you can simple do as follows:\nfrom bayesian_linear_regression_util import *\nIf you do wish to specify your own Bayesian distributions for any parameters, say θ, you can leverage PyMC3 as:\nimport pymc3 as pm\nwith pm.Model() as model:\n    μ = pm.Uniform(\'μ\', lower=0, upper=300)\n    σ = pm.HalfNormal(\'σ\', sd=10)\n    θ = pm.Normal(\'θ\', mu=μ, sd=σ, observed=data[\'price\'].values)\n    trace = pm.sample(1000, tune=1000)\nTakeaways\n\nFew takeaways after digesting this tutorial:\n\nmany distributions specified for Bayesian linear regression are Gaussian because of its nice property in conditioning, multiplication, etc.;\nmaximum-likelihood-estimation (MLE) can be reliable if enough observations are made, but quite vulnerable to limited ones, which leads to a phenomenon of overfitting if someone is interested in the details;\neven the improper or non-informative prior functions are confusing with limited domain knowledge, they play a useful role mathematically and build the connection to an important regression strategy: regularization;\nalthough specifying the prior is sometimes referred as model selection, it is essentially specify the parameters of model, the assumption of the model as linear or non-linear is still arbitrary;\nfor further interest: specifying the design function φ for the design function x can be related to kernel tricks and paves the way to non-parametric modeling.\n\n'], 'url_profile': 'https://github.com/jonwangio', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Regression-Based-Analysis\nAIM of this project is to build a machine learning model to predict revenue over the first year of each customer’s life cycle.\nPROJECT APPRENTICE CHEF\nAIM: Of this project is to build a machine learning model to predict revenue over the first year of each customer’s life cycle.\nWHY: Client want to better understand how much revenue to expect from each customer within their first year of orders.\nBACKGROUND: Apprentice Chef have come to realize that over 90% of their revenue comes from customers that have been ordering meal sets for 12 months or less.\nASSUMPTION: The dataset provided by the engineering team has used dataset engineering techniques and are statistically sound and represent the true picture of Apprentice Chef’s customers.\nLinear Regression Model\nUsing the sklearn linear model linear regression.\nFitting the model on the train data set X_train and y_train\nPredicting the model on the test dataset X_test\nMeasuring the model performance by checking the model training score and testing score\nResult\nModel training score is 0.752 i.e model predicted correct revenue 75% of the time on training dataset Model testing score is 0.712 i.e model predicted correct revenue 71% of the time on testing dataset\nTraining Score: 0.752\nTesting Score: 0.712\n\n'], 'url_profile': 'https://github.com/Shresth0714', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/feiyang-cai', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AmeliaYozu', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Python', 'Updated May 17, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'HTML', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Python', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['House-Price-Prediction-Kaggle\nGetting started with machine learning through Kaggle project - House Prices: Advanced Regression Techniques\n'], 'url_profile': 'https://github.com/zihan97', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'Vancouver', 'stats_list': [], 'contributions': '200 contributions\n        in the last year', 'description': ['Linear-And-Polynomial-Regression-From-Scratch\nJupyter Notebook to accompany my Medium article Implementing Linear and Polynomial Regression From Scratch.\n'], 'url_profile': 'https://github.com/christam96', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['Module 2 Final Project\nThe Goal\nThe goal of this project is to create a multiple linear regression model that accurately predicts house prices in King County, WA.\nNotebook Setup\nFor this project I used general python libraries, geomapping libraries, and statistical or regression libraries.\nGeneral python libraries\n\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport re\n\nGeo mapping libraries\n\nfrom branca.colormap import linear\nimport geopandas as gpd\nfrom ipyleaflet import Map, GeoData, basemaps, LayersControl, Choropleth, Heatmap, FullScreenControl\nfrom ipyleaflet import WidgetControl, GeoJSON\nfrom ipywidgets import Text, HTML\nfrom shapely.geometry import Point, Polygon\n\nStatistics and regression libraries\n\nfrom scipy.stats import zscore\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import minmax_scale\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\n\nMy Approach\n\nUse pandas to initially ingest the datasets and clean them, fill missing values, correct data types, convert certain columns to binary, remove outliers, bin and one-hot encode columns\nUse Sklearn to create baseline model at this point (.954 R-squared)\nThen use numpy to log-transform features\nMin-max scale features using Sklearn\nRemove features that weren\'t statistically significant\nRerun model after changes (.949 R-squared)\nValidate model with k-fold cross validation (10 folds, using negative mean squared error with result of -17.06e9)\nPickle the model for portability\n\nAfter creating model, investigate zipcode feature and try new approach\nA property\'s location, encoded by zipcode, is the most important feature in the dataset but zipcodes are of an arbitrary shape, the government changes them and they are also too big for more detailed location-based analysis.\nSee how various the zipcodes\' sizes and shapes are:\n\nCreate new feature called a ""geobin""\nBecause zipcodes are so unreliable, I split the data into many smaller ""geobins"". These polygons are based on latitude and longitude, are much smaller than the zipcode data that was provided and will allow us to differentiate between different locations\' affect on price more accurately.\n\nConclusion\nOur final geobins-based model had the same r-squared as the original zipcode model but also has the advantage of using a location feature that is standardized, scalable, and not arbitrary.\n'], 'url_profile': 'https://github.com/benratkin', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Differential Pricing Strategy for Ice Cream\nExecutive Summary\nAn ice cream company wants to boost its margin profits through formulating\ndifferential pricing strategies and carrying out effective coupon promotion activities. To\nhelp them achieve their goal, we analyzed a transaction dataset containing transaction\nfeatures such as ‘price paid’, ‘total spend’, product features such as ‘flavor’, ‘size’, and\ndemographic features of households such as ‘income’, ‘age’, and we build multiple\nregression models to explore factors that will impact customers’ willingness to pay,\ndictated by ‘price paid per unit’. Moreover, we design Difference-in-Difference analysis to\nhelp them study the causal effect of coupons to different customer groups, which is not\nobtainable by simply building regression models.\nMajor findings of our analysis are as follows:\n(1) Product features such as flavor and size have significant impacts on customers’\nwillingness to pay\n(2) Demographic features of households such as income, region and household\nsize have significant impacts on customers’ willingness to pay\n(3) Positive interaction exists between coupon and central region regarding\nimpacts on ‘price per unit’\nRecommendations are given as follows:\n(1) Apply differential pricing strategies to different flavors of ice cream, different\nregions and customers with different levels of income\n(2) Collect panel data and conduct Difference-in-Difference analysis to analyze\ncausal effects of coupon to different customer groups, through which they can\nimprove the effectiveness of promotion activities\nIntroduction\nAn ice cream company is seeking ways to formulate differential pricing strategies\nfor different customer groups and improve coupon promotion activities to maximize\nmargin profits. To help them achieve the goal, our analysis will mainly focus on building\nlinear regression models to explore factors that might impact customers’ willingness to\npay and providing recommendations on making smarter pricing strategies based on\ncustomer segmentations. Moreover, we also suggest other statistic methods to help them\nbetter explore how to enhance the efficiency of their promotion activities such as offering\ncoupons to customers with high purchase potential more precisely.\nData Characteristics and Exploration\nThe dataset contains 2,1975 rows of transaction records along with demographic\ninformation of 6385 households. There are 37 variables in total, among which transaction\ndata such as price paid, coupon value, features of products such as flavor, size and\nquantity of ice cream and demographic data of households such as income, region, race,\nage, gender are included.\nWe calculate ‘price per unit’ for each deal as target variable Y and use other\nvariables as explanatory variables X. Then we conducted exploratory analysis on\nrelations between target variable and other variables. Here are what we found. Among all\nthe explanatory variables, ‘coupon value’ positively correlated with ‘price per unit’,\nindicating that higher coupon value may encourage customers to pay higher price per\nunit.\nChart above shows that ‘price per unit’ also varies a lot when the household owns\ndifferent kinds of kitchen appliances such as microwave and dishwasher, showing that\nfamilies holding different machines may have different purchasing power.\nAlso, as the horizontal bar chart shows, ‘Cherry Grca’ is the most popular flavor\nacross all regions, but different regions have different preference for different ice cream\nflavors.\nModel Selection, Evaluation and Interpretation\nMultiple regression model is chosen in this case since it enables us not only to\nidentify the significance of each independent variable but also to quantify the degree of\nimpact on the dependent variable of our interest, thereby increasing the power of\ninterpretation. ‘Price per unit’ is chosen as target variable Y to reflect customers’\nwillingness to pay since it eliminates the effects of quantity on price. 15 variables such as\n‘flavor’, ‘coupon’, ‘marital status’, ‘Hispanic origin’ are chosen as independent variables.\nAfter creating dummy variables for categorical variables, there are 69 independent\nvariables in total.\nResults of the primary regression model shows that whether the household is from\na Hispanic origin or whether it has dishwasher or not are two regressors with insignificant\np-value. These two variables should be removed because they do not influence target\nvariables. Moreover, variables such as whether there is a promotion, household\ncomposition, age, and presence of children are added as they are logically significant and\nhave high correlations with ‘price per unit’.\nHowever, in order to improve the performance of the model and minimize the\npossibility of overfitting and underfitting, stepwise regression method is deployed. And 13\nindependent variables out of 16 are retained. Moreover, interaction terms of ‘coupon per\nunit’ across region, and ‘coupon per unit’ across race which have statistically significant\nimpact on ‘price per unit’ are added.\nAccording to the final model, the features of ice cream such as flavor and size do\nhave significant impacts on target variable. Specifically, some flavors increase customers’\nwillingness to pay higher price. Assuming the cost of the production hold constant, the\ncompany is advised to produce more ice creams with popular flavors among customers.\nIn addition, there are several demographic variables showing statistically significant\nimpacts on ‘price per unit’. Variables such as income, region, and household size are of\noperational use. For example, these demographic characteristics can be utilized by the\ncompany to segment customers and apply differential pricing strategies.\nSince we included 76 independent variables in total, it is possible that some\nunnecessary variables with little explanatory power are mistakenly included in the model.\nBased on the significant level of 95%, the false positive rate is approximately 8%, which\nmeans that approximately 3 out of 32 significant variables under the significant level of\n95% are in fact non-significant. Since 3 false positive variables seems acceptable, we\nkeep the criteria of 95% significance level.\nCoupons can help almost every business type and size. A successful coupon\nmarketing strategy not only drives sales, but also helps build brand image. The scatter\nplot of coupon value and price per unit shows a strong positive correlation. Therefore, it\nis beneficial to focus on studying the causal effects of coupon value with difference-in-\ndifference (DID) technique. In order to further validate both the significance and the\ndirection of the impact of certain variables that might be vital in our implementation of\nstrategy, we designed a difference-in-difference (DID) experiment to identify the causal\nrelationship between those variables and ‘price per unit’. To do the DID analysis,\ncustomers are divided into two groups based on region. Then take months (June, July,\nand August) in summer quarter in one year as the panel data since a year is too long to\nconduct an experiment and sales of ice cream varies greatly across quarters. For the pre-\nintervention period, the company does not give any coupon, and gives coupon to the\ntreatment group, for example, households with children, at the middle of July. By\nperforming DID analysis described above for two groups individually, we can get the\neffects of giving coupon on them. After that, comparison of the effects will reveal\ncustomers in which region should be given coupons to maximize the margins.\nRecommendation\nBased on our analysis, in order to fully exploit customers’ surplus, the company\ncan apply different pricing strategy to different flavors of ice cream, location, and\ncustomers with different levels of income. Specifically, the company can increase the\nprice of ice cream with coffee flavor, ice cream sold in east region, and target coupon at\ncustomer with lower income.\nIn addition, there exists positive interaction between coupon and central region of\nimpact on ‘price per unit’. In other words, handing out coupon can bring company more\nbusiness value in the central region, therefore company is advised to target the coupon\nat customers who lives in central region.\nConclusion\nIn this report, we aim to provide analysis support for formulating differential pricing\nstrategies and effective promotion coupon activities. We build multiple regression models\nto explore factors that impact customers’ willingness to pay and we found that flavor and\nsize of ice cream do have significant impacts on customers’ willingness to pay.\nDemographic variables such as income, region and household size show statistically\nsignificant impact on customers’ willingness to pay.\nBased on the findings, we provide four recommendations:\n(1) set higher prices for popular flavors of ice cream, for example, coffee flavor\n(2) set higher prices in regions where customers’ willingness to pay is higher, for\nexample, east region\n(3) give out more coupons to customers with lower income, for example, students\n(4) collect panel data and carry out Difference-In-Difference analysis to study\ncoupon’s causal effect on customers of different age groups or region groups and give\nout coupons to groups with higher positive effect\n'], 'url_profile': 'https://github.com/yiying-wang', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'Copenhagen', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['ML_proj_2\nA machine learning project from my course Introduction to Machine Learning and Data Mining on the third semester of my BSc Eng. in Data Science and Artificial Intelligence at The Technical University of Denmark\nThe project focus was on evaluation and comparison of different classification and regression models.\nModel accuracy was tested using 2-layer nested cross validation to reduce bias in the generalization errors of the models.\nModels used for classification\n\nLogistic Regression\nDecesion Tree\nBaseline Model\n\nModels used for regression\n\nArtificial Neural Network\nLinear Regression\nBaseline Model\n\nModels were trained on data from http://noegletal.dk.\n'], 'url_profile': 'https://github.com/wdmdev', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'Brampton , Canada', 'stats_list': [], 'contributions': '1,196 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mohd-ahsan-mirza', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['food_truck_profit_prediction\nPredicting profits for a food truck using linear regression\nIn this project, I have implemented linear regression with one\nvariable to predict profits for a food truck.\nUse Case:\nSuppose an owner of a restaurant franchise is considering different cities for opening a new outlet.\nThe franchise already has trucks in various cities and data has been previously collected for profits and populations from those cities.\nYou would like to use this data to help you select which city to expand to next.\n'], 'url_profile': 'https://github.com/gsantiago1618', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/annielkn', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'Birmingham Alabama', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Implementing-ML-model-Linear-Regression-\nHere we are going to predict the amount of money spent on insured customers using Logistic Regression.\nFor this the we will be analyzing insured customers data for an isurance compamy.\nBased on the available sample data that consits of the profile of insured customers, we want to able to predict the dollar amount of money spent by the insurance  company on the insured cusmtomers.\ninsured Customer Data:\nThe insured customers data file is in a csv file. You can find the file in this project. It ha sthe information of\n1 age\n2 sex\n3 BMI\n4 Smoker (yes, no)\n5 Region (Northeast, Northwest, Southeast, Southwest)\n6 Expenses\nThe value we want to predict is the total Expenses.\nSetup\nThis code was tested on python 3, it may work on python 2 but not guaranteed.\n'], 'url_profile': 'https://github.com/yashy23', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}","{'location': 'Toronto, Ontario, Canada', 'stats_list': [], 'contributions': '304 contributions\n        in the last year', 'description': ['WineRating\nImplemented machine learning in small size dataset.\nAbout\nThis project was created with R, utilising machine learning to predict wine price, given countries of origin. Original dataset was taken from Kaggle, in which I practiced machine learning with a different language.\nExample of Data Visuals\n\n'], 'url_profile': 'https://github.com/oliverkpan', 'info_list': ['Jupyter Notebook', 'Updated Mar 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Jun 19, 2020']}"
"{'location': 'Brampton , Canada', 'stats_list': [], 'contributions': '1,196 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mohd-ahsan-mirza', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['food_truck_profit_prediction\nPredicting profits for a food truck using linear regression\nIn this project, I have implemented linear regression with one\nvariable to predict profits for a food truck.\nUse Case:\nSuppose an owner of a restaurant franchise is considering different cities for opening a new outlet.\nThe franchise already has trucks in various cities and data has been previously collected for profits and populations from those cities.\nYou would like to use this data to help you select which city to expand to next.\n'], 'url_profile': 'https://github.com/gsantiago1618', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Lanzhou, China', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lixiongyang', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['LASSO_Ridge_Regression-Cross_Validation\nPython implementation of LASSO & Ridge Regression and k-fold Cross Validation machine learning algorithm from scratch.\nReport\nhttps://github.com/ayushkdwivedi/LASSO_Ridge_Regression-Cross_Validation/blob/master/Report.pdf\n'], 'url_profile': 'https://github.com/ayushkdwivedi', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Predictive-Model-for-Real-Estate-using-GBM-in-R\nPredictive model using in R to predict price of house. Modelling Technique - GBM (Regression).\nProblem Statement -\nPrice of a property is one of the most important decision criterion when people buy homes. Real state firms need to be consistent in their pricing in order to attract buyers . Having a predictive model for the same will be great tool to have , which in turn can also be used to tweak development of properties , putting more emphasis on qualities which increase the value of the property.\nWe have given you two datasets , housing_train.csv and housing_test.csv . You need to use data housing_train to build predictive model for response variable ""Price"". Housing_test data contains all other factors except ""Price"", you need to predict that using the model that you developed and submit your predicted values in a csv files.\nEvaluation Criterion :\nScore will be calculated as:\nScore =212467/RMSE (Note : Dont worry about change in scoring method , this is just a cosmetic change to alter scale of score , passing criterion hasn\'t changed and you dont need to resubmit )\nWhere RMSE is root mean square error on test file.\n'], 'url_profile': 'https://github.com/laveenavalecha', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Machine-Learning---Regression-from-scratch\nImplementing Linear and Logistic Regression from scratch without the use of Machine Learning libraries.\n'], 'url_profile': 'https://github.com/syedhashirali', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Salt Lake City, Utah', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lizzydrysdale', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '301 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MaartenvanderMeulen', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Aix-en-Provence', 'stats_list': [], 'contributions': '509 contributions\n        in the last year', 'description': ['Power Consumption Forecasting\nGoal\nGiven two historics of measures produced by a sensor which performs 96 daily measures:\n\npower consumption from 01/01/2010 to 16/02/2010\ntemperature from 01/01/2010 to 17/02/2010\n\nwe had to forecast power consumption on the 17/02/2010.\nWe used the following models:\n\nlinear regression (LR)\nexponential smoothing (Holt-Winters seasonal method) (HW)\nautoregressive integrated moving average (ARIMA)\nneural network auto-regressive (NNAR)\n\nExploratory Data Analysis\nWe took 46 days as training set and 1 day as validation set, measures of the 16/02/2010. Taking 1 day of data as validation may be a bit suspicious, probably 2 days would have been better. Nevertheless our idea was that since we only had to forecast one day, that is 96 measures of the same day, we should take a validation set which would most likely look similar as the test set.\n\n\n\n\n\n\nWe observe that both time series have seasonality. Although power consumption time series does not have trend while temperature time series has a slight trend.\n\n\n\n\nOn boxplots we notice that for power consumption most of the measures on a day have low variance. Measures near 8 am and 5 pm have high variance though. It could be expected since it is when the people leave and go back home. In the contrary in the case of temperature we notice high variance for each measure of the day. Indeed temperature cannot be explained only by trend and seasonality.\nBaseline models\n\n\n\nThe second baseline model looks like the following (alhough here it has been aggregated over hours instead of daily measures):\n\n\n\nOn validation set we get the following forecasts:\n\n\n\nOn validation set we get the following results:\n\n\n\nLinear Regression models\nWe performed the following models:\n\n\n\nOn validation set we get the following forecasts:\n\n\n\n\n\nOn validation set we get the following results:\n\n\n\nExponential Smoothing models\nWe performed the following models:\n\n\n\nOn validation set we get the following forecasts:\n\n\n\nOn validation set we get the following results:\n\n\n\nARIMA model\nWe performed the autoregressive integrated moving average model for seasonality (SARIMA):\n\n\n\nOn validation set we get the following forecasts:\n\n\n\nOn validation set we get the following results:\n\n\n\nNNAR model\nWe performed the following models:\n\n\n\nOn validation set we get the following forecasts:\n\n\n\nOn validation set we get the following results:\n\n\n\nForecast on test\nWe were supposed to provide two forecasts one without temperature, the second with temperature. It should be noted though that we could not achieve better results with temperature. Best result was achieved by using additive exponential smoothing in log space.\nOn test set we performed the following models:\n\n\n\nOn test set we submitted the following forecasts:\n\n\n\n'], 'url_profile': 'https://github.com/salimandre', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Portland, OR', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bounnoy', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 11, 2020', 'PLSQL', 'Updated Mar 12, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', '1', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Apr 10, 2020', 'Python', 'MIT license', 'Updated Apr 6, 2020', 'R', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020']}"
"{'location': 'Bristol', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['ProjectSeagrass\nA CNN regression project that aims to give coverage estimates for seagrass images\nThis project was built within a virtual environment on Ubuntu 18 that uses Python 3.6.9, Tensorflow 1.14, Keras 2.3.1, NumPy 1.18.2, Scikit-learn 0.22.2.post1 and OpenCV 4.2.0\nThis projects covers the formatting process of images for usage in the CNN model\nSeagrass quadrat images are cropped to remove the quadrat from the image and processed to include a formatted naming scheme.\nVarious feature extractions have been tested on images to discern what features of interest the neural network can learn\nThe main program can be run by using bash scripts on Linux or Windows using a bash command executor. The program can be run without the need to use bash scripts but due to the required command lines arguments it is advantageous to use a script.\nIf the script is set for training, it will load the selected formatted dataset and then train the model and then evaluate its performance. A model will be generated following this training that is stored for later loading.\nGiven a model is trained for use, a directory of images that are formatted to be of the shape 576x576x3 can be passed to the trained model with which coverage estimates are generated and stored in the results folder.\nTo make predictions on a set of custom images that adhere to the required image format (top-down quadrat image).\nUsing the make_predictions.sh script, when prompted if the images need formatting first, select yes and then follow the prompts.\nThe main method is called which crops and resizes the images. Then, given that a model is trained and saved, predictions will be made and saved to a file in the folder you chose.\n'], 'url_profile': 'https://github.com/JamesExeter', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Portland, OR', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bounnoy', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '1,128 contributions\n        in the last year', 'description': ['Multiple Linear Regression\nWe will be solving a simple business task using Multiple Linear Regression.\nTask: Venture capital fund analysis\nWe have been given a dataset that contains a list of 50 companies with their yearly expenditure on R&D (Research & Development), Administration, Marketing and State (New York, California, Florida), alongside their Profit. Our task is to explore which companies we should invest in in order to optimise our profits.\nWe will be using Multiple Linear Regression to carry out this task.\nPros of Linear Regression:\n\nWorks on any size of dataset\nGives us information about relevance of features.\n\nCons of Linear Regression:\n\nLinear regression assumptions\n\nAssumptions of a linear regression:\n\nLinearity\nHomoscedasticity\nMultivariate Normality\nIndependence of errors\nLack of multicollinearity\n\nBackwards Elimination: In order to build an ""optimal"" multiple linear regression model, we use backwards elimination to find the optimal number of independent variables so that each variable has a significant impact on the dependent varaible (profit). In our case, we are using a 5% significance level (p-value = 0.05), therefore any predictor variable that has a p-value > 0.05 should be removed and we will run the Regressor Ordinary Least Squares (Reg_OLS) again and observe the metrics thereafter.\nThe below code will be our first run of the Reg_OLS:\nimport statsmodels.api as sm\nX = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)\nX_opt = X[:,[0,1,2,3,4,5]]\nReg_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nprint(Reg_OLS.summary())\n\nOutput:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.945\nMethod:                 Least Squares   F-statistic:                     169.9\nDate:                Mon, 09 Mar 2020   Prob (F-statistic):           1.34e-27\nTime:                        21:43:31   Log-Likelihood:                -525.38\nNo. Observations:                  50   AIC:                             1063.\nDf Residuals:                      44   BIC:                             1074.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\nx1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\nx2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\nx3             0.8060      0.046     17.369      0.000       0.712       0.900\nx4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\nx5             0.0270      0.017      1.574      0.123      -0.008       0.062\n==============================================================================\nOmnibus:                       14.782   Durbin-Watson:                   1.283\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\nSkew:                          -0.948   Prob(JB):                     2.41e-05\nKurtosis:                       5.572   Cond. No.                     1.45e+06\n==============================================================================\n\nNew York has the highest p-value (0.990 > 0.05) so that variable will be removed, we will run Reg_OLS and review the metrics\nX_opt = X[:,[0,1,3,4,5]]\nReg_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nprint(Reg_OLS.summary())\n\nOutput:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.946\nMethod:                 Least Squares   F-statistic:                     217.2\nDate:                Mon, 09 Mar 2020   Prob (F-statistic):           8.49e-29\nTime:                        21:46:24   Log-Likelihood:                -525.38\nNo. Observations:                  50   AIC:                             1061.\nDf Residuals:                      45   BIC:                             1070.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\nx1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\nx2             0.8060      0.046     17.606      0.000       0.714       0.898\nx3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\nx4             0.0270      0.017      1.592      0.118      -0.007       0.061\n==============================================================================\nOmnibus:                       14.758   Durbin-Watson:                   1.282\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\nSkew:                          -0.948   Prob(JB):                     2.53e-05\nKurtosis:                       5.563   Cond. No.                     1.40e+06\n==============================================================================\n\nFlorida has the highest p-value (0.940 > 0.05) so that variable as been removed, we will run Reg_OLS and review the metrics\nX_opt = X[:,[0,3,4,5]]\nReg_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nprint(Reg_OLS.summary())\n\nOutput:\n                             OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     296.0\nDate:                Mon, 09 Mar 2020   Prob (F-statistic):           4.53e-30\nTime:                        21:50:39   Log-Likelihood:                -525.39\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      46   BIC:                             1066.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\nx1             0.8057      0.045     17.846      0.000       0.715       0.897\nx2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\nx3             0.0272      0.016      1.655      0.105      -0.006       0.060\n==============================================================================\nOmnibus:                       14.838   Durbin-Watson:                   1.282\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\nSkew:                          -0.949   Prob(JB):                     2.21e-05\nKurtosis:                       5.586   Cond. No.                     1.40e+06\n==============================================================================\n\nAdministration has the highest p-value (0.602 > 0.05) so that variable as been removed, we will run Reg_OLS and review the metrics\nX_opt = X[:,[0,3,5]]\nReg_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nprint(Reg_OLS.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.950\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     450.8\nDate:                Mon, 09 Mar 2020   Prob (F-statistic):           2.16e-31\nTime:                        21:53:12   Log-Likelihood:                -525.54\nNo. Observations:                  50   AIC:                             1057.\nDf Residuals:                      47   BIC:                             1063.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\nx1             0.7966      0.041     19.266      0.000       0.713       0.880\nx2             0.0299      0.016      1.927      0.060      -0.001       0.061\n==============================================================================\nOmnibus:                       14.677   Durbin-Watson:                   1.257\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\nSkew:                          -0.939   Prob(JB):                     2.54e-05\nKurtosis:                       5.575   Cond. No.                     5.32e+05\n==============================================================================\n\nMarketing spend has the highest p-value (0.06 > 0.05)  so that variable as been removed, we will run Reg_OLS and review the metrics\nX_opt = X[:,[0,3]]\nReg_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nprint(Reg_OLS.summary())\n\nOutput:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.947\nModel:                            OLS   Adj. R-squared:                  0.945\nMethod:                 Least Squares   F-statistic:                     849.8\nDate:                Mon, 09 Mar 2020   Prob (F-statistic):           3.50e-32\nTime:                        21:55:06   Log-Likelihood:                -527.44\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      48   BIC:                             1063.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\nx1             0.8543      0.029     29.151      0.000       0.795       0.913\n==============================================================================\nOmnibus:                       13.727   Durbin-Watson:                   1.116\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\nSkew:                          -0.911   Prob(JB):                     9.44e-05\nKurtosis:                       5.361   Cond. No.                     1.65e+05\n==============================================================================\n\nAs the adjusted R-squared has decreased, we will add the ""Marketing Spend"" predictor back into our model and declare this our optimal multiple linear regression model. We conclude that we should invest in companies that are investing highly in ""R&D"" (Research and Development), with ""Marketing Spend"" being the additional (but weaker) contributing factor.\nX_opt = X[:,[0,3,5]]\nReg_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nprint(Reg_OLS.summary())\n\nOutput:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.950\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     450.8\nDate:                Mon, 09 Mar 2020   Prob (F-statistic):           2.16e-31\nTime:                        21:57:34   Log-Likelihood:                -525.54\nNo. Observations:                  50   AIC:                             1057.\nDf Residuals:                      47   BIC:                             1063.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\nx1             0.7966      0.041     19.266      0.000       0.713       0.880\nx2             0.0299      0.016      1.927      0.060      -0.001       0.061\n==============================================================================\nOmnibus:                       14.677   Durbin-Watson:                   1.257\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\nSkew:                          -0.939   Prob(JB):                     2.54e-05\nKurtosis:                       5.575   Cond. No.                     5.32e+05\n==============================================================================\n\nwhere X1 is R&D Spend and X2 is Marketing Spend.\nOur regressor therefore takes the following function:\nProfit = 46980 + (0.7966)X1 + (0.0299)X2\n\nNote:\nIt is incorrect to make statements such as ""R&D spend has a much larger impact on profit in comparison to Marketing spend"" or ""R&D\'s impact on profit is over 26 times more than Marketing spend"". This is because, solely looking at the coefficients does not give us the unit measurements for theses predictors. For instance, if R&D spend was in £10,0000\'s and Marketing spend was is in £1,000\'s, the amount invested in each will differ substantially. Therefore, we conclude with the below statement:\nR&D spend has a greater impact on profit per unit of R&D spend than Marketing Spend has per unit of marketing spend. For every one unit increase/decrease in R&D spend, the Profit will increase/decrease by 0.7966 unit pounds.\n'], 'url_profile': 'https://github.com/MohitGoel92', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['The following is project conducted by Alain Duplan and Ariel S. Lutati in hopes of having a better understanding of the financial workings of the University Of Massachusetts, Amherst.\nThis project will use many data science skills and financial analysis to dive into the financial and single audit report of the institution of the past 5 years to look for any trends or note worthy observations.\nThis project was originally intented to be used for INFO-248 at UMass Amherst.\n'], 'url_profile': 'https://github.com/ariel42700', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/parthmaheshwari', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['Boston-housing-price-linear_reg\ncode to check the precision of linear regression on Boston housing data set\n'], 'url_profile': 'https://github.com/Shakthi-Dhar', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Irvine, CA', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': [""AutosRUs\nUsing R to create statistical linear regression t-test on datasets\n##AutosRU's\n##Data Analytics Team\n##3/14/2020\n##MechaCarChallengeRScript.R\n##MechaCarWriteUp.txt\nAnalysis using R Studio multiple linear regresion, designed a linear model that can predict the mpg of MechaCar prototypes using the following variables:\nMechaCar_mpg.csv\nMechaCar_mpg2.csv\n\nmpg\nlength\nweight\nclearance\nangle\nAWD\n\nAfter running the analysis and creating a matrix that compared all the variables. This is helping us to compare the ouput using the summary(), cor() and lm() functions creating metrics from our model. Below the output and analysis.\ncor()\n           weight         mpg   clearance      length       angle         AWD\n\nweight     1.00000000  0.09068314  0.08511338 -0.12271790 -0.11307851 -0.03698098\nmpg        0.09068314  1.00000000  0.32874886  0.60947984 -0.02083999 -0.14166977\nclearance  0.08511338  0.32874886  1.00000000 -0.31663112 -0.21112057 -0.15214456\nlength    -0.12271790  0.60947984 -0.31663112  1.00000000  0.02577114  0.08565668\nangle     -0.11307851 -0.02083999 -0.21112057  0.02577114  1.00000000 -0.09120266\nAWD       -0.03698098 -0.14166977 -0.15214456  0.08565668 -0.09120266  1.00000000\nWe can clearly see there is direct correlation between mpg and lenght as shown in the below graphs. Using this information our analysis continous to take these two variables in our main analysis for linear regresion.\nScatter\nLinearRegresion\nPSIvsDensity\nCall:\nlm(formula = mpg ~ length + AWD + clearance + weight + angle,\ndata = mpgm)\nResiduals:\nMin       1Q   Median       3Q      Max\n-19.4701  -4.4994  -0.0692   5.4433  18.5849\nCoefficients:\nEstimate Std. Error t value Pr(>|t|)\n(Intercept) -1.040e+02  1.585e+01  -6.559 5.08e-08 ***\nlength       6.267e+00  6.553e-01   9.563 2.60e-12 ***\nAWD         -3.411e+00  2.535e+00  -1.346   0.1852\nclearance    3.546e+00  5.412e-01   6.551 5.21e-08 ***\nweight       1.245e-03  6.890e-04   1.807   0.0776 .\nangle        6.877e-02  6.653e-02   1.034   0.3069\nSignif. codes:  0 ‘’ 0.001 ‘’ 0.01 ‘’ 0.05 ‘.’ 0.1 ‘ ’ 1\nResidual standard error: 8.774 on 44 degrees of freedom\nMultiple R-squared:  0.7149,\tAdjusted R-squared:  0.6825\nF-statistic: 22.07 on 5 and 44 DF,  p-value: 5.35e-11\nThis analysis shows the linear regresion, lm() returns our y intercept (Intercept) and slope (length) coefficients. We can find the linear regresion model for our dataset as shown below:\nmpg = 6.267e+00length + -1.040e+02\nWe can also see Pearson p-value 5.35e-11 and our r-squared 0.7149. This is sufficient evidence to reject our null hypotesis, which means that the slope of our linear model is not zero as we can visualize in the linear regresion model using ggplot2 being a positive correlation where x-axis(MPG) increases, the variable on the y-axis (Length) increases as well.\n##----------------------- Coil Suspension Analysis ------- t-test\n##Min, Mean, Max, Median, Standard Deviation, Variance, Num of Vehicles per lot\nManufacturing_Lot\nMin_PSI - Mean_PSI - Maximum_PSI - Med_PSI - StandDev_PSI - \tVariance - Num_Vehicles\n1\tLot1\t1498\t1500.00\t\t1502\t  1500.0\t0.9897433\t0.9795918\t50\n2\tLot2\t1494\t1500.20\t\t1506\t  1500.0\t2.7330181\t7.4693878\t50\n3\tLot3\t1452\t1496.14\t\t1542\t  1498.5\t13.0493725\t170.2861224\t50\nShowing 1 to 3 of 3 entries, 8 total columns\nWe can clearly see a much better product on Lot1, having a perfect Mean 1500 and very small variance. We need to look deeper on what change when Lot2 and Lot3 where produced and perform further analysis and more testing. The same is shown in the Paired t-test below. This is enough evidence to show that Lot 3 mean suspension coil PSI is out of specification and statistically it can be classified as different from the rest of the population mean. We could be within specification if we use the entire dataset that includes all three lots but we have to point out that Lot 3 is statistically different and out of sepecifications.\nPaired t-test\n\ndata:  Lot 1 and Lot2\nt = -0.52031, df = 49, p-value = 0.6052\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n-0.9724591  0.5724591\nsample estimates:\nmean of the differences\n-0.2\nLot3: PSI different from the rest of the population\nt = 2.0728, df = 49, p-value = 0.04347\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n0.1177828 7.6022172\nsample estimates:\nmean of the differences\n3.86\n##New Design Study**\nA new study that will be very intersting to consumers would be to compare wheel size against engine size. It is well known that if we have the incorrect tire size there can be fuel effiency cost. Finding the correct pair it can bring savings for consumers and they will be very insterested in finding that marketing can show this correlation proven scietifically unsing mathematical/machine learning models how we achive this effiency. This data is already available and it could be easily obtained by MechaCar vehicles. The null hypothesis would be that there is no stistical difference between MechaCar fuel effiency and other makers of similar vehicles. The alternative hypothesis would be that there is a statistical difference between fuel efficency of MechaCar when compared to other makers. The population data that we will request MechaCar would be all comparable vehicles using one-sample t-test we can perform this analysis. It will be absolutely necessary that MechaCar provides tire size and engine size to arrive to the conclusion of what is the best combination for fuel effiency.\n""], 'url_profile': 'https://github.com/juan-mpn', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['\n\nSRF: Survival Random Forest\nSRF is built upon grf package for forest-based statistical estimation and inference. SRF currently provides non-parametric approach for estimating restricted mean survival regression.\nSRF supports \'honest\' estimation (where one subset of the data is used for choosing splits, and another for populating the leaves of the tree), and confidence intervals for restricted mean survival regression.\nThis package is currently in beta, and we expect to make continual improvements to its performance and usability.\nAuthors\nSRF is developed and maintained by Mingyang Liu (mingyal@sas.upenn.edu), Hongzhe Li.\nThe repository first started as a fork of the grf repository -- we owe a great deal of thanks to the ranger authors for their useful and free package.\nInstallation\n\nClone srf repo to local machine\nFind build_package.R in r-package folder, and change 5th line setwd(""/Users/lmy/Downloads/SRF-master/r-package"") of build_package.R with your current address\nRunning build_package.R (to compile the code and install the modified version of grf package containing srf)\n\nUsage Examples\nThe following script demonstrates how to use SRF for Restricted Mean Survival Time Prediction.\nlibrary(survival)\nlibrary(survminer)\nlibrary(dplyr)\nlibrary(grf)\n\n\n# Generate training data and test data\n# Note that survival time only depends on first feature for the sake of visualization\n# Survival time follows log(exp(X*beta)+1) +epsilon\nn=500; p = 3; L=6.5; hazardC=0.1; sigma=0.01\nX=matrix(runif(n*p,0,1),n,p); X.test = matrix(runif(n*p,0,1),n,p)\nbetaT = rep(0,p); betaT[1]=2;\nbetaC = rep(0,p); betaC[1]=1;\nsurvtime = log( exp( X%*%betaT+5 )+1 )+rnorm(n,0,sigma); Y.test = log( exp( X.test%*%betaT+5 ) + 1 )+rnorm(n,0,sigma); truth = pmin( log( exp( X.test%*%betaT+5 ) + 1 ),L); Y.test.res.L=pmin(Y.test, L)\ncenstime = rexp( n,exp(X%*%betaC)*hazardC )\nZ =  pmin(survtime, censtime); Z.res.L=pmin(Z, L); \ndelta = (censtime>survtime)+0; delta.res.L=delta;  delta.res.L[Z>=L]=1\nsurv_object <- Surv(time = Z, event = 1-delta)\ndata <- data.frame (x = X)\noutput={}\noutput[[\'predictions\']]=NULL\noutput[[\'sd\']]=NULL\noutput[[\'truth\']]=truth\n\n#training and testing\n#---------------------------\ncox <- coxph(surv_object ~ ., data = data)\nG =c()\nfor(i in 1:n){\n  #if(i%%1000==0){print(i)}\n  cox.summary = summary(survfit(cox, data[i,]))\n  z=Z[i]\n  count = which(sort(c(cox.summary$time, z+1e-10))==z+1e-10)\n  count = count-1\n  if(count <1){count = 1}\n  G =c(G , 1-cox.summary$surv[count])\n}\nG[is.na(G )] <- 0\nG[G==1]<-1-1e-10\ns.forest = custom_forest(X, Z.res.L, delta.res.L, G, num.trees = 1000 )\ns.result  = predict(s.forest, X.test,estimate.variance = TRUE)\ns.predictions =s.result $predictions\ns.var  = s.result $variance.estimates\ncoverage.s = sum(truth<(s.predictions +1.96*sqrt(s.var )) & truth>(s.predictions -1.96*sqrt(s.var ))  )/n\nmse.s = sqrt(sum(( s.predictions -Y.test.res.L)^2))/n\nmae.s = (sum(abs( s.predictions -Y.test.res.L)))/n\nprint(paste0(\'Coverage: \', coverage.s))\nprint(paste0(\'RMSE: \', sqrt(mse.s)))\nprint(paste0(\'MAE: \', mae.s))\n\n\n#plot the true RMST, Predicted RMST and CI with respect to the first feature\n#--------------------------------------\noutput[[\'predictions\']]=s.predictions\noutput[[\'sd\']]=sqrt(s.var)\nplot(sort(X.test[,1]),truth[order(X.test[,1])],\'l\',xlim=c(0,1), ylim=c(4,7),col=""red"",  xlab=""Feature 1"", ylab=""RMST"", main=""Visualization of Performance of SRF"", sub = paste0(""Coverage Prob: "", coverage.s, "",RMSE: "", round(sqrt(mse.s),digits=3), \', MAE: \', round((mae.s),digits=3)))\np = rowMeans(output[[\'predictions\']])\nlines(sort(X.test[,1]),p[order(X.test[,1])],\'l\',col=""green"")\nstd = rowMeans(output[[\'sd\']])\nlines(sort(X.test[,1]),p[order(X.test[,1])]+1.96*std,type=""l"", col=""black"",  cex=0.3)\nlines(sort(X.test[,1]),p[order(X.test[,1])]-1.96*std,type=""l"", col=""black"",  cex=0.3)\nlegend(\'topleft\', legend=c(""True RMST"", ""Predicted RMST"", ""95%-CI""), col=c(""red"", ""green"", ""black""), lty=1:2, cex=0.8)\n\nComparison with RF predictions when censoring presents\n\n\n\nSimulation\nIn Simulation folder, you can run sim.R to reproduce the simulation results for different settings. Simulation result for each setting is provided as a csv file. In Simulation/Visualization for RSF folder, you can run visualization.R to reproduce the visualization of predictions from different models. Visualization result for each setting is provided as a pdf file and csv file.\nReal Data analysis\nThe real data analysis script is in realdata/tcga_ov_2.R. The ov(L, num_features) does a cross-validation analysis with our methods and previous methods. \'L\' stands for the restriction time. \'num_features\' is the number of features we consider in the analysis after feature screening. The feature screening is through univariate pvalue from cox model.\nReferences\nMingyang Liu, Hongzhe Li\nRegression Analysis of  Restricted Mean Survival Time Using Random Forests forthcoming.\n'], 'url_profile': 'https://github.com/lmy1019', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['python-error\nValueError: bad input shape (10605, 74) is occurring in logistic regression, SVC, GaussianNB\n'], 'url_profile': 'https://github.com/amrita1109', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '429 contributions\n        in the last year', 'description': [""Life Expectancy in the US\nExecutive Summary\nWe are taking the position of a consultancy company hired by the government to study the relationship between life expectancy in the US versus various factors related to health and lifestyle. The study's objective is to support the government in formulate healthcare policy based on the life expectancy model that we build.\nLatest data indicates that there are large differences in life expectancy (over 20 years) between some counties: our model will be particularly useful in addressing healthcare issues in vulnerable counties to bring them at par to the rest of the country.\nKey files\n\nLink here : Presentation in Google Slides format\nLife_Expectancy_USA.pdf : Presentation in PDF format\nLife_Expectancy_USA.ipynb : Jupyter notebook file with Python codes + commentaries\nLife_Analysis.py : local Python functions source file for Jupyter notebook\nanalytic_data2019.csv : Raw data source file in CSV format\n\nMethodology\n\nData Import\nData Cleansing\n2.1 Clean-up columns and data\n2.2 Remove outliers\nData Exploration\n3.1 Overview of all data via plots\n3.2 Overview of target (Life expectancy)\n3.3 Split and transform training and test data\nFeature Selection (Part 1): Evaluate predictors\n4.1 Baseline model : calculate k-fold cv with all predictors\n4.2 Baseline model : investigate regularization using Lasso\n4.3 Evaluate predictors (Step 1) : P-value of baseline predictors vs. target\n4.4 Evaluate predictors (Step 2) : Correlation of predictors vs. target\n4.5 Evaluate predictors (Step 3) : Multicollinearity between predictors\n4.6 Model 1 : using Top predictors\n4.7 Evaluate predictors (Step 4) : Interaction between top predictors\n4.8 Evaluate predictors (Step 5): Polynomial terms\n4.9 Add top interaction terms and top polynomial terms into data frame\nFeature Selection (Part 2) : Finalize predictors\n5.1 Model 2 : use Top predictors + interactions + polynomial terms\n5.2 Determine strongest predictor terms (based on correlation)\n5.3 Determine strongest predictor terms (based on standardized coefficient)\n5.4 Evaluate linear regression model assumptions via residual analysis\nFinal Model\n6.1 Prepare final training and test data\n6.2 Final model : run with training and test data\n\nKey findings\nThe first baseline model using all available predictors in the dataset gives a very high accuracy, but similar results would not be achieved in production due to overfitting of sample data. We employed several techniques to reduce the predictors based on the principles of correlation, multicollinearity, interaction between predictors, transforming predictors into polynomial terms, and regularization techniques using Ridge and Lasso.\nOur final model generates an r-squared value of 71% for training data. R-squared value is defined as the proportion of the variance (difference between actual observed data and modelized output) of life expectancy that can be explained by the model's predictor variables.\nFor test data, we obtained an R-squared value of 66%, which suggests that our model does not fall into overfitting trap. In other words, the model is able adapt to unknown data and generates the same level of accuracy as during the development stage.\nHere are the strongest contributing factors to predict life expectancy value in our model. The figures in parenthesis denote the model's absolute coefficient, which measures the relative weight of each predictor to the model's output.\n\n[0.58] Teen births\n[0.56] Adult smoking\n[0.39] Diabetes prevalence\n[0.32] Food insecurity\n[0.29] Median household income\n[0.26] Mental health providers\n[0.25] Physical inactivity\n[0.25] Mammography screening\n\nConclusions\nFrom the study and models conducted on the impact of various health and lifestyle factors to life expectancy in the US, we came up with following key conclusions:\n\nTeen births, smoking and diabetes prevalence are identified as top contributors to lower life expectancy.\nThere is a trade-off that we needed to take between the model’s accuracy and the ability to predict using unseen data input.\nThe accuracy of our model remains fairly unchanged when applied to new data set. We can conclude that it is a reliable model although more refinement can be done to improve its accuracy further.\n\nRecommendations\nHere are the top 3 action plans that we propose to US federal and state agencies to improve life expectancy and to reduce this inequality between states and counties:\n\nSex education in school: improvement in the quality of sex education curriculum in schools to be prioritized in order to reduce teen pregnancy, which came out as top factor in lowering life expectancy.\nFood stamps programme: food stamps programme to be extended to other vulnerable segments of the population to combat food insecurities via affordable access to food, and to combat diabetes via healthier food options.\nSouthern states are in need the most: states in the south of the US, particularly Mississippi, Alabama and Louisiana are the key areas of focus where life expectancy is the lowest in order to bring the level in par with other parts of the country.\n\n""], 'url_profile': 'https://github.com/khairulomar', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Mar 15, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jun 20, 2020', 'R', 'Updated May 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Oct 8, 2020', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['spam-classifier\nClassifying an email as spam or ham, using Naive Bayes and Logistic Regression Algorithms\n'], 'url_profile': 'https://github.com/SuvanshKumar', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NandhiniN85', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['multiclass_classification_nn\nThis project implements one_vs_all logistic regression and neural networks to recognize handwritten digits.\n'], 'url_profile': 'https://github.com/gsantiago1618', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Chicago, Illinois', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['chicago_crime_analysis\nAnalyzing the Chicago crime dataset. (Data Exploration, Data Visualization, Regression, Classification)\n'], 'url_profile': 'https://github.com/vidhyasagar13', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['stock-price-modeling\nTSLA stock price modeling using simple linear regression and KNN algorithm\n'], 'url_profile': 'https://github.com/enoo24', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['truck-logit\nScania Truck problem. Using generalized logistic regression to identify component failures.\n'], 'url_profile': 'https://github.com/benjamin-carter', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['Linear-and-Logistic-Regression-with-L1-and-L2-Lasso-and-Ridge-Regularization-Feature-Selection\nLinear and Logistic Regression with L1 and L2 ( Lasso and Ridge) Regularization for Feature Selection\nDownload Working Files: https://github.com/laxmimerit/Linear-and-Logistic-Regression-with-L1-and-L2-Lasso-and-Ridge-Regularization-Feature-Selection\nLinear regression is a straightforward approach for predicting a quantitative response Y on the basis of a different predictor variable X1, X2, ... Xn. It assumes that there is a linear relationship between X(s) and Y. Mathematically, we can write this linear relationship as Y ≈ β0 + β1X1 + β2X2 + ... + βnXn.\nBasic Assumptions\nLinear relationship with the target y\nFeature space X should have gaussian distribution\nFeatures are not correlated with other\nFeatures are in same scale i.e. have same variance\nLasso (L1) and Ridge (L2) Regularization\nRegularization is a technique to discourage the complexity of the model. It does this by penalizing the loss function. This helps to solve the overfitting problem.\nRegularization adds a penalty on the different parameters of the model to reduce the freedom of the model. Hence, the model will be less likely to fit the noise of the training data and will improve the generalization abilities of the model\nL1 regularization (also called Lasso)\nL2 regularization (also called Ridge)\nL1/L2 regularization (also called Elastic net)\nA regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\nThe key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\nThe L1 regularization adds a penalty equal to the sum of the absolute value of the coefficients.\nThe L1 regularization will shrink some parameters to zero. Hence some variables will not play any role in the model, L1 regression can be seen as a way to select features in a model\nWhat is Ridge Regularisation\nThe L2 regularization adds a penalty equal to the sum of the squared value of the coefficients.\nThe L2 regularization will force the parameters to be relatively small, the bigger the penalization, the smaller (and the more robust) the coefficients are.\nDifference between L1 and L2 regularization\nL1 Regularization\nL1 penalizes sum of absolute value of weights.\nL1 has a sparse solution\nL1 has multiple solutions\nL1 has built in feature selection\nL1 is robust to outliers\nL1 generates model that are simple and interpretable but cannot learn complex patterns\nL2 Regularization\nL2 regularization penalizes sum of square weights.\nL2 has a non sparse solution\nL2 has one solution\nL2 has no feature selection\nL2 is not robust to outliers\nL2 gives better prediction when output variable is a function of all input features\nL2 regularization is able to learn complex data patterns\nLike Facebook Page: https://www.facebook.com/kgptalkie/\nWatch Full Playlists: Feature Selection in Machine Learning using Python: https://www.youtube.com/playlist?list=PLc2rvfiptPSQYzmDIFuq2PqN2n28ZjxDH\nMachine Learning with Theory and Example https://www.youtube.com/playlist?list=PLc2rvfiptPSTvPFbNlT_TGRupzKKhJSIv\nMake Your Own Automated Email Marketing Software in Python: https://www.youtube.com/watch?v=gmYuom6kfoY&list=PLc2rvfiptPSQK9ErKaLqf40iu1A3le9Zr\n'], 'url_profile': 'https://github.com/sachinyar', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Guatemala', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['movie_prediction_model\nProyecto de R para el curso de Coursera Linear Regression models\n'], 'url_profile': 'https://github.com/jpcarranza94', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MPeal', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""Tymit's Task\nDescription and datasets of the task in src folder.\nGetting Started\nPython 3.7 procedures written in jupyter notebook Main.ipynb.\nPrerequisites\nRequired modules in requirements.txt.\nInstalling\nUsed conda environment and conda install package.\nRunning the tests\nNo test needed.\nDeployment\nBuilt With\n\nConda - Package manager\nJupyterNotebook - Web-based interactive development environment\n\nAuthors\nDaniel Coll\nLicense\nNo license\n""], 'url_profile': 'https://github.com/danielcollsol', 'info_list': ['Python', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'MATLAB', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Aug 25, 2019', 'HTML', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}"
"{'location': 'gurugram', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['datatrained\n'], 'url_profile': 'https://github.com/rahul-datahelper', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Waterville, ME', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['DID-confounding-supplementary\nSupplementary materials for ""Confounding and Regression Adjustment in Difference-in-Differences Studies""\nHere, you can find all of our simulation code and web-appendix for our manuscript.\n'], 'url_profile': 'https://github.com/zeldow', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Duke University', 'stats_list': [], 'contributions': '320 contributions\n        in the last year', 'description': [""\nRobust Chauvenet Outlier Rejection (RCR)\n\n\n\n\n\n\nWhat is RCR?\nRCR is advanced, but easy to use, outlier rejection.\nThe simplest form of outlier rejection is sigma clipping, where measurements that are more than a specified number of standard deviations from the mean are rejected from the sample. This number of standard deviations should not be chosen arbitrarily, but is a function of your sample’s size. A simple prescription for this was introduced by William Chauvenet in 1863. Sigma clipping plus this prescription, applied iteratively, is what we call traditional Chauvenet rejection.\nHowever, both sigma clipping and traditional Chauvenet rejection make use of non-robust quantities: the mean and the standard deviation are both sensitive to the very outliers that they are being used to reject. This limits such techniques to samples with small contaminants or small contamination fractions.\nRobust Chauvenet Rejection (RCR) instead first makes use of robust replacements for the mean, such as the median and the half-sample mode, and similar robust replacements that we have developed for the standard deviation.\nRCR has been carefully calibrated, and extensively simulated (see Maples et al. 2018). It can be applied to samples with both large contaminants and large contaminant fractions (sometimes in excess of 90% contaminated).\n\nDocumentation/How to Use RCR\nThe documentation covers all of the RCR API, and provides thorough examples for using RCR in all of its forms.\nWe've also built a web calculator for quick use of RCR, including interactive visualizations. The calculator can be used for either\n\none-dimensional dataset outlier rejection or\noutlier rejection combined with model fitting.\n\n\nInstallation\n\nLinux and macOS\nRCR can be used most easily via Python, installed using python3 -m pip install rcr in the command line.\nThe C++ source code is also included here in /src, with documentation in /docs/cpp_docs.\n\nWindows\nBefore installing, you'll need to have Microsoft Visual C++ 14.0, found under the Microsoft Visual C++ Build Tools. If that doesn't work, you may need the latest Windows SDK. (Both can be installed through the Visual Studio Installer.)\nAfter that, run python3 -m pip install rcr in the command line.\n\nLicensing and Citation\nRCR is free to use for academic and non-commercial applications (see license in this repository). We only ask that you cite Maples et al. 2018 as:\n@article{maples2018robust,\n    title={Robust Chauvenet Outlier Rejection},\n    author={{Maples}, M.P. and {Reichart}, D.E. and {Konz}, N.C. and {Berger}, T.A. and {Trotter}, A.S. and {Martin}, J.R. and {Dutton}, D.A. and {Paggen}, M.L. and {Joyner}, R.E. and {Salemi}, C.P.},\n    journal={The Astrophysical Journal Supplement Series},\n    volume={238},\n    number={1},\n    pages={2},\n    year={2018},\n    publisher={IOP Publishing}\n}\nFor commercial applications, or consultation, feel free to contact us.\nThere is no more fundamental act in science than measurement. There is no more fundamental problem in science than contaminated measurements. RCR is not a complete solution...but it is very close! We hope that you enjoy it.\nNick Konz, Dan Reichart, Michael Maples\nDepartment of Physics and Astronomy\nUniversity of North Carolina at Chapel Hill\n""], 'url_profile': 'https://github.com/nickk124', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Jeddah, Saudi Arabia ', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Linear-Regression-with-Ecommerce-data\nLinear Regression Project with an Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions. Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want.  The company is trying to decide whether to focus their efforts on their mobile app experience or their website.\n'], 'url_profile': 'https://github.com/galsaeedi', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['LinearRegression-on-Boston-Dataset\n'], 'url_profile': 'https://github.com/Koorimikiran369', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['ML_LogisticsRegression-DecisionTree\nData Science using python: Training of Machine Learning Model using LogisticRegression & DecisionTree to predict if the client will subscribe to term deposit for Banks.\n'], 'url_profile': 'https://github.com/s3arajgupta', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'virginia', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['-House-Prices: Advanced-Regression-Techniques\nKaggle Competition https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/alpnelson', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': ['Lab1LinearRegressionReg\nLaboratorio 1 para el curso de Inteligencia Artificial en la Universidad del Valle de Guatemala. Rodrigo Zea - 17058.\n'], 'url_profile': 'https://github.com/RodrigoZea', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['KNN-Naive_Bayes-Logistic_Regression\nPython implementation of K-Nearest Neighbour (KNN), Naive Bayes and Logistic Regression machine learning algorithm from scratch.\nReport:\nhttps://github.com/ayushkdwivedi/KNN-Naive_Bayes-Logistic_Regression/blob/master/Report.pdf\n'], 'url_profile': 'https://github.com/ayushkdwivedi', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marioeid', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated Mar 12, 2020', '2', 'C++', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Python', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['SB10.1-LinearRegression\nSubmission of 10.1 Exercise for Springboard\n'], 'url_profile': 'https://github.com/subbu3642', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['Linear-Regression-For-Machine-Learning\nIn this project housing data for the city of Ames, Iowa, United States from 2006 to 2010 is used to predict the house sale price. This is done by using linear regression model, by using two different approaches to model fitting, after completing data cleaning, transforming, and selecting features.\n'], 'url_profile': 'https://github.com/Shibasrit', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Subru97', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Performing Linear Regression with Python\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': [""Logistic-Regression-and-Decision-Trees\nThere are two parts of this project the first focuses on customer retention. The second part is web robots detection using different variables.\nThe project details are shared in the project report.\nThe data is not shared as I don't own that data.\n""], 'url_profile': 'https://github.com/JiyadRehman', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Preview\nIn this lab, you’ll use a multiple linear regression machine learning algorithm to estimate a person’s medical insurance cost with his or her BMI(Body Mass Index) and whether he or she is a smoker.\nGetting set up\nImport Libraries for Linear Regressions\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nImport Data File from Google Drive\n# Code to read csv file into Colaboratory:\n!pip install -U -q PyDrive\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n# Authenticate and create the PyDrive client.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\nlink = \'https://drive.google.com/open?id=15qJFX87eHO7OWJyjJYIli7VejqW-aGvD\' # The shareable link\nfluff, id = link.split(\'=\')\n# Verify that you have everything after \'=\'\ndownloaded = drive.CreateFile({\'id\':id}) \ndownloaded.GetContentFile(\'insurance2.csv\')  \ndf3 = pd.read_csv(\'insurance2.csv\')\nprint(df3)\n\nDataset is now stored in a Pandas Dataframe.\nThe format of this Pandas Dataframe should be the following.\n\nSmoker = {\'yes\': 1,\'no\': 0} \ndf3.smoker = [Smoker[item] for item in df3.smoker]\nprint(df3)\n\n\nHowever, because we can only perform linear regression with numerical values, we need to convert all the strings into categories. Thus, we need to determine how many categories are needed and make the change in the dataframe.\nConvert all the yes and no under the ""smoker"" column into 1 and 0.\nExpected Output:\n\n\n'], 'url_profile': 'https://github.com/CCAGoogleColab', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nit1n', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': [""[Regression] Home Market Value Prediction\nConstruct a market value prediction model to predict a home's current market value and score a test data set with this model.\nBoth training and test data are home sale transactions from 2015 in King County, WA. The prediction result is a CSV file of ZPIDs and home price predictions in a column named “SaleDollarCnt” for all homes in the test set.\nTwo metrics to meature the accuracy:\n\nAAPE: 𝑚𝑒𝑎𝑛(𝑎𝑏𝑠(𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑−𝑎𝑐𝑡𝑢𝑎𝑙)/𝑎𝑐𝑡𝑢𝑎𝑙)\nMAPE: 𝑚𝑒𝑑𝑖𝑎𝑛(𝑎𝑏𝑠(𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑−𝑎𝑐𝑡𝑢𝑎𝑙)/𝑎𝑐𝑡𝑢𝑎𝑙)\n\nDescription of fields contained in the training and test data sets\n\nPropertyID: Unique ID for home\nTransDate: Date of current sale\nSaleDollarCnt: Price of current sale\nBathroomCnt: Number of bathrooms in home\nBedroomCnt: Number of bedrooms in home\nBuiltYear: Year home was constructed\nFinishedSquarefeet: Finished square footage of the home\nGarageSquareFeet: Size of protected garage space if any\nLotsizeSquarefeet: Lot size of property in square feet\nStoryCnt: Number of stories for the home\nlatitude: Latitude of the home * 1,000,000\nlongitude: Longitude of the home * 1,000,000\nUsecode: Type of home (all homes in both training and test are single-family homes)\nZoneCodeCounty: The intensity of use or density the lot is legally allowed to be built-up to\nviewtypeid: Nominal variable indicating the type of view from the home (blank or NULL\nvalue indicates no view)\ncensusblockgroup: The FIPS code for the census block group this property is located in.\nYou can derive the census tract FIPS by truncating the rightmost digit.\nBGMedHomeValue: The median home value in the block group\nBGMedRent: The median rent value in the block group\nBGMedYearBuilt: The median year structures in the block group were built\nBGPctOwn: Percentage of homes that are owner-occupied in the block group\nBGPctVacant: Percentage of housing that is vacant in the block group\nBGMedIncome: Median income of households residing in the block group\nBGPctKids: Percentage of households with children under 18 years present at home\nBGMedAge: Median age of residents of the block group\n\n""], 'url_profile': 'https://github.com/YikunLiu0801', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['Company-Profits-using-Linear-Regression\n'], 'url_profile': 'https://github.com/bitsofishan', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Atlanta, Georgia, USA', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chinyemba', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', 'R', 'MIT license', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}"
"{'location': 'Atlanta, Georgia, USA', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chinyemba', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saurabh0501', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['SB10.1-LogisticRegression\nSubmission of 10.1 Exercise for Springboard\n'], 'url_profile': 'https://github.com/subbu3642', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'Seoul, South Korea', 'stats_list': [], 'contributions': '329 contributions\n        in the last year', 'description': ['dss12th_regression_Auction\n아파트 경매 예측\n목적\n\n아파트 경매 가격에 영향을 미치는 요소 파악과 예측\n\nDataSource\n\n[금융] 아파트 경매가격 예측\nhttps://dacon.io/competitions/official/17801/data/\n\n\n분석 과정\n\nEDA\nDataWrangling\nRegressionAnalysis\nPredict\nValidation\n\n참여\n\n패스트 캠퍼스 데이터사이언스 스쿨 12기 수강생\n김태형, 조용하, 이진주, 김도경, 안효준\n\n'], 'url_profile': 'https://github.com/loveactualry', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'Wuhan , China', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/naiveyueya', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pawarshubham99', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LeoLeos', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\n'], 'url_profile': 'https://github.com/theirfanrahman', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Data-Science\nEnthusiastic Data Scientist\n'], 'url_profile': 'https://github.com/Sandhya1825', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}","{'location': 'Jeddah, Saudi Arabia ', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/galsaeedi', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 12, 2020', '1', 'Python', 'Updated Mar 10, 2020', 'Updated Mar 15, 2020', 'R', 'Updated Mar 13, 2020', 'Updated Mar 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TemirlanBaimyrza', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshi29', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': '1049 E Duane Ave, Sunnyvale, CA 94085', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PPP180000', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Greater Noida, India', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['#Board Game Review Project\nThis project has imported a dataset of over 80000 games. Each game has some information related to it- id, min_players, max_players, users_rated, max_playtime etc. Using this information, we will predict the average rating of the games using a linear regression model and a random forest regressor.\n'], 'url_profile': 'https://github.com/aparna308', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Tuticorin', 'stats_list': [], 'contributions': '501 contributions\n        in the last year', 'description': ['House_Price_Prediction\nUse a test-driven approach to build a Linear Regression model using Python from scratch. You will use your trained model to predict house sale prices and extend it to a multivariate Linear Regression\n'], 'url_profile': 'https://github.com/Nivitus', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tamercetin', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Stillwater', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': [""Logistic-regression-from-scratch-using-NumPy\nIn this repository, the PIMA Indians diabetes dataset has been used to illustrate how to create a logistic regression model from scratch using vectorization of the predictor variables and gradient descent algorithm for cost minimization with the intuition of neural network.\nThe steps used to create the model are listed below -\n1. Defining the activation function\nThe model algorithm consists of linear transformation of predictor variables to predict the output, which will give a numeric value. Thus, to restrict those values in the range of 0 and 1, an activation function is used here.\nWe have used the sigmoid activation function here which is given by-\nsigmoid(Z)=1/(1+e^(-Z))\n2. Random initialization of parameters (bias term and coefficients)\nThe parameters 'w' and 'b' have been initialized as 0 to start with. With each iteration of the logistic regression algorithm, the    values are updated using the optimization function.The 'w' vector has the shape (no. of features in predictor, 1).\n3. Forward propagation\nIn this step, we do the activation conversion and compute the cost of the model which is given by\n\n4. Optimization\nUsing this function definition, we optimize the cost function mentioned above and update the parameters to minimize the cost. The gradients descent algorithm is used and using the algorithm, the gradients of the 'w' and 'b' parameters are computed and the parameters are updated by using the following formula-\nw := w - learning_rate * dw, and\nb := b - learning_rate * db\nBy running this function using loop for n no. of iterations, we reach a convergence point where the cost function doesn't decrease any further.\nAnother important thing to note is the choice of learning rate. Learning rate shouldn't be too small to increase the computational expense, or too large to overshoot the global minima. Thus, we try the model algorithm with various learning rates to find out the rate which gives optimizes the cost function the best.\nThe final step is to create a 'predict' function and calculate the accuracy of predicted alues from test set.\n""], 'url_profile': 'https://github.com/TrishlaM', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Boise, Idaho', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Logistic-Regression-with-a-Neural-Network-mindset\n'], 'url_profile': 'https://github.com/santosh13579', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '385 contributions\n        in the last year', 'description': ['Basic-Linear-Regression-Using-Iris-Dataset\nSource Code for my medium post\nhttps://medium.com/@darrylsws/linear-regression-using-iris-dataset-hello-world-of-machine-learning-b0feecac9cc1\n'], 'url_profile': 'https://github.com/peanutsee', 'info_list': ['Python', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}"
"{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['Gaussian Process Regression for Energy Generation Prediction\nThe goal of this task is to accurately predict the net hourly generated energy of a Combined Cycle Power Plant. In order to do so, five different measured variables that should be related to the output are provided. These variables are for instance the Temperature, Relative humidity among others.\nAs usual, the provided data has been corrupted with noise, and some values have been lost during their acquisition.\nSolution\nA Gaussian Process Regression (GPR) model with a Matérn kernel (nu = 5/2) was implemented to both estimate the output and the missing values of the data.\nAcknowledgements\nUniversity Carlos III of Madrid, Data Processing (https://www.kaggle.com/c/uc3m-data-processing/overview/description).\n'], 'url_profile': 'https://github.com/javiccano', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sourav2195', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'Greater Noida , knowledge park 3', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amit2020cs', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'Bhubaneswar, Odisha', 'stats_list': [], 'contributions': '184 contributions\n        in the last year', 'description': ['Combined_Cycle_Power_Plant_regression-model.\nData Set Information:\nThe dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant.\nA combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is colected from and has effect on the Steam Turbine, he other three of the ambient variables effect the GT performance.\nFor comparability with our baseline studies, and to allow 5x2 fold statistical tests be carried out, we provide the data shuffled five times. For each shuffling 2-fold CV is carried out and the resulting 10 measurements are used for statistical testing.\nWe provide the data in .xlsx formats.\n'], 'url_profile': 'https://github.com/JeevantheDev', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'Bengaluru, India', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Logistic-Regression-Decision-Tree-Random-Forest\nBusiness Problem-\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n'], 'url_profile': 'https://github.com/nidhifactualai', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Linear_Regression-from-scratch-on-NASA_Dataset\nGiven a NASA data set, obtained from a series of aerodynamic and acoustic tests\nof two and three-dimensional airfoil blade sections. Implement a linear regression model from scratch using gradient descent to predict scaled sound pressure level.\n'], 'url_profile': 'https://github.com/SmritiAgrawal04', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}","{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': ['Regression Analysis to explain median value of homes in Boston.\nNote : Median housing values are used because they are more accurate as compared to mean values. Median values are not affected by outliers.\n506 observations and 13 predictors (features) for prediction.\nThe predictors (X values) are:\n1 per capita crime rate by town\n2 proportion of residential land zoned for lots over 25,000 sq.ft.\n3 proportion of non-retail business acres per town\n4 Charles River dummy variable\n5 nitric oxide concentration (parts per 10 million)\n6 average number of rooms per dwelling\n7 proportion of owner occupied units built prior to 1940\n8 weighted distances to five Boston employment centres\n9 index of accessibility to radial highways\n10 full-value property-tax rate per 10,000\n11 pupil-teacher ratio by town\n12 1000(B − 0.63)2 where B is the proportion of African Americans by town\n13 a numeric vector of percentage values of lower status population\nPreparing the Dataset\nDataset is divided into training data (model building dataset) and testing data.\nExploratory Data Analysis (EDA)\nVizualisations are used to perform EDA to eliminate variables not impacting Median Housing Value (target variable).\n\nVisualizations used:\n\nSummary Statistic (to find a central measure and detect outliers)\nCorrelation between Median Value and feature (to eliminated uncorrelated features)\nBox Plot (to detect outliers)\nHeat Map\n\n\n\nModel Selection and Validation where the correct model is identified.\nDiagnostocs\n\nOutlying Y observations detected\nOutlying X observations detected\nInfluencial Observations detected\n\n'], 'url_profile': 'https://github.com/athiyarastogi', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Linear_Regression-from-scratch-on-NASA_Dataset\nGiven a NASA data set, obtained from a series of aerodynamic and acoustic tests\nof two and three-dimensional airfoil blade sections. Implement a linear regression model from scratch using gradient descent to predict scaled sound pressure level.\n'], 'url_profile': 'https://github.com/SmritiAgrawal04', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': ['Regression Analysis to explain median value of homes in Boston.\nNote : Median housing values are used because they are more accurate as compared to mean values. Median values are not affected by outliers.\n506 observations and 13 predictors (features) for prediction.\nThe predictors (X values) are:\n1 per capita crime rate by town\n2 proportion of residential land zoned for lots over 25,000 sq.ft.\n3 proportion of non-retail business acres per town\n4 Charles River dummy variable\n5 nitric oxide concentration (parts per 10 million)\n6 average number of rooms per dwelling\n7 proportion of owner occupied units built prior to 1940\n8 weighted distances to five Boston employment centres\n9 index of accessibility to radial highways\n10 full-value property-tax rate per 10,000\n11 pupil-teacher ratio by town\n12 1000(B − 0.63)2 where B is the proportion of African Americans by town\n13 a numeric vector of percentage values of lower status population\nPreparing the Dataset\nDataset is divided into training data (model building dataset) and testing data.\nExploratory Data Analysis (EDA)\nVizualisations are used to perform EDA to eliminate variables not impacting Median Housing Value (target variable).\n\nVisualizations used:\n\nSummary Statistic (to find a central measure and detect outliers)\nCorrelation between Median Value and feature (to eliminated uncorrelated features)\nBox Plot (to detect outliers)\nHeat Map\n\n\n\nModel Selection and Validation where the correct model is identified.\nDiagnostocs\n\nOutlying Y observations detected\nOutlying X observations detected\nInfluencial Observations detected\n\n'], 'url_profile': 'https://github.com/athiyarastogi', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Linear_Regression_MLForCloudDeployment_GCP\nhttps://virtual-data-271118.appspot.com\n'], 'url_profile': 'https://github.com/Nayeemuddin-mohd', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['Logistic-Regression-Using-Heights-and-Weights\nThis was done as a part of Springboard Data Science Career Track course (Unit10.1.6. Heights and Weights Using Logistic Regression).\n'], 'url_profile': 'https://github.com/NamikoNa', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Categorical-Grouping-With-Logistic-Regression\nPurpose: To discover which cohorts (groupings of categorical variables) are the most likely to have their binary dependent variables = YES.\nRequirements: 1 .csv with the binary DV in the first column, and at least 2 categorical variables. Continuous variables will be used for modeling, but are not required.\nImport the data in LINE 47: ""FakeData=PATH_TO_YOUR_CSV""\nOptional: An additional .csv file with the same columns, excluding the DV, but including all other columns. Predictions for these rows will be made using the model obtained from the required .csv\nThe optional data is imported at LINE 129: ""NewData=PATH_TO_YOUR_CSV"", and change LINE 117 to ""has_binary=\'Yes\'""\nWhy not use existing algorithms?\nMy original algorithm is unique for 2 reasons:\n\nIt automatically performs one-hot ecoding, entirely removing this painstaking step in the preprocessing procedure!\nIts output is a data frame containing the combination of IV\'s, the size of each such combination, as well as a score for their proximity to the 0.5 cutoff of being considered success/failure\n\n'], 'url_profile': 'https://github.com/ckalra94', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'Bristol, UK', 'stats_list': [], 'contributions': '1,001 contributions\n        in the last year', 'description': ['OpenCV optical flow speed comparison\nI\'ve found OpenCV\'s GPU TVL1 implementation to be much slower in v4 than in v2.\nThis repository serves as an example demonstrating the issue.\nSet up\nEnsure you have docker 19.03+ with an NVIDIA card present on your system. Build\nthe docker images (this handles building the base OpenCV images + the optical\nflow demo application)\n$ make flow-images\nDownload test media and extract frames:\n$ ffmpeg -i ""https://github.com/MarkAYoder/esc-media/raw/master/BigBuckBunny_640x360.m4v"" -t 00:00:10 -qscale 2 frames/frame_%010d.jpg\nRun speed test\nDiscard the first results as they will include the time spent by the nvidia\ndriver generating binaries for the current GPU from the PTX files.\n$ time docker run \\\n    --gpus \'""device=0""\' \\\n    --rm -it \\\n    -v $PWD/frames:/input \\\n    -v $PWD/flow/opencv2:/output \\\n    -v $PWD/.cache-opencv2:/cache/nv \\\n    willprice/furnari-flow:opencv2 \n\n...\n0.03user 0.02system 0:14.57elapsed 0%CPU (0avgtext+0avgdata 63544maxresident)k\n0inputs+0outputs (0major+7956minor)pagefaults 0swaps\n\n$ time docker run \\\n    --gpus \'""device=0""\' \\\n    --rm -it \\\n    -v $PWD/frames:/input \\\n    -v $PWD/flow/opencv4:/output \\\n    -v $PWD/.cache-opencv4:/cache/nv \\\n    willprice/furnari-flow:opencv4\n\n...\n0.04user 0.02system 2:31.88elapsed 0%CPU (0avgtext+0avgdata 63404maxresident)k\n0inputs+0outputs (0major+7877minor)pagefaults 0swaps\n'], 'url_profile': 'https://github.com/willprice', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Advanced-Regression-Modeling-for-House-Prices-Prediction\n• Carried out EDA and Feature Engineering to get the optimal data for the models to trained on.\n• Performed Linear Regression, KNN Regressor, Lasso & Ridge regression on the training dataset.\n• Achieved the best R^2 of 0.93 and MAE of 15149 for the model Lasso with GridsearchCV.\n'], 'url_profile': 'https://github.com/meghashyamnaidub', 'info_list': ['Jupyter Notebook', 'Updated May 12, 2020', 'R', 'Updated Jan 10, 2021', 'Updated Mar 11, 2020', 'CSS', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'C++', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 29, 2020']}"
"{'location': 'Karachi', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['Cars-Analysis-Using-Multiple-Linear-Regression\nDataset easily available in Kaggle website.\n'], 'url_profile': 'https://github.com/syedkashifaliquadri', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': ['Predict Customer Churn with Logistic Regression\nIntroduction\nWhen implementing new features within an application, it is extremely important to analyze the effects of this feature in order to definitively know whether or not the implementation impacted user behavior. This analysis can help companies evaluate these features and properly reallocate resources if the feature is not effective. KyngaCell, a leader in the mobile gaming industry, is interested in assessing the effectiveness of their new online community feature in regard to increased revenue, retention, and customer lifetime value.\nData Description\nData was obtained on various behaviors of 199 users before and after the introduction of the online community. These behaviors include whether the user joined the community (41% joined), the spend of users one month before (avg = $78) and one month after (avg = $121) the introduction of the community, and the number of months a user had been playing the game before the community was introduced (avg = 4). Data was also included on whether each user churned within the first 90 days following the introduction of the online community (59% churned) and how much each user spent in their last 90 days using the app (avg = $80).\nEvaluating Revenue: Model and Results\nThe data provided consist of users’ spending one month before and one month after the launch of the online community feature. Additionally, there is information about whether or not that customer joined the online community. With this data, we can build a\ndifferences-in-differences (diff-in-diff) model where we assess the spending differences of two groups (those who joined the online community and those who did not) and compare those spending differences at two different points in time (one month before and one month after the feature launch). Comparing the group differences at these two points in time will allow us to assess the exact effect of joining the online community on user spending.\n\nBased on the diff-in-diff model created, there is strong evidence that joining the online community feature can increase user revenue. Comparing the group spending differences before and after the launch of the online community, we found out that there is a significant increase of 29 dollars in revenue due to the new feature. Hence we can draw the conclusion that the launch of the online community helped increase sales by a factor of $29; however it is important to note that the small sample size poses a limitation to this conclusion, simply because there is not enough data to reach a definitive conclusion.\nEvaluating Retention: Model and Results\nThe data provided contain customers’ spending and churn status in the three months following the online community launch, as well as the monthly age of the customer at the time of the online community launch. With this information we can create a logistic regression model that predicts the probability of a customer churning or not based on their spending, whether or not they joined the online community, and their monthly age. We chose a logistic regression because it provides a quantified value for the strength of the association while adjusting for other variables (removes confounding effects). The exponential of coefficients correspond to the odds ratios for the given factor, which enables us to achieve a result with strong interpretation power.\nBased on our logistic regression model, the online community will not raise the user retention rate. We found that whether the user joined the online community or not is the only factor that has a statistically significant impact on the probability of churn. The coefficient estimate of this variable “join” being 0.21, means that if a customer joined the online community, their probability of churning increased by 0.21. To assess the model accuracy, we looked at the classification error, which is the proportion of observations that have been misclassified. From this error, we found the accuracy of the model is 0.6281; we can then prove that the prediction is quite accurate and validate our initial conclusion. Intuitively, joining the online community should decrease the churn rate, but our analysis says otherwise. Three months, however, could be too short to see the return on retention, so the data may not be valid for the assumption. Also, it is possible that other unobserved factors may also be interfering with the result.\nEvaluating Customer Lifetime Value: Model and Results\nWhen determining whether joining the online community had any affect on Customer Lifetime Value (CLV), we used predicted churn rates and calculated profit margin from average spending of users in their last 90 days using the app. The regression showed that joining the online community had an effect on CLV, however it was not positive. The results showed that joining the online community was correlated with a $26 decrease in CLV. This finding goes hand-in-hand with the finding that joining the community was related to decreased retention.\nPredicting CLV is incredibly difficult, and relies on an accurate prediction of churn along with other factors. While the direction of the relationship between joining the community and CLV may be accurate (negative), more user data would make the analysis of the size of the effect more accurate. There also may be other factors affecting CLV for which this model does not account for.\nConclusion\nAdding the online community for app users had mixed results on customer behaviors.\nJoining the community led to a $29 increase in spending by users which would not have happened had the community not been introduced. However, those who joined the community were more likely to churn within a 90-day period and overall had a lower Customer Lifetime Value. The company must now decide whether short-term income gains or long-term increases in retention and CLV are of more importance. This also shows that it is important to always back up beliefs with data, otherwise important insights may be missed.\n'], 'url_profile': 'https://github.com/joeyli1997', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ycai4591679', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Linear_regression-study-on-Alumni-data\nPlease check the pdf for the case study details\n'], 'url_profile': 'https://github.com/Srujanaguduru', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/josejohny68', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['Multivariate-Linear-Regression-for-predicting-fish-weight\n'], 'url_profile': 'https://github.com/shafayet98', 'info_list': ['Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'MATLAB', 'Updated Mar 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['linear-KNN\nUsing multiple linear regression and KNN classifier to aid in profiling of properties with high price per square footage\n'], 'url_profile': 'https://github.com/benjamin-carter', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Dalian, China', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Daily-inflow-forecasting\nMulti-step ahead daily inflow forecasting using ERA-Interim reanalysis dataset based on gradient boosting regression trees\n'], 'url_profile': 'https://github.com/DLUTzhanweiliu', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Arlington, TX', 'stats_list': [], 'contributions': '188 contributions\n        in the last year', 'description': [""Overview\nA Linear Regression model implemented from scratch using Vector Calculus approach to predict the class of an Iris plant\nDataset\nThe dataset is obtained from the UCI Machine Learning Repository and contains 3 classes of\n50 instances each, where each class refers to a type of iris plant. The file is comma separated and the fields/attributes in each record are as follows\nFeatures\n\nSepal Length in cm\nSepal Width in cm\nPetal Length in cm\nPetal Width in cm\n\nLabel\nThe class of the plant\n\nIris Setosa\nIris Versicolour\nIris Virginica\n\nSample Data\n\n\n\nsepal length\nsepal width\npetal length\npetal width\nclass\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n\nThe Algorithm\nThis model is implemented using Vector Calculus approach.\n\nWhere,\n\nβ - A column vector of co-efficients\nA - The feature vectors\nY - The label vectors\n\nRequirements\nPlatform\n\nOperating System - Linux or Windows\nPython - 3.6.5\n\nLibraries\nAlthough this model is implemented without using any libraries for the core calculations,\ncsv,random and math are used to conveniently load the data and shuffle it during cross validation.\nThese libraries are available with the default installation of python.\nIn case these libraries aren't available on the target machine, they can be installed using\n$ pip install csv random math\nImplementation\nThe label field of the dataset is transformed into a numeric value by assigning the following numbers for each class\n\n\n\nclass\nnumber\n\n\n\n\nIris-setosa\n0\n\n\nIris-versicolor\n1\n\n\nIris-virginica\n2\n\n\n\nThe predicted numeric value is then approximated to the nearest integer as shown in the examples below\nand are used to look-up for the respective predicted label.\nExample\n\n1.03 is approximated to 1. Hence the label predicted is Iris-versicolor\n1.5 is approximated to 2. Hence the label predicted is Iris-virginia\n\nProject structure\n<project home>\n├── data\n│\xa0\xa0 └── iris.data   # The Dataset\n├── main.py         # To use the model and predict the class of the plant\n├── models.py       # Implementation of the Linear Regression model\n└── README.md\nClasses\nLinearRegression\nThis class contains the code to perform Linear Regression on the given dataset. The logic is divided across the following methods\ninit(file)\nThe class constructor. Loads the dataset into the model, separates the features and labels\n\n\n\nParam\nType\nDescription\n\n\n\n\nfile\nstring\nThe path to the dataset(iris.data)\n\n\n\ndot(mat1, mat2)\nReturn the dot product of matrices mat1 and mat2.\n\n\n\nParameter\nType\nDescription\n\n\n\n\nmat1\nlist[list]\nAn MxN matrix loaded as a list of lists\n\n\nmat2\nlist[list]\nAn NxM matrix loaded as a list of lists\n\n\n\ntranspose(mat)\nReturns the transpose of the matrix mat\n\n\n\nParameter\nType\nDescription\n\n\n\n\nmat\nlist[list]\nThe MxN matrix to be transposed\n\n\n\ninverse(mat)\nReturns the inverse of the matrix mat. Inverse is calculated using determinant & cofactor approach.\n\n\n\nParameter\nType\nDescription\n\n\n\n\nmat\nlist[list]\nThe MxN matrix to be inverted.\n\n\n\nfit(passes, split)\nThis function does the following.\n\nDivides the dataset into training set (A_train, Y_train) and testing set(A_test,Y_test) according to the value of split parameter\nCalculates the co-efficients(beta vector) using the equation described in The Algorithm section using the methods - dot(),transpose() and inverse() on the A_train matrix and Y_train vector\nUses the calculated beta values to predict labels of the test dataset, prints the predicted and actual labels calculates the accuracy of the model as a ratio of number of correctly predicted labels to number of records in the test data set\n\nThe above steps are repeated passes number of times by shuffling the data every-time to observe how the model learns\nwhen trained using different permutations of records in the dataset\n\n\n\nParameter\nType\nDescription\n\n\n\n\nsplit\nfloat[0-1]\nThe percentage of the dataset to be used for training. The remaining will be used as test set\n\n\npasses\nint\nThe number of times the model needs to be trained and evaluated\n\n\n\npredict(A_test, Y_test)\nUses the calculated beta vector to predict the label of a given flower.\n\n\n\nParameter\nType\nDescription\n\n\n\n\nA_test\nlist\nThe feature vector for which the label needs to be predicted. Eg. [6.8,3.2,5.9,2.3]\n\n\nY_test\nlist\nThe actual label of A_test from the dataset. This is used to validate whether the model predicted the label correctly\n\n\n\nHow to Run\n# Unzip the project archive\n# cd into project directory\n# Run the project using the command : python main.py [number of passes] [trian data split percentage]\n\n# Example\n$ unzip project1.zip\n$ cd project1\n$ python main.py 2 0.8\n\nNumber of passes and split percentage are optional. They are set to 2 passes and 72:28 split ratio by default\nafter experimenting with various split ratios several times\n\nCross-Validation\nIn order to verify the model, the dataset is divided into training set and test set in the ratio 72% and 28% by default.\nThis ratio can be changed at runtime by passing arguments for the main.py as shown below.\n$ python main.py 3 0.85\nEvaluation\nThe project was executed several times to observe how the model learns when trained on different permutations of records\nfrom the dataset and here's a summary of observations\n\n\n\nNumber of passes\nTrain data\nTest Data\nAvg. Accuracy\nRemarks\n\n\n\n\n3\n72%\n28%\n95.24%\nSometimes the model learns too well from the data\n\n\n2\n80%\n20%\n94.9%\n\n\n\n2\n85%\n15%\n95.652\nAccuracy fluctuated between 91% to 100%\n\n\n\nSample outputs\n3 passes with 72% of data for training and 28% for testing\n/home/harsha/anaconda3/envs/p365/bin/python /home/harsha/github/projects/python/cse6331/cse6331-p1/main.py\nPass#: 1 \t Train Records: 108 \t Test Records: 42\n\tPredicted: Iris-virginica(1.942), Actual: Iris-virginica(2), Error: -0.058\n\tPredicted: Iris-versicolor(0.857), Actual: Iris-versicolor(1), Error: -0.143\n        ...\n        ...\n        ...\n\tPredicted: Iris-versicolor(1.261), Actual: Iris-versicolor(1), Error: 0.261\n\tPredicted: Iris-setosa(-0.036), Actual: Iris-setosa(0), Error: -0.036\n\tPredicted: Iris-virginica(2.033), Actual: Iris-virginica(2), Error: 0.033\nAccuracy = 100.000 % \t Average Error: 0.068\n\n\nPass#: 2 \t Train Records: 108 \t Test Records: 42\n\tPredicted: Iris-setosa(-0.082), Actual: Iris-setosa(0), Error: -0.082\n\tPredicted: Iris-virginica(1.856), Actual: Iris-virginica(2), Error: -0.144\n\tPredicted: Iris-versicolor(1.353), Actual: Iris-versicolor(1), Error: 0.353\n        ...\n        ...\n        ...\n\tPredicted: Iris-virginica(2.135), Actual: Iris-virginica(2), Error: 0.135\n\tPredicted: Iris-virginica(1.784), Actual: Iris-virginica(2), Error: -0.216\nAccuracy = 97.619 % \t Average Error: 0.006\n\n\nPass#: 3 \t Train Records: 108 \t Test Records: 42\n\tPredicted: Iris-setosa(0.183), Actual: Iris-setosa(0), Error: 0.183\n\tPredicted: Iris-versicolor(1.090), Actual: Iris-versicolor(1), Error: 0.090\n        ...\n        ...\n        ...\n\tPredicted: Iris-versicolor(0.861), Actual: Iris-versicolor(1), Error: -0.139\n\tPredicted: Iris-setosa(-0.031), Actual: Iris-setosa(0), Error: -0.031\nAccuracy = 88.095 % \t Average Error: 0.027\n2 passes with 80% of data for training and 20% for testing\n/home/harsha/anaconda3/envs/p365/bin/python /home/harsha/github/projects/python/cse6331/cse6331-p1/main.py\nPass#: 1 \t Train Records: 120 \t Test Records: 30\n\tPredicted: Iris-virginica(2.015), Actual: Iris-virginica(2), Error: 0.015\n\tPredicted: Iris-versicolor(0.828), Actual: Iris-versicolor(1), Error: -0.172\n\tPredicted: Iris-virginica(1.603), Actual: Iris-versicolor(1), Error: 0.603\n        ...\n        ...\n        ...\t\n\tPredicted: Iris-setosa(-0.096), Actual: Iris-setosa(0), Error: -0.096\n\tPredicted: Iris-setosa(-0.210), Actual: Iris-setosa(0), Error: -0.210\nAccuracy = 96.667 % \t Average Error: 0.000\n\n\nPass#: 2 \t Train Records: 120 \t Test Records: 30\n\tPredicted: Iris-virginica(1.716), Actual: Iris-virginica(2), Error: -0.284\n\tPredicted: Iris-setosa(0.015), Actual: Iris-setosa(0), Error: 0.015\n\tPredicted: Iris-virginica(1.574), Actual: Iris-virginica(2), Error: -0.426\n\tPredicted: Iris-virginica(1.747), Actual: Iris-virginica(2), Error: -0.253\n        ...\n        ...\n        ...\t\n\tPredicted: Iris-versicolor(1.238), Actual: Iris-versicolor(1), Error: 0.238\n\tPredicted: Iris-virginica(1.935), Actual: Iris-virginica(2), Error: -0.065\nAccuracy = 93.333 % \t Average Error: 0.063\n2 passes with 85% data for training and 15% for testing\nPass#: 1 \t Train Records: 127 \t Test Records: 23\n\tPredicted: Iris-virginica(1.714), Actual: Iris-virginica(2), Error: -0.286\n\tPredicted: Iris-virginica(1.547), Actual: Iris-versicolor(1), Error: 0.547\n\tPredicted: Iris-setosa(-0.215), Actual: Iris-setosa(0), Error: -0.215\n        ...\n        ...\n        ...\t\t\n\tPredicted: Iris-virginica(1.839), Actual: Iris-virginica(2), Error: -0.161\n\tPredicted: Iris-virginica(1.830), Actual: Iris-virginica(2), Error: -0.170\nAccuracy = 91.304 % \t Average Error: -0.027\n\n\nPass#: 2 \t Train Records: 127 \t Test Records: 23\n\tPredicted: Iris-virginica(1.553), Actual: Iris-virginica(2), Error: -0.447\n\tPredicted: Iris-versicolor(1.377), Actual: Iris-versicolor(1), Error: 0.377\n\tPredicted: Iris-setosa(-0.084), Actual: Iris-setosa(0), Error: -0.084\n        ...\n        ...\n        ...\t\t\n\tPredicted: Iris-setosa(-0.143), Actual: Iris-setosa(0), Error: -0.143\n\tPredicted: Iris-virginica(1.567), Actual: Iris-virginica(2), Error: -0.433\nAccuracy = 100.000 % \t Average Error: -0.067\nReferences/Resources\n\nhttps://archive.ics.uci.edu/ml/datasets/Iris\nhttps://stackoverflow.com/questions/32114054/matrix-inversion-without-numpy\n\n""], 'url_profile': 'https://github.com/iharshadev', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '1,128 contributions\n        in the last year', 'description': ['HR Salary Justification\nTask:\nWe are working within the HR department in a company and are about to hire a new employee. We are about to negotiate the salary. The new hire has 20+ years of experience and says he/she used to earn a salary of £160,000, therefore being the desired minimum salary for their new role. The HR team has reached out to the new hire\'s previous employer to verify, but they only sent back a table of salaries that the company uses to band their employees. They also stated that he/she was a regional manager for two years and it takes 4 years to go from regional manager to partner. Lets predict if his/her claim is truth or bluff. Our task therefore being to predict the salary of someone who is at level 6.5 (mid way between regional manager and partner).\nWe will be completing this task using Polynomial, Support Vector Machine, Decision Tree and Random Forest Regression.\nPolynomial Linear Regression\nPolynomial regression is also referred to as Polynomial Linear Regression as ""Linear"" refers to the coefficients of the (X)^i terms. We do not require feature scaling as we are only adding polynomial terms to the multiple linear regression function, therefore using the linear regression library (sklearn.linear_model).\nPros of Polynomial Regression:\n\nWorks on any size of dataset\nWorks very well on non-linear problems.\n\nCons of Polynomial Regression:\n\nNeed to choose the right polynomial degree for a good bias/variance trade off.\n\nThe dataset received from the new hire\'s company has been plotted on the scatter graph below.\n\nThe diagram below shows the Polynomial Regressor being applied to the dataset, along with a higher resolution plot (step size 0.01) for a smoother curve.\n\nThe predicted salary for someone at level 6.5 has been predicted to be just under £159,000. Although this is just over £1,000 under the new hire\'s claimed previous salary, we accept the new hire\'s claim and state that the new hire was telling us the truth.\nSupport Vector Machine - SVR\nSupport vector machines (SVM) support linear and non-linear regression that we refer to as SVR. SVR performs linear regression in a higher dimensional space. The vector we get when we evaluate the test point for all points in the training set is the representation of the test point in the higher dimensional space. Once we have that vector we use it to perform a linear regression. The work of the SVM is to approximate the function we used to generate the training set F(X) = y\nThe vectors closest to the test points are referred to as the support vectors. We can evaluate our function anywhere so any vectors could be closest to our test evaluation location. Please see the diagram below for clarification.\n\nNote:\nSVR has a different regressional goal compared to linear regression. In linear regression, we are trying to minimise the error between the prediction and data. In SVR however, our goal is to make sure that errors do not exceed our threshold.\nPros of SVR:\n\nEasily adaptable\nWorks very well on non-linear problems\nNot biased by outliers.\n\nCons of SVR:\n\nCompulsory to apply feature scaling\nNot as well known\nCan be difficult to understand.\n\nIn this analysis, we are choosing the Gaussian RBF Kernel (Radial Basis Function) and regularisation as due to the training sets with noise, the reguliser will prevent wild fluctuations between data points by smoothing out the prior. Any points away from the SVR curve will be treated as an outlier (as illustrated by the salary of the CEO in our dataset).\nThe diagram below shows the SVR being applied to the dataset, along with a higher resolution plot (step size 0.01) for a smoother curve.\n\nThe predicted salary for someone at level 6.5 has been predicted to be just over £170,000, being in support of the new hire\'s claim. We therefore state that the new hire was telling us the truth.\nDecision Tree Regression\nCART: Classification and Regression Trees\nThe diagram below illustrates an example of how the Decision Tree Regression works. The small segments are called ""terminal leaves"".\n\nWe take the average of each terminal leaf and the value obtained will be the value predicted for any point that lands in that terminal leaf.\n\nThe tree diagram below shows how each new point will be assigned a new predicted value.\n\nPros of Decision Tree Regression:\n\nInterpretability\nFeature scaling not being required\nWorks on both linear/non-linear problems\n\nCons of Decision Tree Regression:\n\nPoor results on too small datasets\nOverfitting can easily occur\n\nThe diagram below shows the Decision Tree Regressor model being applied to the dataset.\n\nAs the Decision Tree Regressor is not a continuous regression model, we must visualise the model using a higher resolution. The diagram below shows the Decision Tree Regressor being applied to the dataset, along with a higher resolution plot (step size 0.01).\n\nThe expected salary for someone at level 6.5 has been predicted to be £150,000. This is £10,000 under the new hire\'s claimed previous salary. Therefore, if we base our decision solely on the Decision Tree Regression model, we do not accept the new hire\'s claim and state that the new hire was bluffing. However, as the dataset is small and Decision Tree Regression yield poor results on small datasets, the validity of the result/decision is questionable.\nRandom-Forest-Regression\nThe Random Forest model takes a team of Decision Trees, therefore, multiple trees coming together to make a forest. The ultimate prediction of the Random Forest itself is the average of the different predictions of all the different trees in the forest.\nEnsemble Learning: When you take the same algorithm multiple times and make it more powerful than the original.\n\nStep 1: Pick at random K data points from the training set\nStep 2: Build the Decision Tree associated to these K data points\nStep 3: Choose the number of N trees you want to build and repeat the above two steps\nStep 4: For a new data point, make each one of your N trees predict the value of y for the data point in question and assign the new data point the average across all the predicted y-values. It is common practice to select a large number of trees, for instance, N=300 (300 trees).\n\nAs we\'re getting a high number of predictions for the y-value and then taking the average (predicting based on a forest of trees and not one tree), this improves the accuracy of our predictions as we\'re taking the average of many predictions. If we just take one tree, it may just be an ""okay"" or ""not great"" tree but as we\'re taking the average, it\'s less likely to arrive at a bad prediction. Ensemble algorithms are more stable due to any changes in the data that may impact a tree will unlikely impact a forest of trees.\nPros of Random Forest Regression:\n\nPowerful and accurate\nGood performance on linear and non-linear problems\n\nCons of Random Forest Regression:\n\nNo interpretability\nOverfitting can easily occur\nNeed to choose the number of trees\n\nThe diagram below shows the Random Forest Regressor model being applied to the dataset.\n\nAs the Random Forest Regressor is not a continuous regression model, we must visualise the model using a higher resolution. The diagram below shows the Random Forest Regressor being applied to the dataset, along with a higher resolution plot (step size 0.01).\nNote:\nAs the number of trees N (n_estimators) increases, the number of steps do not increase as the averages converge to a similar number.\n\nThe expected salary for someone at level 6.5 has been predicted to be £160,333. This is in exact agreement with the new hire\'s claimed previous salary. We therefore state that the new hire was tell us the truth.\n'], 'url_profile': 'https://github.com/MohitGoel92', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '325 contributions\n        in the last year', 'description': [""Project_AirbnbRatingsPrediction\n\n\nPredict the ratings of Airbnb rooms with regression methods.\n(Final project for 'Introduction To Business Analytics', BIZ3197-01)\n\n\n*Data source: Airbnb datacenter\n\n1. Data Preprocessing\n\n\nDelete unnecessary variables\n\n\nHandling missing data\n\nDrop the columns with 25% missing values\nDrop the rows with no target variables\nFill up the missing values\n\nCategorical data: use mode\nNumerical data: use median\n\n\n\n\n\nHandling categorical data\n\nConvert data type\nChoose categories\nCreate dummy variables\n\n\n\n2. Model Preparing\n\n\nMulticollinearity check\n\n\nVIF check\n\n\n3. Model Training\n\n\nSimple Linear Regession\n\n\nDropping columns\n\nWith high P-values\n\n\n\n4. Assumption checking\n\n\nHeteroscedasticity\n\n\nNormality of errors\n\n\nLinearity\n\n\n5. Conclusion\n""], 'url_profile': 'https://github.com/sopogen', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mkorn1', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['SYS: Income-Inequality & Personality Facets related to Agression (2019)\nThis dataset used included 1029 adolescents from the Saguenay Youth Study (SYS) who inhabit the Saguenay Lac Saint-Jean region in Quebec, Canada (data is proprietary so it cannot be released here). Information consisted of:\n•\tCensus Tract Area (for the Gini Index of Income-Inequality)\n•\tHousehold Income/Income-to-needs ratio\n•\tPersonality (NEO PI-R)\nThe sample was filtered for participants who contained these features, which resulted in a total of n = 821 adolescents that were included in the analysis.\nCohort profile (SYS): https://www.ncbi.nlm.nih.gov/pubmed/27018016\nThis project examines differences in male-related characteristics among income and income-inequality groups, particularly in personality facets related to aggression.\nSelection for certain personality facets was based off the results of a meta-analysis from Vize et al. (2018).\nParticipants below the median of the income-to-needs were split into the low-income group and those at the median and above were placed in the high-income group. Each income group was split again by income-inequality: those below the median of the Gini Index were placed in the low income-inequality group, and participants at the median and above were placed in the high income-inequality group. These 4 income income-inequality groups were analyzed by each sex (total 8 groups; LI-HII, HI-HII, LI-LII, HI-LII).\nInitially, ANOVA was used to examine if there were any age differences within sexes by income income-inequality group.\nFemales showed no differences (F = 0.35, p = 0.79), however, males displayed age differences (F = 2.69, p = 0.046).\nTo account for the age difference among male groups, residual values were used in the linear mixed effects regression analysis for males.\nThe model examined the effects of age*group, and was adjusted for the randomized effect of Gini.CT to account for potential nesting of data within census tracts.\nTo adjust for multiple comparisons, false discovery rate (FDR) correction was evaluated using the ‘stats’ library in R.\nCurrently searching for the most suitable method of post hoc comparison to examine group differences among personality facets that survived FDR correction (for this particular model).\n'], 'url_profile': 'https://github.com/xmootoo', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Predicting-Heart-Disease\nBuilding machine learning models such as Keras, Logistic Regression, Random Forest and XGBoost to predict heart disease amongst patients.\n'], 'url_profile': 'https://github.com/roshanwahane', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnujKaliraman', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MichielDeRuiter', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'MIT license', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'Jupyter Notebook', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020']}"
"{'location': 'New Jersey', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Spam-filter-using-NLTK\nText classification obtaining:\nK Nearest Neighbors: Accuracy: 93.25197415649676\nDecision Tree: Accuracy: 97.4156496769562\nRandom Forest: Accuracy: 98.63603732950466\nLogistic Regression: Accuracy: 98.63603732950466\nSGD Classifier: Accuracy: 98.27709978463747\nNaive Bayes: Accuracy: 98.42067480258436\nSVN Linear: Accuracy: 98.56424982053123\n'], 'url_profile': 'https://github.com/dyutidave', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Machine-Learning\nImplementation of Logistic Regression, Richardson Algorithm, Support Vector Machines and Neural Networks (using Pytorch)\nHW1 : Richardson ALgorithm, Logistic Regression\nHW2 : Softmax regression, linear SVM dual form, kernel SVM and multi-class SVM\nHW3 : Fully connected neural network and cnn using Pytorch\nHW4 : Blind boosting, Adaboost and Random forest\nHW5 : GAN, VAE, DAE and adversarial examples\n'], 'url_profile': 'https://github.com/ranjabatisen', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kushal1975', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'Taipei, Taiwan', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['COVID19-Epidemic-Curve-Model\nUses LSM regression to model the epidemic curve of COVID-19. Aims to predict curve progression in the near future.\n'], 'url_profile': 'https://github.com/ethanchan2416', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['AIRBNB-Hosts-Need-to-knows\nTable of Contents\n\nLibraries\nProject Motivation\nFile Descriptions\nResults\nLicensing, Authors, and Acknowledgements\n\nLibraries\nThe libraries used in this repository are:\n\npandas\nnumpy\nmatplotlib\ndatetime\nseaborn\nsklearn\n\nProject Motivation\nAribnb is a world wide\nThis project aims to analyze airbnb data in Seattle to provide airbnb hosts a general picture about what airbnb data says. There are three questions I am trying to explore within this project:\n\nHow prices changes according to time and neighborhood?\nWhich factors helps us to predict the price?\nWhich factors helps us to predict the customer satisfaction?\n\nIn this project I hope to help airbnb hosts who are trying to decide prices and improve their guests\' satisfaction.\nFile Descriptions\nThere is one notebook (named ""UdacityP1"") available here to showcase work related to the above questions. This notebook consists of three parts adressing each research question. The detailed descriptions are available within the notebook.\nResults\nThe main findings of the code can be found at the post available here: https://medium.com/@nkibrislioglu/need-to-know-for-airbnb-hosts-5165320d1447?sk=2f94c208186d485e5d624eca3c6675f5\nLicensing, Authors, and Acknowledgements\nAirbnb data set is used in this project. You can find additional information about the data here: https://www.kaggle.com/airbnb/seattle/data\nNote: This repository is part of Udacity Data Science Nano degree program projects\n'], 'url_profile': 'https://github.com/nkibrislioglu', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'Montréal, QC', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': [""Kaggle-Rossman\nTabular data is the bread and butter of data science projects. This repo explores XGBoost for regression which is an algorithm that is well-known by the community and is known to perform really well on tabular data. I also explored further by transforming the categorical variables using the method of Entity Embeddings which was published in a paper in 2016. Check it out on arXiv.\nSummary Results\nPrivate score and puplic score were retreived after submitting the predictions to Kaggle.\n\n\n\nExperiment ID\nCategorical Variables\nNaN-cats\nNaN-cont\nTarget Transformation\nHyperparameter Search\nBacktesting\nPrivate Score\nPublic Score\n\n\n\n\n001\nTarget encoder\nXGBoost\nXGBoost\nLog transform\nDefault\nNo\n0.16925\n0.17975\n\n\n002\nTarget encoder\nXGBoost\nXGBoost\nLog transform\nHyperOpt (100)\nTimeSeriesSplit k = 3\n0.13975\n0.12481\n\n\n003\nEntity Embeddings\n#NAN#\nFastAI\nLog transform\nDefault\nNo\n0.15251\n0.14079\n\n\n004\nEntity Embeddings\n#NAN#\nFastAI\nLog transform\nHyperOpt (100)\nTimeSeriesSplit k = 3\n0.13081\n0.11572\n\n\n\nProject Organization\n├── README.md          <- The top-level README for developers using this project.\n├── data\n│\xa0\xa0 ├── interim        <- Intermediate data that has been transformed.\n│\xa0\xa0 ├── processed      <- The final, canonical data sets for modeling.\n│\xa0\xa0 └── raw            <- The original, immutable data dump.\n│\n├── docs               <- A default Sphinx project; see sphinx-doc.org for details\n│\n├── models             <- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n│\n├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n├── src                <- Source code for use in this project.\n│\xa0\xa0 ├── __init__.py    <- Makes src a Python module\n│   │\n│\xa0\xa0 ├── data           <- Scripts to download or generate data\n│\xa0\xa0 │\xa0\xa0 └── make_dataset.py\n│   │\n│\xa0\xa0 ├── features       <- Scripts to turn raw data into features for modeling\n│\xa0\xa0 │\xa0\xa0 └── build_features.py\n│   │\n│\xa0\xa0 ├── models         <- Scripts to train models and then use trained models to make\n│   │   │                 predictions\n│\xa0\xa0 │\xa0\xa0 ├── predict_model.py\n│\xa0\xa0 │\xa0\xa0 └── train_model.py\n│   │\n│\xa0\xa0 └── visualization  <- Scripts to create exploratory and results oriented visualizations\n│\xa0\xa0     └── visualize.py\n\n\nProject based on the cookiecutter data science project template. #cookiecutterdatascience\n""], 'url_profile': 'https://github.com/JustinCharbonneau', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': [""Perceptron\n 1. Introduzione \n\nIl percettrone venne proposto da Frank Rosenblatt nel 1958. La struttura è la seguente:\n\n\n\nConsiderando un numero p di predittori, il vettore  X = [x1, x2, ... , xp]  rappresenta l'input del percettrone. Quest'ultimo viene poi moltiplicato per il vettore dei pesi  W = [w1, w2, ... , wp] . Detto ciò, l'output può essere espresso matematicamente dalla formula:\n\n\n\n\nin cui la funzione g(), detta  funzione di attivazione , è spesso rappresentata dalla funzione sigmoidea matematicamente esprimibile in questo modo:\n\n\n\n\nQuest'ultima ha un dominio di valori nel range [0,1] rendendo così il percettrone un classificatore binario. In particolare, supponendo di avere due classi A e B, il valore ritornato dalla funzione sigmoidea esprime la probabilità di appartenenza dell'esempio X alla classe A o B.\n\nDi estrema importanza è il vettore dei pesi W. Il loro valore viene determinato durante la fase di apprendimento  (training) , in maniera tale che la differenza (target - output) sia la più piccola possibile. Ma quanto piccola? Considerando (target - output) come la funzione di loss, l'obiettivo è quello di raggiungere il  minimo globale . In particolare con il termine target ci riferiamo al valore con cui labelliamo l'osservazione X  (valore atteso) , al contrario, con il termine output ci riferiamo al valore restituito dal percettrone (predizione) .\n\nI pesi vengono determinati attraverso la regola di aggiornamento  delta rule  (discesa del gradiente). Considerando un solo neurone (appunto il percettrone) possiamo esprimere la delta rule in questo modo:\n\n\n\n\nel i conseguente aggiornamento del vettore peso:\n\n\n\n\n 2. Regressione \n\nModificando la funzione di attivazione possiamo ottenere un modello regressivo. In particolare, sostituiamo la funzione sigmoidea con la funzione ReLU. Quest'ultima viene ad esprimersi matematicamente:\n<\n\n\n\nDetto ciò, il nostro percettrone costituirà un modello definito dal seguente hyperpiano:\n\n\n\n\nin cui  w0  rappresenta l'intercetta mentre w1, ... , wp le relative pendenze.\n\n 3. Plot residui \n\nDi seguito posto il plot dei residui calcolato sul training set sia del percettrone sia eseguendo una LinearRegression. Il dataset utilizzato è Boston house-prices\n\nPerceptronLinearRegression\n\n\n""], 'url_profile': 'https://github.com/GiuseppeCannata', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'Italy', 'stats_list': [], 'contributions': '365 contributions\n        in the last year', 'description': ['fit-COVID19\nEasy model to fit logistic curve to COVID19 data from Italy.\nData is taken from this official repo\nLive demo: https://fit-covid19.herokuapp.com\n(It could be slow because it is a free heroku app)\nFor single regions:\nhttps://fit-covid19.herokuapp.com/regione/nome\n(Ex. Toscana: https://fit-covid19.herokuapp.com/regione/Toscana)\nPredictions for Italy:\nusage: fit.py [-h] [--img IMG] [--avg AVG] [--style STYLE]\n\nModello COVID-19 in Italia.\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --img IMG   y, save imgs - n do not save imgs\n  --avg AVG      if > 1 draw plot of avg last --avg days.\n  --style STYLE  [normal, cyberpunk] : normal, standard mpl - cyberpunk,\n                 cyberpunk style\n  --old_pred OLD_PRED  if True plot also the prediction curve from a week ago.\nPredictions for a region:\nusage: regione_fit.py [-h] --regione REGIONE [--img IMG] [--avg AVG] [--style STYLE]\n\nModello COVID-19 per regione.\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --regione REGIONE  Nome regione su cui effettuare le predizioni.\n  --img IMG          y, save imgs - n do not save imgs\n  --avg AVG      if > 1 draw plot of avg last --avg days.\n  --style STYLE  [normal, cyberpunk] : normal, standard mpl - cyberpunk,\n                 cyberpunk style\n  --old_pred OLD_PRED  if True plot also the prediction curve from a week ago.\nExamples\n\n\nIf you know this stuff and you think you can contribute please just let me know: fork this repo, pull request, star this repo, send me an email.\nRequirements\n\nPython >=3\nPandas\nNumpy\nScyPy\n\n'], 'url_profile': 'https://github.com/LucaAngioloni', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'Kolkata,India', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Board-Game-Review-Prediction\nPredicting board game reviews with linear and random forest regressors\n'], 'url_profile': 'https://github.com/souvikbasak97', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/XuYongi', 'info_list': ['Jupyter Notebook', 'Updated Mar 10, 2020', 'Python', 'Updated Jun 23, 2020', 'Updated Mar 15, 2020', 'MATLAB', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Python', 'Updated Mar 16, 2020', '15', 'Python', 'MIT license', 'Updated Oct 21, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Karachi', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['Black-Friday-Analysis-RandomForestRegression-HyperparameterTuning\nDataset easily available in Kaggle website.\n'], 'url_profile': 'https://github.com/syedkashifaliquadri', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['Binary-Classification-with-Logistic-Regression-Predict-Titanic-Survived\n'], 'url_profile': 'https://github.com/shafayet98', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Linear-Regression-Predicting-Average-GPU-Running-Time\nAs a part of this project, I have worked on\n• The implementation of the gradient descent algorithm to find out the optimal values for the\ncoefficients of the explanatory variables in the dataset.\n• Applied Linear Regression to find out the average computation time. Further, converted the problem statement as a classification problem to find out whether the average GPU computation time is low or high and applied Support Vector Machine (SVM), Decision Trees and XGBoost models to classify the average running time.\n• Plotting learning curves (error metrics with respect to iterations, learning rate, and convergence threshold values).\n• Interpretation of the plots and results.\n'], 'url_profile': 'https://github.com/meghashyamnaidub', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}","{'location': 'Cambridge, UK', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['NLP_NaiveBayes_LogisticRegression_kNearestNeighbors\nNatural Language Processing (NLP) spam/ham email classification via full custom Beta-Binomial Naive Bayes, Gaussian Naive Bayes, Logistic Regression & k-Nearest Neighbors implementation.\nFor usage instructions please check Usage README\nFor full documentation - system design, experiments & findings please read NaiveBayes_LogisticRegression_kNearestNeighborsDoc Report\nGutHub Pages Format\nIntroduction\nIn this report I present my implementations of four pattern recognition methods: Beta-Binomial Naive Bayes, Gaussian Naive Bayes, Logistic Regression with Newton’s Method and L2 regularization learning, K-Nearest Neighbors with cross-validation, as well as, the results and analysis of the SPAM E-mail Dataset, classified with each of the above-mentioned algorithms.\nThe dataset is comprised of 4601 samples, each with 57 features. Out of them 1813(39.4%) are labeled as spam, leaving 2248(60.6%) as non-spam. The full description of the SPAM E-mail dataset and clarification on what is assumed as spam can be seen in [1].\nThe dataset is randomized and divided into two sub-sets – training with 3065 samples and a test (sometimes also used as validation) with 1536 samples. Two data-transformation techniques are applied to the training and test sets. A binarization is performed in order to prepare the dataset for the Beta-Binomial Naive Bayes fit. A log-transform is performed in order to prepare the dataset for the rest of the fits.\nFor classification analysis of the model fit, I observe a few different metrics: accuracy, overall error rate(1-accuracy), false positives error rate and false negatives error rate.\nWhile the main metrics for evaluating the performance of the methods is the resultant accuracy/overall error rate, for the specific case of the SPAM E-mail dataset – a deeper insight can be drawn from the false positives/negatives error rate. As mentioned by [1], the false positives (classifying a non-spam email as spam) is very undesirable as this can lead to the loss of important correspondence. Hence, when I discuss the performance of the model fits, a special attention is given to the false positives rate with the aim of minimizing it.\nAdditionally, the effect that some hyperparameters have on the model fit are studied: for the Beta(a, a)-Binomial Naive Bayes – the effect of the value of the “a” hyperparameter (hence of the prior); for the Logistic Regression method – the amplitude of the weight decay; for K-Nearest Neighbor – the value of “k”.\nThe four methods are implemented in Python from scratch, using NumPy for matrices manipulation and calculations. Four shared helper functions are implemented for extracting the evaluation metrics and plotting a confusion table. Further method-specific helper functions are implemented for wrapped training, testing, displaying results and complete fitting using matplotlib for plotting graphs and some other basic helper packages.\nFor each method graphs of {train/test} {error rate, accuracy, false negatives error rate, false positives error rate} over {(hyperparameter) fit / training(for Logistic Regression)} are plotted. Based on the train/test predictions vs targets, confusion tables are displayed. Last but not least, the optimal value for the hyperparameter at question is chosen and the corresponding best model-fit performance is displayed.\nBeta-Binomial Naive Bayes Classifier\nDesign\nThe classifier is designed to be configurable to either use the Maximum Likelihood (ML) Estimation or to assume a prior Beta(a, a) (Posterior Predictive (PP)) for each – the class priors probability and the class-conditional probabilities. Depending on its configuration, the classifier can use a pure ML, a Bayesian PP or a mixture for training and prediction.\n\n\n\nGaussian Naive Bayes Classifier\nDesign\nFor training the classifier uses the Maximum Likelihood (ML) Estimation for the class-priors and the class-conditional mean and variance for each feature. The prediction is made by utilizing MLE as a plug-in estimator.\n\nLogistic Regression\nDesign\nThe Logistic Regression classifier is built as an artificial neuron from three layers: linear – containing the model parameters, which after a forward step yield the log-odds prediction; logistic – the sigmoid function which squashes the log-odds into [0:1] interval and yields the posterior probabilities; step – the step function with a threshold of 0.5, which translates the logistic decision boundary at 0.5 to binary predictions. The neuron learns though the Newton’s Method by doing a second order derivative cost estimation using both – the Hessian and the Gradient. The learning also utilizes a L2 regularization.\n\n\n\n\nK-Nearest Neighbors\nDesign\nThe classifier measures the “nearness” between train and test samples via the Euclidean distance. The optimal K is chosen via a five-fold cross-validation.\n\n\n\n'], 'url_profile': 'https://github.com/SamyuelDanyo', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sheldon-pixel', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Linear_Regression-from-scratch-to-predict-apparent-temperature\nGiven a dataset containing historical weather information of certain area, implement a linear regression model from scratch using gradient descent to predict the apparent temperature.\n'], 'url_profile': 'https://github.com/SmritiAgrawal04', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '595 contributions\n        in the last year', 'description': ['Jumia-kilimall\nThis is a web application that implements a web scrapper to get data for products from jumia and killimall, and use linear regression to recomment the best products to clients\nTechnologies used\n\nDjango\nBeautiful soup 4\nSelenium\nCelery\nHtml/css\n\nHow To Set Up Locally\nInstall pipenv\npip install virtualenv\n\nCreate virtual environment\nvirtualenv -p python3 env  && source env/bin/activate\n\nInstall dependencies\npip install -r requirements.txt\n\nMigrate to DB\npython manage.py migrate\n\nLoad static data\npython manage.py loaddata sites\n\nRun the server\npython manage.py runserver\n\nRun redis server\nredis-server\n\nRun celery\ncelery -A jkm worker --loglevel=info --concurrency=1 --beat\n\nEnv Variables\nexport CELERY_TIME_J=*/60\nexport CELERY_TIME_K=*/1200\nexport DB_NAME=name\nexport DB_USER=user\nexport REDIS_URL=redis://127.0.0.1:6379\nexport CURRENT_ENV=development\n\n'], 'url_profile': 'https://github.com/dev-jey', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Massy, France', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ZoeYou', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Isfahan, Iran', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Titanic-prediction-in-python\nIn this project I am going to use some classification such as logistic regression, KNN and Discriminant analysis to predict it.\nThe train.csv and python file are added. please open it in spyder or jupyter notebook or other editors.\nsome classification such as Logistic regression, Knn, discriminant analysis are applied and there are some visualizationof features.\nIt is not complete and there are some other classification which should be applied to find the best classification and have accurate prediction.\n'], 'url_profile': 'https://github.com/MahsaMozafariNia', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JudeWells', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['flood-risk-and-real-estate-prices\nData processing and regression for an academic research study on whether the rise in coastal flood risk is capitalized in real estate prices.\n'], 'url_profile': 'https://github.com/BairuChen2019', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Performing Classification Tasks with Python\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['Machine-Learning-in-Python\nLogistic regression, binary classifiers, K-means clustering in Python etc are used in this project dealt with data containing index prices for predicting the stock market.\n'], 'url_profile': 'https://github.com/Shibasrit', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '145 contributions\n        in the last year', 'description': ['House_Price_Prediction_Model\nBuilt a linear regression model to predict the price of house using two features- Size of House and total number of rooms.\nThe file predict_price.py contains the model for 2 features and ex1data2 is the datset file.\nI used both gradient Descent method and Normal equation to find the appropriate hypothesis.\n'], 'url_profile': 'https://github.com/gauravmadan583', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""Case-Study-On-NYC-Taxi-Fare-Prediction\nGiven a ride's pickup time, pickup and dropoff coordinates along with the total passengers riding, build a model to predict the fare for a NYC taxi (regression)\n""], 'url_profile': 'https://github.com/Kirankendre', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '2', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 12, 2020', 'Python', 'Updated Mar 9, 2020', 'R', 'Updated Apr 2, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Mar 17, 2020', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Customer-Analytics\nExecutive summary\nKyngaCell’s latest game, Nicht-Soporific, contains a new online community feature that is assumed to have increased user revenue, retention, and customer lifetime value within the game. However, instead of blindly believing this claim, the firm would like to conduct additional  analysis that can provide a more definitive answer about the effectiveness of this new feature.\nIn order to test the effect on user revenue, we created a differences-in-differences model that compares the revenue of those who joined the community and those who did not at one month before and after the launch of the new feature. We found a significant increase in user revenue of 29 dollars accredited to the new online community feature; however, the sample size of 199 presents a potential limitation because it is not large enough for a definitive conclusion.\nFor testing the effect on retention, we created a logistic regression model that predicts the probability of a customer churning based on spending, churn status, and monthly age. We found that if a user joined the community, they had a 0.21 higher probability of churning (i.e. the online community had a negative effect on retention). The data provided could present a limitation on this conclusion because three months could potentially be too short to see the return.\nLooking at customer lifetime value (CLV), we ran a regression based on various customer attributes and found that the new community feature decreases CLV by $26. This conclusion coincides with the conclusion of joining the community decreasing retention and therefore could indicate that there is a need for additional data for more accurate interpretations.\nWith the new “Campaign/Organic” variable, we could introduce analysis that indicates the value of marketing campaigns. This insight could potentially help the CFO in determining the allocation of funds and whether or not there is value in additional marketing expenditures.\nIntroduction\nWhen implementing new features within an application, it is extremely important to analyze the effects of this feature in order to definitively know whether or not the implementation impacted user behavior. This analysis can help companies evaluate these features and properly reallocate resources if the feature is not effective. KyngaCell, a leader in the mobile gaming industry, is interested in assessing the effectiveness of their new online community feature in regard to increased revenue, retention, and customer lifetime value.\nData Description\nData was obtained on various behaviors of 199 users before and after the introduction of the online community. These behaviors include whether the user joined the community (41% joined), the spend of users one month before (avg = $78) and one month after (avg = $121) the introduction of the community, and the number of months a user had been playing the game before the community was introduced (avg = 4). Data was also included on whether each user churned within the first 90 days following the introduction of the online community (59% churned) and how much each user spent in their last 90 days using the app (avg = $80).\nEvaluating Revenue: Model and Results\nThe data provided consist of users’ spending one month before and one month after the launch of the online community feature. Additionally, there is information about whether or not that customer joined the online community. With this data, we can build a differences-in-differences (diff-in-diff) model where we assess the spending differences of two groups (those who joined the online community and those who did not) and compare those spending differences at two different points in time (one month before and one month after the feature launch). Comparing the group differences at these two points in time will allow us to assess the exact effect of joining the online community on user spending.\nBased on the diff-in-diff model created, there is strong evidence that joining the online community feature can increase user revenue. Comparing the group spending differences before and after the launch of the online community, we found out that there is a significant increase of 29 dollars in revenue due to the new feature. Hence we can draw the conclusion that the launch of the online community helped increase sales by a factor of $29; however it is important to note that the small sample size poses a limitation to this conclusion, simply because there is not enough data to reach a definitive conclusion.\nEvaluating Retention: Model and Results\nThe data provided contain customers’ spending and churn status in the three months following the online community launch, as well as the monthly age of the customer at the time of the online community launch. With this information we can create a logistic regression model that predicts the probability of a customer churning or not based on their spending, whether or not they joined the online community, and their monthly age. We chose a logistic regression because it provides a quantified value for the strength of the association while adjusting for other variables (removes confounding effects). The exponential of coefficients correspond to the odds ratios for the given factor, which enables us to achieve a result with strong interpretation power.\nBased on our logistic regression model, the online community will not raise the user retention rate. We found that whether the user joined the online community or not is the only factor that has a statistically significant impact on the probability of churn. The coefficient estimate of this variable “join” being 0.21, means that if a customer joined the online community, their probability of churning increased by 0.21. To assess the model accuracy, we looked at the classification error, which is the proportion of observations that have been misclassified. From this error, we found the accuracy of the model is 0.6281; we can then prove that the prediction is quite accurate and validate our initial conclusion.  Intuitively, joining the online community should decrease the churn rate, but our analysis says otherwise. Three months, however, could be too short to see the return on retention, so the data may not be valid for the assumption. Also, it is possible that other unobserved factors may also be interfering with the result.\nEvaluating Customer Lifetime Value: Model and Results\nWhen determining whether joining the online community had any affect on Customer Lifetime Value (CLV), we used predicted churn rates and calculated profit margin from average spending of users in their last 90 days using the app. The regression showed that joining the online community had an effect on CLV, however it was not positive. The results showed that joining the online community was correlated with a $26 decrease in CLV. This finding goes hand-in-hand with the finding that joining the community was related to decreased retention. Predicting CLV is incredibly difficult, and relies on an accurate prediction of churn along with other factors. While the direction of the relationship between joining the community and CLV may be accurate (negative), more user data would make the analysis of the size of the effect more accurate. There also may be other factors affecting CLV for which this model does not account for. Additional data on whether the user joined the company organically or via a marketing campaign was included in the dataset, so further analysis on whether this impacts CLV is included in Appendix A.\nEvaluating Marketing Campaigns: Potential Metrics\nAs mentioned above, the addition of whether a user joined the app via a marketing campaign or organically into analysis of income, retention, and CLV can provide insight into the value of the marketing campaigns. Ideally, users who download the app from a marketing campaign will spend more than organic users and have a higher CLV, as they had a higher cost to attract. It could also be interesting to determine if how a user downloaded the app had an effect on whether or not the user joined the online community. If there are key differences in behaviors based on this factor, the team may need to target these groups of users differently. If there are not any differences, the CFO may wish to reevaluate the current marketing strategy.\nConclusion\nAdding the online community for app users had mixed results on customer behaviors. Joining the community led to a $29 increase in spending by users which would not have happened had the community not been introduced. However, those who joined the community were more likely to churn within a 90-day period and overall had a lower Customer Lifetime Value. The company must now decide whether short-term income gains or long-term increases in retention and CLV are of more importance. This also shows that it is important to always back up beliefs with data, otherwise important insights may be missed.\n'], 'url_profile': 'https://github.com/yiying-wang', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mwhittle968', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '223 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harmanBaincha', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Financial Distress & Factors analysis\nHere, I have built Ridge and Lasso Regression in R Studio to predict if a company is in financial distress by analyzing given financial & non-financial parameters to predict the financial distress estimator.\n'], 'url_profile': 'https://github.com/adityadiwane', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['bartBMA\nThe goal of bartBMA is to provide an implementation of Bayesian Additive Regression Trees Using Bayesian Model Averaging (BART-BMA) (Hernandez et al. 2018)\nHernández, B., Raftery, A. E., Pennington, S. R., & Parnell, A. C. (2018). Bayesian additive regression trees using Bayesian model averaging. Statistics and computing, 28(4), 869-890.\nInstallation\nlibrary(devtools)\ninstall_github(""bartBMA"")\nExample\nlibrary(bartBMA)\n## basic example code\n\nN <- 100\np<- 100\nset.seed(100)\n\nepsilon <- rnorm(N)\nxcov <- matrix(runif(N*p), nrow=N)\ny <- sin(pi*xcov[,1]*xcov[,2]) + 20*(xcov[,3]-0.5)^2+10*xcov[,4]+5*xcov[,5]+epsilon\n\nepsilontest <- rnorm(N)\nxcovtest <- matrix(runif(N*p), nrow=N)\nytest <- sin(pi*xcovtest[,1]*xcovtest[,2]) + 20*(xcovtest[,3]-0.5)^2+10*xcovtest[,4]+5*xcovtest[,5]+epsilontest\n\n\nbart_bma_example <- bartBMA(x.train = xcov,y.train=y,x.test=xcovtest, \n                    zero_split = 1, only_max_num_trees = 1,split_rule_node = 0)\n\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'Belgium', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Master\'s thesis\nContains the code, the text and the presentation of my master\'s thesis ""Hyperparameter Adjustment in Regression-Based Neural Networks for Predicting Support Case Durations""\nHow to run the code yourself:\n\ndownload Master_Thesis_Code.ipynb\ndownload and unzip incident_event_log.rar\nchange path df = pd.read_csv(\'/your/local/drive/incident_event_log.csv\' ...\nrun notebook.\n\n'], 'url_profile': 'https://github.com/hristochr', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['RoboticsMatchPredictor\nUses TensorFlow library to create a regression based on past match data from teams, to predict outcome of a new match\n'], 'url_profile': 'https://github.com/greamy', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Mar 12, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'R', 'Updated Jan 9, 2021', 'C++', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Python', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['Univariate-Linear-Regression-model-for-salary-prediction-over-the-year-of-experience\n'], 'url_profile': 'https://github.com/shafayet98', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'Medan, Indonesia', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Prediction-Correlation-of-pisa-score-to-OECD-using-Pearson-and-Linear-Regression\nThis study case based on Data Science - ITFEST 2020\nImplementation Linear Regression Algorithm to solve problem.\n'], 'url_profile': 'https://github.com/Kristovwaruwu', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '713 contributions\n        in the last year', 'description': [""Optimization and Learning\n\n The first four problems\n The fifth problem\n The final problem\n\n\n\nProblem 1 demonstrates the use of second order methods to calculate an optimal learning rate for gradient descent. We see how convergence is affected with change in the learning rate.\n\n\nIn Problems 2 and 3, various kinds of algorithms for optimizing functions have been tried out. A contrast has been shown among normal gradient descent, gradient descent with Polyak's learning rate, Nesterov's accelerated gradient descent, and the Adam optimizer. All of these have been implemented from scratch and applied for regression on two bivariate functions with MSE loss.\n\n\nProblem 4 shows how data normalization can lead to faster training. A further analysis on the structure of datasets and 'good' learning rates has been provided.\n\n\nProblem 5 explores the gradient ascent technique to calculate the local maxima of functions.\n\n\nProblem 6 shows the use of Rprop and Quickprop for a regression task. We compare a one hidden-layer neural network architecture for the task, with different number of hidden neurons, different activation functions, with normal batch backpropagation, Rprop and Quickprop. The dataset used for this problem is the Concrete Compressive Strength dataset which can be found here.\n\n\n\nThe notebooks contain required equations and explanations for all the problems.\n\n""], 'url_profile': 'https://github.com/sayarghoshroy', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'MATLAB', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated May 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '346 contributions\n        in the last year', 'description': ['datascience\nMy data science and machine learning projects are stored here\n'], 'url_profile': 'https://github.com/vladutmd', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '160 contributions\n        in the last year', 'description': [""Analysis of Major League Baseball (MLB) Statistics with Supervised Learning Models\nWork by Song Ying and Antoine Ghilissen\nFiles\n\nindex: Jupyter notebook detailing the analysis.\nsource/Dataset: Original dataset in a csv format.\nsource/data_cleaning: Jupyter notebook detailing the data cleaning process.\ndf.csv: Cleaned dataset used for the analysis.\npresentation: Presentation slide deck.\n\nExecutive Summary\nAnalysing Major League Baseball (MLB) World Series statistics, ranging from 1946 to 2016, and formulate coaching strategies to maximise chances of winning matches.\nThis analysis encompasses the following:\n\nData Exploration\nData Cleaning & Feature Engineering\nLogistic Regression ROC-AUC score: 61.5 %\nDecision Tree ROC-AUC score: 72.2 %\nRandom Forest ROC-AUC score: 74.2 %\nXGBoost ROC-AUC score: 66.69 %\nVariable & Model Selection\nThreshold Selection\nFinal Evaluation\nActionable Insights\n\nActionable Insights\n\nInvest in top pitchers for defensive plays to reduce opponent’s RBI.\nEmploy the most consistent batter to improve RBI.\nDo not focus on hitting ambitious strikes (doubles, triples, home runs), consistency is preferred and getting the bat on the ball as frequently as possible is more efficient.\nApproaching the end of the innings, batters should focus on taking risk, to reduce number of players left on base so they could complete a run.\n\nHigh Level Overview\nThe project started by exploring the data and transforming the dataset into a desirable format. As the original dataset was given in match-by-match basis, we divide each match (row) into 2, namely the winning team and losing team with their respective statistics.\nAs a result, we doubled our number of rows but halved the number of columns, additionally we defined our target variable, the match outcome in a boolean format.\nAs the original dataset was given match-by-match, we had no class imbalance issue as we had a perfect 50:50 split of winners and losers.\nAfter the data cleaning process, we did a preliminary round of feature selection based on correlation matrix to remove trivial variables and also removed several other variables using VIF (variance inflation factor) to ensure there would low multicollinearity between the multiple variables.\nNext, we trained a logistic regression model as our baseline model. Followed by a few additional models, their ROC_AUC scores are summarized in the following table:\n\n\n\nType\nROC_AUC\n\n\n\n\nLogistic Regression\n61.5%\n\n\nDecision Trees\n72.2%\n\n\nRandom Forest\n74.2%\n\n\nXGBoost\n66.69%\n\n\n\n_All these models have gone through hyperparameter tuning, using scikit-learn's GridSearchCV() optimisation method.\nThe Random Forest was chosen as our final model.\nThe final technical step in our process was to select a threshold to optimize for both Type 1 and Type 2 errors.\nFinally, we evaluated the performance of the model and derived actionable insights for our stakeholders.\nMethodology\nThis project uses Python3 and is documented with Jupyter Notebook.\nWe have used a combination of numpy and pandas for data cleaning, filtering and feature engineering and seaborn was used for data visualisation.\nThe initial stage of feature selection included a multicollinearity check on the cleaned data. This was performed using a correlation matrix, variance inflation factor (VIF) and the decisions were made based on our business expertise.\nA nested 5-fold cross validation within a train-test split with a ratio of 70:30 was employed for all our models.\nA logistic regression was used as our baseline model and various other models were trained, such as a Decision Tree, a Random Forest and an XGBoost.\nThese models were used to adjust the feature selection as well as to improve performance.\nscikit-learn's GridSearchCV() was used for the hyperparameter tuning of the relevant models: the Decision Trees, the Random Forest and the XGBoost model.\nModels were compared using the ROC-AUC score.\nData Source\nThe data is the game log of MLB matches performed between 1871 and 2016. It was compiled by Retrosheet. The original dataset can be found on Dataquest.\nLimitations\n\nData samples from 1970-1979 and 1990-1999 were missing and therefore not included in our analysis.\nMissing players individual statistics were not provided so we couldn't evaluate the influence of individual factors on match outcome.\nDue to our business case, we removed a few games from the original database: multiheaded games, games that ended in a draw, protested and interrupted games.\n\nFuture Work\n\nIn-depth analysis of linescore paired with match statistics per innings of the game from other sources to investigate probability of winning the match as the match progresses.\nEvaluation of model on other baseball leagues to ensure consistency and scalability of our model\nApplication of dimensionality reduction techniques like Support Vector Machines to the model\nApplication of unsupervised learning model using Principle Component Analysis & Clustering\nApplying our method to winning statistics and also losing statistics in order to compare and emphasise which feature really impacts the game outcome.\n\n""], 'url_profile': 'https://github.com/aghilissen', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'chennai / jaipur', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['""# Investigating-IMDB-dataset-using-Sentiment-Analysis""\n'], 'url_profile': 'https://github.com/anirudh786', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['Independent_Research_Long_Term_Care\nThis project consists of econometric modeling to distill the impact of Electronic Health Record in Healthcare Facilities on health outcomes of the patients. Built Difference in Differences regression models on various clinical performance parameters to analyze and infer causal effects.\n'], 'url_profile': 'https://github.com/AnkitRajSri', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': [""simplelinearregression\nBuilt a simple linear regression model which predicts the amount of CO2 emitted taking one attribute from different set of attributes. In this project I have predicted CO2 emissions based on engine size of a vehicle.\nSteps to run this file:\nOpen the Simple_Linear_Regression.ipynb file in Jupyter notebook.\nUpload 'Fuel_Consumption.csv' data file in same directory in which .ipynb is uploaded and run the file.\n""], 'url_profile': 'https://github.com/santoshsahini19', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'Changchun', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Mege\nThis project use some widely used feature selection algorithm to analyse the binary classfication problem. We use the feature construction method(some regression method) to structure the new feature.And then we find the result accuracy is well good.\n'], 'url_profile': 'https://github.com/Carroll105', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'bengaluru', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YuvrajShekhar', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'Chicago, IL, United States', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['BlackFridaySalesPrediction\nBuilt different classification models like KNN , Naïve Bayes and logistic regression to find a better model among the three to predict the customer purchase pattern with the highest accuracy using MAE and AIC metrics.\n'], 'url_profile': 'https://github.com/sagarippili', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'Washington D.C. - Baltimore', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Social-Media-Logistic-Classification\nUsed logistic regression models (k-NN, SVM, Kernel SVM, Naive Bayes, Decision tree, Random Forest) to classify whether or not an individual purchased a brand new car that was shown on a social media platform. I classified the participants in the study based on their age and estimated salaries. The two results were either yes - they purchased the car or no -they did not purchase the car. I viewed the correct number of predictions by constructing a confusion matrix and plotted the boundaries of the classifications using matplotlib.\nThe kernel SVM and Naive Bayes classifiers were the most effective in classifying the correct number of yes and no customers (about 90% accuracy), while minimizing overfitting. The decision tree and random forest models were the most accurate (92% accuracy), however, the models overfitted the data set. In conclusion, I would deploy the kernel SVM or the Naive Bayes for a larger version of this data set with more features and participants.\n'], 'url_profile': 'https://github.com/justinyu1', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Predicting-price-of-used-cars-using-XGB-regressor\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Sep 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Mar 11, 2020', 'Updated Mar 15, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': [""CensusTractProject\nThe goal of this project is to predict the median income of a Census Tract, and find suitable places to open 'Basic' and 'Premium' store based on median Income of each census tract.\nThis porject first Compares the clusters obtained by K-Means and BIRCH.\nNext, the project tries different numbers of clusters, and for each cluster, the project uses Gradiant Boosting Regressor, Random Forrest, Linear Regression,Ridge Regression, Lasso Regression, KNN Regressor, Artificial Neural Network, and Decision Tree to make final prediction.\nThe project is capable of finding the overall optimal number of clusters and the optimal regressor model for each cluster within the optimal clusters, the criteria used is MSE.\n""], 'url_profile': 'https://github.com/financetocoder', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'Ave. San Claudio s/n, esquina Blvrd 24 Sur, Cd Universitaria BUAP, 72592 Puebla, Pue. México.', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Biomechanics_Modelos_Automaticos\nbuendiaenr1@gmail.com. BUAP, México. Crear modelos de manera automática usando como criterio el menor AIC\nRequisitos:\nfit <- lm(fcm~edad+peso+est,data=fcm)  #### colocar todas las variables en el modelo inicial\n                                  #### estas variables estan todas en el archivo csv que usaremos y que contiene\n                                  #### las mediciones de cada variable\n\n'], 'url_profile': 'https://github.com/EnriqueBuendiaL', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Supervised Learning Models with Python\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'Munich, Germany', 'stats_list': [], 'contributions': '304 contributions\n        in the last year', 'description': ['\n\n\n\n\n\nHyperspectral Processing Scripts for the HydReSGeo Dataset\nThis repository includes the processing scripts of the HydReSGeo dataset\nfor the hyperspectral, LWIR, and soil moisture data.\n\n\nLicense:3-Clause BSD license\n\nAuthor:Felix M. Riese\n\nRequirements:Python 3 with these packages\n\nCitation:see Citation and in the bibtex file\n\nDocumentation:Documentation\n\n\n\n\nSensors\n\nHyperspectral sensors: Cubert UHD 285 (VNIR), FLIR Tau2 640 (LWIR), ASD FieldSpec 4 Sensors (VNIR & SWIR)\nHydrological sensor: IMKO Pico32 (TDR)\n\n\nExemplary notebooks\n\nExample_Plots.ipynb\nProcess_HydReSGeo_Dataset.ipynb\n\n\n\nCitation\nCode:\n[1] F. M. Riese, ""Hyperspectral Processing Scripts for HydReSGeo Dataset,""\nZenodo, 2020. DOI:10.5281/zenodo.3706418\n@misc{riese2020hyperspectral,\n    author = {Riese, Felix~M.},\n    title = {{Hyperspectral Processing Scripts for the HydReSGeo Dataset}},\n    year = {2020},\n    DOI = {10.5281/zenodo.3706418},\n    publisher = {Zenodo},\n    howpublished = {\\href{https://doi.org/10.5281/zenodo.3706418}{doi.org/10.5281/zenodo.3706418}}\n}\nDataset:\n[2] S. Keller, F. M. Riese, N. Allroggen, and C. Jackisch, ""HydReSGeo: Field\nexperiment dataset of surface-subsurface infiltration dynamics acquired by\nhydrological, remote sensing, and geophysical measurement techniques,"" GFZ Data\nServices, 2020. DOI:10.5880/fidgeo.2020.015\n@misc{keller2020hydresgeo,\n    author = {Keller, Sina and Riese, Felix~M. and Allroggen, Niklas and\n              Jackisch, Conrad},\n    title = {{HydReSGeo: Field experiment dataset of surface-subsurface\n              infiltration dynamics acquired by hydrological, remote\n              sensing, and geophysical measurement techniques}},\n    year = {2020},\n    publisher = {GFZ Data Services},\n    DOI = {10.5880/fidgeo.2020.015},\n}\n\nCode is Supplementary Material to\n[3] S. Keller, F. M. Riese, N. Allroggen, C. Jackisch, and S. Hinz, “Modeling\nsubsurface soil moisture based on hyperspectral data: First results of a\nmultilateral field campaign,” in Tagungsband der 37. Wissenschaftlich-\nTechnische Jahrestagung der DGPF e.V., vol. 27, Munich, Germany, 2018, pp.\n34–48. Link\n[4] S. Keller, F. M. Riese, J. Stötzer, P. M. Maier, and S. Hinz, “Developing\na machine learning framework for estimating soil moisture with VNIR\nhyperspectral data,” ISPRS Annals of Photogrammetry, Remote Sensing and\nSpatial Information Sciences, vol. IV-1, pp. 101–108, 2018.\nDOI:10.5194/isprs-annals-IV-1-101-2018\n[5] F. M. Riese and S. Keller, “Fusion of hyperspectral and ground penetrating\nradar data to estimate soil moisture,” in 2018 9th Workshop on Hyperspectral\nImage and Signal Processing: Evolution in Remote Sensing (WHISPERS), Amsterdam,\nNetherlands, 2018, pp. 1–5. DOI:10.1109/WHISPERS.2018.8747076\n[6] S. Keller, Fusion hyperspektraler, LWIR- und Bodenradar-Daten mit\nmaschinellen Lernverfahren zur Bodenfeuchteschätzung, 5th ed. Wichmann, Berlin,\n2019, p. 217–250.\n\n\nTo do:\n\n[ ] Include plots with masks and bars into the documentation\n[ ] Speed-up the script by opening dataframes only once\n[ ] Describe data from rs/hyp/ and rs/lwir/ in the documentation\n\n'], 'url_profile': 'https://github.com/felixriese', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/joseconde1997', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'Bhopal, India', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': [""Smart Phone Based Human Activity Recognition\nIn this project,I explored this dataset. This is basically a\nclassification problem. I have tried various classical machine learning models like SVM, K-NN, Desicion tree and logistic regressionto get state-of-the-art accuracy.Finally I have also used deep learning models like multilayer perceptron and 1D conv-net. Uniqueness of the datset set is that it has 512 features that is the feature space has 512 dimensions.\nResult of the predictive models:\n\n\n\nModel\nAccuracy\n\n\n\n\nlogistic regression\nTest Accuracy: 0.9500316255534472\n\n\nMLP\nTest accuracy: 0.932\n\n\n1D Conv\nTest accuracy: 0.917\n\n\nSVM ker = rbf\nTest accuracy: 0.941808981657179\n\n\nSVM ker = linear\nTest accuracy: 0.9487666034155597\n\n\nSVM ker = Polynomial\nTest accuracy: 0.92662871600253\n\n\nKNN\nTest set Accuracy:  0.8548387096774194\n\n\nDecision Tree\nDecisionTrees's  Test Accuracy:  0.8358633776091081\n\n\n\ndeep-learning model\n\nclassical machine-leaning models\n\n\n""], 'url_profile': 'https://github.com/deepacefic', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'Azerbaijan', 'stats_list': [], 'contributions': '338 contributions\n        in the last year', 'description': ['Keras-Assignment\nIn this project, I have built a regression model using the deep learning Keras library, and then I have experiment with increasing the number of training epochs and changing number of hidden layers and you will see how changing these parameters impacts the performance of the model.\nFor your convenience, the data can be found here: https://cocl.us/concrete_data.\n'], 'url_profile': 'https://github.com/NijatZeynalov', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['MultiDataAnalysis\nThis package facilitates combining data from multiple files, experiments, or datasets into one analysis. In particular, this package has two main functionalities: 1. Merging files that are split by sample into one data set by experiment/measurement type, and 2. Applying a regression model to all rows of one or more matrices using data from multiple data sets.\n'], 'url_profile': 'https://github.com/okg3', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'Philadelphia', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['Screening-Tool-Chronic-Kidney-Disease\nABSTRACT\nChronic kidney disease\u202f(CKD) is a type of\u202fkidney disease\u202fin which there is gradual loss of\u202fkidney function\u202fover a period of months to years. Initially there are generally no symptoms; later, symptoms may include\u202fleg swelling, feeling tired,\u202fvomiting, loss of appetite, and\u202fconfusion. Complications include an increased risk of\u202fheart disease,\u202fhigh blood pressure,\u202fbone disease, and\u202fanemia. CKD can affect almost every body system. Early recognition and intervention are essential to slowing disease progression, maintaining quality of life, and improving outcomes.\nOur study implements logistic regression and develops a model to identify whether a person has CKD. We cited various research papers and consulted various Doctors and implemented logistic regression to measure the model’s accuracy. We conducted (a) Correlation Analysis, (b) Lasso Regression and Scientific citation for feature Selection. Logistic regression gave a training accuracy of 80.3 % with a validation accuracy of 79.46%. The study also consists of a simple screening tool which most likely indicates the presence of CKD. This study concludes by using predictive model and the screening tool to predict the risk of CKD in 2819 people.\nMETHODOLOGY\nDATA DESCRPTION: The dataset for the case study consists of responses for specifically designed questionnaire from 8819 individuals, aged 20 years or older taken between 1999-2000 and 2001-2002 during a survey across various states in USA.  The dataset is divided into two sets 1. Training set with 6000 observations in which 33 variables along with the status of CKD is provided. 2. Testing set consisting of 2819 observations with same set of variables in which the CKD has to be predicted. Table1 has all the 33 variables given in our dataset.\nDATA DESCRIPTION\n\nMISSING DATA\nMissing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (""subject""). Some items are more likely to generate a nonresponse than others: for example, items about private subjects such as income.\nOur dataset consists of 8819 responses against 33 attributes (8819 x 33) 291027 individual responses are to be recorded. But only 283285 are recorded and 7742 records are missing (which is about 2.6 % of the data set). Four dummy variables have been created for Race group (Black, White, Hispanic and others).\n\nThe above picture shows the number of missing values in each of the variables. Further the missing values are analysed to observe any patterns or combinations occur within the data as shown below.\n\nIMPUTATION\nMissing data reduces the representativeness of the sample and can therefore distort inferences about the population. The choice of\u202fmethod to impute missing values, largely influences the model’s predictive ability.\u202fIf the data is missing completely at random then deletion does not add any bias, but it might decrease the power of the analysis by decreasing the sample size. To deal with the missing data here, MICE package has been used with mean imputation so that the overall mean will not be affected.\nVARIABLE SELECTION\nAttribute selection methods are used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model. We have used correlation analysis and cited many research papers online and eliminated following variables: Income, Unmarried, CareSource, Insured, Education, Height, Weight, LDL, Total Cholesterol for the initial selection. Then to further filter out the insignificant variables we have used several approaches.\n\nANALYTICAL APPROACH\nWe used Lasso regression for feature selection in remaining 24 variables. Based on the lasso model, following 13 variables have higher significance: Age, DBP, HDL, PVD, Activity, Hypertension, Diabetes, Stroke, CVD, Anemia, Racegroup Hispanic.\nSCIENTIFIC APPROACH\nConsidering real life scenario, we cross validated the variables with Nephrologist and modified the above variables and finalized the following 13 variables for our predictive model:  Age, Female, BMI, Dyslipidemia, PVD, Hypertension, Diabetes, Family Diabetes, Stroke, Family CVD, CHF, Anemia and Race Group (Black, White, Hispanic and others).\nCRITERION BASED APPROACH\nThe Akaike Information Criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. We want to minimize AIC. Larger models will fit better and so have smaller RSS but use more parameters. Thus, the best choice of model will balance fit with model size.\nDifferent Logit models were run on the different combination of variable selected. We used AIC as an estimator on the all the models and picked out the best one with the lowest AIC value.\nPREDICTION MODEL\nThis logit model gave an AIC value of 1478.8. Then to train and test the model we split the 6000 training set data into three sets 1) Main Training Set (4000) 2) Testing (1000) Set 3) Validation Set (1000).\nFRAMEWORK OF THE APPROACH\n\nACCURACY OF THE MODEL\nAfter training the model, we tested it with the Testing set consisting of 1000 responses to check the accuracy of the model and select the threshold for prediction. We generated a for loop on this set to predict CKD for this testing set for various thresholds and compared with the actual CKD values to get confusion matrix, accuracies and corresponding costs.\nTHRESHOLD SELECTION\nWe can convert the probabilities to predictions using what’s called a threshold value,\u202ft. If the probability of CKD is greater than this threshold value,\u202ft, we predict that person has CKD. But if the probability of CKD is less than the threshold value,\u202ft, then we predict that the person does not have CKD.\nHow to select the value for t: The threshold value,\u202ft, is often selected based on which errors are better. This would imply that\u202ft\u202fwould be best for no errors but it’s rare to have a model that predicts perfectly. In this model we have selected based on money as well. For taking test it takes 100 dollars for FP and TP we would lose 100$ and for TP the hospital will gain 1000$.\n\nAUC VALUE: We selected the threshold based on the ROC curve. AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represent degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, higher the AUC, better the model is at distinguishing between patients with CKD and without CKD. The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis. The AUC value for the above ROC curve 87%.\n\nOur main aim was to reduce the FP(False Positive), so we went with a threshold which gave us less FP at the same time more profit. That threshold came out to be 0.07 with an accuracy of 77.2%.\n\nFor further validation this logit model was validated with the validation set of 1000. The model gave out an accuracy of 78.2%.\n\nWith the above threshold we got a cost of $74,200. Running the model on the Validation set further confirmed our model’s accuracy and AUC were almost similar for both the Testing and Validation sets. So, we went with the threshold of 0.07 and finally ran the model to predict whether a person has CKD on the 2819 Prediction Set.\nLIMITATIONS OF THE MODEL\n\n\nThe data set doesn’t talk about the severity of CKD (Symptoms like CVD develop only at the final stage. So, if a person who is prone to CKD might not have CVD at the initial stage).\n\n\nData about main causes of CKD are missing (Protein Urea, IHD, eGFR). Two simple tests can detect CKD urine, albumin and serum, creatinine. We don’t have these in the data set.\n\n\nThe variables selected might not be the best combination as more research work is needed on each variable.\n\n\nUnder-representation of certain race may lead bias in the prediction.\n\n\nImbalance in the data set -We have less people with CKD. This will lead to bias again.\n\n\nThe data set is focused only for people of US state and during a certain time period. So, putting a general Prediction model is very tough.\n\n\nThe model was focused on reducing FP and maximizing the profit but the most dangerous one is FN (False Negative).\n\n\nSCREENING TOOL\n\nCONCLUSION\nThe model can accurately identify patients receiving low-quality care with test set accuracy being equal to 78 % with 13 attributes. In practice, the probabilities returned by the logistic regression model can be used to prioritize patients for intervention.\u202f Any individual’s risk can be estimated as the probability of that individual with the questions used in the simple screening tool. Further research is needed to simultaneously assess the role of multiple risk factors which were not provided in the case study (as mentioned in the limitation section) to validate this model in other population.\nREFERENCES\n\n\nChronic Kidney Disease: Early Education Intervention by Judy Kauffman, MSN, RN, CNN Charlottesville, Virginia ( A DNP Scholarly Project presented to the Graduate Faculty of the University of Virginia in Candidacy for the Degree of Doctor of Nursing Practice School of Nursing University of Virginia May 2017).\n\n\nDetection of Chronic Kidney Disease and Selecting Important Predictive Attributes by Asif Salekin and John Stankovic (Department of Computer Science University of Virginia Charlottesville, Virginia).\n\n\nAn Introduction to Statistical Learning with Applications in R by Gareth James and Daniela Witten.\n\n\nCentre for Disease Control and Prevention ( https://www.cdc.gov/kidneydisease/publications-resources/2019-national-facts.html )\n\n\nAfrican Health Sciences https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4915439/)\n\n\n'], 'url_profile': 'https://github.com/HemachandarN', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}","{'location': 'Philadelphia', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Screening-Tool-to-Identify-the-patients-with-Chronic-Kidney-Disease\nABSTRACT\nChronic kidney disease\u202f(CKD) is a type of\u202fkidney disease\u202fin which there is gradual loss of\u202fkidney function\u202fover a period of months to years. Initially there are generally no symptoms; later, symptoms may include\u202fleg swelling, feeling tired,\u202fvomiting, loss of appetite, and\u202fconfusion. Complications include an increased risk of\u202fheart disease,\u202fhigh blood pressure,\u202fbone disease, and\u202fanemia. CKD can affect almost every body system. Early recognition and intervention are essential to slowing disease progression, maintaining quality of life, and improving outcomes.\nOur study implements logistic regression and develops a model to identify whether a person has CKD. We cited various research papers and consulted various Doctors and implemented logistic regression to measure the model’s accuracy. We conducted (a) Correlation Analysis, (b) Lasso Regression and Scientific citation for feature Selection. Logistic regression gave a training accuracy of 80.3 % with a validation accuracy of 79.46%. The study also consists of a simple screening tool which most likely indicates the presence of CKD. This study concludes by using predictive model and the screening tool to predict the risk of CKD in 2819 people.\nMETHODOLOGY\nDATA DESCRPTION: The dataset for the case study consists of responses for specifically designed questionnaire from 8819 individuals, aged 20 years or older taken between 1999-2000 and 2001-2002 during a survey across various states in USA. The dataset is divided into two sets 1. Training set with 6000 observations in which 33 variables along with the status of CKD is provided. 2. Testing set consisting of 2819 observations with same set of variables in which the CKD has to be predicted. Table1 has all the 33 variables given in our dataset.\nMISSING DATA\nMissing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (""subject""). Some items are more likely to generate a nonresponse than others: for example, items about private subjects such as income.\nOur dataset consists of 8819 responses against 33 attributes (8819 x 33) 291027 individual responses are to be recorded. But only 283285 are recorded and 7742 records are missing (which is about 2.6 % of the data set). Four dummy variables have been created for Race group (Black, White, Hispanic and others).\nIMPUTATION\nMissing data reduces the representativeness of the sample and can therefore distort inferences about the population. The choice of\u202fmethod to impute missing values, largely influences the model’s predictive ability.\u202fIf the data is missing completely at random then deletion does not add any bias, but it might decrease the power of the analysis by decreasing the sample size. To deal with the missing data here, MICE package has been used with mean imputation so that the overall mean will not be affected.\nVARIABLE SELECTION\nAttribute selection methods are used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model. We have used correlation analysis and cited many research papers online and eliminated following variables: Income, Unmarried, CareSource, Insured, Education, Height, Weight, LDL, Total Cholesterol for the initial selection. Then to further filter out the insignificant variables we have used several approaches.\nANALYTICAL APPROACH\nWe used Lasso regression for feature selection in remaining 24 variables. Based on the lasso model, following 13 variables have higher significance: Age, DBP, HDL, PVD, Activity, Hypertension, Diabetes, Stroke, CVD, Anemia, Racegroup Hispanic.\nSCIENTIFIC APPROACH\nConsidering real life scenario, we cross validated the variables with Nephrologist and modified the above variables and finalized the following 13 variables for our predictive model: Age, Female, BMI, Dyslipidemia, PVD, Hypertension, Diabetes, Family Diabetes, Stroke, Family CVD, CHF, Anemia and Race Group (Black, White, Hispanic and others).\nCRITERION BASED APPROACH\nThe Akaike Information Criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. We want to minimize AIC. Larger models will fit better and so have smaller RSS but use more parameters. Thus, the best choice of model will balance fit with model size.\nDifferent Logit models were run on the different combination of variable selected. We used AIC as an estimator on the all the models and picked out the best one with the lowest AIC value.\nPREDICTION MODEL\nThis logit model gave an AIC value of 1478.8. Then to train and test the model we split the 6000 training set data into three sets 1) Main Training Set (4000) 2) Testing (1000) Set 3) Validation Set (1000).\nACCURACY OF THE MODEL\nAfter training the model, we tested it with the Testing set consisting of 1000 responses to check the accuracy of the model and select the threshold for prediction. We generated a for loop on this set to predict CKD for this testing set for various thresholds and compared with the actual CKD values to get confusion matrix, accuracies and corresponding costs.\nTHRESHOLD SELECTION\nWe can convert the probabilities to predictions using what’s called a threshold value,\u202ft. If the probability of CKD is greater than this threshold value,\u202ft, we predict that person has CKD. But if the probability of CKD is less than the threshold value,\u202ft, then we predict that the person does not have CKD.\nHow to select the value for t: The threshold value,\u202ft, is often selected based on which errors are better. This would imply that\u202ft\u202fwould be best for no errors but it’s rare to have a model that predicts perfectly. In this model we have selected based on money as well. For taking test it takes 100 dollars for FP and TP we would lose 100$ and for TP the hospital will gain 1000$.\nAUC VALUE: We selected the threshold based on the ROC curve. AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represent degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, higher the AUC, better the model is at distinguishing between patients with CKD and without CKD. The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis. The AUC value for the above ROC curve 82 %.\nOur main aim was to reduce the FP(False Positive), so we went with a threshold which gave us less FP at the same time more profit. That threshold came out to be 0.07 with an accuracy of 77.2%.\nFor further validation this logit model was validated with the validation set of 1000. The model gave out an accuracy of 78.2%.\nWith the above threshold we got a cost of $74,200. Running the model on the Validation set further confirmed our model’s accuracy and AUC were almost similar for both the Testing and Validation sets. So, we went with the threshold of 0.07 and finally ran the model to predict whether a person has CKD on the 2819 Prediction Set.\nLIMITATIONS OF THE MODEL\nThe data set doesn’t talk about the severity of CKD (Symptoms like CVD develop only at the final stage. So, if a person who is prone to CKD might not have CVD at the initial stage).\nData about main causes of CKD are missing (Protein Urea, IHD, eGFR). Two simple tests can detect CKD urine, albumin and serum, creatinine. We don’t have these in the data set.\nThe variables selected might not be the best combination as more research work is needed on each variable.\nUnder-representation of certain race may lead bias in the prediction.\nImbalance in the data set -We have less people with CKD. This will lead to bias again.\nThe data set is focused only for people of US state and during a certain time period. So, putting a general Prediction model is very tough.\nThe model was focused on reducing FP and maximizing the profit but the most dangerous one is FN (False Negative).\nCONCLUSION\nThe model can accurately identify patients receiving low-quality care with test set accuracy being equal to 78 % with 13 attributes. In practice, the probabilities returned by the logistic regression model can be used to prioritize patients for intervention.\u202f Any individual’s risk can be estimated as the probability of that individual with the questions used in the simple screening tool. Further research is needed to simultaneously assess the role of multiple risk factors which were not provided in the case study (as mentioned in the limitation section) to validate this model in other population.\nREFERENCES\nChronic Kidney Disease: Early Education Intervention by Judy Kauffman, MSN, RN, CNN Charlottesville, Virginia ( A DNP Scholarly Project presented to the Graduate Faculty of the University of Virginia in Candidacy for the Degree of Doctor of Nursing Practice School of Nursing University of Virginia May 2017).\nDetection of Chronic Kidney Disease and Selecting Important Predictive Attributes by Asif Salekin and John Stankovic (Department of Computer Science University of Virginia Charlottesville, Virginia).\nAn Introduction to Statistical Learning with Applications in R by Gareth James and Daniela Witten.\nCentre for Disease Control and Prevention ( https://www.cdc.gov/kidneydisease/publications-resources/2019-national-facts.html )\nAfrican Health Sciences https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4915439/)\n'], 'url_profile': 'https://github.com/Sasidhar-Sirivella', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 13, 2020', '1', 'Python', 'BSD-3-Clause license', 'Updated Sep 19, 2020', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Sep 26, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 20, 2020', 'R', 'Updated Mar 23, 2020', 'R', 'Updated Nov 4, 2020', 'R', 'Updated Mar 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""Deep-Neural-Network-for-Image-Classification-Application\nYou will use the functions you'd implemented in the previous assignment to build a deep network, and apply it to cat vs non-cat classification. Hopefully, you will see an improvement in accuracy relative to your previous logistic regression implementation.\n""], 'url_profile': 'https://github.com/sourav2195', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saidul-islam98', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'St. Louis, Missouri', 'stats_list': [], 'contributions': '225 contributions\n        in the last year', 'description': ['What\'s the Weather Like?\nBackground\nWhether financial, political, or social -- data\'s true power lies in its ability to answer questions definitively. So let\'s take what you\'ve learned about Python requests, APIs, and JSON traversals to answer a fundamental question: ""What\'s the weather like as we approach the equator?""\n\nBefore You Begin\n\n\nCreate a new repository for this project called python-api-challenge.\n\n\nClone the new repository to your computer.\n\n\nInside your local git repository, create a directory for both of the  Python Challenges. Use folder names corresponding to the challenges: WeatherPy.\n\n\nInside the folder that you just created, add new files called WeatherPy.ipynb and VacationPy.ipynb. These will be the main scripts to run for each analysis.\n\n\nPush the above changes to GitHub.\n\n\nPart I - WeatherPy\nIn this example, you\'ll be creating a Python script to visualize the weather of 500+ cities across the world of varying distance from the equator. To accomplish this, you\'ll be utilizing a simple Python library, the OpenWeatherMap API, and a little common sense to create a representative model of weather across world cities.\nYour first objective is to build a series of scatter plots to showcase the following relationships:\n\nTemperature (F) vs. Latitude\nHumidity (%) vs. Latitude\nCloudiness (%) vs. Latitude\nWind Speed (mph) vs. Latitude\n\nAfter each plot add a sentence or too explaining what the code is and analyzing.\nYour next objective is to run linear regression on each relationship, only this time separating them into Northern Hemisphere (greater than or equal to 0 degrees latitude) and Southern Hemisphere (less than 0 degrees latitude):\n\nNorthern Hemisphere - Temperature (F) vs. Latitude\nSouthern Hemisphere - Temperature (F) vs. Latitude\nNorthern Hemisphere - Humidity (%) vs. Latitude\nSouthern Hemisphere - Humidity (%) vs. Latitude\nNorthern Hemisphere - Cloudiness (%) vs. Latitude\nSouthern Hemisphere - Cloudiness (%) vs. Latitude\nNorthern Hemisphere - Wind Speed (mph) vs. Latitude\nSouthern Hemisphere - Wind Speed (mph) vs. Latitude\n\nAfter each pair of plots explain what the linear regression is modelling such as any relationships you notice and any other analysis you may have.\nYour final notebook must:\n\nRandomly select at least 500 unique (non-repeat) cities based on latitude and longitude.\nPerform a weather check on each of the cities using a series of successive API calls.\nInclude a print log of each city as it\'s being processed with the city number and city name.\nSave a CSV of all retrieved data and a PNG image for each scatter plot.\n\nPart II - VacationPy\nNow let\'s use your skills in working with weather data to plan future vacations. Use jupyter-gmaps and the Google Places API for this part of the assignment.\n\n\nNote: if you having trouble displaying the maps try running jupyter nbextension enable --py gmaps in your environment and retry.\n\n\nCreate a heat map that displays the humidity for every city from the part I of the homework.\n\n\n\nNarrow down the DataFrame to find your ideal weather condition. For example:\n\n\nA max temperature lower than 80 degrees but higher than 70.\n\n\nWind speed less than 10 mph.\n\n\nZero cloudiness.\n\n\nDrop any rows that don\'t contain all three conditions. You want to be sure the weather is ideal.\n\n\nNote: Feel free to adjust to your specifications but be sure to limit the number of rows returned by your API requests to a reasonable number.\n\n\n\n\nUsing Google Places API to find the first hotel for each city located within 5000 meters of your coordinates.\n\n\nPlot the hotels on top of the humidity heatmap with each pin containing the Hotel Name, City, and Country.\n\n\n\nAs final considerations:\n\nCreate a new GitHub repository for this project called API-Challenge (note the kebab-case). Do not add to an existing repo\nYou must complete your analysis using a Jupyter notebook.\nYou must use the Matplotlib or Pandas plotting libraries.\nFor Part I, you must include a written description of three observable trends based on the data.\nYou must use proper labeling of your plots, including aspects like: Plot Titles (with date of analysis) and Axes Labels.\nFor max intensity in the heat map, try setting it to the highest humidity found in the data set.\n\nHints and Considerations\n\n\nThe city data you generate is based on random coordinates as well as different query times; as such, your outputs will not be an exact match to the provided starter notebook.\n\n\nYou may want to start this assignment by refreshing yourself on the geographic coordinate system.\n\n\nNext, spend the requisite time necessary to study the OpenWeatherMap API. Based on your initial study, you should be able to answer  basic questions about the API: Where do you request the API key? Which Weather API in particular will you need? What URL endpoints does it expect? What JSON structure does it respond with? Before you write a line of code, you should be aiming to have a crystal clear understanding of your intended outcome.\n\n\nA starter code for Citipy has been provided. However, if you\'re craving an extra challenge, push yourself to learn how it works: citipy Python library. Before you try to incorporate the library into your analysis, start by creating simple test cases outside your main script to confirm that you are using it correctly. Too often, when introduced to a new library, students get bogged down by the most minor of errors -- spending hours investigating their entire code -- when, in fact, a simple and focused test would have shown their basic utilization of the library was wrong from the start. Don\'t let this be you!\n\n\nPart of our expectation in this challenge is that you will use critical thinking skills to understand how and why we\'re recommending the tools we are. What is Citipy for? Why would you use it in conjunction with the OpenWeatherMap API? How would you do so?\n\n\nIn building your script, pay attention to the cities you are using in your query pool. Are you getting coverage of the full gamut of latitudes and longitudes? Or are you simply choosing 500 cities concentrated in one region of the world? Even if you were a geographic genius, simply rattling 500 cities based on your human selection would create a biased dataset. Be thinking of how you should counter this. (Hint: Consider the full range of latitudes).\n\n\nOnce you have computed the linear regression for one chart, the process will be similar for all others. As a bonus, try to create a function that will create these charts based on different parameters.\n\n\nRemember that each coordinate will trigger a separate call to the Google API. If you\'re creating your own criteria to plan your vacation, try to reduce the results in your DataFrame to 10 or fewer cities.\n\n\nLastly, remember -- this is a challenging activity. Push yourself! If you complete this task, then you can safely say that you\'ve gained a strong mastery of the core foundations of data analytics and it will only go better from here. Good luck!\n\n\nCopyright\nTrilogy Education Services © 2019. All Rights Reserved.\n'], 'url_profile': 'https://github.com/cindyreznikov', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Bhubaneshwar', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yo-sayantan', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ravikanth99', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/reyear', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ravikanth99', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}",,,
